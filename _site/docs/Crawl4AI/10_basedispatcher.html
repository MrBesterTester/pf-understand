<h1 id="chapter-10-orchestrating-the-crawl---basedispatcher">Chapter 10: Orchestrating the Crawl - BaseDispatcher</h1>

<p>In <a href="09_cachecontext___cachemode.md">Chapter 9: Smart Fetching with Caching - CacheContext / CacheMode</a>, we learned how Crawl4AI uses caching to cleverly avoid re-fetching the same webpage multiple times, which is especially helpful when crawling many URLs. We’ve also seen how methods like <code class="language-plaintext highlighter-rouge">arun_many()</code> (<a href="02_asyncwebcrawler.md">Chapter 2: Meet the General Manager - AsyncWebCrawler</a>) or strategies like <a href="08_deepcrawlstrategy.md">DeepCrawlStrategy</a> can lead to potentially hundreds or thousands of individual URLs needing to be crawled.</p>

<p>This raises a question: if we have 1000 URLs to crawl, does Crawl4AI try to crawl all 1000 simultaneously? That would likely overwhelm your computer’s resources (like memory and CPU) and could also flood the target website with too many requests, potentially getting you blocked! How does Crawl4AI manage running many crawls efficiently and responsibly?</p>

<h2 id="what-problem-does-basedispatcher-solve">What Problem Does <code class="language-plaintext highlighter-rouge">BaseDispatcher</code> Solve?</h2>

<p>Imagine you’re managing a fleet of delivery drones (<code class="language-plaintext highlighter-rouge">AsyncWebCrawler</code> tasks) that need to pick up packages from many different addresses (URLs). If you launch all 1000 drones at the exact same moment:</p>

<ul>
  <li>Your control station (your computer) might crash due to the processing load.</li>
  <li>The central warehouse (the target website) might get overwhelmed by simultaneous arrivals.</li>
  <li>Some drones might collide or interfere with each other.</li>
</ul>

<p>You need a <strong>Traffic Controller</strong> or a <strong>Dispatch Center</strong> to manage the fleet. This controller decides:</p>

<ol>
  <li>How many drones can be active in the air at any one time.</li>
  <li>When to launch the next drone, maybe based on available airspace (system resources) or just a simple count limit.</li>
  <li>How to handle potential delays or issues (like rate limiting from a specific website).</li>
</ol>

<p>In Crawl4AI, the <code class="language-plaintext highlighter-rouge">BaseDispatcher</code> acts as this <strong>Traffic Controller</strong> or <strong>Task Scheduler</strong> for concurrent crawling operations, primarily when using <code class="language-plaintext highlighter-rouge">arun_many()</code>. It manages <em>how</em> multiple crawl tasks are executed concurrently, ensuring the process is efficient without overwhelming your system or the target websites.</p>

<h2 id="what-is-basedispatcher">What is <code class="language-plaintext highlighter-rouge">BaseDispatcher</code>?</h2>

<p><code class="language-plaintext highlighter-rouge">BaseDispatcher</code> is an abstract concept (a blueprint or job description) in Crawl4AI. It defines <em>that</em> we need a system for managing the execution of multiple, concurrent crawling tasks. It specifies the <em>interface</em> for how the main <code class="language-plaintext highlighter-rouge">AsyncWebCrawler</code> interacts with such a system, but the specific <em>logic</em> for managing concurrency can vary.</p>

<p>Think of it as the control panel for our drone fleet – the panel exists, but the specific rules programmed into it determine how drones are dispatched.</p>

<h2 id="the-different-controllers-ways-to-dispatch-tasks">The Different Controllers: Ways to Dispatch Tasks</h2>

<p>Crawl4AI provides concrete implementations (the actual traffic control systems) based on the <code class="language-plaintext highlighter-rouge">BaseDispatcher</code> blueprint:</p>

<ol>
  <li><strong><code class="language-plaintext highlighter-rouge">SemaphoreDispatcher</code> (The Simple Counter):</strong>
    <ul>
      <li><strong>Analogy:</strong> A parking garage with a fixed number of spots (e.g., 10). A gate (<code class="language-plaintext highlighter-rouge">asyncio.Semaphore</code>) only lets a new car in if one of the 10 spots is free.</li>
      <li><strong>How it works:</strong> You tell it the maximum number of crawls that can run <em>at the same time</em> (e.g., <code class="language-plaintext highlighter-rouge">semaphore_count=10</code>). It uses a simple counter (a semaphore) to ensure that no more than this number of crawls are active simultaneously. When one crawl finishes, it allows another one from the queue to start.</li>
      <li><strong>Good for:</strong> Simple, direct control over concurrency when you know a specific limit works well for your system and the target sites.</li>
    </ul>
  </li>
  <li><strong><code class="language-plaintext highlighter-rouge">MemoryAdaptiveDispatcher</code> (The Resource-Aware Controller - Default):</strong>
    <ul>
      <li><strong>Analogy:</strong> A smart parking garage attendant who checks not just the number of cars, but also the <em>total space</em> they occupy (system memory). They might stop letting cars in if the garage is nearing its memory capacity, even if some numbered spots are technically free.</li>
      <li><strong>How it works:</strong> This dispatcher monitors your system’s available memory. It tries to run multiple crawls concurrently (up to a configurable maximum like <code class="language-plaintext highlighter-rouge">max_session_permit</code>), but it will pause launching new crawls if the system memory usage exceeds a certain threshold (e.g., <code class="language-plaintext highlighter-rouge">memory_threshold_percent=90.0</code>). It adapts the concurrency level based on available resources.</li>
      <li><strong>Good for:</strong> Automatically adjusting concurrency to prevent out-of-memory errors, especially when crawl tasks vary significantly in resource usage. <strong>This is the default dispatcher used by <code class="language-plaintext highlighter-rouge">arun_many</code> if you don’t specify one.</strong></li>
    </ul>
  </li>
</ol>

<p>These dispatchers can also optionally work with a <code class="language-plaintext highlighter-rouge">RateLimiter</code> component, which adds politeness rules for specific websites (e.g., slowing down requests to a domain if it returns “429 Too Many Requests”).</p>

<h2 id="how-arun_many-uses-the-dispatcher">How <code class="language-plaintext highlighter-rouge">arun_many</code> Uses the Dispatcher</h2>

<p>When you call <code class="language-plaintext highlighter-rouge">crawler.arun_many(urls=...)</code>, here’s the basic flow involving the dispatcher:</p>

<ol>
  <li><strong>Get URLs:</strong> <code class="language-plaintext highlighter-rouge">arun_many</code> receives the list of URLs you want to crawl.</li>
  <li><strong>Select Dispatcher:</strong> It checks if you provided a specific <code class="language-plaintext highlighter-rouge">dispatcher</code> instance. If not, it creates an instance of the default <code class="language-plaintext highlighter-rouge">MemoryAdaptiveDispatcher</code>.</li>
  <li><strong>Delegate Execution:</strong> It hands over the list of URLs and the <code class="language-plaintext highlighter-rouge">CrawlerRunConfig</code> to the chosen dispatcher’s <code class="language-plaintext highlighter-rouge">run_urls</code> (or <code class="language-plaintext highlighter-rouge">run_urls_stream</code>) method.</li>
  <li><strong>Manage Tasks:</strong> The dispatcher takes charge:
    <ul>
      <li>It iterates through the URLs.</li>
      <li>For each URL, it decides <em>when</em> to start the actual crawl based on its rules (semaphore count, memory usage, rate limits).</li>
      <li>When ready, it typically calls the single-page <code class="language-plaintext highlighter-rouge">crawler.arun(url, config)</code> method internally for that specific URL, wrapped within its concurrency control mechanism.</li>
      <li>It manages the running tasks (e.g., using <code class="language-plaintext highlighter-rouge">asyncio.create_task</code> and <code class="language-plaintext highlighter-rouge">asyncio.wait</code>).</li>
    </ul>
  </li>
  <li><strong>Collect Results:</strong> As individual <code class="language-plaintext highlighter-rouge">arun</code> calls complete, the dispatcher collects their <code class="language-plaintext highlighter-rouge">CrawlResult</code> objects.</li>
  <li><strong>Return:</strong> Once all URLs are processed, the dispatcher returns the list of results (or yields them if streaming).</li>
</ol>

<pre><code class="language-mermaid">sequenceDiagram
    participant User
    participant AWC as AsyncWebCrawler
    participant Dispatcher as BaseDispatcher (e.g., MemoryAdaptive)
    participant TaskPool as Concurrency Manager

    User-&gt;&gt;AWC: arun_many(urls, config, dispatcher?)
    AWC-&gt;&gt;Dispatcher: run_urls(crawler=AWC, urls, config)
    Dispatcher-&gt;&gt;TaskPool: Initialize (e.g., set max concurrency)
    loop For each URL in urls
        Dispatcher-&gt;&gt;TaskPool: Can I start a new task? (Checks limits)
        alt Yes
            TaskPool--&gt;&gt;Dispatcher: OK
            Note over Dispatcher: Create task: call AWC.arun(url, config) internally
            Dispatcher-&gt;&gt;TaskPool: Add new task
        else No
            TaskPool--&gt;&gt;Dispatcher: Wait
            Note over Dispatcher: Waits for a running task to finish
        end
    end
    Note over Dispatcher: Manages running tasks, collects results
    Dispatcher--&gt;&gt;AWC: List of CrawlResults
    AWC--&gt;&gt;User: List of CrawlResults
</code></pre>

<h2 id="using-the-dispatcher-often-implicitly">Using the Dispatcher (Often Implicitly!)</h2>

<p>Most of the time, you don’t need to think about the dispatcher explicitly. When you use <code class="language-plaintext highlighter-rouge">arun_many</code>, the default <code class="language-plaintext highlighter-rouge">MemoryAdaptiveDispatcher</code> handles things automatically.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># chapter10_example_1.py
</span><span class="kn">import</span> <span class="nn">asyncio</span>
<span class="kn">from</span> <span class="nn">crawl4ai</span> <span class="kn">import</span> <span class="n">AsyncWebCrawler</span><span class="p">,</span> <span class="n">CrawlerRunConfig</span>

<span class="k">async</span> <span class="k">def</span> <span class="nf">main</span><span class="p">():</span>
    <span class="n">urls_to_crawl</span> <span class="o">=</span> <span class="p">[</span>
        <span class="s">"https://httpbin.org/html"</span><span class="p">,</span>
        <span class="s">"https://httpbin.org/links/5/0"</span><span class="p">,</span> <span class="c1"># Page with 5 links
</span>        <span class="s">"https://httpbin.org/robots.txt"</span><span class="p">,</span>
        <span class="s">"https://httpbin.org/status/200"</span><span class="p">,</span>
    <span class="p">]</span>

    <span class="c1"># We DON'T specify a dispatcher here.
</span>    <span class="c1"># arun_many will use the default MemoryAdaptiveDispatcher.
</span>    <span class="k">async</span> <span class="k">with</span> <span class="n">AsyncWebCrawler</span><span class="p">()</span> <span class="k">as</span> <span class="n">crawler</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Crawling </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">urls_to_crawl</span><span class="p">)</span><span class="si">}</span><span class="s"> URLs using the default dispatcher..."</span><span class="p">)</span>
        <span class="n">config</span> <span class="o">=</span> <span class="n">CrawlerRunConfig</span><span class="p">(</span><span class="n">stream</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span> <span class="c1"># Get results as a list at the end
</span>
        <span class="c1"># The MemoryAdaptiveDispatcher manages concurrency behind the scenes.
</span>        <span class="n">results</span> <span class="o">=</span> <span class="k">await</span> <span class="n">crawler</span><span class="p">.</span><span class="n">arun_many</span><span class="p">(</span><span class="n">urls</span><span class="o">=</span><span class="n">urls_to_crawl</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">)</span>

        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"</span><span class="se">\n</span><span class="s">Finished! Got </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">results</span><span class="p">)</span><span class="si">}</span><span class="s"> results."</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">result</span> <span class="ow">in</span> <span class="n">results</span><span class="p">:</span>
            <span class="n">status</span> <span class="o">=</span> <span class="s">"✅"</span> <span class="k">if</span> <span class="n">result</span><span class="p">.</span><span class="n">success</span> <span class="k">else</span> <span class="s">"❌"</span>
            <span class="n">url_short</span> <span class="o">=</span> <span class="n">result</span><span class="p">.</span><span class="n">url</span><span class="p">.</span><span class="n">split</span><span class="p">(</span><span class="s">'/'</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
            <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"  </span><span class="si">{</span><span class="n">status</span><span class="si">}</span><span class="s"> </span><span class="si">{</span><span class="n">url_short</span><span class="si">:</span><span class="o">&lt;</span><span class="mi">15</span><span class="si">}</span><span class="s"> | Title: </span><span class="si">{</span><span class="n">result</span><span class="p">.</span><span class="n">metadata</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">'title'</span><span class="p">,</span> <span class="s">'N/A'</span><span class="p">)</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>

<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="s">"__main__"</span><span class="p">:</span>
    <span class="n">asyncio</span><span class="p">.</span><span class="n">run</span><span class="p">(</span><span class="n">main</span><span class="p">())</span>
</code></pre></div></div>

<p><strong>Explanation:</strong></p>

<ul>
  <li>We call <code class="language-plaintext highlighter-rouge">crawler.arun_many</code> without passing a <code class="language-plaintext highlighter-rouge">dispatcher</code> argument.</li>
  <li>Crawl4AI automatically creates and uses a <code class="language-plaintext highlighter-rouge">MemoryAdaptiveDispatcher</code>.</li>
  <li>This dispatcher runs the crawls concurrently, adapting to your system’s memory, and returns all the results once completed (because <code class="language-plaintext highlighter-rouge">stream=False</code>). You benefit from concurrency without explicit setup.</li>
</ul>

<h2 id="explicitly-choosing-a-dispatcher">Explicitly Choosing a Dispatcher</h2>

<p>What if you want simpler, fixed concurrency? You can explicitly create and pass a <code class="language-plaintext highlighter-rouge">SemaphoreDispatcher</code>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># chapter10_example_2.py
</span><span class="kn">import</span> <span class="nn">asyncio</span>
<span class="kn">from</span> <span class="nn">crawl4ai</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">AsyncWebCrawler</span><span class="p">,</span>
    <span class="n">CrawlerRunConfig</span><span class="p">,</span>
    <span class="n">SemaphoreDispatcher</span> <span class="c1"># 1. Import the specific dispatcher
</span><span class="p">)</span>

<span class="k">async</span> <span class="k">def</span> <span class="nf">main</span><span class="p">():</span>
    <span class="n">urls_to_crawl</span> <span class="o">=</span> <span class="p">[</span>
        <span class="s">"https://httpbin.org/delay/1"</span><span class="p">,</span> <span class="c1"># Takes 1 second
</span>        <span class="s">"https://httpbin.org/delay/1"</span><span class="p">,</span>
        <span class="s">"https://httpbin.org/delay/1"</span><span class="p">,</span>
        <span class="s">"https://httpbin.org/delay/1"</span><span class="p">,</span>
        <span class="s">"https://httpbin.org/delay/1"</span><span class="p">,</span>
    <span class="p">]</span>

    <span class="c1"># 2. Create an instance of the SemaphoreDispatcher
</span>    <span class="c1">#    Allow only 2 crawls to run at the same time.
</span>    <span class="n">semaphore_controller</span> <span class="o">=</span> <span class="n">SemaphoreDispatcher</span><span class="p">(</span><span class="n">semaphore_count</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Using SemaphoreDispatcher with limit: </span><span class="si">{</span><span class="n">semaphore_controller</span><span class="p">.</span><span class="n">semaphore_count</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>

    <span class="k">async</span> <span class="k">with</span> <span class="n">AsyncWebCrawler</span><span class="p">()</span> <span class="k">as</span> <span class="n">crawler</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Crawling </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">urls_to_crawl</span><span class="p">)</span><span class="si">}</span><span class="s"> URLs with explicit dispatcher..."</span><span class="p">)</span>
        <span class="n">config</span> <span class="o">=</span> <span class="n">CrawlerRunConfig</span><span class="p">(</span><span class="n">stream</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

        <span class="c1"># 3. Pass the dispatcher instance to arun_many
</span>        <span class="n">results</span> <span class="o">=</span> <span class="k">await</span> <span class="n">crawler</span><span class="p">.</span><span class="n">arun_many</span><span class="p">(</span>
            <span class="n">urls</span><span class="o">=</span><span class="n">urls_to_crawl</span><span class="p">,</span>
            <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">,</span>
            <span class="n">dispatcher</span><span class="o">=</span><span class="n">semaphore_controller</span> <span class="c1"># Pass our controller
</span>        <span class="p">)</span>

        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"</span><span class="se">\n</span><span class="s">Finished! Got </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">results</span><span class="p">)</span><span class="si">}</span><span class="s"> results."</span><span class="p">)</span>
        <span class="c1"># This crawl likely took around 3 seconds (5 tasks, 1s each, 2 concurrent = ceil(5/2)*1s)
</span>        <span class="k">for</span> <span class="n">result</span> <span class="ow">in</span> <span class="n">results</span><span class="p">:</span>
            <span class="n">status</span> <span class="o">=</span> <span class="s">"✅"</span> <span class="k">if</span> <span class="n">result</span><span class="p">.</span><span class="n">success</span> <span class="k">else</span> <span class="s">"❌"</span>
            <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"  </span><span class="si">{</span><span class="n">status</span><span class="si">}</span><span class="s"> </span><span class="si">{</span><span class="n">result</span><span class="p">.</span><span class="n">url</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>

<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="s">"__main__"</span><span class="p">:</span>
    <span class="n">asyncio</span><span class="p">.</span><span class="n">run</span><span class="p">(</span><span class="n">main</span><span class="p">())</span>
</code></pre></div></div>

<p><strong>Explanation:</strong></p>

<ol>
  <li><strong>Import:</strong> We import <code class="language-plaintext highlighter-rouge">SemaphoreDispatcher</code>.</li>
  <li><strong>Instantiate:</strong> We create <code class="language-plaintext highlighter-rouge">SemaphoreDispatcher(semaphore_count=2)</code>, limiting concurrency to 2 simultaneous crawls.</li>
  <li><strong>Pass Dispatcher:</strong> We pass our <code class="language-plaintext highlighter-rouge">semaphore_controller</code> instance directly to the <code class="language-plaintext highlighter-rouge">dispatcher</code> parameter of <code class="language-plaintext highlighter-rouge">arun_many</code>.</li>
  <li><strong>Execution:</strong> Now, <code class="language-plaintext highlighter-rouge">arun_many</code> uses our <code class="language-plaintext highlighter-rouge">SemaphoreDispatcher</code>. It will start the first two crawls. As one finishes, it will start the next one from the list, always ensuring no more than two are running concurrently.</li>
</ol>

<h2 id="a-glimpse-under-the-hood">A Glimpse Under the Hood</h2>

<p>Where are these dispatchers defined? In <code class="language-plaintext highlighter-rouge">crawl4ai/async_dispatcher.py</code>.</p>

<p><strong>The Blueprint (<code class="language-plaintext highlighter-rouge">BaseDispatcher</code>):</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Simplified from crawl4ai/async_dispatcher.py
</span><span class="kn">from</span> <span class="nn">abc</span> <span class="kn">import</span> <span class="n">ABC</span><span class="p">,</span> <span class="n">abstractmethod</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">List</span><span class="p">,</span> <span class="n">Optional</span>
<span class="c1"># ... other imports like CrawlerRunConfig, CrawlerTaskResult, AsyncWebCrawler ...
</span>
<span class="k">class</span> <span class="nc">BaseDispatcher</span><span class="p">(</span><span class="n">ABC</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">rate_limiter</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">RateLimiter</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
        <span class="n">monitor</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">CrawlerMonitor</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">crawler</span> <span class="o">=</span> <span class="bp">None</span> <span class="c1"># Will be set by arun_many
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">rate_limiter</span> <span class="o">=</span> <span class="n">rate_limiter</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">monitor</span> <span class="o">=</span> <span class="n">monitor</span>
        <span class="c1"># ... other common state ...
</span>
    <span class="o">@</span><span class="n">abstractmethod</span>
    <span class="k">async</span> <span class="k">def</span> <span class="nf">crawl_url</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">url</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">config</span><span class="p">:</span> <span class="n">CrawlerRunConfig</span><span class="p">,</span>
        <span class="n">task_id</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="c1"># ... maybe other internal params ...
</span>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">CrawlerTaskResult</span><span class="p">:</span>
        <span class="s">"""Crawls a single URL, potentially handling concurrency primitives."""</span>
        <span class="c1"># This is often the core worker method called by run_urls
</span>        <span class="k">pass</span>

    <span class="o">@</span><span class="n">abstractmethod</span>
    <span class="k">async</span> <span class="k">def</span> <span class="nf">run_urls</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">urls</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
        <span class="n">crawler</span><span class="p">:</span> <span class="s">"AsyncWebCrawler"</span><span class="p">,</span>
        <span class="n">config</span><span class="p">:</span> <span class="n">CrawlerRunConfig</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">CrawlerTaskResult</span><span class="p">]:</span>
        <span class="s">"""Manages the concurrent execution of crawl_url for multiple URLs."""</span>
        <span class="c1"># This is the main entry point called by arun_many
</span>        <span class="k">pass</span>

    <span class="k">async</span> <span class="k">def</span> <span class="nf">run_urls_stream</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">urls</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
        <span class="n">crawler</span><span class="p">:</span> <span class="s">"AsyncWebCrawler"</span><span class="p">,</span>
        <span class="n">config</span><span class="p">:</span> <span class="n">CrawlerRunConfig</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">AsyncGenerator</span><span class="p">[</span><span class="n">CrawlerTaskResult</span><span class="p">,</span> <span class="bp">None</span><span class="p">]:</span>
         <span class="s">""" Streaming version of run_urls (might be implemented in base or subclasses) """</span>
         <span class="c1"># Example default implementation (subclasses might override)
</span>         <span class="n">results</span> <span class="o">=</span> <span class="k">await</span> <span class="bp">self</span><span class="p">.</span><span class="n">run_urls</span><span class="p">(</span><span class="n">urls</span><span class="p">,</span> <span class="n">crawler</span><span class="p">,</span> <span class="n">config</span><span class="p">)</span>
         <span class="k">for</span> <span class="n">res</span> <span class="ow">in</span> <span class="n">results</span><span class="p">:</span> <span class="k">yield</span> <span class="n">res</span> <span class="c1"># Naive stream, real one is more complex
</span>
    <span class="c1"># ... other potential helper methods ...
</span></code></pre></div></div>

<p><strong>Example Implementation (<code class="language-plaintext highlighter-rouge">SemaphoreDispatcher</code>):</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Simplified from crawl4ai/async_dispatcher.py
</span><span class="kn">import</span> <span class="nn">asyncio</span>
<span class="kn">import</span> <span class="nn">uuid</span>
<span class="kn">import</span> <span class="nn">psutil</span> <span class="c1"># For memory tracking in crawl_url
</span><span class="kn">import</span> <span class="nn">time</span>   <span class="c1"># For timing in crawl_url
# ... other imports ...
</span>
<span class="k">class</span> <span class="nc">SemaphoreDispatcher</span><span class="p">(</span><span class="n">BaseDispatcher</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">semaphore_count</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span>
        <span class="c1"># ... other params like rate_limiter, monitor ...
</span>    <span class="p">):</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">(...)</span> <span class="c1"># Pass rate_limiter, monitor to base
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">semaphore_count</span> <span class="o">=</span> <span class="n">semaphore_count</span>

    <span class="k">async</span> <span class="k">def</span> <span class="nf">crawl_url</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">url</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">config</span><span class="p">:</span> <span class="n">CrawlerRunConfig</span><span class="p">,</span>
        <span class="n">task_id</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">semaphore</span><span class="p">:</span> <span class="n">asyncio</span><span class="p">.</span><span class="n">Semaphore</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span> <span class="c1"># Takes the semaphore
</span>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">CrawlerTaskResult</span><span class="p">:</span>
        <span class="c1"># ... (Code to track start time, memory usage - similar to MemoryAdaptiveDispatcher's version)
</span>        <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="n">time</span><span class="p">()</span>
        <span class="n">error_message</span> <span class="o">=</span> <span class="s">""</span>
        <span class="n">memory_usage</span> <span class="o">=</span> <span class="n">peak_memory</span> <span class="o">=</span> <span class="mf">0.0</span>
        <span class="n">result</span> <span class="o">=</span> <span class="bp">None</span>

        <span class="k">try</span><span class="p">:</span>
            <span class="c1"># Update monitor state if used
</span>            <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">monitor</span><span class="p">:</span> <span class="bp">self</span><span class="p">.</span><span class="n">monitor</span><span class="p">.</span><span class="n">update_task</span><span class="p">(</span><span class="n">task_id</span><span class="p">,</span> <span class="n">status</span><span class="o">=</span><span class="n">CrawlStatus</span><span class="p">.</span><span class="n">IN_PROGRESS</span><span class="p">)</span>

            <span class="c1"># Wait for rate limiter if used
</span>            <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">rate_limiter</span><span class="p">:</span> <span class="k">await</span> <span class="bp">self</span><span class="p">.</span><span class="n">rate_limiter</span><span class="p">.</span><span class="n">wait_if_needed</span><span class="p">(</span><span class="n">url</span><span class="p">)</span>

            <span class="c1"># --- Core Semaphore Logic ---
</span>            <span class="k">async</span> <span class="k">with</span> <span class="n">semaphore</span><span class="p">:</span> <span class="c1"># Acquire a spot from the semaphore
</span>                <span class="c1"># Now that we have a spot, run the actual crawl
</span>                <span class="n">process</span> <span class="o">=</span> <span class="n">psutil</span><span class="p">.</span><span class="n">Process</span><span class="p">()</span>
                <span class="n">start_memory</span> <span class="o">=</span> <span class="n">process</span><span class="p">.</span><span class="n">memory_info</span><span class="p">().</span><span class="n">rss</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1024</span> <span class="o">*</span> <span class="mi">1024</span><span class="p">)</span>

                <span class="c1"># Call the single-page crawl method of the main crawler
</span>                <span class="n">result</span> <span class="o">=</span> <span class="k">await</span> <span class="bp">self</span><span class="p">.</span><span class="n">crawler</span><span class="p">.</span><span class="n">arun</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">,</span> <span class="n">session_id</span><span class="o">=</span><span class="n">task_id</span><span class="p">)</span>

                <span class="n">end_memory</span> <span class="o">=</span> <span class="n">process</span><span class="p">.</span><span class="n">memory_info</span><span class="p">().</span><span class="n">rss</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1024</span> <span class="o">*</span> <span class="mi">1024</span><span class="p">)</span>
                <span class="n">memory_usage</span> <span class="o">=</span> <span class="n">peak_memory</span> <span class="o">=</span> <span class="n">end_memory</span> <span class="o">-</span> <span class="n">start_memory</span>
            <span class="c1"># --- Semaphore spot is released automatically on exiting 'async with' ---
</span>
            <span class="c1"># Update rate limiter based on result status if used
</span>            <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">rate_limiter</span> <span class="ow">and</span> <span class="n">result</span><span class="p">.</span><span class="n">status_code</span><span class="p">:</span>
                 <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="p">.</span><span class="n">rate_limiter</span><span class="p">.</span><span class="n">update_delay</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">result</span><span class="p">.</span><span class="n">status_code</span><span class="p">):</span>
                    <span class="c1"># Handle retry limit exceeded
</span>                    <span class="n">error_message</span> <span class="o">=</span> <span class="s">"Rate limit retry count exceeded"</span>
                    <span class="c1"># ... update monitor, prepare error result ...
</span>
            <span class="c1"># Update monitor status (success/fail)
</span>            <span class="k">if</span> <span class="n">result</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">result</span><span class="p">.</span><span class="n">success</span><span class="p">:</span> <span class="n">error_message</span> <span class="o">=</span> <span class="n">result</span><span class="p">.</span><span class="n">error_message</span>
            <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">monitor</span><span class="p">:</span> <span class="bp">self</span><span class="p">.</span><span class="n">monitor</span><span class="p">.</span><span class="n">update_task</span><span class="p">(</span><span class="n">task_id</span><span class="p">,</span> <span class="n">status</span><span class="o">=</span><span class="n">CrawlStatus</span><span class="p">.</span><span class="n">COMPLETED</span> <span class="k">if</span> <span class="n">result</span><span class="p">.</span><span class="n">success</span> <span class="k">else</span> <span class="n">CrawlStatus</span><span class="p">.</span><span class="n">FAILED</span><span class="p">)</span>

        <span class="k">except</span> <span class="nb">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="c1"># Handle unexpected errors during the crawl
</span>            <span class="n">error_message</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">monitor</span><span class="p">:</span> <span class="bp">self</span><span class="p">.</span><span class="n">monitor</span><span class="p">.</span><span class="n">update_task</span><span class="p">(</span><span class="n">task_id</span><span class="p">,</span> <span class="n">status</span><span class="o">=</span><span class="n">CrawlStatus</span><span class="p">.</span><span class="n">FAILED</span><span class="p">)</span>
            <span class="c1"># Create a failed CrawlResult if needed
</span>            <span class="k">if</span> <span class="ow">not</span> <span class="n">result</span><span class="p">:</span> <span class="n">result</span> <span class="o">=</span> <span class="n">CrawlResult</span><span class="p">(</span><span class="n">url</span><span class="o">=</span><span class="n">url</span><span class="p">,</span> <span class="n">html</span><span class="o">=</span><span class="s">""</span><span class="p">,</span> <span class="n">success</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">error_message</span><span class="o">=</span><span class="n">error_message</span><span class="p">)</span>

        <span class="k">finally</span><span class="p">:</span>
            <span class="c1"># Final monitor update with timing, memory etc.
</span>             <span class="n">end_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="n">time</span><span class="p">()</span>
             <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">monitor</span><span class="p">:</span> <span class="bp">self</span><span class="p">.</span><span class="n">monitor</span><span class="p">.</span><span class="n">update_task</span><span class="p">(...)</span>

        <span class="c1"># Package everything into CrawlerTaskResult
</span>        <span class="k">return</span> <span class="n">CrawlerTaskResult</span><span class="p">(...)</span>


    <span class="k">async</span> <span class="k">def</span> <span class="nf">run_urls</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">crawler</span><span class="p">:</span> <span class="s">"AsyncWebCrawler"</span><span class="p">,</span>
        <span class="n">urls</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
        <span class="n">config</span><span class="p">:</span> <span class="n">CrawlerRunConfig</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">CrawlerTaskResult</span><span class="p">]:</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">crawler</span> <span class="o">=</span> <span class="n">crawler</span> <span class="c1"># Store the crawler instance
</span>        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">monitor</span><span class="p">:</span> <span class="bp">self</span><span class="p">.</span><span class="n">monitor</span><span class="p">.</span><span class="n">start</span><span class="p">()</span>

        <span class="k">try</span><span class="p">:</span>
            <span class="c1"># Create the semaphore with the specified count
</span>            <span class="n">semaphore</span> <span class="o">=</span> <span class="n">asyncio</span><span class="p">.</span><span class="n">Semaphore</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">semaphore_count</span><span class="p">)</span>
            <span class="n">tasks</span> <span class="o">=</span> <span class="p">[]</span>

            <span class="c1"># Create a crawl task for each URL, passing the semaphore
</span>            <span class="k">for</span> <span class="n">url</span> <span class="ow">in</span> <span class="n">urls</span><span class="p">:</span>
                <span class="n">task_id</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">uuid</span><span class="p">.</span><span class="n">uuid4</span><span class="p">())</span>
                <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">monitor</span><span class="p">:</span> <span class="bp">self</span><span class="p">.</span><span class="n">monitor</span><span class="p">.</span><span class="n">add_task</span><span class="p">(</span><span class="n">task_id</span><span class="p">,</span> <span class="n">url</span><span class="p">)</span>
                <span class="c1"># Create an asyncio task to run crawl_url
</span>                <span class="n">task</span> <span class="o">=</span> <span class="n">asyncio</span><span class="p">.</span><span class="n">create_task</span><span class="p">(</span>
                    <span class="bp">self</span><span class="p">.</span><span class="n">crawl_url</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">config</span><span class="p">,</span> <span class="n">task_id</span><span class="p">,</span> <span class="n">semaphore</span><span class="o">=</span><span class="n">semaphore</span><span class="p">)</span>
                <span class="p">)</span>
                <span class="n">tasks</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">task</span><span class="p">)</span>

            <span class="c1"># Wait for all created tasks to complete
</span>            <span class="c1"># asyncio.gather runs them concurrently, respecting the semaphore limit
</span>            <span class="n">results</span> <span class="o">=</span> <span class="k">await</span> <span class="n">asyncio</span><span class="p">.</span><span class="n">gather</span><span class="p">(</span><span class="o">*</span><span class="n">tasks</span><span class="p">,</span> <span class="n">return_exceptions</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

            <span class="c1"># Process results (handle potential exceptions returned by gather)
</span>            <span class="n">final_results</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">res</span> <span class="ow">in</span> <span class="n">results</span><span class="p">:</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">res</span><span class="p">,</span> <span class="nb">Exception</span><span class="p">):</span>
                    <span class="c1"># Handle case where gather caught an exception from a task
</span>                    <span class="c1"># You might create a failed CrawlerTaskResult here
</span>                    <span class="k">pass</span>
                <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">res</span><span class="p">,</span> <span class="n">CrawlerTaskResult</span><span class="p">):</span>
                    <span class="n">final_results</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">res</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">final_results</span>
        <span class="k">finally</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">monitor</span><span class="p">:</span> <span class="bp">self</span><span class="p">.</span><span class="n">monitor</span><span class="p">.</span><span class="n">stop</span><span class="p">()</span>

    <span class="c1"># run_urls_stream would have similar logic but use asyncio.as_completed
</span>    <span class="c1"># or manage tasks manually to yield results as they finish.
</span></code></pre></div></div>

<p>The key takeaway is that the <code class="language-plaintext highlighter-rouge">Dispatcher</code> orchestrates calls to the single-page <code class="language-plaintext highlighter-rouge">crawler.arun</code> method, wrapping them with concurrency controls (like the <code class="language-plaintext highlighter-rouge">async with semaphore:</code> block) before running them using <code class="language-plaintext highlighter-rouge">asyncio</code>’s concurrency tools (<code class="language-plaintext highlighter-rouge">asyncio.create_task</code>, <code class="language-plaintext highlighter-rouge">asyncio.gather</code>, etc.).</p>

<h2 id="conclusion">Conclusion</h2>

<p>You’ve learned about <code class="language-plaintext highlighter-rouge">BaseDispatcher</code>, the crucial “Traffic Controller” that manages concurrent crawls in Crawl4AI, especially for <code class="language-plaintext highlighter-rouge">arun_many</code>.</p>

<ul>
  <li>It solves the problem of efficiently running many crawls without overloading systems or websites.</li>
  <li>It acts as a <strong>blueprint</strong> for managing concurrency.</li>
  <li>Key implementations:
    <ul>
      <li><strong><code class="language-plaintext highlighter-rouge">SemaphoreDispatcher</code></strong>: Uses a simple count limit.</li>
      <li><strong><code class="language-plaintext highlighter-rouge">MemoryAdaptiveDispatcher</code></strong>: Adjusts concurrency based on system memory (the default for <code class="language-plaintext highlighter-rouge">arun_many</code>).</li>
    </ul>
  </li>
  <li>The dispatcher is used <strong>automatically</strong> by <code class="language-plaintext highlighter-rouge">arun_many</code>, but you can provide a specific instance if needed.</li>
  <li>It orchestrates the execution of individual crawl tasks, respecting defined limits.</li>
</ul>

<p>Understanding the dispatcher helps appreciate how Crawl4AI handles large-scale crawling tasks responsibly and efficiently.</p>

<p>This concludes our tour of the core concepts in Crawl4AI! We’ve covered how pages are fetched, how the process is managed, how content is cleaned, filtered, and extracted, how deep crawls are performed, how caching optimizes fetches, and finally, how concurrency is managed. You now have a solid foundation to start building powerful web data extraction and processing applications with Crawl4AI. Happy crawling!</p>

<hr />

<p>Generated by <a href="https://github.com/The-Pocket/Tutorial-Codebase-Knowledge">AI Codebase Knowledge Builder</a></p>
