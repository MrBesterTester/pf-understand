<h1 id="chapter-8-exploring-websites---deepcrawlstrategy">Chapter 8: Exploring Websites - DeepCrawlStrategy</h1>

<p>In <a href="07_crawlresult.md">Chapter 7: Understanding the Results - CrawlResult</a>, we saw the final report (<code class="language-plaintext highlighter-rouge">CrawlResult</code>) that Crawl4AI gives us after processing a single URL. This report contains cleaned content, links, metadata, and maybe even extracted data.</p>

<p>But what if you want to explore a website <em>beyond</em> just the first page? Imagine you land on a blog’s homepage. You don’t just want the homepage content; you want to automatically discover and crawl all the individual blog posts linked from it. How can you tell Crawl4AI to act like an explorer, following links and venturing deeper into the website?</p>

<h2 id="what-problem-does-deepcrawlstrategy-solve">What Problem Does <code class="language-plaintext highlighter-rouge">DeepCrawlStrategy</code> Solve?</h2>

<p>Think of the <code class="language-plaintext highlighter-rouge">AsyncWebCrawler.arun()</code> method we’ve used so far like visiting just the entrance hall of a vast library. You get information about that specific hall, but you don’t automatically explore the adjoining rooms or different floors.</p>

<p>What if you want to systematically explore the library? You need a plan:</p>

<ul>
  <li>Do you explore room by room on the current floor before going upstairs? (Level by level)</li>
  <li>Do you pick one wing and explore all its rooms down to the very end before exploring another wing? (Go deep first)</li>
  <li>Do you have a map highlighting potentially interesting sections and prioritize visiting those first? (Prioritize promising paths)</li>
</ul>

<p><code class="language-plaintext highlighter-rouge">DeepCrawlStrategy</code> provides this <strong>exploration plan</strong>. It defines the logic for how Crawl4AI should discover and crawl new URLs starting from the initial one(s) by following the links it finds on each page. It turns the crawler from a single-page visitor into a website explorer.</p>

<h2 id="what-is-deepcrawlstrategy">What is <code class="language-plaintext highlighter-rouge">DeepCrawlStrategy</code>?</h2>

<p><code class="language-plaintext highlighter-rouge">DeepCrawlStrategy</code> is a concept (a blueprint) in Crawl4AI that represents the <strong>method or logic used to navigate and crawl multiple pages by following links</strong>. It tells the crawler <em>which links</em> to follow and in <em>what order</em> to visit them.</p>

<p>It essentially takes over the process when you call <code class="language-plaintext highlighter-rouge">arun()</code> if a deep crawl is requested, managing a queue or list of URLs to visit and coordinating the crawling of those URLs, potentially up to a certain depth or number of pages.</p>

<h2 id="different-exploration-plans-the-strategies">Different Exploration Plans: The Strategies</h2>

<p>Crawl4AI provides several concrete exploration plans (implementations) for <code class="language-plaintext highlighter-rouge">DeepCrawlStrategy</code>:</p>

<ol>
  <li><strong><code class="language-plaintext highlighter-rouge">BFSDeepCrawlStrategy</code> (Level-by-Level Explorer):</strong>
    <ul>
      <li><strong>Analogy:</strong> Like ripples spreading in a pond.</li>
      <li><strong>How it works:</strong> It first crawls the starting URL (Level 0). Then, it crawls all the valid links found on that page (Level 1). Then, it crawls all the valid links found on <em>those</em> pages (Level 2), and so on. It explores the website layer by layer.</li>
      <li><strong>Good for:</strong> Finding the shortest path to all reachable pages, getting a broad overview quickly near the start page.</li>
    </ul>
  </li>
  <li><strong><code class="language-plaintext highlighter-rouge">DFSDeepCrawlStrategy</code> (Deep Path Explorer):</strong>
    <ul>
      <li><strong>Analogy:</strong> Like exploring one specific corridor in a maze all the way to the end before backtracking and trying another corridor.</li>
      <li><strong>How it works:</strong> It starts at the initial URL, follows one link, then follows a link from <em>that</em> page, and continues going deeper down one path as far as possible (or until a specified depth limit). Only when it hits a dead end or the limit does it backtrack and try another path.</li>
      <li><strong>Good for:</strong> Exploring specific branches of a website thoroughly, potentially reaching deeper pages faster than BFS (if the target is down a specific path).</li>
    </ul>
  </li>
  <li><strong><code class="language-plaintext highlighter-rouge">BestFirstCrawlingStrategy</code> (Priority Explorer):</strong>
    <ul>
      <li><strong>Analogy:</strong> Like using a treasure map where some paths are marked as more promising than others.</li>
      <li><strong>How it works:</strong> This strategy uses a <strong>scoring system</strong>. It looks at all the discovered (but not yet visited) links and assigns a score to each one based on how “promising” it seems (e.g., does the URL contain relevant keywords? Is it from a trusted domain?). It then crawls the link with the <em>best</em> score first, regardless of its depth.</li>
      <li><strong>Good for:</strong> Focusing the crawl on the most relevant or important pages first, especially useful when you can’t crawl the entire site and need to prioritize.</li>
    </ul>
  </li>
</ol>

<p><strong>Guiding the Explorer: Filters and Scorers</strong></p>

<p>Deep crawl strategies often work together with:</p>

<ul>
  <li><strong>Filters:</strong> Rules that decide <em>if</em> a discovered link should even be considered for crawling. Examples:
    <ul>
      <li><code class="language-plaintext highlighter-rouge">DomainFilter</code>: Only follow links within the starting website’s domain.</li>
      <li><code class="language-plaintext highlighter-rouge">URLPatternFilter</code>: Only follow links matching a specific pattern (e.g., <code class="language-plaintext highlighter-rouge">/blog/posts/...</code>).</li>
      <li><code class="language-plaintext highlighter-rouge">ContentTypeFilter</code>: Avoid following links to non-HTML content like PDFs or images.</li>
    </ul>
  </li>
  <li><strong>Scorers:</strong> (Used mainly by <code class="language-plaintext highlighter-rouge">BestFirstCrawlingStrategy</code>) Rules that assign a score to a potential link to help prioritize it. Examples:
    <ul>
      <li><code class="language-plaintext highlighter-rouge">KeywordRelevanceScorer</code>: Scores links higher if the URL contains certain keywords.</li>
      <li><code class="language-plaintext highlighter-rouge">PathDepthScorer</code>: Might score links differently based on how deep they are.</li>
    </ul>
  </li>
</ul>

<p>These act like instructions for the explorer: “Only explore rooms on this floor (filter),” “Ignore corridors marked ‘Staff Only’ (filter),” or “Check rooms marked with a star first (scorer).”</p>

<h2 id="how-to-use-a-deepcrawlstrategy">How to Use a <code class="language-plaintext highlighter-rouge">DeepCrawlStrategy</code></h2>

<p>You enable deep crawling by adding a <code class="language-plaintext highlighter-rouge">DeepCrawlStrategy</code> instance to your <code class="language-plaintext highlighter-rouge">CrawlerRunConfig</code>. Let’s try exploring a website layer by layer using <code class="language-plaintext highlighter-rouge">BFSDeepCrawlStrategy</code>, going only one level deep from the start page.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># chapter8_example_1.py
</span><span class="kn">import</span> <span class="nn">asyncio</span>
<span class="kn">from</span> <span class="nn">crawl4ai</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">AsyncWebCrawler</span><span class="p">,</span>
    <span class="n">CrawlerRunConfig</span><span class="p">,</span>
    <span class="n">BFSDeepCrawlStrategy</span><span class="p">,</span> <span class="c1"># 1. Import the desired strategy
</span>    <span class="n">DomainFilter</span>          <span class="c1"># Import a filter to stay on the same site
</span><span class="p">)</span>

<span class="k">async</span> <span class="k">def</span> <span class="nf">main</span><span class="p">():</span>
    <span class="c1"># 2. Create an instance of the strategy
</span>    <span class="c1">#    - max_depth=1: Crawl start URL (depth 0) + links found (depth 1)
</span>    <span class="c1">#    - filter_chain: Use DomainFilter to only follow links on the same website
</span>    <span class="n">bfs_explorer</span> <span class="o">=</span> <span class="n">BFSDeepCrawlStrategy</span><span class="p">(</span>
        <span class="n">max_depth</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">filter_chain</span><span class="o">=</span><span class="p">[</span><span class="n">DomainFilter</span><span class="p">()]</span> <span class="c1"># Stay within the initial domain
</span>    <span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Strategy: BFS, Max Depth: </span><span class="si">{</span><span class="n">bfs_explorer</span><span class="p">.</span><span class="n">max_depth</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>

    <span class="c1"># 3. Create CrawlerRunConfig and set the deep_crawl_strategy
</span>    <span class="c1">#    Also set stream=True to get results as they come in.
</span>    <span class="n">run_config</span> <span class="o">=</span> <span class="n">CrawlerRunConfig</span><span class="p">(</span>
        <span class="n">deep_crawl_strategy</span><span class="o">=</span><span class="n">bfs_explorer</span><span class="p">,</span>
        <span class="n">stream</span><span class="o">=</span><span class="bp">True</span> <span class="c1"># Get results one by one using async for
</span>    <span class="p">)</span>

    <span class="c1"># 4. Run the crawl - arun now handles the deep crawl!
</span>    <span class="k">async</span> <span class="k">with</span> <span class="n">AsyncWebCrawler</span><span class="p">()</span> <span class="k">as</span> <span class="n">crawler</span><span class="p">:</span>
        <span class="n">start_url</span> <span class="o">=</span> <span class="s">"https://httpbin.org/links/10/0"</span> <span class="c1"># A page with 10 internal links
</span>        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"</span><span class="se">\n</span><span class="s">Starting deep crawl from: </span><span class="si">{</span><span class="n">start_url</span><span class="si">}</span><span class="s">..."</span><span class="p">)</span>

        <span class="n">crawl_results_generator</span> <span class="o">=</span> <span class="k">await</span> <span class="n">crawler</span><span class="p">.</span><span class="n">arun</span><span class="p">(</span><span class="n">url</span><span class="o">=</span><span class="n">start_url</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">run_config</span><span class="p">)</span>

        <span class="n">crawled_count</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="c1"># Iterate over the results as they are yielded
</span>        <span class="k">async</span> <span class="k">for</span> <span class="n">result</span> <span class="ow">in</span> <span class="n">crawl_results_generator</span><span class="p">:</span>
            <span class="n">crawled_count</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="n">status</span> <span class="o">=</span> <span class="s">"✅"</span> <span class="k">if</span> <span class="n">result</span><span class="p">.</span><span class="n">success</span> <span class="k">else</span> <span class="s">"❌"</span>
            <span class="n">depth</span> <span class="o">=</span> <span class="n">result</span><span class="p">.</span><span class="n">metadata</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">"depth"</span><span class="p">,</span> <span class="s">"N/A"</span><span class="p">)</span>
            <span class="n">parent</span> <span class="o">=</span> <span class="n">result</span><span class="p">.</span><span class="n">metadata</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">"parent_url"</span><span class="p">,</span> <span class="s">"Start"</span><span class="p">)</span>
            <span class="n">url_short</span> <span class="o">=</span> <span class="n">result</span><span class="p">.</span><span class="n">url</span><span class="p">.</span><span class="n">split</span><span class="p">(</span><span class="s">'/'</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="c1"># Show last part of URL
</span>            <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"  </span><span class="si">{</span><span class="n">status</span><span class="si">}</span><span class="s"> Crawled: </span><span class="si">{</span><span class="n">url_short</span><span class="si">:</span><span class="o">&lt;</span><span class="mi">6</span><span class="si">}</span><span class="s"> (Depth: </span><span class="si">{</span><span class="n">depth</span><span class="si">}</span><span class="s">)"</span><span class="p">)</span>

        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"</span><span class="se">\n</span><span class="s">Finished deep crawl. Total pages processed: </span><span class="si">{</span><span class="n">crawled_count</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
        <span class="c1"># Expecting 1 (start URL) + 10 (links) = 11 results
</span>
<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="s">"__main__"</span><span class="p">:</span>
    <span class="n">asyncio</span><span class="p">.</span><span class="n">run</span><span class="p">(</span><span class="n">main</span><span class="p">())</span>
</code></pre></div></div>

<p><strong>Explanation:</strong></p>

<ol>
  <li><strong>Import:</strong> We import <code class="language-plaintext highlighter-rouge">AsyncWebCrawler</code>, <code class="language-plaintext highlighter-rouge">CrawlerRunConfig</code>, <code class="language-plaintext highlighter-rouge">BFSDeepCrawlStrategy</code>, and <code class="language-plaintext highlighter-rouge">DomainFilter</code>.</li>
  <li><strong>Instantiate Strategy:</strong> We create <code class="language-plaintext highlighter-rouge">BFSDeepCrawlStrategy</code>.
    <ul>
      <li><code class="language-plaintext highlighter-rouge">max_depth=1</code>: We tell it to crawl the starting URL (depth 0) and any valid links it finds on that page (depth 1), but not to go any further.</li>
      <li><code class="language-plaintext highlighter-rouge">filter_chain=[DomainFilter()]</code>: We provide a list containing <code class="language-plaintext highlighter-rouge">DomainFilter</code>. This tells the strategy to only consider following links that point to the same domain as the <code class="language-plaintext highlighter-rouge">start_url</code>. Links to external sites will be ignored.</li>
    </ul>
  </li>
  <li><strong>Configure Run:</strong> We create a <code class="language-plaintext highlighter-rouge">CrawlerRunConfig</code> and pass our <code class="language-plaintext highlighter-rouge">bfs_explorer</code> instance to the <code class="language-plaintext highlighter-rouge">deep_crawl_strategy</code> parameter. We also set <code class="language-plaintext highlighter-rouge">stream=True</code> so we can process results as soon as they are ready, rather than waiting for the entire crawl to finish.</li>
  <li><strong>Crawl:</strong> We call <code class="language-plaintext highlighter-rouge">await crawler.arun(url=start_url, config=run_config)</code>. Because the config contains a <code class="language-plaintext highlighter-rouge">deep_crawl_strategy</code>, <code class="language-plaintext highlighter-rouge">arun</code> doesn’t just crawl the single <code class="language-plaintext highlighter-rouge">start_url</code>. Instead, it activates the deep crawl logic defined by <code class="language-plaintext highlighter-rouge">BFSDeepCrawlStrategy</code>.</li>
  <li><strong>Process Results:</strong> Since we used <code class="language-plaintext highlighter-rouge">stream=True</code>, the return value is an asynchronous generator. We use <code class="language-plaintext highlighter-rouge">async for result in crawl_results_generator:</code> to loop through the <code class="language-plaintext highlighter-rouge">CrawlResult</code> objects as they are produced by the deep crawl. For each result, we print its status and depth.</li>
</ol>

<p>You’ll see the output showing the crawl starting, then processing the initial page (<code class="language-plaintext highlighter-rouge">links/10/0</code> at depth 0), followed by the 10 linked pages (e.g., <code class="language-plaintext highlighter-rouge">9</code>, <code class="language-plaintext highlighter-rouge">8</code>, … <code class="language-plaintext highlighter-rouge">0</code> at depth 1).</p>

<h2 id="how-it-works-under-the-hood">How It Works (Under the Hood)</h2>

<p>How does simply putting a strategy in the config change <code class="language-plaintext highlighter-rouge">arun</code>’s behavior? It involves a bit of Python magic called a <strong>decorator</strong>.</p>

<ol>
  <li><strong>Decorator:</strong> When you create an <code class="language-plaintext highlighter-rouge">AsyncWebCrawler</code>, its <code class="language-plaintext highlighter-rouge">arun</code> method is automatically wrapped by a <code class="language-plaintext highlighter-rouge">DeepCrawlDecorator</code>.</li>
  <li><strong>Check Config:</strong> When you call <code class="language-plaintext highlighter-rouge">await crawler.arun(url=..., config=...)</code>, this decorator checks if <code class="language-plaintext highlighter-rouge">config.deep_crawl_strategy</code> is set.</li>
  <li><strong>Delegate or Run Original:</strong>
    <ul>
      <li>If a strategy <strong>is set</strong>, the decorator <em>doesn’t</em> run the original single-page crawl logic. Instead, it calls the <code class="language-plaintext highlighter-rouge">arun</code> method of your chosen <code class="language-plaintext highlighter-rouge">DeepCrawlStrategy</code> instance (e.g., <code class="language-plaintext highlighter-rouge">bfs_explorer.arun(...)</code>), passing it the <code class="language-plaintext highlighter-rouge">crawler</code> itself, the <code class="language-plaintext highlighter-rouge">start_url</code>, and the <code class="language-plaintext highlighter-rouge">config</code>.</li>
      <li>If no strategy is set, the decorator simply calls the original <code class="language-plaintext highlighter-rouge">arun</code> logic to crawl the single page.</li>
    </ul>
  </li>
  <li><strong>Strategy Takes Over:</strong> The <code class="language-plaintext highlighter-rouge">DeepCrawlStrategy</code>’s <code class="language-plaintext highlighter-rouge">arun</code> method now manages the crawl.
    <ul>
      <li>It maintains a list or queue of URLs to visit (e.g., <code class="language-plaintext highlighter-rouge">current_level</code> in BFS, a stack in DFS, a priority queue in BestFirst).</li>
      <li>It repeatedly takes batches of URLs from its list/queue.</li>
      <li>For each batch, it calls <code class="language-plaintext highlighter-rouge">crawler.arun_many(urls=batch_urls, config=batch_config)</code> (with deep crawling disabled in <code class="language-plaintext highlighter-rouge">batch_config</code> to avoid infinite loops!).</li>
      <li>As results come back from <code class="language-plaintext highlighter-rouge">arun_many</code>, the strategy processes them:
        <ul>
          <li>It yields the <code class="language-plaintext highlighter-rouge">CrawlResult</code> if running in stream mode.</li>
          <li>It extracts links using its <code class="language-plaintext highlighter-rouge">link_discovery</code> method.</li>
          <li><code class="language-plaintext highlighter-rouge">link_discovery</code> uses <code class="language-plaintext highlighter-rouge">can_process_url</code> (which applies filters) to validate links.</li>
          <li>Valid new links are added to the list/queue for future crawling.</li>
        </ul>
      </li>
      <li>This continues until the list/queue is empty, the max depth/pages limit is reached, or it’s cancelled.</li>
    </ul>
  </li>
</ol>

<pre><code class="language-mermaid">sequenceDiagram
    participant User
    participant Decorator as DeepCrawlDecorator
    participant Strategy as DeepCrawlStrategy (e.g., BFS)
    participant AWC as AsyncWebCrawler

    User-&gt;&gt;Decorator: arun(start_url, config_with_strategy)
    Decorator-&gt;&gt;Strategy: arun(start_url, crawler=AWC, config)
    Note over Strategy: Initialize queue/level with start_url
    loop Until Queue Empty or Limits Reached
        Strategy-&gt;&gt;Strategy: Get next batch of URLs from queue
        Note over Strategy: Create batch_config (deep_crawl=None)
        Strategy-&gt;&gt;AWC: arun_many(batch_urls, config=batch_config)
        AWC--&gt;&gt;Strategy: batch_results (List/Stream of CrawlResult)
        loop For each result in batch_results
            Strategy-&gt;&gt;Strategy: Process result (yield if streaming)
            Strategy-&gt;&gt;Strategy: Discover links (apply filters)
            Strategy-&gt;&gt;Strategy: Add valid new links to queue
        end
    end
    Strategy--&gt;&gt;Decorator: Final result (List or Generator)
    Decorator--&gt;&gt;User: Final result
</code></pre>

<h2 id="code-glimpse">Code Glimpse</h2>

<p>Let’s peek at the simplified structure:</p>

<p><strong>1. The Decorator (<code class="language-plaintext highlighter-rouge">deep_crawling/base_strategy.py</code>)</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Simplified from deep_crawling/base_strategy.py
</span><span class="kn">from</span> <span class="nn">contextvars</span> <span class="kn">import</span> <span class="n">ContextVar</span>
<span class="kn">from</span> <span class="nn">functools</span> <span class="kn">import</span> <span class="n">wraps</span>
<span class="c1"># ... other imports
</span>
<span class="k">class</span> <span class="nc">DeepCrawlDecorator</span><span class="p">:</span>
    <span class="n">deep_crawl_active</span> <span class="o">=</span> <span class="n">ContextVar</span><span class="p">(</span><span class="s">"deep_crawl_active"</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">crawler</span><span class="p">:</span> <span class="n">AsyncWebCrawler</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">crawler</span> <span class="o">=</span> <span class="n">crawler</span>

    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">original_arun</span><span class="p">):</span>
        <span class="o">@</span><span class="n">wraps</span><span class="p">(</span><span class="n">original_arun</span><span class="p">)</span>
        <span class="k">async</span> <span class="k">def</span> <span class="nf">wrapped_arun</span><span class="p">(</span><span class="n">url</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">CrawlerRunConfig</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
            <span class="c1"># Is a strategy present AND not already inside a deep crawl?
</span>            <span class="k">if</span> <span class="n">config</span> <span class="ow">and</span> <span class="n">config</span><span class="p">.</span><span class="n">deep_crawl_strategy</span> <span class="ow">and</span> <span class="ow">not</span> <span class="bp">self</span><span class="p">.</span><span class="n">deep_crawl_active</span><span class="p">.</span><span class="n">get</span><span class="p">():</span>
                <span class="c1"># Mark that we are starting a deep crawl
</span>                <span class="n">token</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">deep_crawl_active</span><span class="p">.</span><span class="nb">set</span><span class="p">(</span><span class="bp">True</span><span class="p">)</span>
                <span class="k">try</span><span class="p">:</span>
                    <span class="c1"># Call the STRATEGY's arun method instead of the original
</span>                    <span class="n">strategy_result</span> <span class="o">=</span> <span class="k">await</span> <span class="n">config</span><span class="p">.</span><span class="n">deep_crawl_strategy</span><span class="p">.</span><span class="n">arun</span><span class="p">(</span>
                        <span class="n">crawler</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">crawler</span><span class="p">,</span>
                        <span class="n">start_url</span><span class="o">=</span><span class="n">url</span><span class="p">,</span>
                        <span class="n">config</span><span class="o">=</span><span class="n">config</span>
                    <span class="p">)</span>
                    <span class="c1"># Handle streaming if needed
</span>                    <span class="k">if</span> <span class="n">config</span><span class="p">.</span><span class="n">stream</span><span class="p">:</span>
                        <span class="c1"># Return an async generator that resets the context var on exit
</span>                        <span class="k">async</span> <span class="k">def</span> <span class="nf">result_wrapper</span><span class="p">():</span>
                            <span class="k">try</span><span class="p">:</span>
                                <span class="k">async</span> <span class="k">for</span> <span class="n">result</span> <span class="ow">in</span> <span class="n">strategy_result</span><span class="p">:</span> <span class="k">yield</span> <span class="n">result</span>
                            <span class="k">finally</span><span class="p">:</span> <span class="bp">self</span><span class="p">.</span><span class="n">deep_crawl_active</span><span class="p">.</span><span class="n">reset</span><span class="p">(</span><span class="n">token</span><span class="p">)</span>
                        <span class="k">return</span> <span class="n">result_wrapper</span><span class="p">()</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="k">return</span> <span class="n">strategy_result</span> <span class="c1"># Return the list of results directly
</span>                <span class="k">finally</span><span class="p">:</span>
                    <span class="c1"># Reset the context var if not streaming (or handled in wrapper)
</span>                    <span class="k">if</span> <span class="ow">not</span> <span class="n">config</span><span class="p">.</span><span class="n">stream</span><span class="p">:</span> <span class="bp">self</span><span class="p">.</span><span class="n">deep_crawl_active</span><span class="p">.</span><span class="n">reset</span><span class="p">(</span><span class="n">token</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># No strategy or already deep crawling, call the original single-page arun
</span>                <span class="k">return</span> <span class="k">await</span> <span class="n">original_arun</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">wrapped_arun</span>
</code></pre></div></div>

<p><strong>2. The Strategy Blueprint (<code class="language-plaintext highlighter-rouge">deep_crawling/base_strategy.py</code>)</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Simplified from deep_crawling/base_strategy.py
</span><span class="kn">from</span> <span class="nn">abc</span> <span class="kn">import</span> <span class="n">ABC</span><span class="p">,</span> <span class="n">abstractmethod</span>
<span class="c1"># ... other imports
</span>
<span class="k">class</span> <span class="nc">DeepCrawlStrategy</span><span class="p">(</span><span class="n">ABC</span><span class="p">):</span>

    <span class="o">@</span><span class="n">abstractmethod</span>
    <span class="k">async</span> <span class="k">def</span> <span class="nf">_arun_batch</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">start_url</span><span class="p">,</span> <span class="n">crawler</span><span class="p">,</span> <span class="n">config</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">CrawlResult</span><span class="p">]:</span>
        <span class="c1"># Implementation for non-streaming mode
</span>        <span class="k">pass</span>

    <span class="o">@</span><span class="n">abstractmethod</span>
    <span class="k">async</span> <span class="k">def</span> <span class="nf">_arun_stream</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">start_url</span><span class="p">,</span> <span class="n">crawler</span><span class="p">,</span> <span class="n">config</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">AsyncGenerator</span><span class="p">[</span><span class="n">CrawlResult</span><span class="p">,</span> <span class="bp">None</span><span class="p">]:</span>
        <span class="c1"># Implementation for streaming mode
</span>        <span class="k">pass</span>

    <span class="k">async</span> <span class="k">def</span> <span class="nf">arun</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">start_url</span><span class="p">,</span> <span class="n">crawler</span><span class="p">,</span> <span class="n">config</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">RunManyReturn</span><span class="p">:</span>
        <span class="c1"># Decides whether to call _arun_batch or _arun_stream
</span>        <span class="k">if</span> <span class="n">config</span><span class="p">.</span><span class="n">stream</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">_arun_stream</span><span class="p">(</span><span class="n">start_url</span><span class="p">,</span> <span class="n">crawler</span><span class="p">,</span> <span class="n">config</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="k">await</span> <span class="bp">self</span><span class="p">.</span><span class="n">_arun_batch</span><span class="p">(</span><span class="n">start_url</span><span class="p">,</span> <span class="n">crawler</span><span class="p">,</span> <span class="n">config</span><span class="p">)</span>

    <span class="o">@</span><span class="n">abstractmethod</span>
    <span class="k">async</span> <span class="k">def</span> <span class="nf">can_process_url</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">url</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">depth</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
        <span class="c1"># Applies filters to decide if a URL is valid to crawl
</span>        <span class="k">pass</span>

    <span class="o">@</span><span class="n">abstractmethod</span>
    <span class="k">async</span> <span class="k">def</span> <span class="nf">link_discovery</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">result</span><span class="p">,</span> <span class="n">source_url</span><span class="p">,</span> <span class="n">current_depth</span><span class="p">,</span> <span class="n">visited</span><span class="p">,</span> <span class="n">next_level</span><span class="p">,</span> <span class="n">depths</span><span class="p">):</span>
        <span class="c1"># Extracts, validates, and prepares links for the next step
</span>        <span class="k">pass</span>

    <span class="o">@</span><span class="n">abstractmethod</span>
    <span class="k">async</span> <span class="k">def</span> <span class="nf">shutdown</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># Cleanup logic
</span>        <span class="k">pass</span>
</code></pre></div></div>

<p><strong>3. Example: BFS Implementation (<code class="language-plaintext highlighter-rouge">deep_crawling/bfs_strategy.py</code>)</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Simplified from deep_crawling/bfs_strategy.py
# ... imports ...
</span><span class="kn">from</span> <span class="nn">.base_strategy</span> <span class="kn">import</span> <span class="n">DeepCrawlStrategy</span> <span class="c1"># Import the base class
</span>
<span class="k">class</span> <span class="nc">BFSDeepCrawlStrategy</span><span class="p">(</span><span class="n">DeepCrawlStrategy</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">max_depth</span><span class="p">,</span> <span class="n">filter_chain</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">url_scorer</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="p">...):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">max_depth</span> <span class="o">=</span> <span class="n">max_depth</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">filter_chain</span> <span class="o">=</span> <span class="n">filter_chain</span> <span class="ow">or</span> <span class="n">FilterChain</span><span class="p">()</span> <span class="c1"># Use default if none
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">url_scorer</span> <span class="o">=</span> <span class="n">url_scorer</span>
        <span class="c1"># ... other init ...
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">_pages_crawled</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">async</span> <span class="k">def</span> <span class="nf">can_process_url</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">url</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">depth</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
        <span class="c1"># ... (validation logic using self.filter_chain) ...
</span>        <span class="n">is_valid</span> <span class="o">=</span> <span class="bp">True</span> <span class="c1"># Placeholder
</span>        <span class="k">if</span> <span class="n">depth</span> <span class="o">!=</span> <span class="mi">0</span> <span class="ow">and</span> <span class="ow">not</span> <span class="k">await</span> <span class="bp">self</span><span class="p">.</span><span class="n">filter_chain</span><span class="p">.</span><span class="nb">apply</span><span class="p">(</span><span class="n">url</span><span class="p">):</span>
            <span class="n">is_valid</span> <span class="o">=</span> <span class="bp">False</span>
        <span class="k">return</span> <span class="n">is_valid</span>

    <span class="k">async</span> <span class="k">def</span> <span class="nf">link_discovery</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">result</span><span class="p">,</span> <span class="n">source_url</span><span class="p">,</span> <span class="n">current_depth</span><span class="p">,</span> <span class="n">visited</span><span class="p">,</span> <span class="n">next_level</span><span class="p">,</span> <span class="n">depths</span><span class="p">):</span>
        <span class="c1"># ... (logic to get links from result.links) ...
</span>        <span class="n">links</span> <span class="o">=</span> <span class="n">result</span><span class="p">.</span><span class="n">links</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">"internal"</span><span class="p">,</span> <span class="p">[])</span> <span class="c1"># Example: only internal
</span>        <span class="k">for</span> <span class="n">link_data</span> <span class="ow">in</span> <span class="n">links</span><span class="p">:</span>
            <span class="n">url</span> <span class="o">=</span> <span class="n">link_data</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">"href"</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">url</span> <span class="ow">and</span> <span class="n">url</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">visited</span><span class="p">:</span>
                <span class="k">if</span> <span class="k">await</span> <span class="bp">self</span><span class="p">.</span><span class="n">can_process_url</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">current_depth</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
                    <span class="c1"># Check scoring, max_pages limit etc.
</span>                    <span class="n">depths</span><span class="p">[</span><span class="n">url</span><span class="p">]</span> <span class="o">=</span> <span class="n">current_depth</span> <span class="o">+</span> <span class="mi">1</span>
                    <span class="n">next_level</span><span class="p">.</span><span class="n">append</span><span class="p">((</span><span class="n">url</span><span class="p">,</span> <span class="n">source_url</span><span class="p">))</span> <span class="c1"># Add (url, parent) tuple
</span>
    <span class="k">async</span> <span class="k">def</span> <span class="nf">_arun_batch</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">start_url</span><span class="p">,</span> <span class="n">crawler</span><span class="p">,</span> <span class="n">config</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">CrawlResult</span><span class="p">]:</span>
        <span class="n">visited</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
        <span class="n">current_level</span> <span class="o">=</span> <span class="p">[(</span><span class="n">start_url</span><span class="p">,</span> <span class="bp">None</span><span class="p">)]</span> <span class="c1"># List of (url, parent_url)
</span>        <span class="n">depths</span> <span class="o">=</span> <span class="p">{</span><span class="n">start_url</span><span class="p">:</span> <span class="mi">0</span><span class="p">}</span>
        <span class="n">all_results</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="k">while</span> <span class="n">current_level</span><span class="p">:</span> <span class="c1"># While there are pages in the current level
</span>            <span class="n">next_level</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">urls_in_level</span> <span class="o">=</span> <span class="p">[</span><span class="n">url</span> <span class="k">for</span> <span class="n">url</span><span class="p">,</span> <span class="n">parent</span> <span class="ow">in</span> <span class="n">current_level</span><span class="p">]</span>
            <span class="n">visited</span><span class="p">.</span><span class="n">update</span><span class="p">(</span><span class="n">urls_in_level</span><span class="p">)</span>

            <span class="c1"># Create config for this batch (no deep crawl recursion)
</span>            <span class="n">batch_config</span> <span class="o">=</span> <span class="n">config</span><span class="p">.</span><span class="n">clone</span><span class="p">(</span><span class="n">deep_crawl_strategy</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">stream</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
            <span class="c1"># Crawl all URLs in the current level
</span>            <span class="n">batch_results</span> <span class="o">=</span> <span class="k">await</span> <span class="n">crawler</span><span class="p">.</span><span class="n">arun_many</span><span class="p">(</span><span class="n">urls</span><span class="o">=</span><span class="n">urls_in_level</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">batch_config</span><span class="p">)</span>

            <span class="k">for</span> <span class="n">result</span> <span class="ow">in</span> <span class="n">batch_results</span><span class="p">:</span>
                <span class="c1"># Add metadata (depth, parent)
</span>                <span class="n">depth</span> <span class="o">=</span> <span class="n">depths</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="n">result</span><span class="p">.</span><span class="n">url</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
                <span class="n">result</span><span class="p">.</span><span class="n">metadata</span> <span class="o">=</span> <span class="n">result</span><span class="p">.</span><span class="n">metadata</span> <span class="ow">or</span> <span class="p">{}</span>
                <span class="n">result</span><span class="p">.</span><span class="n">metadata</span><span class="p">[</span><span class="s">"depth"</span><span class="p">]</span> <span class="o">=</span> <span class="n">depth</span>
                <span class="c1"># ... find parent ...
</span>                <span class="n">all_results</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
                <span class="c1"># Discover links for the *next* level
</span>                <span class="k">if</span> <span class="n">result</span><span class="p">.</span><span class="n">success</span><span class="p">:</span>
                     <span class="k">await</span> <span class="bp">self</span><span class="p">.</span><span class="n">link_discovery</span><span class="p">(</span><span class="n">result</span><span class="p">,</span> <span class="n">result</span><span class="p">.</span><span class="n">url</span><span class="p">,</span> <span class="n">depth</span><span class="p">,</span> <span class="n">visited</span><span class="p">,</span> <span class="n">next_level</span><span class="p">,</span> <span class="n">depths</span><span class="p">)</span>

            <span class="n">current_level</span> <span class="o">=</span> <span class="n">next_level</span> <span class="c1"># Move to the next level
</span>
        <span class="k">return</span> <span class="n">all_results</span>

    <span class="k">async</span> <span class="k">def</span> <span class="nf">_arun_stream</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">start_url</span><span class="p">,</span> <span class="n">crawler</span><span class="p">,</span> <span class="n">config</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">AsyncGenerator</span><span class="p">[</span><span class="n">CrawlResult</span><span class="p">,</span> <span class="bp">None</span><span class="p">]:</span>
        <span class="c1"># Similar logic to _arun_batch, but uses 'yield result'
</span>        <span class="c1"># and processes results as they come from arun_many stream
</span>        <span class="n">visited</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
        <span class="n">current_level</span> <span class="o">=</span> <span class="p">[(</span><span class="n">start_url</span><span class="p">,</span> <span class="bp">None</span><span class="p">)]</span> <span class="c1"># List of (url, parent_url)
</span>        <span class="n">depths</span> <span class="o">=</span> <span class="p">{</span><span class="n">start_url</span><span class="p">:</span> <span class="mi">0</span><span class="p">}</span>

        <span class="k">while</span> <span class="n">current_level</span><span class="p">:</span>
             <span class="n">next_level</span> <span class="o">=</span> <span class="p">[]</span>
             <span class="n">urls_in_level</span> <span class="o">=</span> <span class="p">[</span><span class="n">url</span> <span class="k">for</span> <span class="n">url</span><span class="p">,</span> <span class="n">parent</span> <span class="ow">in</span> <span class="n">current_level</span><span class="p">]</span>
             <span class="n">visited</span><span class="p">.</span><span class="n">update</span><span class="p">(</span><span class="n">urls_in_level</span><span class="p">)</span>

             <span class="c1"># Use stream=True for arun_many
</span>             <span class="n">batch_config</span> <span class="o">=</span> <span class="n">config</span><span class="p">.</span><span class="n">clone</span><span class="p">(</span><span class="n">deep_crawl_strategy</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">stream</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
             <span class="n">batch_results_gen</span> <span class="o">=</span> <span class="k">await</span> <span class="n">crawler</span><span class="p">.</span><span class="n">arun_many</span><span class="p">(</span><span class="n">urls</span><span class="o">=</span><span class="n">urls_in_level</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">batch_config</span><span class="p">)</span>

             <span class="k">async</span> <span class="k">for</span> <span class="n">result</span> <span class="ow">in</span> <span class="n">batch_results_gen</span><span class="p">:</span>
                  <span class="c1"># Add metadata
</span>                  <span class="n">depth</span> <span class="o">=</span> <span class="n">depths</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="n">result</span><span class="p">.</span><span class="n">url</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
                  <span class="n">result</span><span class="p">.</span><span class="n">metadata</span> <span class="o">=</span> <span class="n">result</span><span class="p">.</span><span class="n">metadata</span> <span class="ow">or</span> <span class="p">{}</span>
                  <span class="n">result</span><span class="p">.</span><span class="n">metadata</span><span class="p">[</span><span class="s">"depth"</span><span class="p">]</span> <span class="o">=</span> <span class="n">depth</span>
                  <span class="c1"># ... find parent ...
</span>                  <span class="k">yield</span> <span class="n">result</span> <span class="c1"># Yield result immediately
</span>                  <span class="c1"># Discover links for the next level
</span>                  <span class="k">if</span> <span class="n">result</span><span class="p">.</span><span class="n">success</span><span class="p">:</span>
                      <span class="k">await</span> <span class="bp">self</span><span class="p">.</span><span class="n">link_discovery</span><span class="p">(</span><span class="n">result</span><span class="p">,</span> <span class="n">result</span><span class="p">.</span><span class="n">url</span><span class="p">,</span> <span class="n">depth</span><span class="p">,</span> <span class="n">visited</span><span class="p">,</span> <span class="n">next_level</span><span class="p">,</span> <span class="n">depths</span><span class="p">)</span>

             <span class="n">current_level</span> <span class="o">=</span> <span class="n">next_level</span>
    <span class="c1"># ... shutdown method ...
</span></code></pre></div></div>

<h2 id="conclusion">Conclusion</h2>

<p>You’ve learned about <code class="language-plaintext highlighter-rouge">DeepCrawlStrategy</code>, the component that turns Crawl4AI into a website explorer!</p>

<ul>
  <li>It solves the problem of crawling beyond a single starting page by following links.</li>
  <li>It defines the <strong>exploration plan</strong>:
    <ul>
      <li><code class="language-plaintext highlighter-rouge">BFSDeepCrawlStrategy</code>: Level by level.</li>
      <li><code class="language-plaintext highlighter-rouge">DFSDeepCrawlStrategy</code>: Deep paths first.</li>
      <li><code class="language-plaintext highlighter-rouge">BestFirstCrawlingStrategy</code>: Prioritized by score.</li>
    </ul>
  </li>
  <li><strong>Filters</strong> and <strong>Scorers</strong> help guide the exploration.</li>
  <li>You enable it by setting <code class="language-plaintext highlighter-rouge">deep_crawl_strategy</code> in the <code class="language-plaintext highlighter-rouge">CrawlerRunConfig</code>.</li>
  <li>A decorator mechanism intercepts <code class="language-plaintext highlighter-rouge">arun</code> calls to activate the strategy.</li>
  <li>The strategy manages the queue of URLs and uses <code class="language-plaintext highlighter-rouge">crawler.arun_many</code> to crawl them in batches.</li>
</ul>

<p>Deep crawling allows you to gather information from multiple related pages automatically. But how does Crawl4AI avoid re-fetching the same page over and over again, especially during these deeper crawls? The answer lies in caching.</p>

<p><strong>Next:</strong> Let’s explore how Crawl4AI smartly caches results with <a href="09_cachecontext___cachemode.md">Chapter 9: Smart Fetching with Caching - CacheContext / CacheMode</a>.</p>

<hr />

<p>Generated by <a href="https://github.com/The-Pocket/Tutorial-Codebase-Knowledge">AI Codebase Knowledge Builder</a></p>
