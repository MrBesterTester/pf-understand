<h1 id="chapter-4-url-filtering--scoring">Chapter 4: URL Filtering &amp; Scoring</h1>

<p>In <a href="03_content_extraction_pipeline_.md">Chapter 3: Content Extraction Pipeline</a>, we learned how to process and extract useful information from web pages. But how do we decide which URLs to crawl in the first place, and in what order? That’s where URL filtering and scoring come in!</p>

<h2 id="understanding-url-filtering-and-scoring">Understanding URL Filtering and Scoring</h2>

<p>Imagine you’re hosting a party with a VIP section. You need two people:</p>
<ol>
  <li>A bouncer who decides who gets in based on specific rules (the URL filter)</li>
  <li>A host who decides the order people enter based on their importance (the URL scorer)</li>
</ol>

<p>When crawling the web, you don’t want to crawl everything - that would be inefficient and might get you blocked! Instead, you want to:</p>
<ul>
  <li>Filter out URLs you don’t care about (like advertisements or irrelevant domains)</li>
  <li>Prioritize the most valuable URLs to crawl first</li>
</ul>

<p>Let’s see how <code class="language-plaintext highlighter-rouge">crawl4ai</code> helps us do this!</p>

<h2 id="url-filters-the-bouncers">URL Filters: The Bouncers</h2>

<p>URL filters are simple yes/no decision makers. They look at a URL and decide: “Should we crawl this or not?”</p>

<h3 id="creating-a-simple-domain-filter">Creating a Simple Domain Filter</h3>

<p>Let’s say you only want to crawl pages from <code class="language-plaintext highlighter-rouge">example.com</code>:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">crawl4ai.deep_crawling.filters</span> <span class="kn">import</span> <span class="n">DomainFilter</span>

<span class="c1"># Create a filter that only allows example.com
</span><span class="n">domain_filter</span> <span class="o">=</span> <span class="n">DomainFilter</span><span class="p">(</span><span class="n">allowed_domains</span><span class="o">=</span><span class="p">[</span><span class="s">"example.com"</span><span class="p">])</span>

<span class="c1"># Test it
</span><span class="n">is_allowed</span> <span class="o">=</span> <span class="n">domain_filter</span><span class="p">.</span><span class="nb">apply</span><span class="p">(</span><span class="s">"https://example.com/page1"</span><span class="p">)</span>  <span class="c1"># True
</span><span class="n">is_blocked</span> <span class="o">=</span> <span class="n">domain_filter</span><span class="p">.</span><span class="nb">apply</span><span class="p">(</span><span class="s">"https://other-site.com/page1"</span><span class="p">)</span>  <span class="c1"># False
</span></code></pre></div></div>

<p>This filter acts like a bouncer who only lets in people from a specific city!</p>

<h3 id="filtering-by-content-type">Filtering by Content Type</h3>

<p>Maybe you only want HTML pages and PDFs, not images or other files:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">crawl4ai.deep_crawling.filters</span> <span class="kn">import</span> <span class="n">ContentTypeFilter</span>

<span class="c1"># Create a filter for HTML and PDF files
</span><span class="n">content_filter</span> <span class="o">=</span> <span class="n">ContentTypeFilter</span><span class="p">(</span><span class="n">allowed_types</span><span class="o">=</span><span class="p">[</span><span class="s">"text/html"</span><span class="p">,</span> <span class="s">"application/pdf"</span><span class="p">])</span>

<span class="c1"># Now only HTML and PDF URLs will pass through
</span></code></pre></div></div>

<p>This filter checks the content type (often inferred from the URL extension) and only allows specified types.</p>

<h3 id="pattern-based-filtering">Pattern-Based Filtering</h3>

<p>You can also filter URLs based on patterns:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">crawl4ai.deep_crawling.filters</span> <span class="kn">import</span> <span class="n">URLPatternFilter</span>

<span class="c1"># Exclude admin pages and pages with "login" in the URL
</span><span class="n">pattern_filter</span> <span class="o">=</span> <span class="n">URLPatternFilter</span><span class="p">(</span>
    <span class="n">patterns</span><span class="o">=</span><span class="p">[</span><span class="s">"/admin/*"</span><span class="p">,</span> <span class="s">"*/login*"</span><span class="p">],</span> 
    <span class="n">reverse</span><span class="o">=</span><span class="bp">True</span>  <span class="c1"># Exclude matching patterns instead of including them
</span><span class="p">)</span>
</code></pre></div></div>

<p>This filter looks for specific patterns in URLs and rejects or accepts them accordingly.</p>

<h2 id="url-scorers-the-judges">URL Scorers: The Judges</h2>

<p>While filters make yes/no decisions, scorers assign a numerical value to each URL. Higher scores mean higher priority!</p>

<h3 id="scoring-by-keyword-relevance">Scoring by Keyword Relevance</h3>

<p>Let’s score URLs higher if they contain keywords we’re interested in:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">crawl4ai.deep_crawling.scorers</span> <span class="kn">import</span> <span class="n">KeywordRelevanceScorer</span>

<span class="c1"># URLs with these keywords will score higher
</span><span class="n">keyword_scorer</span> <span class="o">=</span> <span class="n">KeywordRelevanceScorer</span><span class="p">(</span>
    <span class="n">keywords</span><span class="o">=</span><span class="p">[</span><span class="s">"python"</span><span class="p">,</span> <span class="s">"tutorial"</span><span class="p">,</span> <span class="s">"programming"</span><span class="p">],</span>
    <span class="n">weight</span><span class="o">=</span><span class="mf">1.0</span>
<span class="p">)</span>
</code></pre></div></div>

<p>This scorer gives higher scores to URLs containing our keywords. The <code class="language-plaintext highlighter-rouge">weight</code> parameter lets us control how important this particular scoring factor is.</p>

<h3 id="scoring-by-freshness">Scoring by Freshness</h3>

<p>For news or time-sensitive information, you might want to prioritize recent content:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">crawl4ai.deep_crawling.scorers</span> <span class="kn">import</span> <span class="n">FreshnessScorer</span>

<span class="c1"># Prioritize URLs with recent dates in them
</span><span class="n">freshness_scorer</span> <span class="o">=</span> <span class="n">FreshnessScorer</span><span class="p">(</span><span class="n">weight</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">current_year</span><span class="o">=</span><span class="mi">2024</span><span class="p">)</span>
</code></pre></div></div>

<p>This scorer looks for dates in URLs (like <code class="language-plaintext highlighter-rouge">/2024/04/</code> or <code class="language-plaintext highlighter-rouge">2023-news</code>) and scores newer content higher.</p>

<h3 id="combining-multiple-scorers">Combining Multiple Scorers</h3>

<p>You’ll often want to combine multiple scoring criteria:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">crawl4ai.deep_crawling.scorers</span> <span class="kn">import</span> <span class="n">CompositeScorer</span>

<span class="c1"># Combine our scorers
</span><span class="n">combined_scorer</span> <span class="o">=</span> <span class="n">CompositeScorer</span><span class="p">([</span>
    <span class="n">keyword_scorer</span><span class="p">,</span>  <span class="c1"># From previous example
</span>    <span class="n">freshness_scorer</span><span class="p">,</span>  <span class="c1"># From previous example
</span>    <span class="c1"># Add more scorers as needed
</span><span class="p">])</span>
</code></pre></div></div>

<p>Each scorer contributes to the final score, letting you balance different factors when deciding crawling priority.</p>

<h2 id="putting-it-all-together">Putting It All Together</h2>

<p>Now let’s see how to use filters and scorers together in a complete crawling setup:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">crawl4ai</span> <span class="kn">import</span> <span class="n">AsyncWebCrawler</span><span class="p">,</span> <span class="n">CrawlerRunConfig</span>
<span class="kn">from</span> <span class="nn">crawl4ai.deep_crawling</span> <span class="kn">import</span> <span class="n">BestFirstCrawlingStrategy</span>
<span class="kn">from</span> <span class="nn">crawl4ai.deep_crawling.filters</span> <span class="kn">import</span> <span class="n">FilterChain</span>

<span class="c1"># Create a chain of filters
</span><span class="n">filter_chain</span> <span class="o">=</span> <span class="n">FilterChain</span><span class="p">([</span><span class="n">domain_filter</span><span class="p">,</span> <span class="n">content_filter</span><span class="p">,</span> <span class="n">pattern_filter</span><span class="p">])</span>

<span class="c1"># Create a crawler strategy with our filters and scorer
</span><span class="n">strategy</span> <span class="o">=</span> <span class="n">BestFirstCrawlingStrategy</span><span class="p">(</span>
    <span class="n">max_depth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>  <span class="c1"># How deep to crawl
</span>    <span class="n">filter_chain</span><span class="o">=</span><span class="n">filter_chain</span><span class="p">,</span>  <span class="c1"># Our filters
</span>    <span class="n">url_scorer</span><span class="o">=</span><span class="n">combined_scorer</span><span class="p">,</span>  <span class="c1"># Our scorer
</span>    <span class="n">max_pages</span><span class="o">=</span><span class="mi">100</span>  <span class="c1"># Limit total pages
</span><span class="p">)</span>

<span class="c1"># Set up the crawler with our strategy
</span><span class="n">config</span> <span class="o">=</span> <span class="n">CrawlerRunConfig</span><span class="p">(</span><span class="n">deep_crawl_strategy</span><span class="o">=</span><span class="n">strategy</span><span class="p">)</span>

<span class="c1"># Start crawling!
</span><span class="k">async</span> <span class="k">with</span> <span class="n">AsyncWebCrawler</span><span class="p">()</span> <span class="k">as</span> <span class="n">crawler</span><span class="p">:</span>
    <span class="n">results</span> <span class="o">=</span> <span class="k">await</span> <span class="n">crawler</span><span class="p">.</span><span class="n">arun</span><span class="p">(</span>
        <span class="n">url</span><span class="o">=</span><span class="s">"https://example.com"</span><span class="p">,</span> 
        <span class="n">config</span><span class="o">=</span><span class="n">config</span>
    <span class="p">)</span>
</code></pre></div></div>

<p>This sets up a complete crawling system that:</p>
<ol>
  <li>Starts at <code class="language-plaintext highlighter-rouge">example.com</code></li>
  <li>Only crawls URLs that pass all our filters</li>
  <li>Prioritizes URLs based on our scoring criteria</li>
  <li>Stops after 100 pages or when it reaches depth 3</li>
</ol>

<h2 id="what-happens-under-the-hood">What Happens Under the Hood</h2>

<p>Let’s look at what happens when you run a crawler with filters and scorers:</p>

<pre><code class="language-mermaid">sequenceDiagram
    participant WC as WebCrawler
    participant ST as BestFirstStrategy
    participant FC as FilterChain
    participant SC as URLScorer
    participant Q as PriorityQueue

    WC-&gt;&gt;ST: arun(start_url)
    ST-&gt;&gt;Q: add start_url (score=0)
    loop For each URL in queue
        ST-&gt;&gt;Q: get highest-scored URL
        ST-&gt;&gt;FC: can_process_url(url)?
        FC-&gt;&gt;ST: yes/no
        Note over ST: Skip if filter says no
        ST-&gt;&gt;WC: crawl(url)
        WC-&gt;&gt;ST: crawl_result
        ST-&gt;&gt;ST: extract links from result
        loop For each new link
            ST-&gt;&gt;FC: can_process_url(link)?
            FC-&gt;&gt;ST: yes/no
            Note over ST: Skip if filter says no
            ST-&gt;&gt;SC: score(link)
            SC-&gt;&gt;ST: priority score
            ST-&gt;&gt;Q: add link with score
        end
    end
</code></pre>

<p>When you run a crawler with URL filtering and scoring:</p>

<ol>
  <li>The crawler adds the start URL to a priority queue</li>
  <li>It takes the highest-scored URL from the queue</li>
  <li>It checks if the URL passes all filters</li>
  <li>If it passes, the URL is crawled</li>
  <li>New URLs found during crawling go through the same process:
    <ul>
      <li>Check if they pass the filters</li>
      <li>Score them to determine priority</li>
      <li>Add them to the priority queue</li>
    </ul>
  </li>
  <li>The process repeats until the queue is empty or limits are reached</li>
</ol>

<h2 id="implementation-details">Implementation Details</h2>

<p>Let’s look at how the <code class="language-plaintext highlighter-rouge">BestFirstCrawlingStrategy</code> implements this process:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># From crawl4ai/deep_crawling/bff_strategy.py
</span><span class="k">async</span> <span class="k">def</span> <span class="nf">_arun_best_first</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">start_url</span><span class="p">,</span> <span class="n">crawler</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
    <span class="n">queue</span> <span class="o">=</span> <span class="n">asyncio</span><span class="p">.</span><span class="n">PriorityQueue</span><span class="p">()</span>
    <span class="c1"># Start with the initial URL (score 0, depth 0)
</span>    <span class="k">await</span> <span class="n">queue</span><span class="p">.</span><span class="n">put</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">start_url</span><span class="p">,</span> <span class="bp">None</span><span class="p">))</span>
    <span class="n">visited</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
    
    <span class="k">while</span> <span class="ow">not</span> <span class="n">queue</span><span class="p">.</span><span class="n">empty</span><span class="p">():</span>
        <span class="c1"># Get highest priority URL
</span>        <span class="n">score</span><span class="p">,</span> <span class="n">depth</span><span class="p">,</span> <span class="n">url</span><span class="p">,</span> <span class="n">parent_url</span> <span class="o">=</span> <span class="k">await</span> <span class="n">queue</span><span class="p">.</span><span class="n">get</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">url</span> <span class="ow">in</span> <span class="n">visited</span><span class="p">:</span>
            <span class="k">continue</span>
        <span class="n">visited</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">url</span><span class="p">)</span>
        
        <span class="c1"># Crawl the URL
</span>        <span class="n">result</span> <span class="o">=</span> <span class="k">await</span> <span class="n">crawler</span><span class="p">.</span><span class="n">arun</span><span class="p">(</span><span class="n">url</span><span class="o">=</span><span class="n">url</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">)</span>
        <span class="k">yield</span> <span class="n">result</span>
        
        <span class="c1"># Only process successful crawls
</span>        <span class="k">if</span> <span class="n">result</span><span class="p">.</span><span class="n">success</span><span class="p">:</span>
            <span class="c1"># Discover new links
</span>            <span class="n">new_links</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">await</span> <span class="bp">self</span><span class="p">.</span><span class="n">link_discovery</span><span class="p">(</span><span class="n">result</span><span class="p">,</span> <span class="n">url</span><span class="p">,</span> <span class="n">depth</span><span class="p">,</span> <span class="n">visited</span><span class="p">,</span> <span class="n">new_links</span><span class="p">)</span>
            
            <span class="c1"># Score and add links to queue
</span>            <span class="k">for</span> <span class="n">new_url</span><span class="p">,</span> <span class="n">new_parent</span> <span class="ow">in</span> <span class="n">new_links</span><span class="p">:</span>
                <span class="n">new_score</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">url_scorer</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">new_url</span><span class="p">)</span>
                <span class="k">await</span> <span class="n">queue</span><span class="p">.</span><span class="n">put</span><span class="p">((</span><span class="n">new_score</span><span class="p">,</span> <span class="n">depth</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">new_url</span><span class="p">,</span> <span class="n">new_parent</span><span class="p">))</span>
</code></pre></div></div>

<p>The key parts of this implementation are:</p>
<ol>
  <li>Using a priority queue to always process the highest-scored URL next</li>
  <li>Checking if URLs have been visited to avoid duplicates</li>
  <li>Using filters during link discovery to filter out unwanted URLs</li>
  <li>Scoring new URLs to determine their priority in the queue</li>
</ol>

<p>Let’s also look at how filters are applied in the filter chain:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># From crawl4ai/deep_crawling/filters.py
</span><span class="k">async</span> <span class="k">def</span> <span class="nf">apply</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">url</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
    <span class="s">"""Apply all filters concurrently when possible"""</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">stats</span><span class="p">.</span><span class="n">_counters</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>  <span class="c1"># Count total URLs processed
</span>    
    <span class="c1"># Apply each filter
</span>    <span class="k">for</span> <span class="n">filter_</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">filters</span><span class="p">:</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">filter_</span><span class="p">.</span><span class="nb">apply</span><span class="p">(</span><span class="n">url</span><span class="p">)</span>
        
        <span class="c1"># If filter returns False, URL is rejected
</span>        <span class="k">if</span> <span class="ow">not</span> <span class="n">result</span><span class="p">:</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">stats</span><span class="p">.</span><span class="n">_counters</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>  <span class="c1"># Count rejections
</span>            <span class="k">return</span> <span class="bp">False</span>
    
    <span class="c1"># All filters passed
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">stats</span><span class="p">.</span><span class="n">_counters</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>  <span class="c1"># Count accepted URLs
</span>    <span class="k">return</span> <span class="bp">True</span>
</code></pre></div></div>

<p>This shows how the filter chain works - it applies each filter in sequence, and if any filter says “no,” the URL is immediately rejected.</p>

<h2 id="real-world-examples">Real-World Examples</h2>

<h3 id="technical-documentation-crawler">Technical Documentation Crawler</h3>

<p>Let’s say you want to crawl Python documentation but only care about tutorials and library references:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Filter for docs.python.org
</span><span class="n">domain_filter</span> <span class="o">=</span> <span class="n">DomainFilter</span><span class="p">(</span><span class="n">allowed_domains</span><span class="o">=</span><span class="p">[</span><span class="s">"docs.python.org"</span><span class="p">])</span>

<span class="c1"># Only interested in tutorial and library sections
</span><span class="n">pattern_filter</span> <span class="o">=</span> <span class="n">URLPatternFilter</span><span class="p">(</span><span class="n">patterns</span><span class="o">=</span><span class="p">[</span><span class="s">"/tutorial/*"</span><span class="p">,</span> <span class="s">"/library/*"</span><span class="p">])</span>

<span class="c1"># Prioritize based on keywords
</span><span class="n">keyword_scorer</span> <span class="o">=</span> <span class="n">KeywordRelevanceScorer</span><span class="p">([</span>
    <span class="s">"beginners"</span><span class="p">,</span> <span class="s">"tutorial"</span><span class="p">,</span> <span class="s">"examples"</span><span class="p">,</span> <span class="s">"howto"</span>
<span class="p">],</span> <span class="n">weight</span><span class="o">=</span><span class="mf">1.5</span><span class="p">)</span>  <span class="c1"># Give higher weight to beginner-friendly content
</span></code></pre></div></div>

<h3 id="news-crawler">News Crawler</h3>

<p>For a news crawler that focuses on recent technology articles:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Only allow news sites
</span><span class="n">domain_filter</span> <span class="o">=</span> <span class="n">DomainFilter</span><span class="p">(</span><span class="n">allowed_domains</span><span class="o">=</span><span class="p">[</span>
    <span class="s">"techcrunch.com"</span><span class="p">,</span> <span class="s">"theverge.com"</span><span class="p">,</span> <span class="s">"wired.com"</span>
<span class="p">])</span>

<span class="c1"># Exclude category pages and author pages
</span><span class="n">pattern_filter</span> <span class="o">=</span> <span class="n">URLPatternFilter</span><span class="p">(</span>
    <span class="n">patterns</span><span class="o">=</span><span class="p">[</span><span class="s">"/author/*"</span><span class="p">,</span> <span class="s">"/category/*"</span><span class="p">,</span> <span class="s">"/tag/*"</span><span class="p">],</span> 
    <span class="n">reverse</span><span class="o">=</span><span class="bp">True</span>
<span class="p">)</span>

<span class="c1"># Prioritize recent articles and tech keywords
</span><span class="n">freshness</span> <span class="o">=</span> <span class="n">FreshnessScorer</span><span class="p">(</span><span class="n">weight</span><span class="o">=</span><span class="mf">1.2</span><span class="p">)</span>  <span class="c1"># High weight on recency
</span><span class="n">tech_terms</span> <span class="o">=</span> <span class="n">KeywordRelevanceScorer</span><span class="p">(</span>
    <span class="p">[</span><span class="s">"ai"</span><span class="p">,</span> <span class="s">"python"</span><span class="p">,</span> <span class="s">"machine learning"</span><span class="p">,</span> <span class="s">"data science"</span><span class="p">],</span> 
    <span class="n">weight</span><span class="o">=</span><span class="mf">1.0</span>
<span class="p">)</span>
</code></pre></div></div>

<h2 id="conclusion">Conclusion</h2>

<p>URL filtering and scoring are powerful tools that help you focus your web crawling on the most relevant and valuable content. Filters act as gatekeepers that eliminate unwanted URLs, while scorers help you prioritize the most important content first.</p>

<p>By combining different filters and scorers, you can create sophisticated crawling strategies that efficiently collect exactly the data you need. This makes your crawlers more effective and less likely to waste resources on irrelevant content.</p>

<p>In the next chapter, <a href="05_deep_crawling_system_.md">Deep Crawling System</a>, we’ll explore how to use these filtering and scoring techniques to build complete web crawling systems that can navigate complex website structures.</p>

<hr />

<p>Generated by <a href="https://github.com/The-Pocket/Tutorial-Codebase-Knowledge">AI Codebase Knowledge Builder</a></p>
