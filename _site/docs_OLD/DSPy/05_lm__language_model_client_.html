<h1 id="chapter-5-lm-language-model-client---the-engine-room">Chapter 5: LM (Language Model Client) - The Engine Room</h1>

<p>In <a href="04_predict.md">Chapter 4: Predict</a>, we saw how <code class="language-plaintext highlighter-rouge">dspy.Predict</code> takes a <a href="02_signature.md">Signature</a> and input data to magically generate an output. We used our <code class="language-plaintext highlighter-rouge">translator</code> example:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># translator = dspy.Predict(TranslateToFrench)
# result = translator(english_sentence="Hello, how are you?")
# print(result.french_sentence) # --&gt; Bonjour, comment ça va?
</span></code></pre></div></div>

<p>But wait… how did <code class="language-plaintext highlighter-rouge">dspy.Predict</code> <em>actually</em> produce that French sentence? It didn’t just invent it! It needed to talk to a powerful Language Model (LM) like GPT-3.5, GPT-4, Claude, Llama, or some other AI brain.</p>

<p>How does DSPy connect your program (<code class="language-plaintext highlighter-rouge">dspy.Predict</code> in this case) to these external AI brains? That’s the job of the <strong>LM (Language Model Client)</strong> abstraction!</p>

<p>Think of the LM Client as:</p>

<ul>
  <li><strong>The Engine:</strong> It’s the core component that provides the “thinking” power to your DSPy modules.</li>
  <li><strong>The Translator:</strong> It speaks the specific language (API calls, parameters) required by different LM providers (like OpenAI, Anthropic, Cohere, Hugging Face, or models running locally).</li>
  <li><strong>The Connection:</strong> It bridges the gap between your abstract DSPy code and the concrete LM service.</li>
</ul>

<p>In this chapter, you’ll learn:</p>

<ul>
  <li>What the LM Client does and why it’s crucial.</li>
  <li>How to tell DSPy which Language Model to use.</li>
  <li>How this setup lets you easily switch between different LMs.</li>
  <li>A peek under the hood at how the connection works.</li>
</ul>

<p>Let’s connect our program to an AI brain!</p>

<h2 id="what-does-the-lm-client-do">What Does the LM Client Do?</h2>

<p>When a module like <code class="language-plaintext highlighter-rouge">dspy.Predict</code> needs an LM to generate text, it doesn’t make the raw API call itself. Instead, it relies on the configured <strong>LM Client</strong>. The LM Client handles several important tasks:</p>

<ol>
  <li><strong>API Interaction:</strong> It knows how to format the request (the prompt, parameters like <code class="language-plaintext highlighter-rouge">temperature</code>, <code class="language-plaintext highlighter-rouge">max_tokens</code>) in the exact way the target LM provider expects. It then makes the actual network call to the provider’s API (or interacts with a local model).</li>
  <li><strong>Parameter Management:</strong> You can set standard parameters like <code class="language-plaintext highlighter-rouge">temperature</code> (controlling randomness) or <code class="language-plaintext highlighter-rouge">max_tokens</code> (limiting output length) when you configure the LM Client. It ensures these are sent correctly with each request.</li>
  <li><strong>Authentication:</strong> It usually handles sending your API keys securely (often by reading them from environment variables).</li>
  <li><strong>Retries:</strong> If an API call fails due to a temporary issue (like a network glitch or the LM service being busy), the LM Client often automatically retries the request a few times.</li>
  <li><strong>Standard Interface:</strong> It provides a consistent way for DSPy modules (<code class="language-plaintext highlighter-rouge">Predict</code>, <code class="language-plaintext highlighter-rouge">ChainOfThought</code>, etc.) to interact with <em>any</em> supported LM. This means you can swap the underlying LM without changing your module code.</li>
  <li><strong>Caching:</strong> To save time and money, the LM Client usually caches responses. If you make the exact same request again, it can return the saved result instantly instead of calling the LM API again.</li>
</ol>

<p>Essentially, the LM Client abstracts away all the messy details of talking to different AI models, giving your DSPy program a clean and consistent engine to rely on.</p>

<h2 id="configuring-which-lm-to-use">Configuring Which LM to Use</h2>

<p>So, how do you tell DSPy <em>which</em> LM engine to use? You do this using <code class="language-plaintext highlighter-rouge">dspy.settings.configure</code>.</p>

<p>First, you need to import and create an instance of the specific client for your desired LM provider. DSPy integrates with many models primarily through the <code class="language-plaintext highlighter-rouge">litellm</code> library, but also provides direct wrappers for common ones like OpenAI.</p>

<p><strong>Example: Configuring OpenAI’s GPT-3.5 Turbo</strong></p>

<p>Let’s say you want to use OpenAI’s <code class="language-plaintext highlighter-rouge">gpt-3.5-turbo</code> model.</p>

<ol>
  <li><strong>Import the client:</strong>
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">dspy</span>
</code></pre></div>    </div>
    <p><em>(Note: For many common providers like OpenAI, Anthropic, Cohere, etc., you can use the general <code class="language-plaintext highlighter-rouge">dspy.LM</code> client which leverages <code class="language-plaintext highlighter-rouge">litellm</code>)</em></p>
  </li>
  <li>
    <p><strong>Create an instance:</strong> You specify the model name. API keys are typically picked up automatically from environment variables (e.g., <code class="language-plaintext highlighter-rouge">OPENAI_API_KEY</code>). You can also set default parameters here.</p>

    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Use the generic dspy.LM for LiteLLM integration
# Model name follows 'provider/model_name' format for many models
</span><span class="n">turbo</span> <span class="o">=</span> <span class="n">dspy</span><span class="p">.</span><span class="n">LM</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s">'openai/gpt-3.5-turbo'</span><span class="p">,</span> <span class="n">max_tokens</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>

<span class="c1"># Or, if you prefer the dedicated OpenAI client wrapper (functionally similar for basic use)
# from dspy.models.openai import OpenAI
# turbo = OpenAI(model='gpt-3.5-turbo', max_tokens=100)
</span></code></pre></div>    </div>
    <p>This creates an object <code class="language-plaintext highlighter-rouge">turbo</code> that knows how to talk to the <code class="language-plaintext highlighter-rouge">gpt-3.5-turbo</code> model via OpenAI’s API (using <code class="language-plaintext highlighter-rouge">litellm</code>’s connection logic) and will limit responses to 100 tokens by default.</p>
  </li>
  <li>
    <p><strong>Configure DSPy settings:</strong> You tell DSPy globally that this is the LM engine to use for subsequent calls.</p>

    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">dspy</span><span class="p">.</span><span class="n">settings</span><span class="p">.</span><span class="n">configure</span><span class="p">(</span><span class="n">lm</span><span class="o">=</span><span class="n">turbo</span><span class="p">)</span>
</code></pre></div>    </div>
    <p>That’s it! Now, any DSPy module (like <code class="language-plaintext highlighter-rouge">dspy.Predict</code>) that needs to call an LM will automatically use the <code class="language-plaintext highlighter-rouge">turbo</code> instance we just configured.</p>
  </li>
</ol>

<p><strong>Using Other Models (via <code class="language-plaintext highlighter-rouge">dspy.LM</code> and LiteLLM)</strong></p>

<p>The <code class="language-plaintext highlighter-rouge">dspy.LM</code> client is very powerful because it uses <code class="language-plaintext highlighter-rouge">litellm</code> under the hood, which supports a vast numberk of models from providers like Anthropic, Cohere, Google, Hugging Face, Ollama (for local models), and more. You generally just need to change the <code class="language-plaintext highlighter-rouge">model</code> string.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Example: Configure Anthropic's Claude 3 Haiku
# (Assumes ANTHROPIC_API_KEY environment variable is set)
# Note: Provider prefix 'anthropic/' is often optional if model name is unique
</span><span class="n">claude_haiku</span> <span class="o">=</span> <span class="n">dspy</span><span class="p">.</span><span class="n">LM</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s">'anthropic/claude-3-haiku-20240307'</span><span class="p">,</span> <span class="n">max_tokens</span><span class="o">=</span><span class="mi">200</span><span class="p">)</span>
<span class="n">dspy</span><span class="p">.</span><span class="n">settings</span><span class="p">.</span><span class="n">configure</span><span class="p">(</span><span class="n">lm</span><span class="o">=</span><span class="n">claude_haiku</span><span class="p">)</span>

<span class="c1"># Now DSPy modules will use Claude 3 Haiku
</span>
<span class="c1"># Example: Configure a local model served via Ollama
# (Assumes Ollama server is running and has the 'llama3' model)
</span><span class="n">local_llama</span> <span class="o">=</span> <span class="n">dspy</span><span class="p">.</span><span class="n">LM</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s">'ollama/llama3'</span><span class="p">,</span> <span class="n">max_tokens</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
<span class="n">dspy</span><span class="p">.</span><span class="n">settings</span><span class="p">.</span><span class="n">configure</span><span class="p">(</span><span class="n">lm</span><span class="o">=</span><span class="n">local_llama</span><span class="p">)</span>

<span class="c1"># Now DSPy modules will use the local Llama 3 model via Ollama
</span></code></pre></div></div>

<p>You only need to configure the LM <strong>once</strong> (usually at the start of your script).</p>

<h2 id="how-modules-use-the-configured-lm">How Modules Use the Configured LM</h2>

<p>Remember our <code class="language-plaintext highlighter-rouge">translator</code> module from <a href="04_predict.md">Chapter 4: Predict</a>?</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Define signature (same as before)
</span><span class="k">class</span> <span class="nc">TranslateToFrench</span><span class="p">(</span><span class="n">dspy</span><span class="p">.</span><span class="n">Signature</span><span class="p">):</span>
    <span class="s">"""Translates English text to French."""</span>
    <span class="n">english_sentence</span> <span class="o">=</span> <span class="n">dspy</span><span class="p">.</span><span class="n">InputField</span><span class="p">()</span>
    <span class="n">french_sentence</span> <span class="o">=</span> <span class="n">dspy</span><span class="p">.</span><span class="n">OutputField</span><span class="p">()</span>

<span class="c1"># Configure the LM (e.g., using OpenAI)
# turbo = dspy.LM(model='openai/gpt-3.5-turbo', max_tokens=100)
# dspy.settings.configure(lm=turbo)
</span>
<span class="c1"># Create the Predict module
</span><span class="n">translator</span> <span class="o">=</span> <span class="n">dspy</span><span class="p">.</span><span class="n">Predict</span><span class="p">(</span><span class="n">TranslateToFrench</span><span class="p">)</span>

<span class="c1"># Use the module - NO need to pass the LM here!
</span><span class="n">result</span> <span class="o">=</span> <span class="n">translator</span><span class="p">(</span><span class="n">english_sentence</span><span class="o">=</span><span class="s">"Hello, how are you?"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">result</span><span class="p">.</span><span class="n">french_sentence</span><span class="p">)</span>
</code></pre></div></div>

<p>Notice that we didn’t pass <code class="language-plaintext highlighter-rouge">turbo</code> or <code class="language-plaintext highlighter-rouge">claude_haiku</code> or <code class="language-plaintext highlighter-rouge">local_llama</code> directly to <code class="language-plaintext highlighter-rouge">dspy.Predict</code>. When <code class="language-plaintext highlighter-rouge">translator(...)</code> is called, <code class="language-plaintext highlighter-rouge">dspy.Predict</code> internally asks <code class="language-plaintext highlighter-rouge">dspy.settings</code> for the currently configured <code class="language-plaintext highlighter-rouge">lm</code>. It then uses that client object to handle the actual LM interaction.</p>

<h2 id="the-power-of-swapping-lms">The Power of Swapping LMs</h2>

<p>This setup makes it incredibly easy to experiment with different language models. Want to see if Claude does a better job at translation than GPT-3.5? Just change the configuration!</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># --- Experiment 1: Using GPT-3.5 Turbo ---
</span><span class="k">print</span><span class="p">(</span><span class="s">"Testing with GPT-3.5 Turbo..."</span><span class="p">)</span>
<span class="n">turbo</span> <span class="o">=</span> <span class="n">dspy</span><span class="p">.</span><span class="n">LM</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s">'openai/gpt-3.5-turbo'</span><span class="p">,</span> <span class="n">max_tokens</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">dspy</span><span class="p">.</span><span class="n">settings</span><span class="p">.</span><span class="n">configure</span><span class="p">(</span><span class="n">lm</span><span class="o">=</span><span class="n">turbo</span><span class="p">)</span>

<span class="n">translator</span> <span class="o">=</span> <span class="n">dspy</span><span class="p">.</span><span class="n">Predict</span><span class="p">(</span><span class="n">TranslateToFrench</span><span class="p">)</span>
<span class="n">result_turbo</span> <span class="o">=</span> <span class="n">translator</span><span class="p">(</span><span class="n">english_sentence</span><span class="o">=</span><span class="s">"Where is the library?"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"GPT-3.5: </span><span class="si">{</span><span class="n">result_turbo</span><span class="p">.</span><span class="n">french_sentence</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>


<span class="c1"># --- Experiment 2: Using Claude 3 Haiku ---
</span><span class="k">print</span><span class="p">(</span><span class="s">"</span><span class="se">\n</span><span class="s">Testing with Claude 3 Haiku..."</span><span class="p">)</span>
<span class="n">claude_haiku</span> <span class="o">=</span> <span class="n">dspy</span><span class="p">.</span><span class="n">LM</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="s">'anthropic/claude-3-haiku-20240307'</span><span class="p">,</span> <span class="n">max_tokens</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">dspy</span><span class="p">.</span><span class="n">settings</span><span class="p">.</span><span class="n">configure</span><span class="p">(</span><span class="n">lm</span><span class="o">=</span><span class="n">claude_haiku</span><span class="p">)</span>

<span class="c1"># We can reuse the SAME translator object, or create a new one
# It will pick up the NEWLY configured LM from settings
</span><span class="n">result_claude</span> <span class="o">=</span> <span class="n">translator</span><span class="p">(</span><span class="n">english_sentence</span><span class="o">=</span><span class="s">"Where is the library?"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Claude 3 Haiku: </span><span class="si">{</span><span class="n">result_claude</span><span class="p">.</span><span class="n">french_sentence</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
</code></pre></div></div>

<p><strong>Expected Output:</strong></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Testing with GPT-3.5 Turbo...
GPT-3.5: Où est la bibliothèque?

Testing with Claude 3 Haiku...
Claude 3 Haiku: Où se trouve la bibliothèque ?
</code></pre></div></div>

<p>Look at that! We changed the underlying AI brain just by modifying the <code class="language-plaintext highlighter-rouge">dspy.settings.configure</code> call. The core logic of our <code class="language-plaintext highlighter-rouge">translator</code> module remained untouched. This flexibility is a key advantage of DSPy.</p>

<h2 id="how-it-works-under-the-hood-a-peek">How It Works Under the Hood (A Peek)</h2>

<p>Let’s trace what happens when <code class="language-plaintext highlighter-rouge">translator(english_sentence=...)</code> runs:</p>

<ol>
  <li><strong>Module Execution:</strong> The <code class="language-plaintext highlighter-rouge">forward</code> method of the <code class="language-plaintext highlighter-rouge">dspy.Predict</code> module (<code class="language-plaintext highlighter-rouge">translator</code>) starts executing.</li>
  <li><strong>Get LM Client:</strong> Inside its logic, <code class="language-plaintext highlighter-rouge">Predict</code> needs to call an LM. It accesses <code class="language-plaintext highlighter-rouge">dspy.settings.lm</code>. This returns the currently configured LM client object (e.g., the <code class="language-plaintext highlighter-rouge">claude_haiku</code> instance we set).</li>
  <li><strong>Format Prompt:</strong> <code class="language-plaintext highlighter-rouge">Predict</code> uses the <a href="02_signature.md">Signature</a> and the input (<code class="language-plaintext highlighter-rouge">english_sentence</code>) to prepare the text prompt.</li>
  <li><strong>LM Client Call:</strong> <code class="language-plaintext highlighter-rouge">Predict</code> calls the LM client object, passing the formatted prompt and any necessary parameters (like <code class="language-plaintext highlighter-rouge">max_tokens</code> which might come from the client’s defaults or be overridden). Let’s say it calls <code class="language-plaintext highlighter-rouge">claude_haiku(prompt, max_tokens=100, ...)</code>.</li>
  <li><strong>API Interaction (Inside LM Client):</strong>
    <ul>
      <li>The <code class="language-plaintext highlighter-rouge">claude_haiku</code> object (an instance of <code class="language-plaintext highlighter-rouge">dspy.LM</code>) checks its cache first. If the same request was made recently, it might return the cached response directly.</li>
      <li>If not cached, it constructs the specific API request for Anthropic’s Claude 3 Haiku model (using <code class="language-plaintext highlighter-rouge">litellm</code>). This includes setting headers, API keys, and formatting the prompt/parameters correctly for Anthropic.</li>
      <li>It makes the HTTPS request to the Anthropic API endpoint.</li>
      <li>It handles potential retries if the API returns specific errors.</li>
      <li>It receives the raw response from the API.</li>
    </ul>
  </li>
  <li><strong>Parse Response (Inside LM Client):</strong> The client extracts the generated text content from the API response structure.</li>
  <li><strong>Return to Module:</strong> The LM client returns the generated text (e.g., <code class="language-plaintext highlighter-rouge">"Où se trouve la bibliothèque ?"</code>) back to the <code class="language-plaintext highlighter-rouge">dspy.Predict</code> module.</li>
  <li><strong>Module Finishes:</strong> <code class="language-plaintext highlighter-rouge">Predict</code> takes this text, parses it according to the <code class="language-plaintext highlighter-rouge">OutputField</code> (<code class="language-plaintext highlighter-rouge">french_sentence</code>) in the signature, and returns the final <code class="language-plaintext highlighter-rouge">Prediction</code> object.</li>
</ol>

<p>Here’s a simplified sequence diagram:</p>

<pre><code class="language-mermaid">sequenceDiagram
    participant User
    participant PredictModule as translator (Predict)
    participant Settings as dspy.settings
    participant LMClient as LM Client (e.g., dspy.LM instance)
    participant ActualAPI as Actual LM API (e.g., Anthropic)

    User-&gt;&gt;PredictModule: Call translator(english_sentence="...")
    PredictModule-&gt;&gt;Settings: Get configured lm
    Settings--&gt;&gt;PredictModule: Return LMClient instance
    PredictModule-&gt;&gt;PredictModule: Format prompt for LM
    PredictModule-&gt;&gt;LMClient: __call__(prompt, **params)
    LMClient-&gt;&gt;LMClient: Check Cache (Cache Miss)
    LMClient-&gt;&gt;ActualAPI: Send formatted API request (prompt, key, params)
    ActualAPI--&gt;&gt;LMClient: Return API response
    LMClient-&gt;&gt;LMClient: Parse response, extract text
    LMClient--&gt;&gt;PredictModule: Return generated text
    PredictModule-&gt;&gt;PredictModule: Parse text into output fields
    PredictModule--&gt;&gt;User: Return Prediction object
</code></pre>

<p><strong>Relevant Code Files:</strong></p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">dspy/clients/lm.py</code>: Defines the main <code class="language-plaintext highlighter-rouge">dspy.LM</code> class which uses <code class="language-plaintext highlighter-rouge">litellm</code> for broad compatibility. It handles caching (in-memory and disk via <code class="language-plaintext highlighter-rouge">litellm</code>), retries, parameter mapping, and calling the appropriate <code class="language-plaintext highlighter-rouge">litellm</code> functions.</li>
  <li><code class="language-plaintext highlighter-rouge">dspy/clients/base_lm.py</code>: Defines the <code class="language-plaintext highlighter-rouge">BaseLM</code> abstract base class that all LM clients inherit from. It includes the basic <code class="language-plaintext highlighter-rouge">__call__</code> structure, history tracking, and requires subclasses to implement the core <code class="language-plaintext highlighter-rouge">forward</code> method for making the actual API call. It also defines <code class="language-plaintext highlighter-rouge">inspect_history</code>.</li>
  <li><code class="language-plaintext highlighter-rouge">dspy/models/openai.py</code> (and others like <code class="language-plaintext highlighter-rouge">anthropic.py</code>, <code class="language-plaintext highlighter-rouge">cohere.py</code> - though <code class="language-plaintext highlighter-rouge">dspy.LM</code> is often preferred now): Specific client implementations (often inheriting from <code class="language-plaintext highlighter-rouge">BaseLM</code> or using <code class="language-plaintext highlighter-rouge">dspy.LM</code> internally).</li>
  <li><code class="language-plaintext highlighter-rouge">dspy/dsp/utils/settings.py</code>: Defines the <code class="language-plaintext highlighter-rouge">Settings</code> singleton object where the configured <code class="language-plaintext highlighter-rouge">lm</code> (and other components like <code class="language-plaintext highlighter-rouge">rm</code>) are stored and accessed globally or via thread-local context.</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Simplified structure from dspy/clients/base_lm.py
</span><span class="k">class</span> <span class="nc">BaseLM</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">model</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">kwargs</span> <span class="o">=</span> <span class="n">kwargs</span> <span class="c1"># Default params like temp, max_tokens
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">history</span> <span class="o">=</span> <span class="p">[]</span> <span class="c1"># Stores records of calls
</span>
    <span class="o">@</span><span class="n">with_callbacks</span> <span class="c1"># Handles logging, potential custom hooks
</span>    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">prompt</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">messages</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="c1"># 1. Call the actual request logic (implemented by subclasses)
</span>        <span class="n">response</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">forward</span><span class="p">(</span><span class="n">prompt</span><span class="o">=</span><span class="n">prompt</span><span class="p">,</span> <span class="n">messages</span><span class="o">=</span><span class="n">messages</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="c1"># 2. Extract the output text(s)
</span>        <span class="n">outputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">choice</span><span class="p">.</span><span class="n">message</span><span class="p">.</span><span class="n">content</span> <span class="k">for</span> <span class="n">choice</span> <span class="ow">in</span> <span class="n">response</span><span class="p">.</span><span class="n">choices</span><span class="p">]</span> <span class="c1"># Simplified
</span>
        <span class="c1"># 3. Log the interaction (prompt, response, cost, etc.)
</span>        <span class="c1">#    (self.history.append(...))
</span>
        <span class="c1"># 4. Return the list of generated texts
</span>        <span class="k">return</span> <span class="n">outputs</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">prompt</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">messages</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="c1"># Subclasses MUST implement this method to make the actual API call
</span>        <span class="c1"># It should return an object similar to OpenAI's API response structure
</span>        <span class="k">raise</span> <span class="nb">NotImplementedError</span>

<span class="c1"># Simplified structure from dspy/clients/lm.py
</span><span class="kn">import</span> <span class="nn">litellm</span>

<span class="k">class</span> <span class="nc">LM</span><span class="p">(</span><span class="n">BaseLM</span><span class="p">):</span> <span class="c1"># Inherits from BaseLM
</span>    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">model_type</span><span class="o">=</span><span class="s">"chat"</span><span class="p">,</span> <span class="p">...,</span> <span class="n">num_retries</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">model_type</span> <span class="o">=</span> <span class="n">model_type</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">num_retries</span> <span class="o">=</span> <span class="n">num_retries</span>
        <span class="c1"># ... other setup ...
</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">prompt</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">messages</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="c1"># Combine default and call-specific kwargs
</span>        <span class="n">request_kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="o">**</span><span class="bp">self</span><span class="p">.</span><span class="n">kwargs</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">}</span>
        <span class="n">messages</span> <span class="o">=</span> <span class="n">messages</span> <span class="ow">or</span> <span class="p">[{</span><span class="s">"role"</span><span class="p">:</span> <span class="s">"user"</span><span class="p">,</span> <span class="s">"content"</span><span class="p">:</span> <span class="n">prompt</span><span class="p">}]</span>

        <span class="c1"># Use litellm to make the call, handles different providers
</span>        <span class="c1"># Simplified - handles caching, retries, model types under the hood
</span>        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">model_type</span> <span class="o">==</span> <span class="s">"chat"</span><span class="p">:</span>
            <span class="n">response</span> <span class="o">=</span> <span class="n">litellm</span><span class="p">.</span><span class="n">completion</span><span class="p">(</span>
                <span class="n">model</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">,</span>
                <span class="n">messages</span><span class="o">=</span><span class="n">messages</span><span class="p">,</span>
                <span class="c1"># Pass combined parameters
</span>                <span class="o">**</span><span class="n">request_kwargs</span><span class="p">,</span>
                <span class="c1"># Configure retries and caching via litellm
</span>                <span class="n">num_retries</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">num_retries</span><span class="p">,</span>
                <span class="c1"># cache=...
</span>            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span> <span class="c1"># Text completion model type
</span>             <span class="n">response</span> <span class="o">=</span> <span class="n">litellm</span><span class="p">.</span><span class="n">text_completion</span><span class="p">(...)</span> <span class="c1"># Simplified
</span>
        <span class="c1"># LiteLLM returns an object compatible with BaseLM's expectations
</span>        <span class="k">return</span> <span class="n">response</span>

<span class="c1"># Simplified Usage in a Module (like Predict)
# from dspy.dsp.utils import settings
</span>
<span class="c1"># Inside Predict's forward method:
# lm_client = settings.lm # Get the globally configured client
# prompt_text = self._generate_prompt(...) # Format the prompt
# parameters = self.config # Get parameters specific to this Predict instance
# generated_texts = lm_client(prompt_text, **parameters) # Call the LM Client!
# output_text = generated_texts[0]
# parsed_result = self._parse_output(output_text) # Parse based on signature
# return Prediction(**parsed_result)
</span></code></pre></div></div>

<p>The key is that modules interact with the standard <code class="language-plaintext highlighter-rouge">BaseLM</code> interface (primarily its <code class="language-plaintext highlighter-rouge">__call__</code> method), and the specific LM client implementation handles the rest.</p>

<h2 id="conclusion">Conclusion</h2>

<p>You’ve now demystified the <strong>LM (Language Model Client)</strong>! It’s the essential engine connecting your DSPy programs to the power of large language models.</p>

<ul>
  <li>The LM Client acts as a <strong>translator</strong> and <strong>engine</strong>, handling API calls, parameters, retries, and caching.</li>
  <li>You configure which LM to use <strong>globally</strong> via <code class="language-plaintext highlighter-rouge">dspy.settings.configure(lm=...)</code>, usually using <code class="language-plaintext highlighter-rouge">dspy.LM</code> for broad compatibility via <code class="language-plaintext highlighter-rouge">litellm</code>.</li>
  <li>DSPy modules like <code class="language-plaintext highlighter-rouge">dspy.Predict</code> automatically <strong>use the configured LM</strong> without needing it passed explicitly.</li>
  <li>This makes it easy to <strong>swap out different LMs</strong> (like GPT-4, Claude, Llama) with minimal code changes, facilitating experimentation.</li>
</ul>

<p>Now that we know how to connect to the “brain” (LM), what about connecting to external knowledge sources like databases or document collections? That’s where the <strong>RM (Retrieval Model Client)</strong> comes in.</p>

<p><strong>Next:</strong> <a href="06_rm__retrieval_model_client_.md">Chapter 6: RM (Retrieval Model Client)</a></p>

<hr />

<p>Generated by <a href="https://github.com/The-Pocket/Tutorial-Codebase-Knowledge">AI Codebase Knowledge Builder</a></p>
