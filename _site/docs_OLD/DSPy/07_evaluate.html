<h1 id="chapter-7-evaluate---grading-your-program">Chapter 7: Evaluate - Grading Your Program</h1>

<p>In the previous chapter, <a href="06_rm__retrieval_model_client_.md">Chapter 6: RM (Retrieval Model Client)</a>, we learned how to connect our DSPy program to external knowledge sources using Retrieval Models (RMs). We saw how combining RMs with Language Models (LMs) allows us to build sophisticated programs like Retrieval-Augmented Generation (RAG) systems.</p>

<p>Now that we can build these powerful programs, a crucial question arises: <strong>How good are they?</strong> If we build a RAG system to answer questions, how often does it get the answer right? How do we measure its performance objectively?</p>

<p>This is where <strong><code class="language-plaintext highlighter-rouge">dspy.Evaluate</code></strong> comes in! It’s DSPy’s built-in tool for testing and grading your programs.</p>

<p>Think of <code class="language-plaintext highlighter-rouge">dspy.Evaluate</code> as:</p>

<ul>
  <li><strong>An Automated Grader:</strong> Like a teacher grading a batch of homework assignments based on an answer key.</li>
  <li><strong>A Test Suite Runner:</strong> Similar to how software developers use test suites to check if their code works correctly.</li>
  <li><strong>Your Program’s Report Card:</strong> It gives you a score that tells you how well your DSPy program is performing on a specific set of tasks.</li>
</ul>

<p>In this chapter, you’ll learn:</p>

<ul>
  <li>What you need to evaluate a DSPy program.</li>
  <li>How to define a metric (a grading rule).</li>
  <li>How to use <code class="language-plaintext highlighter-rouge">dspy.Evaluate</code> to run the evaluation and get a score.</li>
  <li>How it works behind the scenes.</li>
</ul>

<p>Let’s learn how to grade our DSPy creations!</p>

<h2 id="the-ingredients-for-evaluation">The Ingredients for Evaluation</h2>

<p>To grade your program using <code class="language-plaintext highlighter-rouge">dspy.Evaluate</code>, you need three main ingredients:</p>

<ol>
  <li><strong>Your DSPy <code class="language-plaintext highlighter-rouge">Program</code>:</strong> The program you want to test. This could be a simple <code class="language-plaintext highlighter-rouge">dspy.Predict</code> module or a complex multi-step program like the <code class="language-plaintext highlighter-rouge">SimpleRAG</code> we sketched out in the last chapter.</li>
  <li><strong>A Dataset (<code class="language-plaintext highlighter-rouge">devset</code>):</strong> A list of <code class="language-plaintext highlighter-rouge">dspy.Example</code> objects (<a href="03_example.md">Chapter 3: Example</a>). Crucially, these examples must contain not only the <strong>inputs</strong> your program expects but also the <strong>gold standard outputs</strong> (the correct answers or desired results) that you want to compare against. This dataset is often called a “development set” or “dev set”.</li>
  <li><strong>A Metric Function (<code class="language-plaintext highlighter-rouge">metric</code>):</strong> A Python function you define. This function takes one gold standard <code class="language-plaintext highlighter-rouge">Example</code> and the <code class="language-plaintext highlighter-rouge">Prediction</code> generated by your program for that example’s inputs. It then compares them and returns a score indicating how well the prediction matched the gold standard. The score is often <code class="language-plaintext highlighter-rouge">1.0</code> for a perfect match and <code class="language-plaintext highlighter-rouge">0.0</code> for a mismatch, but it can also be a fractional score (e.g., for F1 score).</li>
</ol>

<p><code class="language-plaintext highlighter-rouge">dspy.Evaluate</code> takes these three ingredients, runs your program on all examples in the dataset, uses your metric function to score each prediction against the gold standard, and finally reports the average score across the entire dataset.</p>

<h2 id="evaluating-a-simple-question-answering-program">Evaluating a Simple Question Answering Program</h2>

<p>Let’s illustrate this with a simple example. Suppose we have a basic DSPy program that’s supposed to answer simple questions.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">dspy</span>

<span class="c1"># Assume we have configured an LM client (Chapter 5)
# gpt3_turbo = dspy.LM(model='openai/gpt-3.5-turbo')
# dspy.settings.configure(lm=gpt3_turbo)
</span>
<span class="c1"># A simple program using dspy.Predict (Chapter 4)
</span><span class="k">class</span> <span class="nc">BasicQA</span><span class="p">(</span><span class="n">dspy</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
        <span class="c1"># Use a simple signature: question -&gt; answer
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">predictor</span> <span class="o">=</span> <span class="n">dspy</span><span class="p">.</span><span class="n">Predict</span><span class="p">(</span><span class="s">'question -&gt; answer'</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">question</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">predictor</span><span class="p">(</span><span class="n">question</span><span class="o">=</span><span class="n">question</span><span class="p">)</span>

<span class="c1"># Create an instance of our program
</span><span class="n">qa_program</span> <span class="o">=</span> <span class="n">BasicQA</span><span class="p">()</span>
</code></pre></div></div>

<p>Now, let’s prepare the other ingredients for evaluation.</p>

<p><strong>1. Prepare the Dataset (<code class="language-plaintext highlighter-rouge">devset</code>)</strong></p>

<p>We need a list of <code class="language-plaintext highlighter-rouge">dspy.Example</code> objects, each containing a <code class="language-plaintext highlighter-rouge">question</code> (input) and the correct <code class="language-plaintext highlighter-rouge">answer</code> (gold standard output).</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Create example data points with questions and gold answers
</span><span class="n">dev_example1</span> <span class="o">=</span> <span class="n">dspy</span><span class="p">.</span><span class="n">Example</span><span class="p">(</span><span class="n">question</span><span class="o">=</span><span class="s">"What color is the sky?"</span><span class="p">,</span> <span class="n">answer</span><span class="o">=</span><span class="s">"blue"</span><span class="p">)</span>
<span class="n">dev_example2</span> <span class="o">=</span> <span class="n">dspy</span><span class="p">.</span><span class="n">Example</span><span class="p">(</span><span class="n">question</span><span class="o">=</span><span class="s">"What is 2 + 2?"</span><span class="p">,</span> <span class="n">answer</span><span class="o">=</span><span class="s">"4"</span><span class="p">)</span>
<span class="n">dev_example3</span> <span class="o">=</span> <span class="n">dspy</span><span class="p">.</span><span class="n">Example</span><span class="p">(</span><span class="n">question</span><span class="o">=</span><span class="s">"What is the capital of France?"</span><span class="p">,</span> <span class="n">answer</span><span class="o">=</span><span class="s">"Paris"</span><span class="p">)</span>
<span class="n">dev_example_wrong</span> <span class="o">=</span> <span class="n">dspy</span><span class="p">.</span><span class="n">Example</span><span class="p">(</span><span class="n">question</span><span class="o">=</span><span class="s">"Who wrote Hamlet?"</span><span class="p">,</span> <span class="n">answer</span><span class="o">=</span><span class="s">"Shakespeare"</span><span class="p">)</span> <span class="c1"># Let's assume our QA program might get this wrong
</span>
<span class="c1"># Create the development set (list of examples)
</span><span class="n">devset</span> <span class="o">=</span> <span class="p">[</span><span class="n">dev_example1</span><span class="p">,</span> <span class="n">dev_example2</span><span class="p">,</span> <span class="n">dev_example3</span><span class="p">,</span> <span class="n">dev_example_wrong</span><span class="p">]</span>

<span class="c1"># We need to tell DSPy which fields are inputs vs outputs for evaluation
# The .with_inputs() method marks the input keys.
# The remaining keys ('answer' in this case) are treated as labels.
</span><span class="n">devset</span> <span class="o">=</span> <span class="p">[</span><span class="n">d</span><span class="p">.</span><span class="n">with_inputs</span><span class="p">(</span><span class="s">'question'</span><span class="p">)</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">devset</span><span class="p">]</span>
</code></pre></div></div>
<p>Here, we’ve created a small dataset <code class="language-plaintext highlighter-rouge">devset</code> with four question-answer pairs. We used <code class="language-plaintext highlighter-rouge">.with_inputs('question')</code> to mark the <code class="language-plaintext highlighter-rouge">question</code> field as the input; <code class="language-plaintext highlighter-rouge">dspy.Evaluate</code> will automatically treat the remaining field (<code class="language-plaintext highlighter-rouge">answer</code>) as the gold label to compare against.</p>

<p><strong>2. Define a Metric Function (<code class="language-plaintext highlighter-rouge">metric</code>)</strong></p>

<p>We need a function that compares the program’s predicted answer to the gold answer in an example. Let’s create a simple “exact match” metric.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">simple_exact_match_metric</span><span class="p">(</span><span class="n">gold_example</span><span class="p">,</span> <span class="n">prediction</span><span class="p">,</span> <span class="n">trace</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="c1"># Does the predicted 'answer' EXACTLY match the gold 'answer'?
</span>    <span class="c1"># '.answer' field comes from our Predict signature 'question -&gt; answer'
</span>    <span class="c1"># 'gold_example.answer' is the gold label from the devset example
</span>    <span class="k">return</span> <span class="n">prediction</span><span class="p">.</span><span class="n">answer</span> <span class="o">==</span> <span class="n">gold_example</span><span class="p">.</span><span class="n">answer</span>

<span class="c1"># Note: DSPy often provides common metrics too, like dspy.evaluate.answer_exact_match
# import dspy.evaluate
# metric = dspy.evaluate.answer_exact_match
</span></code></pre></div></div>
<p>Our <code class="language-plaintext highlighter-rouge">simple_exact_match_metric</code> function takes the gold <code class="language-plaintext highlighter-rouge">dspy.Example</code> (<code class="language-plaintext highlighter-rouge">gold_example</code>) and the program’s output <code class="language-plaintext highlighter-rouge">dspy.Prediction</code> (<code class="language-plaintext highlighter-rouge">prediction</code>). It returns <code class="language-plaintext highlighter-rouge">True</code> (which Python treats as <code class="language-plaintext highlighter-rouge">1.0</code>) if the predicted <code class="language-plaintext highlighter-rouge">answer</code> matches the gold <code class="language-plaintext highlighter-rouge">answer</code>, and <code class="language-plaintext highlighter-rouge">False</code> (<code class="language-plaintext highlighter-rouge">0.0</code>) otherwise. The <code class="language-plaintext highlighter-rouge">trace</code> argument is optional and can be ignored for basic metrics; it sometimes contains information about the program’s execution steps.</p>

<p><strong>3. Create and Run <code class="language-plaintext highlighter-rouge">dspy.Evaluate</code></strong></p>

<p>Now we have all the ingredients: <code class="language-plaintext highlighter-rouge">qa_program</code>, <code class="language-plaintext highlighter-rouge">devset</code>, and <code class="language-plaintext highlighter-rouge">simple_exact_match_metric</code>. Let’s use <code class="language-plaintext highlighter-rouge">dspy.Evaluate</code>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">dspy.evaluate</span> <span class="kn">import</span> <span class="n">Evaluate</span>

<span class="c1"># 1. Create the Evaluator instance
</span><span class="n">evaluator</span> <span class="o">=</span> <span class="n">Evaluate</span><span class="p">(</span>
    <span class="n">devset</span><span class="o">=</span><span class="n">devset</span><span class="p">,</span>            <span class="c1"># The dataset to evaluate on
</span>    <span class="n">metric</span><span class="o">=</span><span class="n">simple_exact_match_metric</span><span class="p">,</span> <span class="c1"># The function to score predictions
</span>    <span class="n">num_threads</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>            <span class="c1"># Run 4 evaluations in parallel (optional)
</span>    <span class="n">display_progress</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>    <span class="c1"># Show a progress bar (optional)
</span>    <span class="n">display_table</span><span class="o">=</span><span class="bp">True</span>        <span class="c1"># Display results in a table (optional)
</span><span class="p">)</span>

<span class="c1"># 2. Run the evaluation by calling the evaluator with the program
# This will run qa_program on each example in devset,
# score it using simple_exact_match_metric, and return the average score.
</span><span class="n">average_score</span> <span class="o">=</span> <span class="n">evaluator</span><span class="p">(</span><span class="n">qa_program</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Average Score: </span><span class="si">{</span><span class="n">average_score</span><span class="si">}</span><span class="s">%"</span><span class="p">)</span>
</code></pre></div></div>

<p><strong>What happens here?</strong></p>

<ol>
  <li>We create an <code class="language-plaintext highlighter-rouge">Evaluate</code> object, providing our dataset and metric. We also request parallel execution (<code class="language-plaintext highlighter-rouge">num_threads=4</code>) for speed and ask for progress/table display.</li>
  <li>We call the <code class="language-plaintext highlighter-rouge">evaluator</code> instance with our <code class="language-plaintext highlighter-rouge">qa_program</code>.</li>
  <li><code class="language-plaintext highlighter-rouge">Evaluate</code> iterates through <code class="language-plaintext highlighter-rouge">devset</code>:
    <ul>
      <li>For <code class="language-plaintext highlighter-rouge">dev_example1</code>, it calls <code class="language-plaintext highlighter-rouge">qa_program(question="What color is the sky?")</code>. Let’s assume the program predicts <code class="language-plaintext highlighter-rouge">answer="blue"</code>.</li>
      <li>It calls <code class="language-plaintext highlighter-rouge">simple_exact_match_metric(dev_example1, predicted_output)</code>. Since <code class="language-plaintext highlighter-rouge">"blue" == "blue"</code>, the score is <code class="language-plaintext highlighter-rouge">1.0</code>.</li>
      <li>It does the same for <code class="language-plaintext highlighter-rouge">dev_example2</code> (input: “What is 2 + 2?”). Assume prediction is <code class="language-plaintext highlighter-rouge">answer="4"</code>. Score: <code class="language-plaintext highlighter-rouge">1.0</code>.</li>
      <li>It does the same for <code class="language-plaintext highlighter-rouge">dev_example3</code> (input: “What is the capital of France?”). Assume prediction is <code class="language-plaintext highlighter-rouge">answer="Paris"</code>. Score: <code class="language-plaintext highlighter-rouge">1.0</code>.</li>
      <li>It does the same for <code class="language-plaintext highlighter-rouge">dev_example_wrong</code> (input: “Who wrote Hamlet?”). Maybe the simple LM messes up and predicts <code class="language-plaintext highlighter-rouge">answer="William Shakespeare"</code>. Since <code class="language-plaintext highlighter-rouge">"William Shakespeare" != "Shakespeare"</code>, the score is <code class="language-plaintext highlighter-rouge">0.0</code>.</li>
    </ul>
  </li>
  <li><code class="language-plaintext highlighter-rouge">Evaluate</code> calculates the average score: <code class="language-plaintext highlighter-rouge">(1.0 + 1.0 + 1.0 + 0.0) / 4 = 0.75</code>.</li>
  <li>It prints the average score as a percentage.</li>
</ol>

<p><strong>Expected Output:</strong></p>

<p>A progress bar will be shown (if <code class="language-plaintext highlighter-rouge">tqdm</code> is installed), followed by a table like this (requires <code class="language-plaintext highlighter-rouge">pandas</code>):</p>

<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Average Metric: 3 / 4  (75.0%)
  question                           answer      simple_exact_match_metric
0 What color is the sky?           blue        ✔️ [True]
1 What is 2 + 2?                   4           ✔️ [True]
2 What is the capital of France?   Paris       ✔️ [True]
3 Who wrote Hamlet?                Shakespeare 
</code></pre></div></div>
<p><em>(Note: The table shows the predicted answer if different, otherwise just the metric outcome. The exact table format might vary slightly).</em></p>

<p>And finally:</p>
<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Average Score: 75.0%
</code></pre></div></div>

<p>This tells us our simple QA program achieved 75% accuracy on our small development set using the exact match criterion.</p>

<h2 id="getting-more-details-optional-flags">Getting More Details (Optional Flags)</h2>

<p>Sometimes, just the average score isn’t enough. You might want to see the score for each individual example or the actual predictions made by the program. <code class="language-plaintext highlighter-rouge">Evaluate</code> provides flags for this:</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">return_all_scores=True</code>: Returns the average score <em>and</em> a list containing the individual score for each example.</li>
  <li><code class="language-plaintext highlighter-rouge">return_outputs=True</code>: Returns the average score <em>and</em> a list of tuples, where each tuple contains <code class="language-plaintext highlighter-rouge">(example, prediction, score)</code>.</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Re-run evaluation asking for more details
</span><span class="n">evaluator_detailed</span> <span class="o">=</span> <span class="n">Evaluate</span><span class="p">(</span><span class="n">devset</span><span class="o">=</span><span class="n">devset</span><span class="p">,</span> <span class="n">metric</span><span class="o">=</span><span class="n">simple_exact_match_metric</span><span class="p">)</span>

<span class="c1"># Get individual scores
</span><span class="n">avg_score</span><span class="p">,</span> <span class="n">individual_scores</span> <span class="o">=</span> <span class="n">evaluator_detailed</span><span class="p">(</span><span class="n">qa_program</span><span class="p">,</span> <span class="n">return_all_scores</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Individual Scores: </span><span class="si">{</span><span class="n">individual_scores</span><span class="si">}</span><span class="s">"</span><span class="p">)</span> <span class="c1"># Output: [True, True, True, False]
</span>
<span class="c1"># Get full outputs
</span><span class="n">avg_score</span><span class="p">,</span> <span class="n">outputs_list</span> <span class="o">=</span> <span class="n">evaluator_detailed</span><span class="p">(</span><span class="n">qa_program</span><span class="p">,</span> <span class="n">return_outputs</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="c1"># outputs_list[0] would be roughly: (dev_example1, Prediction(answer='blue'), True)
# outputs_list[3] would be roughly: (dev_example_wrong, Prediction(answer='William Shakespeare'), False)
</span><span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Number of outputs returned: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">outputs_list</span><span class="p">)</span><span class="si">}</span><span class="s">"</span><span class="p">)</span> <span class="c1"># Output: 4
</span></code></pre></div></div>

<p>These flags are useful for more detailed error analysis to understand <em>where</em> your program is failing.</p>

<h2 id="how-it-works-under-the-hood">How It Works Under the Hood</h2>

<p>What happens internally when you call <code class="language-plaintext highlighter-rouge">evaluator(program)</code>?</p>

<ol>
  <li><strong>Initialization:</strong> The <code class="language-plaintext highlighter-rouge">Evaluate</code> instance stores the <code class="language-plaintext highlighter-rouge">devset</code>, <code class="language-plaintext highlighter-rouge">metric</code>, <code class="language-plaintext highlighter-rouge">num_threads</code>, and other settings.</li>
  <li><strong>Parallel Executor:</strong> It creates a <code class="language-plaintext highlighter-rouge">ParallelExecutor</code> (if <code class="language-plaintext highlighter-rouge">num_threads &gt; 1</code>) to manage running the evaluations concurrently.</li>
  <li><strong>Iteration:</strong> It iterates through each <code class="language-plaintext highlighter-rouge">example</code> in the <code class="language-plaintext highlighter-rouge">devset</code>.</li>
  <li><strong>Program Execution:</strong> For each <code class="language-plaintext highlighter-rouge">example</code>, it calls <code class="language-plaintext highlighter-rouge">program(**example.inputs())</code> (e.g., <code class="language-plaintext highlighter-rouge">qa_program(question=example.question)</code>). This runs your DSPy program’s <code class="language-plaintext highlighter-rouge">forward</code> method to get a <code class="language-plaintext highlighter-rouge">prediction</code>.</li>
  <li><strong>Metric Calculation:</strong> It calls the provided <code class="language-plaintext highlighter-rouge">metric</code> function, passing it the original <code class="language-plaintext highlighter-rouge">example</code> (which contains the gold labels) and the <code class="language-plaintext highlighter-rouge">prediction</code> object returned by the program (e.g., <code class="language-plaintext highlighter-rouge">metric(example, prediction)</code>). This yields a <code class="language-plaintext highlighter-rouge">score</code>.</li>
  <li><strong>Error Handling:</strong> If running the program or the metric causes an error for a specific example, <code class="language-plaintext highlighter-rouge">Evaluate</code> catches it (up to <code class="language-plaintext highlighter-rouge">max_errors</code>), records a default <code class="language-plaintext highlighter-rouge">failure_score</code> (usually 0.0), and continues with the rest of the dataset.</li>
  <li><strong>Aggregation:</strong> It collects all the individual scores (including failure scores).</li>
  <li><strong>Calculate Average:</strong> It computes the average score by summing all scores and dividing by the total number of examples in the <code class="language-plaintext highlighter-rouge">devset</code>.</li>
  <li><strong>Return Results:</strong> It returns the average score (and optionally the individual scores or full output tuples based on the flags).</li>
</ol>

<p>Here’s a simplified sequence diagram:</p>

<pre><code class="language-mermaid">sequenceDiagram
    participant User
    participant Evaluator as dspy.Evaluate
    participant Executor as ParallelExecutor
    participant Program as Your DSPy Program
    participant Metric as Your Metric Function

    User-&gt;&gt;Evaluator: __call__(program)
    Evaluator-&gt;&gt;Executor: Create (manages threads)
    loop For each example in devset
        Executor-&gt;&gt;Executor: Assign task to a thread
        Note over Executor, Program: In parallel thread:
        Executor-&gt;&gt;Program: Call program(**example.inputs())
        Program--&gt;&gt;Executor: Return prediction
        Executor-&gt;&gt;Metric: Call metric(example, prediction)
        Metric--&gt;&gt;Executor: Return score
    end
    Executor-&gt;&gt;Evaluator: Collect all results (predictions, scores)
    Evaluator-&gt;&gt;Evaluator: Calculate average score
    Evaluator--&gt;&gt;User: Return average score (and other requested data)

</code></pre>

<p><strong>Relevant Code Files:</strong></p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">dspy/evaluate/evaluate.py</code>: Defines the <code class="language-plaintext highlighter-rouge">Evaluate</code> class.
    <ul>
      <li>The <code class="language-plaintext highlighter-rouge">__init__</code> method stores the configuration.</li>
      <li>The <code class="language-plaintext highlighter-rouge">__call__</code> method orchestrates the evaluation: sets up the <code class="language-plaintext highlighter-rouge">ParallelExecutor</code>, defines the <code class="language-plaintext highlighter-rouge">process_item</code> function (which runs the program and metric for one example), executes it over the <code class="language-plaintext highlighter-rouge">devset</code>, aggregates results, and handles display/return logic.</li>
    </ul>
  </li>
  <li><code class="language-plaintext highlighter-rouge">dspy/utils/parallelizer.py</code>: Contains the <code class="language-plaintext highlighter-rouge">ParallelExecutor</code> class used for running tasks concurrently across multiple threads or processes.</li>
  <li><code class="language-plaintext highlighter-rouge">dspy/evaluate/metrics.py</code>: Contains implementations of common metrics like <code class="language-plaintext highlighter-rouge">answer_exact_match</code>.</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Simplified view from dspy/evaluate/evaluate.py
</span>
<span class="c1"># ... imports ...
</span><span class="kn">from</span> <span class="nn">dspy.utils.parallelizer</span> <span class="kn">import</span> <span class="n">ParallelExecutor</span>

<span class="k">class</span> <span class="nc">Evaluate</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">devset</span><span class="p">,</span> <span class="n">metric</span><span class="p">,</span> <span class="n">num_threads</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="p">...,</span> <span class="n">failure_score</span><span class="o">=</span><span class="mf">0.0</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">devset</span> <span class="o">=</span> <span class="n">devset</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">metric</span> <span class="o">=</span> <span class="n">metric</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">num_threads</span> <span class="o">=</span> <span class="n">num_threads</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">display_progress</span> <span class="o">=</span> <span class="p">...</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">display_table</span> <span class="o">=</span> <span class="p">...</span>
        <span class="c1"># ... store other flags ...
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">failure_score</span> <span class="o">=</span> <span class="n">failure_score</span>

    <span class="c1"># @with_callbacks # Decorator handles optional logging/callbacks
</span>    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">program</span><span class="p">,</span> <span class="n">metric</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">devset</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="p">...):</span>
        <span class="c1"># Use provided args or fall back to instance attributes
</span>        <span class="n">metric</span> <span class="o">=</span> <span class="n">metric</span> <span class="k">if</span> <span class="n">metric</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span> <span class="k">else</span> <span class="bp">self</span><span class="p">.</span><span class="n">metric</span>
        <span class="n">devset</span> <span class="o">=</span> <span class="n">devset</span> <span class="k">if</span> <span class="n">devset</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span> <span class="k">else</span> <span class="bp">self</span><span class="p">.</span><span class="n">devset</span>
        <span class="n">num_threads</span> <span class="o">=</span> <span class="p">...</span> <span class="c1"># Similar logic for other args
</span>
        <span class="c1"># Create executor for parallelism
</span>        <span class="n">executor</span> <span class="o">=</span> <span class="n">ParallelExecutor</span><span class="p">(</span><span class="n">num_threads</span><span class="o">=</span><span class="n">num_threads</span><span class="p">,</span> <span class="p">...)</span>

        <span class="c1"># Define the work to be done for each example
</span>        <span class="k">def</span> <span class="nf">process_item</span><span class="p">(</span><span class="n">example</span><span class="p">):</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="c1"># Run the program with the example's inputs
</span>                <span class="n">prediction</span> <span class="o">=</span> <span class="n">program</span><span class="p">(</span><span class="o">**</span><span class="n">example</span><span class="p">.</span><span class="n">inputs</span><span class="p">())</span>
                <span class="c1"># Call the metric function with the gold example and prediction
</span>                <span class="n">score</span> <span class="o">=</span> <span class="n">metric</span><span class="p">(</span><span class="n">example</span><span class="p">,</span> <span class="n">prediction</span><span class="p">)</span>
                <span class="k">return</span> <span class="n">prediction</span><span class="p">,</span> <span class="n">score</span>
            <span class="k">except</span> <span class="nb">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                <span class="c1"># Handle errors during program/metric execution
</span>                <span class="c1"># Log error, return None or failure score
</span>                <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Error processing example: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
                <span class="k">return</span> <span class="bp">None</span> <span class="c1"># Executor will handle None later
</span>
        <span class="c1"># Execute process_item for all examples in devset using the executor
</span>        <span class="n">raw_results</span> <span class="o">=</span> <span class="n">executor</span><span class="p">.</span><span class="n">execute</span><span class="p">(</span><span class="n">process_item</span><span class="p">,</span> <span class="n">devset</span><span class="p">)</span>

        <span class="c1"># Process results, handle failures (replace None with failure score)
</span>        <span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">r</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">raw_results</span><span class="p">):</span>
            <span class="n">example</span> <span class="o">=</span> <span class="n">devset</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
            <span class="k">if</span> <span class="n">r</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span> <span class="c1"># Execution failed for this example
</span>                <span class="n">prediction</span><span class="p">,</span> <span class="n">score</span> <span class="o">=</span> <span class="n">dspy</span><span class="p">.</span><span class="n">Prediction</span><span class="p">(),</span> <span class="bp">self</span><span class="p">.</span><span class="n">failure_score</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">prediction</span><span class="p">,</span> <span class="n">score</span> <span class="o">=</span> <span class="n">r</span>
            <span class="n">results</span><span class="p">.</span><span class="n">append</span><span class="p">((</span><span class="n">example</span><span class="p">,</span> <span class="n">prediction</span><span class="p">,</span> <span class="n">score</span><span class="p">))</span>

        <span class="c1"># Calculate the average score
</span>        <span class="n">total_score</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">score</span> <span class="k">for</span> <span class="o">*</span><span class="n">_</span><span class="p">,</span> <span class="n">score</span> <span class="ow">in</span> <span class="n">results</span><span class="p">)</span>
        <span class="n">num_examples</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">devset</span><span class="p">)</span>
        <span class="n">average_score</span> <span class="o">=</span> <span class="nb">round</span><span class="p">(</span><span class="mi">100</span> <span class="o">*</span> <span class="n">total_score</span> <span class="o">/</span> <span class="n">num_examples</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="k">if</span> <span class="n">num_examples</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="mi">0</span>

        <span class="c1"># Display table if requested
</span>        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">display_table</span><span class="p">:</span>
             <span class="bp">self</span><span class="p">.</span><span class="n">_display_result_table</span><span class="p">(...)</span> <span class="c1"># Internal helper function
</span>
        <span class="c1"># Return results based on flags (return_all_scores, return_outputs)
</span>        <span class="c1"># ... logic to construct return tuple ...
</span>        <span class="k">return</span> <span class="n">average_score</span> <span class="c1"># Base return value
</span></code></pre></div></div>

<p>The core logic involves running the program and the metric function for each data point, handling potential errors, and averaging the results, with parallel processing to speed things up.</p>

<h2 id="conclusion">Conclusion</h2>

<p>You’ve now learned about <code class="language-plaintext highlighter-rouge">dspy.Evaluate</code>, the standard way to measure the performance of your DSPy programs!</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">Evaluate</code> acts as an <strong>automated grader</strong> for your DSPy modules.</li>
  <li>It requires three ingredients: your <strong>program</strong>, a <strong>dataset (<code class="language-plaintext highlighter-rouge">devset</code>)</strong> with gold labels, and a <strong>metric function</strong> to compare predictions against labels.</li>
  <li>It runs the program on the dataset, applies the metric, and reports the <strong>average score</strong>.</li>
  <li>It supports <strong>parallel execution</strong> for speed and offers options to display progress, show results tables, and return detailed outputs.</li>
</ul>

<p>Knowing how well your program performs is essential. But what if the score isn’t good enough? How can we <em>improve</em> the program, perhaps by automatically finding better prompts or few-shot examples?</p>

<p>That’s precisely what <strong>Teleprompters</strong> (Optimizers) are designed for! Let’s dive into how DSPy can help automatically optimize your programs next.</p>

<p><strong>Next:</strong> <a href="08_teleprompter___optimizer.md">Chapter 8: Teleprompter / Optimizer</a></p>

<hr />

<p>Generated by <a href="https://github.com/The-Pocket/Tutorial-Codebase-Knowledge">AI Codebase Knowledge Builder</a></p>
