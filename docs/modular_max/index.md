---
layout: default
title: My Tutorial for Modular's Max 
nav_order: 2
has_children: true
permalink: /modular_max/
---
# Tutorial: modular

The project, `modular`, is a **high-performance serving framework** for *Large Language Models (LLMs)*. It allows users to easily deploy and run LLMs, handling incoming requests, orchestrating the generation process, and streaming back responses. Key features include efficient *request batching*, advanced *KV cache management* for speed, and a *modular architecture* to manage different parts of the serving stack like API endpoints, model execution, and telemetry.


**Source Repository:** [None](None)

```mermaid
flowchart TD
    A0["Serving API Layer (FastAPI App & Routers)
"]
    A1["LLM Pipeline Orchestrator (`TokenGeneratorPipeline`)
"]
    A2["Model Worker
"]
    A3["Scheduler (`TokenGenerationScheduler`, `EmbeddingsScheduler`)
"]
    A4["EngineQueue
"]
    A5["Settings (`Settings` class)
"]
    A6["Telemetry and Metrics (`METRICS`, `MetricClient`)
"]
    A7["KV Cache Management
"]
    A0 -- "Forwards requests" --> A1
    A0 -- "Reads config" --> A5
    A1 -- "Sends tasks to" --> A4
    A1 -- "Reports metrics" --> A6
    A2 -- "Uses" --> A3
    A2 -- "Uses for inference" --> A7
    A3 -- "Uses for tasks" --> A4
    A3 -- "Manages cache via" --> A7
    A4 -- "Delivers tasks to" --> A2
    A5 -- "Configures" --> A0
    A5 -- "Configures telemetry" --> A6
    A7 -- "Agent started by" --> A0
```

---

Generated by [AI Codebase Knowledge Builder](https://github.com/The-Pocket/Tutorial-Codebase-Knowledge)