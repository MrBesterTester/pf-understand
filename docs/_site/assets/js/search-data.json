{"0": {
    "doc": "Chapter 1: AddressSpace",
    "title": "Chapter 1: Understanding Memory Neighborhoods with AddressSpace",
    "content": "Welcome to your first step into the world of Mojo! In this series, we’ll explore some fundamental concepts that help Mojo achieve its impressive performance, especially when dealing with different kinds of hardware like CPUs and GPUs. Our first topic is AddressSpace. It might sound a bit technical, but we’ll break it down with simple analogies. ",
    "url": "/mojo-v1/01_addressspace_.html#chapter-1-understanding-memory-neighborhoods-with-addressspace",
    
    "relUrl": "/mojo-v1/01_addressspace_.html#chapter-1-understanding-memory-neighborhoods-with-addressspace"
  },"1": {
    "doc": "Chapter 1: AddressSpace",
    "title": "What is AddressSpace?",
    "content": "Imagine a big city. This city has many different neighborhoods: . | Downtown (CPU RAM): This is your general-purpose area. Lots of things happen here, it’s accessible to everyone, and it’s where most everyday tasks are carried out. | Industrial Zone (GPU Global Memory): A large area on specialized hardware (like a Graphics Processing Unit, or GPU) designed for heavy-duty work. | Specialized Workshops (GPU Shared/Constant Memory): Smaller, super-fast areas within the Industrial Zone, designed for specific tasks or for workers (GPU threads) to collaborate very quickly. | . In Mojo, AddressSpace is a way to tell the system which “neighborhood” in memory a piece of data lives in. The official description you saw earlier puts it nicely: . AddressSpace is a parameter that specifies the type or region of memory where the data an NDBuffer (a type we’ll see later) points to is located. Think of it like different neighborhoods in a city, each with its own characteristics and accessibility rules. For instance, AddressSpace.GENERIC might refer to standard CPU RAM, while other address spaces like _GPUAddressSpace.SHARED or _GPUAddressSpace.CONSTANT would indicate memory on a GPU with specific properties (e.g., shared among GPU threads or read-only constant data). The choice of AddressSpace is critical for performance and data management in systems with diverse memory types, particularly when programming for GPUs. So, AddressSpace helps Mojo understand the properties and location of memory. ",
    "url": "/mojo-v1/01_addressspace_.html#what-is-addressspace",
    
    "relUrl": "/mojo-v1/01_addressspace_.html#what-is-addressspace"
  },"2": {
    "doc": "Chapter 1: AddressSpace",
    "title": "Why Do We Need Different Memory “Neighborhoods”?",
    "content": "Why not just have one giant memory area for everything? . | Speed and Performance: Different types of memory have different speeds. | CPU RAM (our “Downtown”) is generally fast for a wide variety of tasks. | GPU memory can be incredibly fast for certain parallel computations but might have different access patterns. Specialized parts of GPU memory (like “Workshops”) are even faster for specific uses. Telling Mojo where data is allows it to optimize how that data is accessed. | . | Special Capabilities: Some memory regions have special properties. | For example, “shared memory” on a GPU allows multiple GPU processing units (threads) to share data very quickly, which is great for collaborative tasks. | “Constant memory” on a GPU is for data that doesn’t change; the GPU can often cache this aggressively for faster reads. | . | Hardware Differences: Modern computers often have both a CPU and one or more GPUs. These components have their own memory systems. AddressSpace helps manage data across these different pieces of hardware. | . Knowing the AddressSpace allows Mojo (and you, the programmer!) to make smart decisions about how to store and access data for optimal performance and correctness. ",
    "url": "/mojo-v1/01_addressspace_.html#why-do-we-need-different-memory-neighborhoods",
    
    "relUrl": "/mojo-v1/01_addressspace_.html#why-do-we-need-different-memory-neighborhoods"
  },"3": {
    "doc": "Chapter 1: AddressSpace",
    "title": "AddressSpace in Mojo: A Closer Look",
    "content": "Let’s peek at how AddressSpace is defined in Mojo’s standard library. You’ll find it in a file named stdlib/src/memory/pointer.mojo. Don’t worry about understanding all the code details yet; we’ll focus on the main ideas. // From stdlib/src/memory/pointer.mojo @value @register_passable(\"trivial\") struct AddressSpace(...): \"\"\"Address space of the pointer.\"\"\" var _value: Int // Each address space has an underlying integer ID alias GENERIC = AddressSpace(0) // The most common one! \"\"\"Generic address space.\"\"\" // ... other details and functions ... @always_inline(\"builtin\") fn __init__(out self, value: Int): \"\"\"Initializes the address space from the underlying integral value.\"\"\" self._value = value @always_inline(\"builtin\") fn __init__(out self, value: _GPUAddressSpace): // Can also be made from a _GPUAddressSpace type \"\"\"Initializes the address space from the underlying integral value.\"\"\" self._value = value._value // ... later in the same file ... @value @register_passable(\"trivial\") struct _GPUAddressSpace(EqualityComparable): var _value: Int // These are like pre-defined AddressSpace values for common GPU memory types alias GENERIC = AddressSpace(0) // GPUs can also access generic memory alias GLOBAL = AddressSpace(1) \"\"\"Global address space (typically main GPU memory).\"\"\" alias SHARED = AddressSpace(3) \"\"\"Shared address space (fast memory for GPU thread groups).\"\"\" alias CONSTANT = AddressSpace(4) \"\"\"Constant address space (read-only, often cached).\"\"\" alias LOCAL = AddressSpace(5) \"\"\"Local address space (private to a single GPU thread).\"\"\" // ... other details ... Here’s what this means for a beginner: . | struct AddressSpace: This defines the AddressSpace type. | The decorators @value and @register_passable(\"trivial\") tell Mojo that AddressSpace is a simple value type that can be handled very efficiently. Think of it like a number. | var _value: Int: Internally, each AddressSpace is represented by an integer. For example, 0 means generic, 1 might mean global GPU memory, and so on. | alias GENERIC = AddressSpace(0): This is a very important line! AddressSpace.GENERIC is the most common address space. It usually refers to standard CPU memory. If you don’t specify an address space, this is often the default. | . | struct _GPUAddressSpace: This struct helps define common address spaces used with GPUs. | alias GLOBAL = AddressSpace(1): _GPUAddressSpace.GLOBAL is a convenient name for AddressSpace(1), which typically represents the main memory on a GPU. | alias SHARED = AddressSpace(3): _GPUAddressSpace.SHARED refers to AddressSpace(3), a special, fast memory region that groups of GPU threads can use to share data. | alias CONSTANT = AddressSpace(4): _GPUAddressSpace.CONSTANT refers to AddressSpace(4), used for data that won’t change during a computation. GPUs can optimize access to constant memory. | alias LOCAL = AddressSpace(5): _GPUAddressSpace.LOCAL refers to AddressSpace(5), representing memory private to individual GPU threads. | . | . So, you have AddressSpace.GENERIC for general CPU memory. For GPU-specific memory, you can use handy aliases like _GPUAddressSpace.GLOBAL or _GPUAddressSpace.SHARED. These aliases are actually AddressSpace values themselves. ",
    "url": "/mojo-v1/01_addressspace_.html#addressspace-in-mojo-a-closer-look",
    
    "relUrl": "/mojo-v1/01_addressspace_.html#addressspace-in-mojo-a-closer-look"
  },"4": {
    "doc": "Chapter 1: AddressSpace",
    "title": "How is AddressSpace Used?",
    "content": "You’ll encounter AddressSpace as a parameter when you define types that manage memory, like Pointer (which represents a direct memory address) and NDBuffer (which represents an N-dimensional array of data, like a tensor or matrix). Look at how Pointer and NDBuffer use it (simplified from their definitions): . // From stdlib/src/memory/pointer.mojo struct Pointer[ // ... other parameters ... type: AnyType, address_space: AddressSpace = AddressSpace.GENERIC, // Default is GENERIC! ]: // ... // From stdlib/src/buffer/buffer.mojo struct NDBuffer[ // ... other parameters ... type: DType, address_space: AddressSpace = AddressSpace.GENERIC, // Default is GENERIC! // ... ]: // ... Notice the address_space: AddressSpace = AddressSpace.GENERIC part. This means: . | Both Pointer and NDBuffer need to know the AddressSpace of the memory they are dealing with. | By default, if you don’t specify otherwise, they assume the memory is in AddressSpace.GENERIC (i.e., standard CPU RAM). | . Conceptual Example: . Imagine you’re creating a buffer for some calculations. // This is conceptual Mojo-like code to illustrate the idea // A buffer in standard CPU memory (GENERIC is the default) // let cpu_buffer = NDBuffer[Float32, ...]() // A buffer specifically in GPU's global memory // let gpu_global_buffer = NDBuffer[Float32, ..., address_space: _GPUAddressSpace.GLOBAL]() // A buffer in fast GPU shared memory // let gpu_shared_buffer = NDBuffer[Float32, ..., address_space: _GPUAddressSpace.SHARED]() . By specifying the address_space, you’re giving Mojo crucial information that it can use to manage and access the data efficiently. Real-world Impact in NDBuffer: . The NDBuffer code itself makes decisions based on AddressSpace. For example, there’s a helper function in stdlib/src/buffer/buffer.mojo: . @always_inline fn _use_32bit_indexing[address_space: AddressSpace]() -&gt; Bool: return is_gpu() and address_space in ( _GPUAddressSpace.SHARED, _GPUAddressSpace.LOCAL, _GPUAddressSpace.CONSTANT, ) . This function checks if the code is running on a GPU (is_gpu()) and if the NDBuffer’s data is in SHARED, LOCAL, or CONSTANT GPU memory. If so, it might decide to use 32-bit numbers for indexing into the buffer (which can sometimes be faster on GPUs for these memory types) instead of the usual 64-bit numbers. This is a direct example of AddressSpace influencing performance optimizations! . ",
    "url": "/mojo-v1/01_addressspace_.html#how-is-addressspace-used",
    
    "relUrl": "/mojo-v1/01_addressspace_.html#how-is-addressspace-used"
  },"5": {
    "doc": "Chapter 1: AddressSpace",
    "title": "Key Takeaways",
    "content": ". | AddressSpace tells Mojo where data lives in memory (like different neighborhoods in a city). | AddressSpace.GENERIC is the common default, usually meaning standard CPU RAM. | For GPUs, there are specific address spaces like _GPUAddressSpace.GLOBAL (main GPU memory), _GPUAddressSpace.SHARED (fast shared memory for GPU threads), and _GPUAddressSpace.CONSTANT (read-only cached memory). | Knowing the AddressSpace is crucial for performance and correct data handling, especially when working with diverse hardware like GPUs. | You’ll see AddressSpace as a parameter in memory-related types like Pointer and NDBuffer. | . Understanding AddressSpace is a foundational piece in learning how Mojo manages memory and achieves high performance. It’s all about giving the system enough information to make smart choices! . ",
    "url": "/mojo-v1/01_addressspace_.html#key-takeaways",
    
    "relUrl": "/mojo-v1/01_addressspace_.html#key-takeaways"
  },"6": {
    "doc": "Chapter 1: AddressSpace",
    "title": "Next Steps",
    "content": "Now that you have a basic understanding of AddressSpace, we’re ready to explore how Mojo represents direct memory locations. In the next chapter, we’ll dive into UnsafePointer, a fundamental building block for memory operations in Mojo. Generated by AI Codebase Knowledge Builder . ",
    "url": "/mojo-v1/01_addressspace_.html#next-steps",
    
    "relUrl": "/mojo-v1/01_addressspace_.html#next-steps"
  },"7": {
    "doc": "Chapter 1: AddressSpace",
    "title": "Chapter 1: AddressSpace",
    "content": " ",
    "url": "/mojo-v1/01_addressspace_.html",
    
    "relUrl": "/mojo-v1/01_addressspace_.html"
  },"8": {
    "doc": "AsyncCrawlerStrategy",
    "title": "Chapter 1: How We Fetch Webpages - AsyncCrawlerStrategy",
    "content": "Welcome to the Crawl4AI tutorial series! Our goal is to build intelligent agents that can understand and extract information from the web. The very first step in this process is actually getting the content from a webpage. This chapter explains how Crawl4AI handles that fundamental task. Imagine you need to pick up a package from a specific address. How do you get there and retrieve it? . | You could send a simple, fast drone that just grabs the package off the porch (if it’s easily accessible). This is quick but might fail if the package is inside or requires a signature. | Or, you could send a full delivery truck with a driver. The driver can ring the bell, wait, sign for the package, and even handle complex instructions. This is more versatile but takes more time and resources. | . In Crawl4AI, the AsyncCrawlerStrategy is like choosing your delivery vehicle. It defines how the crawler fetches the raw content (like the HTML, CSS, and maybe JavaScript results) of a webpage. ",
    "url": "/Crawl4AI/01_asynccrawlerstrategy.html#chapter-1-how-we-fetch-webpages---asynccrawlerstrategy",
    
    "relUrl": "/Crawl4AI/01_asynccrawlerstrategy.html#chapter-1-how-we-fetch-webpages---asynccrawlerstrategy"
  },"9": {
    "doc": "AsyncCrawlerStrategy",
    "title": "What Exactly is AsyncCrawlerStrategy?",
    "content": "AsyncCrawlerStrategy is a core concept in Crawl4AI that represents the method or technique used to download the content of a given URL. Think of it as a blueprint: it specifies that we need a way to fetch content, but the specific details of how it’s done can vary. This “blueprint” approach is powerful because it allows us to swap out the fetching mechanism depending on our needs, without changing the rest of our crawling logic. ",
    "url": "/Crawl4AI/01_asynccrawlerstrategy.html#what-exactly-is-asynccrawlerstrategy",
    
    "relUrl": "/Crawl4AI/01_asynccrawlerstrategy.html#what-exactly-is-asynccrawlerstrategy"
  },"10": {
    "doc": "AsyncCrawlerStrategy",
    "title": "The Default: AsyncPlaywrightCrawlerStrategy (The Delivery Truck)",
    "content": "By default, Crawl4AI uses AsyncPlaywrightCrawlerStrategy. This strategy uses a real, automated web browser engine (like Chrome, Firefox, or WebKit) behind the scenes. Why use a full browser? . | Handles JavaScript: Modern websites rely heavily on JavaScript to load content, change the layout, or fetch data after the initial page load. AsyncPlaywrightCrawlerStrategy runs this JavaScript, just like your normal browser does. | Simulates User Interaction: It can wait for elements to appear, handle dynamic content, and see the page after scripts have run. | Gets the “Final” View: It fetches the content as a user would see it in their browser. | . This is our “delivery truck” – powerful and capable of handling complex websites. However, like a real truck, it’s slower and uses more memory and CPU compared to simpler methods. You generally don’t need to do anything to use it, as it’s the default! When you start Crawl4AI, it picks this strategy automatically. ",
    "url": "/Crawl4AI/01_asynccrawlerstrategy.html#the-default-asyncplaywrightcrawlerstrategy-the-delivery-truck",
    
    "relUrl": "/Crawl4AI/01_asynccrawlerstrategy.html#the-default-asyncplaywrightcrawlerstrategy-the-delivery-truck"
  },"11": {
    "doc": "AsyncCrawlerStrategy",
    "title": "Another Option: AsyncHTTPCrawlerStrategy (The Delivery Drone)",
    "content": "Crawl4AI also offers AsyncHTTPCrawlerStrategy. This strategy is much simpler. It directly requests the URL and downloads the initial HTML source code that the web server sends back. Why use this simpler strategy? . | Speed: It’s significantly faster because it doesn’t need to start a browser, render the page, or execute JavaScript. | Efficiency: It uses much less memory and CPU. | . This is our “delivery drone” – super fast and efficient for simple tasks. What’s the catch? . | No JavaScript: It won’t run any JavaScript on the page. If content is loaded dynamically by scripts, this strategy will likely miss it. | Basic HTML Only: You get the raw HTML source, not necessarily what a user sees after the browser processes everything. | . This strategy is great for websites with simple, static HTML content or when you only need the basic structure and metadata very quickly. ",
    "url": "/Crawl4AI/01_asynccrawlerstrategy.html#another-option-asynchttpcrawlerstrategy-the-delivery-drone",
    
    "relUrl": "/Crawl4AI/01_asynccrawlerstrategy.html#another-option-asynchttpcrawlerstrategy-the-delivery-drone"
  },"12": {
    "doc": "AsyncCrawlerStrategy",
    "title": "Why Have Different Strategies? (The Power of Abstraction)",
    "content": "Having AsyncCrawlerStrategy as a distinct concept offers several advantages: . | Flexibility: You can choose the best tool for the job. Need to crawl complex, dynamic sites? Use the default AsyncPlaywrightCrawlerStrategy. Need to quickly fetch basic HTML from thousands of simple pages? Switch to AsyncHTTPCrawlerStrategy. | Maintainability: The logic for fetching content is kept separate from the logic for processing it. | Extensibility: Advanced users could even create their own custom strategies for specialized fetching needs (though that’s beyond this beginner tutorial). | . ",
    "url": "/Crawl4AI/01_asynccrawlerstrategy.html#why-have-different-strategies-the-power-of-abstraction",
    
    "relUrl": "/Crawl4AI/01_asynccrawlerstrategy.html#why-have-different-strategies-the-power-of-abstraction"
  },"13": {
    "doc": "AsyncCrawlerStrategy",
    "title": "How It Works Conceptually",
    "content": "When you ask Crawl4AI to crawl a URL, the main AsyncWebCrawler doesn’t fetch the content itself. Instead, it delegates the task to the currently selected AsyncCrawlerStrategy. Here’s a simplified flow: . sequenceDiagram participant C as AsyncWebCrawler participant S as AsyncCrawlerStrategy participant W as Website C-&gt;&gt;S: Please crawl(\"https://example.com\") Note over S: I'm using my method (e.g., Browser or HTTP) S-&gt;&gt;W: Request Page Content W--&gt;&gt;S: Return Raw Content (HTML, etc.) S--&gt;&gt;C: Here's the result (AsyncCrawlResponse) . The AsyncWebCrawler only needs to know how to talk to any strategy through a common interface (the crawl method). The strategy handles the specific details of the fetching process. ",
    "url": "/Crawl4AI/01_asynccrawlerstrategy.html#how-it-works-conceptually",
    
    "relUrl": "/Crawl4AI/01_asynccrawlerstrategy.html#how-it-works-conceptually"
  },"14": {
    "doc": "AsyncCrawlerStrategy",
    "title": "Using the Default Strategy (You’re Already Doing It!)",
    "content": "Let’s see how you use the default AsyncPlaywrightCrawlerStrategy without even needing to specify it. # main_example.py import asyncio from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode async def main(): # When you create AsyncWebCrawler without specifying a strategy, # it automatically uses AsyncPlaywrightCrawlerStrategy! async with AsyncWebCrawler() as crawler: print(\"Crawler is ready using the default strategy (Playwright).\") # Let's crawl a simple page that just returns HTML # We use CacheMode.BYPASS to ensure we fetch it fresh each time for this demo. config = CrawlerRunConfig(cache_mode=CacheMode.BYPASS) result = await crawler.arun( url=\"https://httpbin.org/html\", config=config ) if result.success: print(\"\\nSuccessfully fetched content!\") # The strategy fetched the raw HTML. # AsyncWebCrawler then processes it (more on that later). print(f\"First 100 chars of fetched HTML: {result.html[:100]}...\") else: print(f\"\\nFailed to fetch content: {result.error_message}\") if __name__ == \"__main__\": asyncio.run(main()) . Explanation: . | We import AsyncWebCrawler and supporting classes. | We create an instance of AsyncWebCrawler() inside an async with block (this handles setup and cleanup). Since we didn’t tell it which strategy to use, it defaults to AsyncPlaywrightCrawlerStrategy. | We call crawler.arun() to crawl the URL. Under the hood, the AsyncPlaywrightCrawlerStrategy starts a browser, navigates to the page, gets the content, and returns it. | We print the first part of the fetched HTML from the result. | . ",
    "url": "/Crawl4AI/01_asynccrawlerstrategy.html#using-the-default-strategy-youre-already-doing-it",
    
    "relUrl": "/Crawl4AI/01_asynccrawlerstrategy.html#using-the-default-strategy-youre-already-doing-it"
  },"15": {
    "doc": "AsyncCrawlerStrategy",
    "title": "Explicitly Choosing the HTTP Strategy",
    "content": "What if you know the page is simple and want the speed of the “delivery drone”? You can explicitly tell AsyncWebCrawler to use AsyncHTTPCrawlerStrategy. # http_strategy_example.py import asyncio from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode # Import the specific strategies we want to use from crawl4ai.async_crawler_strategy import AsyncHTTPCrawlerStrategy async def main(): # 1. Create an instance of the strategy you want http_strategy = AsyncHTTPCrawlerStrategy() # 2. Pass the strategy instance when creating the AsyncWebCrawler async with AsyncWebCrawler(crawler_strategy=http_strategy) as crawler: print(\"Crawler is ready using the explicit HTTP strategy.\") # Crawl the same simple page config = CrawlerRunConfig(cache_mode=CacheMode.BYPASS) result = await crawler.arun( url=\"https://httpbin.org/html\", config=config ) if result.success: print(\"\\nSuccessfully fetched content using HTTP strategy!\") print(f\"First 100 chars of fetched HTML: {result.html[:100]}...\") else: print(f\"\\nFailed to fetch content: {result.error_message}\") if __name__ == \"__main__\": asyncio.run(main()) . Explanation: . | We now also import AsyncHTTPCrawlerStrategy. | We create an instance: http_strategy = AsyncHTTPCrawlerStrategy(). | We pass this instance to the AsyncWebCrawler constructor: AsyncWebCrawler(crawler_strategy=http_strategy). | The rest of the code is the same, but now crawler.arun() will use the faster, simpler HTTP GET request method defined by AsyncHTTPCrawlerStrategy. | . For a simple page like httpbin.org/html, both strategies will likely return the same HTML content, but the HTTP strategy would generally be faster and use fewer resources. On a complex JavaScript-heavy site, the HTTP strategy might fail to get the full content, while the Playwright strategy would handle it correctly. ",
    "url": "/Crawl4AI/01_asynccrawlerstrategy.html#explicitly-choosing-the-http-strategy",
    
    "relUrl": "/Crawl4AI/01_asynccrawlerstrategy.html#explicitly-choosing-the-http-strategy"
  },"16": {
    "doc": "AsyncCrawlerStrategy",
    "title": "A Glimpse Under the Hood",
    "content": "You don’t need to know the deep internals to use the strategies, but it helps to understand the structure. Inside the crawl4ai library, you’d find a file like async_crawler_strategy.py. It defines the “blueprint” (an Abstract Base Class): . # Simplified from async_crawler_strategy.py from abc import ABC, abstractmethod from .models import AsyncCrawlResponse # Defines the structure of the result class AsyncCrawlerStrategy(ABC): \"\"\" Abstract base class for crawler strategies. \"\"\" @abstractmethod async def crawl(self, url: str, **kwargs) -&gt; AsyncCrawlResponse: \"\"\"Fetch content from the URL.\"\"\" pass # Each specific strategy must implement this . And then the specific implementations: . # Simplified from async_crawler_strategy.py from playwright.async_api import Page # Playwright library for browser automation # ... other imports class AsyncPlaywrightCrawlerStrategy(AsyncCrawlerStrategy): # ... (Initialization code to manage browsers) async def crawl(self, url: str, config: CrawlerRunConfig, **kwargs) -&gt; AsyncCrawlResponse: # Uses Playwright to: # 1. Get a browser page # 2. Navigate to the url (page.goto(url)) # 3. Wait for content, run JS, etc. # 4. Get the final HTML (page.content()) # 5. Optionally take screenshots, etc. # 6. Return an AsyncCrawlResponse # ... implementation details ... pass . # Simplified from async_crawler_strategy.py import aiohttp # Library for making HTTP requests asynchronously # ... other imports class AsyncHTTPCrawlerStrategy(AsyncCrawlerStrategy): # ... (Initialization code to manage HTTP sessions) async def crawl(self, url: str, config: CrawlerRunConfig, **kwargs) -&gt; AsyncCrawlResponse: # Uses aiohttp to: # 1. Make an HTTP GET (or other method) request to the url # 2. Read the response body (HTML) # 3. Get response headers and status code # 4. Return an AsyncCrawlResponse # ... implementation details ... pass . The key takeaway is that both strategies implement the same crawl method, allowing AsyncWebCrawler to use them interchangeably. ",
    "url": "/Crawl4AI/01_asynccrawlerstrategy.html#a-glimpse-under-the-hood",
    
    "relUrl": "/Crawl4AI/01_asynccrawlerstrategy.html#a-glimpse-under-the-hood"
  },"17": {
    "doc": "AsyncCrawlerStrategy",
    "title": "Conclusion",
    "content": "You’ve learned about AsyncCrawlerStrategy, the core concept defining how Crawl4AI fetches webpage content. | It’s like choosing a vehicle: a powerful browser (AsyncPlaywrightCrawlerStrategy, the default) or a fast, simple HTTP request (AsyncHTTPCrawlerStrategy). | This abstraction gives you flexibility to choose the right fetching method for your task. | You usually don’t need to worry about it, as the default handles most modern websites well. | . Now that we understand how the raw content is fetched, the next step is to look at the main class that orchestrates the entire crawling process. Next: Let’s dive into the AsyncWebCrawler itself! . Generated by AI Codebase Knowledge Builder . ",
    "url": "/Crawl4AI/01_asynccrawlerstrategy.html#conclusion",
    
    "relUrl": "/Crawl4AI/01_asynccrawlerstrategy.html#conclusion"
  },"18": {
    "doc": "AsyncCrawlerStrategy",
    "title": "AsyncCrawlerStrategy",
    "content": " ",
    "url": "/Crawl4AI/01_asynccrawlerstrategy.html",
    
    "relUrl": "/Crawl4AI/01_asynccrawlerstrategy.html"
  },"19": {
    "doc": "Chapter 1: Configuration System",
    "title": "Chapter 1: Configuration System",
    "content": "Welcome to the first chapter of our Crawl4AI tutorial! Today, we’re going to learn about the Configuration System, which is the foundation for controlling how our web crawler works. ",
    "url": "/Crawl4AI/01_configuration_system_.html",
    
    "relUrl": "/Crawl4AI/01_configuration_system_.html"
  },"20": {
    "doc": "Chapter 1: Configuration System",
    "title": "What is the Configuration System?",
    "content": "Imagine you’re setting up a smart camera to take photos of a building. First, you need to adjust the camera settings (like focus, zoom, and flash), and then you decide how to process the photos (like applying filters or cropping). The Configuration System in Crawl4AI works in a similar way: . | BrowserConfig: Like adjusting your camera settings, this configures the web browser that will visit websites | CrawlerRunConfig: Like choosing how to process your photos, this determines how the web content is processed after it’s fetched | . Let’s explore how to use these configurations to control our web crawler! . ",
    "url": "/Crawl4AI/01_configuration_system_.html#what-is-the-configuration-system",
    
    "relUrl": "/Crawl4AI/01_configuration_system_.html#what-is-the-configuration-system"
  },"21": {
    "doc": "Chapter 1: Configuration System",
    "title": "BrowserConfig: Setting Up Your Browser",
    "content": "The BrowserConfig class allows you to customize the browser that Crawl4AI will use to visit websites. Here’s a simple example: . from crawl4ai import BrowserConfig, AsyncWebCrawler # Create a browser configuration browser_config = BrowserConfig( browser_type=\"chromium\", headless=True, user_agent=\"Mozilla/5.0 ...\" ) # Create a crawler with our browser configuration crawler = AsyncWebCrawler(config=browser_config) . In this example, we’re configuring: . | browser_type: We’re using “chromium” (you could also use “firefox”) | headless: When set to True, the browser runs in the background without a visible window | user_agent: This is how the browser identifies itself to websites | . Think of BrowserConfig as setting up your tools before you start working. ",
    "url": "/Crawl4AI/01_configuration_system_.html#browserconfig-setting-up-your-browser",
    
    "relUrl": "/Crawl4AI/01_configuration_system_.html#browserconfig-setting-up-your-browser"
  },"22": {
    "doc": "Chapter 1: Configuration System",
    "title": "CrawlerRunConfig: Processing the Content",
    "content": "Once the browser has loaded a webpage, CrawlerRunConfig controls what happens next. This includes how content is extracted, transformed, and cached: . from crawl4ai import CrawlerRunConfig from crawl4ai.cache_context import CacheMode # Create a crawler configuration crawler_config = CrawlerRunConfig( cache_mode=CacheMode.ENABLED, screenshot=True, verbose=True ) . In this example: . | cache_mode: Determines if and how to use the cache (store previously visited pages) | screenshot: Whether to capture a screenshot of the webpage | verbose: Whether to provide detailed logging information | . Think of CrawlerRunConfig as giving instructions to a photographer about what kind of photos you want. ",
    "url": "/Crawl4AI/01_configuration_system_.html#crawlerrunconfig-processing-the-content",
    
    "relUrl": "/Crawl4AI/01_configuration_system_.html#crawlerrunconfig-processing-the-content"
  },"23": {
    "doc": "Chapter 1: Configuration System",
    "title": "Putting It All Together: A Complete Example",
    "content": "Let’s see how to use both configurations together: . import asyncio from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig from crawl4ai.cache_context import CacheMode async def crawl_website(): # Set up the browser browser_config = BrowserConfig(browser_type=\"chromium\", headless=True) # Set up the crawler behavior crawler_config = CrawlerRunConfig(cache_mode=CacheMode.ENABLED, screenshot=True) # Create and start the crawler async with AsyncWebCrawler(config=browser_config) as crawler: # Crawl a website with our configuration result = await crawler.arun(url=\"https://example.com\", config=crawler_config) # Print the markdown content we extracted print(result.markdown) # Run the async function asyncio.run(crawl_website()) . This code: . | Sets up a headless Chromium browser | Configures the crawler to use caching and take screenshots | Creates a crawler with our browser configuration | Runs the crawler on example.com using our crawler configuration | Prints the extracted markdown content | . ",
    "url": "/Crawl4AI/01_configuration_system_.html#putting-it-all-together-a-complete-example",
    
    "relUrl": "/Crawl4AI/01_configuration_system_.html#putting-it-all-together-a-complete-example"
  },"24": {
    "doc": "Chapter 1: Configuration System",
    "title": "Understanding Cache Modes",
    "content": "One important aspect of configuration is controlling how caching works. The CacheMode enum offers several options: . from crawl4ai.cache_context import CacheMode # Use the cache (read and write) config = CrawlerRunConfig(cache_mode=CacheMode.ENABLED) # Don't use the cache at all config = CrawlerRunConfig(cache_mode=CacheMode.DISABLED) # Only read from cache, don't write config = CrawlerRunConfig(cache_mode=CacheMode.READ_ONLY) . Caching can greatly improve performance by avoiding unnecessary repeated downloads of the same content. ",
    "url": "/Crawl4AI/01_configuration_system_.html#understanding-cache-modes",
    
    "relUrl": "/Crawl4AI/01_configuration_system_.html#understanding-cache-modes"
  },"25": {
    "doc": "Chapter 1: Configuration System",
    "title": "What Happens Under the Hood?",
    "content": "To understand how these configurations work together, let’s look at the flow of a typical crawl operation: . sequenceDiagram participant User participant WebCrawler participant Browser participant Cache participant Processor User-&gt;&gt;WebCrawler: Create with BrowserConfig WebCrawler-&gt;&gt;Browser: Initialize with settings User-&gt;&gt;WebCrawler: arun(url, CrawlerRunConfig) WebCrawler-&gt;&gt;Cache: Check if URL is cached alt URL is cached and cache_mode allows reading Cache-&gt;&gt;WebCrawler: Return cached content else URL not cached or bypass cache WebCrawler-&gt;&gt;Browser: Request URL Browser-&gt;&gt;WebCrawler: Return HTML content WebCrawler-&gt;&gt;Cache: Store in cache if enabled end WebCrawler-&gt;&gt;Processor: Process content (extract, transform) Processor-&gt;&gt;WebCrawler: Return processed result WebCrawler-&gt;&gt;User: Return final result . Let’s explore the implementation details: . When you create an AsyncWebCrawler with a BrowserConfig, the crawler initializes a browser instance with your specified settings: . # From crawl4ai/async_webcrawler.py def __init__( self, crawler_strategy: AsyncCrawlerStrategy = None, config: BrowserConfig = None, base_directory: str = str( os.getenv(\"CRAWL4_AI_BASE_DIRECTORY\", Path.home())), thread_safe: bool = False, logger: AsyncLoggerBase = None, **kwargs, ): # Handle browser configuration browser_config = config or BrowserConfig() self.browser_config = browser_config # Initialize crawler strategy self.crawler_strategy = crawler_strategy or AsyncPlaywrightCrawlerStrategy( browser_config=browser_config, logger=self.logger, **params, ) . Then, when you call arun() with a URL and CrawlerRunConfig, the crawler first checks if it should use a cached version: . # Create cache context cache_context = CacheContext(url, config.cache_mode, False) # Try to get cached result if appropriate if cache_context.should_read(): cached_result = await async_db_manager.aget_cached_url(url) . If the content isn’t cached or shouldn’t be read from cache, the crawler retrieves it using the browser: . async_response = await self.crawler_strategy.crawl( url, config=config, # Pass the entire config object ) . Finally, the HTML content is processed according to your configuration: . crawl_result: CrawlResult = await self.aprocess_html( url=url, html=html, extracted_content=extracted_content, config=config, screenshot_data=screenshot_data, pdf_data=pdf_data, verbose=config.verbose, is_raw_html=True if url.startswith(\"raw:\") else False, redirected_url=async_response.redirected_url, **kwargs, ) . ",
    "url": "/Crawl4AI/01_configuration_system_.html#what-happens-under-the-hood",
    
    "relUrl": "/Crawl4AI/01_configuration_system_.html#what-happens-under-the-hood"
  },"26": {
    "doc": "Chapter 1: Configuration System",
    "title": "Common Configuration Options",
    "content": "Let’s look at some common configuration options you might want to use: . BrowserConfig Options . browser_config = BrowserConfig( browser_type=\"chromium\", # \"chromium\" or \"firefox\" headless=True, # Run without a visible window viewport_size=(1280, 720), # Width and height of browser window user_agent=\"Custom User Agent\", # Browser identification timeout=30000, # Timeout in milliseconds verbose=False # Detailed logging ) . CrawlerRunConfig Options . from crawl4ai.extraction_strategy import SimpleExtractionStrategy from crawl4ai.chunking_strategy import RegexChunking crawler_config = CrawlerRunConfig( cache_mode=CacheMode.ENABLED, # Cache behavior screenshot=True, # Take screenshots pdf=False, # Generate PDF extraction_strategy=SimpleExtractionStrategy(), # How to extract content chunking_strategy=RegexChunking(), # How to chunk content ) . ",
    "url": "/Crawl4AI/01_configuration_system_.html#common-configuration-options",
    
    "relUrl": "/Crawl4AI/01_configuration_system_.html#common-configuration-options"
  },"27": {
    "doc": "Chapter 1: Configuration System",
    "title": "Conclusion",
    "content": "In this chapter, we learned about the Configuration System in Crawl4AI, which consists of two main components: . | BrowserConfig: Controls how the browser behaves when visiting websites | CrawlerRunConfig: Controls how the content is processed after it’s fetched | . By using these configurations together, you can customize how Crawl4AI interacts with websites and processes their content. This flexibility allows you to adapt the crawler to different use cases and requirements. In the next chapter, AsyncWebCrawler, we’ll dive deeper into how to use the AsyncWebCrawler class to fetch and process web content using the configurations we’ve learned about here. Generated by AI Codebase Knowledge Builder . ",
    "url": "/Crawl4AI/01_configuration_system_.html#conclusion",
    
    "relUrl": "/Crawl4AI/01_configuration_system_.html#conclusion"
  },"28": {
    "doc": "Chapter 1: Settings Class",
    "title": "Chapter 1: Settings (Settings class)",
    "content": "Welcome to the modular tutorial! We’re excited to help you understand how this powerful serving application works, piece by piece. In this first chapter, we’ll dive into one of the most fundamental concepts: the Settings class. Imagine you’re about to launch a new web application. You might need to decide: . | “What web address (host) and port number should it use?” | “How much detail should its logs show?” | “Should it offer an OpenAI-compatible API, or perhaps a SageMaker one?” | . Changing these kinds of parameters directly in the main code can be messy and error-prone, especially as the application grows. This is where the Settings class comes in! . ",
    "url": "/modular_max/01_settings___settings__class__.html#chapter-1-settings-settings-class",
    
    "relUrl": "/modular_max/01_settings___settings__class__.html#chapter-1-settings-settings-class"
  },"29": {
    "doc": "Chapter 1: Settings Class",
    "title": "What Problem Do Settings Solve?",
    "content": "The Settings class acts as the central control panel for the entire modular serving application. Think of it like the main dashboard of a complex machine, like a spaceship. This dashboard has various dials and switches that allow an operator (that’s you!) to adjust how the machine runs without needing to open up the engine and rewire things. For example, you might want to: . | Run the server on port 8080 instead of the default 8000. | Tell the application to expose only an OpenAI-compatible API. | Increase the logging detail to help debug an issue. | . The Settings class allows you to make these kinds of adjustments easily and safely. ",
    "url": "/modular_max/01_settings___settings__class__.html#what-problem-do-settings-solve",
    
    "relUrl": "/modular_max/01_settings___settings__class__.html#what-problem-do-settings-solve"
  },"30": {
    "doc": "Chapter 1: Settings Class",
    "title": "Meet the Settings Class",
    "content": "At its core, the Settings class (found in src/max/serve/config.py) uses a popular Python library called Pydantic. Pydantic helps define what settings are available, what types they should be (e.g., a number, text, a list), and ensures that any values you provide are valid. These settings can be loaded from a few places: . | Environment Variables: These are variables set in your system’s environment (e.g., MAX_SERVE_PORT=8080). | A .env file: This is a special file (literally named .env) you can create in your project directory to list your settings. | Directly in code: When the application starts, it can provide default values or override others. | . Let’s look at some key aspects. Defining a Setting . Here’s a simplified peek at how settings like the server host and port are defined within the Settings class: . # From: src/max/serve/config.py from pydantic import Field from pydantic_settings import BaseSettings, SettingsConfigDict class Settings(BaseSettings): # This tells Pydantic where to look for a .env file # and to allow other environment variables not explicitly defined here. model_config = SettingsConfigDict( env_file=\".env\", extra=\"allow\", ) # Server host configuration host: str = Field( description=\"Hostname to use\", default=\"0.0.0.0\", # Default if not specified elsewhere alias=\"MAX_SERVE_HOST\" # Environment variable to look for ) # Server port configuration port: int = Field( description=\"Port to use\", default=8000, # Default if not specified elsewhere alias=\"MAX_SERVE_PORT\" # Environment variable to look for ) # ... many other settings for logging, model workers, etc... In this snippet: . | host: str means the host setting is expected to be a string (text). | default=\"0.0.0.0\" means if you don’t specify a host, it will use “0.0.0.0”. | alias=\"MAX_SERVE_HOST\" means Pydantic will look for an environment variable named MAX_SERVE_HOST to get the value for host. The Settings class internally uses host, but externally (like in environment variables), you’d use MAX_SERVE_HOST. | . Automatic Validation . Pydantic is smart! If you try to set port to a text value like \"eight-thousand\", Pydantic will raise an error because it expects an integer (int). The Settings class can also have custom validation. For example, it checks if the chosen port is already in use: . # From: src/max/serve/config.py # (Inside the Settings class) @field_validator(\"port\") # This function validates the 'port' field def validate_port(cls, port: int): # (Simplified: code to check if port is available) # If port is in use, it raises a ValueError. # Otherwise, it returns the port. # ... return port . This helps catch configuration mistakes early on. Different Types of Settings . Settings aren’t just numbers and text. They can be more complex, like a list of API types to enable: . # From: src/max/serve/config.py from enum import Enum class APIType(Enum): # Defines allowed API types KSERVE = \"kserve\" OPENAI = \"openai\" SAGEMAKER = \"sagemaker\" class Settings(BaseSettings): # ... other settings ... api_types: list[APIType] = Field( description=\"List of exposed API types.\", default=[APIType.OPENAI, APIType.SAGEMAKER] # Default is a list ) # ... Here, api_types is expected to be a list, and each item in the list must be one of the predefined APIType values. ",
    "url": "/modular_max/01_settings___settings__class__.html#meet-the-settings-class",
    
    "relUrl": "/modular_max/01_settings___settings__class__.html#meet-the-settings-class"
  },"31": {
    "doc": "Chapter 1: Settings Class",
    "title": "How to Use and Change Settings",
    "content": "Let’s say you want to change the server port from the default 8000 to 8080. You have a few ways to do this: . 1. Using Environment Variables: Before running your application, you can set an environment variable in your terminal: . export MAX_SERVE_PORT=8080 # Now run your modular application . When the Settings class is initialized, it will pick up MAX_SERVE_PORT and use 8080 for the port. 2. Using a .env file: Create a file named .env in the root directory of your modular project and add the line: . MAX_SERVE_PORT=8080 . or you can also use the attribute name directly: . port=8080 . The Settings class will automatically read this file when the application starts. 3. Initializing Settings in Code (Less Common for End-Users): Sometimes, settings are specified when the Settings object is created directly in the code. This is often used for internal defaults or specific configurations within the application’s entrypoints. # Simplified from: src/max/entrypoints/cli/serve.py from max.serve.config import Settings # When starting the server, 'modular' might initialize settings like this: # This particular setting 'MAX_SERVE_USE_HEARTBEAT' is an example; # it would override any environment or .env file setting for this specific run. settings_object = Settings(MAX_SERVE_USE_HEARTBEAT=False) print(f\"Host: {settings_object.host}\") # Will show host from env/.env/default print(f\"Port: {settings_object.port}\") # Will show port from env/.env/default print(f\"Use Heartbeat: {settings_object.use_heartbeat}\") # Will show False . Accessing Settings in the Application: Once the Settings object is created and populated, other parts of the modular application can access these values. For example, when the API server is being set up: . # Simplified from: src/max/serve/api_server.py from max.serve.config import Settings # Assume Settings class is defined def configure_my_server(current_settings: Settings): # The application uses the loaded settings host_to_use = current_settings.host port_to_use = current_settings.port print(f\"Server will run on: http://{host_to_use}:{port_to_use}\") # ... actual server setup using these values ... # When the application starts: app_settings = Settings() # Loads from .env, environment variables, or defaults configure_my_server(app_settings) . If you had set MAX_SERVE_PORT=8080, the output would be: . Server will run on: http://0.0.0.0:8080 . (Assuming host remained its default “0.0.0.0”). ",
    "url": "/modular_max/01_settings___settings__class__.html#how-to-use-and-change-settings",
    
    "relUrl": "/modular_max/01_settings___settings__class__.html#how-to-use-and-change-settings"
  },"32": {
    "doc": "Chapter 1: Settings Class",
    "title": "Under the Hood: How Settings Are Loaded",
    "content": "When you create an instance of the Settings class (e.g., my_settings = Settings()), Pydantic performs a sequence of steps to determine the final value for each setting: . | Direct Initialization: If values are passed directly when creating the Settings object (e.g., Settings(MAX_SERVE_PORT=9000)), these take the highest precedence for those specific settings. | Environment Variables: Pydantic then checks for environment variables that match the alias defined for each field (e.g., MAX_SERVE_HOST, MAX_SERVE_PORT). If found, these override values from the .env file and defaults. | .env File: If not found in environment variables, Pydantic looks for the setting in the .env file (specified by model_config = SettingsConfigDict(env_file=\".env\", ...)). It can match either the alias (e.g., MAX_SERVE_PORT=8001) or the field name directly (e.g., port=8001). | Default Values: If the setting isn’t found in any of the above, the default value specified in the class definition is used (e.g., default=8000 for port). | Validation: After a value is determined, any associated validators (like validate_port) are run. | . Here’s a simplified diagram illustrating this loading process: . sequenceDiagram participant App as Application Code participant SettingsObj as Settings() Instance participant Pydantic as Pydantic Logic participant InitArgs as Direct Arguments participant EnvVars as Environment Variables participant DotEnv as .env File participant Defaults as Default Values (in class) App-&gt;&gt;SettingsObj: Create (e.g., Settings(MAX_SERVE_PORT=9000)) SettingsObj-&gt;&gt;Pydantic: Process settings fields Pydantic-&gt;&gt;InitArgs: Check for 'MAX_SERVE_PORT' alt Value provided in InitArgs (e.g., 9000) InitArgs--&gt;&gt;Pydantic: Use 9000 else No value in InitArgs Pydantic-&gt;&gt;EnvVars: Check for 'MAX_SERVE_PORT' alt EnvVar 'MAX_SERVE_PORT' exists (e.g., 8080) EnvVars--&gt;&gt;Pydantic: Use 8080 else EnvVar not found Pydantic-&gt;&gt;DotEnv: Check '.env' for 'MAX_SERVE_PORT' or 'port' alt Value in '.env' (e.g., 8001) DotEnv--&gt;&gt;Pydantic: Use 8001 else Value not in '.env' Pydantic-&gt;&gt;Defaults: Use default for 'port' (e.g., 8000) Defaults--&gt;&gt;Pydantic: Use 8000 end end end Pydantic--&gt;&gt;SettingsObj: Populated 'port' field SettingsObj--&gt;&gt;App: Ready-to-use settings . The actual Settings class is defined in src/max/serve/config.py. It inherits from BaseSettings provided by pydantic-settings, which gives it all this powerful loading and validation capability. # From: src/max/serve/config.py from pydantic_settings import BaseSettings, SettingsConfigDict class Settings(BaseSettings): # This 'model_config' is crucial. # It tells Pydantic to look for a \".env\" file and # to use an empty prefix for environment variables by default # (so 'MAX_SERVE_HOST' maps to 'host', not 'SOMETHING_host'). model_config = SettingsConfigDict( env_file=\".env\", env_prefix=\"\", # For aliases like MAX_SERVE_HOST extra=\"allow\", # Allow other env vars not listed here populate_by_name=False, # Important for alias behavior ) # Example of a setting: host: str = Field( default=\"0.0.0.0\", alias=\"MAX_SERVE_HOST\" # Matches env var MAX_SERVE_HOST ) # ... other settings definitions . When code like settings = Settings() is executed (as seen in src/max/serve/api_server.py’s main function or src/max/entrypoints/cli/serve.py’s serve_pipeline), Pydantic does its magic to collect all the values. ",
    "url": "/modular_max/01_settings___settings__class__.html#under-the-hood-how-settings-are-loaded",
    
    "relUrl": "/modular_max/01_settings___settings__class__.html#under-the-hood-how-settings-are-loaded"
  },"33": {
    "doc": "Chapter 1: Settings Class",
    "title": "Common Settings You Might Tweak",
    "content": "While there are many settings, here are a few common ones you might interact with: . | host (Env: MAX_SERVE_HOST): The IP address the server listens on (e.g., “0.0.0.0” for all available interfaces, “127.0.0.1” for localhost). | port (Env: MAX_SERVE_PORT): The port number the server uses (e.g., 8000). | api_types: A list specifying which API styles to enable (e.g., [APIType.OPENAI, APIType.SAGEMAKER]). You could set this via an environment variable like MAX_SERVE_API_TYPES='[\"openai\"]' (note the string representation of a list). | logs_console_level (Env: MAX_SERVE_LOGS_CONSOLE_LEVEL): Controls the verbosity of logs printed to your console (e.g., “INFO”, “DEBUG”, “WARNING”). | disable_telemetry (Env: MAX_SERVE_DISABLE_TELEMETRY): A boolean (True or False) to turn off remote telemetry. | . ",
    "url": "/modular_max/01_settings___settings__class__.html#common-settings-you-might-tweak",
    
    "relUrl": "/modular_max/01_settings___settings__class__.html#common-settings-you-might-tweak"
  },"34": {
    "doc": "Chapter 1: Settings Class",
    "title": "Conclusion",
    "content": "You’ve now learned about the Settings class, the central control panel for the modular application! You’ve seen how it allows for flexible configuration through environment variables, .env files, or code defaults, all validated by the power of Pydantic. This system ensures that you can tailor the application’s behavior to your needs without modifying its core code. With this foundation in how the application is configured, you’re ready to explore how these settings are put to use. In the next chapter, we’ll look at the Serving API Layer (FastAPI App &amp; Routers), which is responsible for handling incoming requests based on these configurations. Generated by AI Codebase Knowledge Builder . ",
    "url": "/modular_max/01_settings___settings__class__.html#conclusion",
    
    "relUrl": "/modular_max/01_settings___settings__class__.html#conclusion"
  },"35": {
    "doc": "Chapter 1: Settings Class",
    "title": "Chapter 1: Settings Class",
    "content": " ",
    "url": "/modular_max/01_settings___settings__class__.html",
    
    "relUrl": "/modular_max/01_settings___settings__class__.html"
  },"36": {
    "doc": "Chapter 1: UnsafePointer",
    "title": "Chapter 1: Meet UnsafePointer - Your Direct Line to Memory",
    "content": "Welcome to your Mojo journey! In this first chapter, we’ll explore a fundamental concept that’s crucial for understanding how Mojo can work with data efficiently, especially when dealing with complex structures like NDBuffer. We’re talking about UnsafePointer. If you’re new to programming concepts like pointers, don’t worry! We’ll break it down step by step. ",
    "url": "/mojo-v2/01_unsafepointer__as_used_by_ndbuffer__.html#chapter-1-meet-unsafepointer---your-direct-line-to-memory",
    
    "relUrl": "/mojo-v2/01_unsafepointer__as_used_by_ndbuffer__.html#chapter-1-meet-unsafepointer---your-direct-line-to-memory"
  },"37": {
    "doc": "Chapter 1: UnsafePointer",
    "title": "What’s an NDBuffer Anyway? (A Quick Peek)",
    "content": "Imagine you’re working with a grid of numbers, like: . | A list of scores: [10, 20, 30] | A spreadsheet or a simple image: 1, 2, 3 4, 5, 6 . | Or even more complex, multi-dimensional data (like a video, which is a sequence of images). | . In Mojo, NDBuffer (which stands for N-Dimensional Buffer) is a powerful structure designed to handle such collections of data. It helps you organize and access elements in these structures. But for an NDBuffer to know where its actual data (the numbers, pixels, etc.) lives in your computer’s memory, it needs an address. And that’s precisely where UnsafePointer comes into the picture. ",
    "url": "/mojo-v2/01_unsafepointer__as_used_by_ndbuffer__.html#whats-an-ndbuffer-anyway-a-quick-peek",
    
    "relUrl": "/mojo-v2/01_unsafepointer__as_used_by_ndbuffer__.html#whats-an-ndbuffer-anyway-a-quick-peek"
  },"38": {
    "doc": "Chapter 1: UnsafePointer",
    "title": "UnsafePointer: The Street Address for Your Data",
    "content": "Let’s use an analogy. Think of your computer’s memory as a giant city full of warehouses. Each warehouse can store some data. An UnsafePointer is like the exact street address of one of these warehouses. The NDBuffer holds this “street address” (the UnsafePointer) to know exactly where its data begins. Here’s a key description to keep in mind: . NDBuffer contains a field data which is an UnsafePointer. This pointer directly references the starting address of the memory region that the NDBuffer describes. So, inside every NDBuffer, there’s a special variable, typically named data. The type of this data variable is UnsafePointer. This data variable doesn’t hold the numbers themselves, but rather the memory location where the first number (or element) of the NDBuffer is stored. Familiar Territory? (For C/C++ Users) . If you’ve ever worked with pointers in languages like C or C++, the idea of an UnsafePointer will seem quite familiar. It provides direct memory access. This means you’re operating very close to the computer’s hardware, which can be very powerful. Why “Unsafe”? The Power and Responsibility . The “unsafe” part of UnsafePointer is a crucial distinction. Mojo, by default, is designed with many safety features to help you avoid common programming errors. For example: . | Bounds Checking: If you have an array of 3 items, Mojo usually stops you if you try to access a 4th item (which doesn’t exist). | Lifetime Management: Mojo often helps manage when memory is no longer in use and can be cleared up or reused, preventing “memory leaks” or using “stale” memory. | . UnsafePointer bypasses these built-in safety nets. Why? . | Performance: Sometimes, these safety checks can add a tiny bit of overhead. For very performance-critical tasks, removing this overhead can be beneficial. | Interoperability: When Mojo needs to work with C/C++ code or low-level hardware, it needs a way to handle raw memory addresses. | . However, this power comes with responsibility. When you use an UnsafePointer, you, the programmer, are responsible for: . | Ensuring the memory address is actually valid (it points to a real, allocated piece of memory). | Making sure you don’t read or write past the end of the allocated memory (no automatic bounds checking). | Knowing if the memory is still “alive” or if it has been “freed” (deallocated). Using a pointer to freed memory can lead to crashes or weird behavior. | . Let’s revisit our warehouse analogy: . Think of UnsafePointer as the exact street address of a warehouse where your data is stored. The NDBuffer holds this address to know where its data begins. However, the NDBuffer doesn’t own the warehouse or manage its contents; it just has the key to the front door and relies on the address being correct. This is a very important concept: the NDBuffer uses the UnsafePointer to find its data, but it typically doesn’t “own” or “manage” the lifecycle of that memory. It trusts that the UnsafePointer it’s given is correct and valid. Anatomy of an UnsafePointer (Its Parameters) . When you declare or use an UnsafePointer in Mojo code, it’s often parameterized. These parameters give more specific information about the pointer and the data it points to: . | type: AnyType: This specifies the kind of data that the pointer points to. Is it an integer (Int), a floating-point number (Float64), a character, or some other custom type? . | Example: UnsafePointer[Int] points to a memory location that is expected to hold an integer. | . | mut: Bool: This stands for “mutability.” It indicates whether the data at the memory location can be changed (mutated) through this pointer. | mut=True: The data can be modified. | mut=False: The data is read-only through this pointer. | Example: UnsafePointer[Int, mut=True] points to an integer that you are allowed to change. | . | address_space: AddressSpace: This tells Mojo about the memory system where the data resides. For instance, data could be in the main CPU memory, or on a GPU, or in some other special memory region. | Example: AddressSpace.GENERIC is a common default, referring to general-purpose memory. | . | Origin: This is a more advanced Mojo concept related to its ownership and borrowing system. It essentially helps track “where did the permission to access this memory come from?” We won’t dive deep into Origin in this chapter, but it’s good to know it exists. | alignment: Int: This specifies memory alignment, which can be important for performance, especially on certain hardware. It ensures the data starts at a memory address that’s a multiple of a certain number. | . So, a declaration like UnsafePointer[Float64, mut=True, address_space=AddressSpace.GENERIC] describes a pointer that: . | Points to a Float64 value. | Allows modification of that Float64 value. | Resides in the generic memory address space. | . ",
    "url": "/mojo-v2/01_unsafepointer__as_used_by_ndbuffer__.html#unsafepointer-the-street-address-for-your-data",
    
    "relUrl": "/mojo-v2/01_unsafepointer__as_used_by_ndbuffer__.html#unsafepointer-the-street-address-for-your-data"
  },"39": {
    "doc": "Chapter 1: UnsafePointer",
    "title": "A Glimpse into the Mojo Standard Library Code",
    "content": "You don’t need to understand all the internal details now, but let’s peek at how UnsafePointer is defined. This code lives in a file named unsafe_pointer.mojo within Mojo’s standard library. // This is a *simplified* look at the definition from: // stdlib/src/memory/unsafe_pointer.mojo @register_passable(\"trivial\") struct UnsafePointer[ type: AnyType, // The data type it points to *, // Indicates subsequent parameters are keyword-only address_space: AddressSpace = AddressSpace.GENERIC, // Default to generic memory alignment: Int = _default_alignment[type](), // Default alignment mut: Bool = True, // Default to mutable origin: Origin[mut] = Origin[mut].cast_from[MutableAnyOrigin].result, // ... other traits it conforms to ... ]{ // This is the core: it holds the actual memory address! var address: Self._mlir_type; // --- Life cycle methods --- // fn __init__(out self): // Creates a null (empty) pointer // fn __init__(out self, *, ref to: type): // Points to an existing variable // --- Factory methods --- // @staticmethod // fn alloc(count: Int) -&gt; UnsafePointer[type, ...]: // Allocates new memory // --- Operator dunders --- // fn __getitem__(self) -&gt; ref type: // Access data (dereference) // fn offset[I: Indexer](self, idx: I) -&gt; Self: // Pointer arithmetic // --- Methods --- // fn load[...](self) -&gt; SIMD[...]: // Reads data from memory // fn store[...](self, ..., val: SIMD[...]): // Writes data to memory // fn free(self): // Deallocates memory this pointer might manage // ... and many other helpful (but unsafe!) methods ... } . Key things for a beginner to notice: . | The parameters we just discussed (type, mut, address_space, alignment, origin) are all there in the struct UnsafePointer[...] definition. | The line var address: Self._mlir_type; is the crucial field. This is where the actual raw memory address is stored. (_mlir_type is an internal representation detail). | There are many methods (functions associated with the struct) like alloc (to request new memory), free (to release memory), load (to read data from the memory location), and store (to write data). Using these methods correctly is part of your “unsafe” responsibility. | . ",
    "url": "/mojo-v2/01_unsafepointer__as_used_by_ndbuffer__.html#a-glimpse-into-the-mojo-standard-library-code",
    
    "relUrl": "/mojo-v2/01_unsafepointer__as_used_by_ndbuffer__.html#a-glimpse-into-the-mojo-standard-library-code"
  },"40": {
    "doc": "Chapter 1: UnsafePointer",
    "title": "How NDBuffer Uses UnsafePointer",
    "content": "Now, let’s connect this back to NDBuffer. Here’s a simplified look at the NDBuffer structure, found in stdlib/src/buffer/buffer.mojo: . // This is a *simplified* look at the definition from: // stdlib/src/buffer/buffer.mojo @value @register_passable(\"trivial\") struct NDBuffer[ mut: Bool, // Is the NDBuffer's data mutable? type: DType, // The data type of elements (e.g., DType.float64) rank: Int, // Number of dimensions (e.g., 2 for a 2D matrix) origin: Origin[mut], // Memory origin, tied to the UnsafePointer's origin shape: DimList = DimList.create_unknown[rank](), // Static shape info (optional) strides: DimList = DimList.create_unknown[rank](), // Static stride info (optional) *, alignment: Int = 1, address_space: AddressSpace = AddressSpace.GENERIC, // ... other traits ... ]{ // This is the star of our show for this chapter! // The NDBuffer's direct link to its underlying data. var data: UnsafePointer[ Scalar[type], // It points to individual elements of the NDBuffer's 'type' address_space=address_space, // Inherits address space mut=mut, // Inherits mutability origin=origin // Inherits origin ]; // These fields help interpret the data pointed to by `data` var dynamic_shape: IndexList[rank, ...]; // How large each dimension is (e.g., [3, 4] for 3x4) var dynamic_stride: IndexList[rank, ...]; // How to \"jump\" in memory to get to the next element // in each dimension. // ... other methods for initialization, access, etc... } . Do you see the var data: UnsafePointer[...] line within NDBuffer? That’s it! When an NDBuffer is created or used, its data field holds an UnsafePointer. This pointer tells the NDBuffer the starting memory location of all its elements. The NDBuffer then uses its other information, like dynamic_shape (e.g., “this is a 3x4 matrix”) and dynamic_stride (e.g., “to get to the next row, jump forward 4 elements in memory”), to correctly access and interpret the block of memory that starts at the address held by data. We’ll explore shape and strides in much more detail in later chapters. For now, the key is that UnsafePointer provides the starting point. ",
    "url": "/mojo-v2/01_unsafepointer__as_used_by_ndbuffer__.html#how-ndbuffer-uses-unsafepointer",
    
    "relUrl": "/mojo-v2/01_unsafepointer__as_used_by_ndbuffer__.html#how-ndbuffer-uses-unsafepointer"
  },"41": {
    "doc": "Chapter 1: UnsafePointer",
    "title": "Key Takeaways for Chapter 1",
    "content": "That was a deep dive for a first chapter, but you’ve learned some very important Mojo concepts! Here are the main takeaways: . | UnsafePointer is like a raw, direct memory address. It tells Mojo precisely where some data is stored in the computer’s memory. | It’s called “unsafe” because it operates outside of Mojo’s usual safety mechanisms (like automatic bounds checking or memory lifetime management). This gives you power and performance but requires you, the programmer, to be very careful and responsible for using it correctly. | NDBuffer, Mojo’s structure for handling N-dimensional data (like arrays and matrices), has an important field named data. This data field is an UnsafePointer. | This UnsafePointer within NDBuffer points to the starting memory location of the NDBuffer’s elements. | Typically, an NDBuffer itself doesn’t “own” or manage the memory it points to via its UnsafePointer. It simply holds the “key” (the address) and trusts that the memory is valid and correctly managed elsewhere. | . Understanding UnsafePointer is a fundamental step in seeing how Mojo can achieve high performance and interact with memory at a low level, especially for data-intensive structures like NDBuffer. ",
    "url": "/mojo-v2/01_unsafepointer__as_used_by_ndbuffer__.html#key-takeaways-for-chapter-1",
    
    "relUrl": "/mojo-v2/01_unsafepointer__as_used_by_ndbuffer__.html#key-takeaways-for-chapter-1"
  },"42": {
    "doc": "Chapter 1: UnsafePointer",
    "title": "What’s Next?",
    "content": "Now that you know how an NDBuffer finds the start of its data using an UnsafePointer, you might be curious about: . | How does it know the dimensions (like 3 rows, 4 columns)? | How are the elements arranged in memory? | How does it use this information to find a specific element, say, at row 2, column 1? | . These questions lead us directly to concepts like DimList, shape, and strides. We’ll start unraveling these in the next chapter! Keep up the great work! . Generated by AI Codebase Knowledge Builder . ",
    "url": "/mojo-v2/01_unsafepointer__as_used_by_ndbuffer__.html#whats-next",
    
    "relUrl": "/mojo-v2/01_unsafepointer__as_used_by_ndbuffer__.html#whats-next"
  },"43": {
    "doc": "Chapter 1: UnsafePointer",
    "title": "Chapter 1: UnsafePointer",
    "content": " ",
    "url": "/mojo-v2/01_unsafepointer__as_used_by_ndbuffer__.html",
    
    "relUrl": "/mojo-v2/01_unsafepointer__as_used_by_ndbuffer__.html"
  },"44": {
    "doc": "AsyncWebCrawler",
    "title": "Chapter 2: Meet the General Manager - AsyncWebCrawler",
    "content": "In Chapter 1: How We Fetch Webpages - AsyncCrawlerStrategy, we learned about the different ways Crawl4AI can fetch the raw content of a webpage, like choosing between a fast drone (AsyncHTTPCrawlerStrategy) or a versatile delivery truck (AsyncPlaywrightCrawlerStrategy). But who decides which delivery vehicle to use? Who tells it which address (URL) to go to? And who takes the delivered package (the raw HTML) and turns it into something useful? . That’s where the AsyncWebCrawler comes in. Think of it as the General Manager of the entire crawling operation. ",
    "url": "/Crawl4AI/02_asyncwebcrawler.html#chapter-2-meet-the-general-manager---asyncwebcrawler",
    
    "relUrl": "/Crawl4AI/02_asyncwebcrawler.html#chapter-2-meet-the-general-manager---asyncwebcrawler"
  },"45": {
    "doc": "AsyncWebCrawler",
    "title": "What Problem Does AsyncWebCrawler Solve?",
    "content": "Imagine you want to get information from a website. You need to: . | Decide how to fetch the page (like choosing the drone or truck from Chapter 1). | Actually fetch the page content. | Maybe clean up the messy HTML. | Perhaps extract specific pieces of information (like product prices or article titles). | Maybe save the results so you don’t have to fetch them again immediately (caching). | Finally, give you the final, processed result. | . Doing all these steps manually for every URL would be tedious and complex. AsyncWebCrawler acts as the central coordinator, managing all these steps for you. You just tell it what URL to crawl and maybe some preferences, and it handles the rest. ",
    "url": "/Crawl4AI/02_asyncwebcrawler.html#what-problem-does-asyncwebcrawler-solve",
    
    "relUrl": "/Crawl4AI/02_asyncwebcrawler.html#what-problem-does-asyncwebcrawler-solve"
  },"46": {
    "doc": "AsyncWebCrawler",
    "title": "What is AsyncWebCrawler?",
    "content": "AsyncWebCrawler is the main class you’ll interact with when using Crawl4AI. It’s the primary entry point for starting any crawling task. Key Responsibilities: . | Initialization: Sets up the necessary components, like the browser (if needed). | Coordination: Takes your request (a URL and configuration) and orchestrates the different parts: . | Delegates fetching to an AsyncCrawlerStrategy. | Manages caching using CacheContext / CacheMode. | Uses a ContentScrapingStrategy to clean and parse HTML. | Applies a RelevantContentFilter if configured. | Uses an ExtractionStrategy to pull out specific data if needed. | . | Result Packaging: Bundles everything up into a neat CrawlResult object. | Resource Management: Handles starting and stopping resources (like browsers) cleanly. | . It’s the “conductor” making sure all the different instruments play together harmoniously. ",
    "url": "/Crawl4AI/02_asyncwebcrawler.html#what-is-asyncwebcrawler",
    
    "relUrl": "/Crawl4AI/02_asyncwebcrawler.html#what-is-asyncwebcrawler"
  },"47": {
    "doc": "AsyncWebCrawler",
    "title": "Your First Crawl: Using arun",
    "content": "Let’s see the AsyncWebCrawler in action. The most common way to use it is with an async with block, which automatically handles setup and cleanup. The main method to crawl a single URL is arun. # chapter2_example_1.py import asyncio from crawl4ai import AsyncWebCrawler # Import the General Manager async def main(): # Create the General Manager instance using 'async with' # This handles setup (like starting a browser if needed) # and cleanup (closing the browser). async with AsyncWebCrawler() as crawler: print(\"Crawler is ready!\") # Tell the manager to crawl a specific URL url_to_crawl = \"https://httpbin.org/html\" # A simple example page print(f\"Asking the crawler to fetch: {url_to_crawl}\") result = await crawler.arun(url=url_to_crawl) # Check if the crawl was successful if result.success: print(\"\\nSuccess! Crawler got the content.\") # The result object contains the processed data # We'll learn more about CrawlResult in Chapter 7 print(f\"Page Title: {result.metadata.get('title', 'N/A')}\") print(f\"First 100 chars of Markdown: {result.markdown.raw_markdown[:100]}...\") else: print(f\"\\nFailed to crawl: {result.error_message}\") if __name__ == \"__main__\": asyncio.run(main()) . Explanation: . | import AsyncWebCrawler: We import the main class. | async def main():: Crawl4AI uses Python’s asyncio for efficiency, so our code needs to be in an async function. | async with AsyncWebCrawler() as crawler:: This is the standard way to create and manage the crawler. The async with statement ensures that resources (like the underlying browser used by the default AsyncPlaywrightCrawlerStrategy) are properly started and stopped, even if errors occur. | crawler.arun(url=url_to_crawl): This is the core command. We tell our crawler instance (the General Manager) to run (arun) the crawling process for the specified url. await is used because fetching webpages takes time, and asyncio allows other tasks to run while waiting. | result: The arun method returns a CrawlResult object. This object contains all the information gathered during the crawl (HTML, cleaned text, metadata, etc.). We’ll explore this object in detail in Chapter 7: Understanding the Results - CrawlResult. | result.success: We check this boolean flag to see if the crawl completed without critical errors. | Accessing Data: If successful, we can access processed information like the page title (result.metadata['title']) or the content formatted as Markdown (result.markdown.raw_markdown). | . ",
    "url": "/Crawl4AI/02_asyncwebcrawler.html#your-first-crawl-using-arun",
    
    "relUrl": "/Crawl4AI/02_asyncwebcrawler.html#your-first-crawl-using-arun"
  },"48": {
    "doc": "AsyncWebCrawler",
    "title": "Configuring the Crawl",
    "content": "Sometimes, the default behavior isn’t quite what you need. Maybe you want to use the faster “drone” strategy from Chapter 1, or perhaps you want to ensure you always fetch a fresh copy of the page, ignoring any saved cache. You can customize the behavior of a specific arun call by passing a CrawlerRunConfig object. Think of this as giving specific instructions to the General Manager for this particular job. # chapter2_example_2.py import asyncio from crawl4ai import AsyncWebCrawler from crawl4ai import CrawlerRunConfig # Import configuration class from crawl4ai import CacheMode # Import cache options async def main(): async with AsyncWebCrawler() as crawler: print(\"Crawler is ready!\") url_to_crawl = \"https://httpbin.org/html\" # Create a specific configuration for this run # Tell the crawler to BYPASS the cache (fetch fresh) run_config = CrawlerRunConfig( cache_mode=CacheMode.BYPASS ) print(\"Configuration: Bypass cache for this run.\") # Pass the config object to the arun method result = await crawler.arun( url=url_to_crawl, config=run_config # Pass the specific instructions ) if result.success: print(\"\\nSuccess! Crawler got fresh content (cache bypassed).\") print(f\"Page Title: {result.metadata.get('title', 'N/A')}\") else: print(f\"\\nFailed to crawl: {result.error_message}\") if __name__ == \"__main__\": asyncio.run(main()) . Explanation: . | from crawl4ai import CrawlerRunConfig, CacheMode: We import the necessary classes for configuration. | run_config = CrawlerRunConfig(...): We create an instance of CrawlerRunConfig. This object holds various settings for a specific crawl job. | cache_mode=CacheMode.BYPASS: We set the cache_mode. CacheMode.BYPASS tells the crawler to ignore any previously saved results for this URL and fetch it directly from the web server. We’ll learn all about caching options in Chapter 9: Smart Fetching with Caching - CacheContext / CacheMode. | crawler.arun(..., config=run_config): We pass our custom run_config object to the arun method using the config parameter. | . The CrawlerRunConfig is very powerful and lets you control many aspects of the crawl, including which scraping or extraction methods to use. We’ll dive deep into it in the next chapter: Chapter 3: Giving Instructions - CrawlerRunConfig. ",
    "url": "/Crawl4AI/02_asyncwebcrawler.html#configuring-the-crawl",
    
    "relUrl": "/Crawl4AI/02_asyncwebcrawler.html#configuring-the-crawl"
  },"49": {
    "doc": "AsyncWebCrawler",
    "title": "What Happens When You Call arun? (The Flow)",
    "content": "When you call crawler.arun(url=\"...\"), the AsyncWebCrawler (our General Manager) springs into action and coordinates several steps behind the scenes: . sequenceDiagram participant U as User participant AWC as AsyncWebCrawler (Manager) participant CC as Cache Check participant CS as AsyncCrawlerStrategy (Fetcher) participant SP as Scraping/Processing participant CR as CrawlResult (Final Report) U-&gt;&gt;AWC: arun(\"https://example.com\", config) AWC-&gt;&gt;CC: Need content for \"https://example.com\"? (Respect CacheMode in config) alt Cache Hit &amp; Cache Mode allows reading CC--&gt;&gt;AWC: Yes, here's the cached result. AWC--&gt;&gt;CR: Package cached result. AWC--&gt;&gt;U: Here is the CrawlResult else Cache Miss or Cache Mode prevents reading CC--&gt;&gt;AWC: No cached result / Cannot read cache. AWC-&gt;&gt;CS: Please fetch \"https://example.com\" (using configured strategy) CS--&gt;&gt;AWC: Here's the raw response (HTML, etc.) AWC-&gt;&gt;SP: Process this raw content (Scrape, Filter, Extract based on config) SP--&gt;&gt;AWC: Here's the processed data (Markdown, Metadata, etc.) AWC-&gt;&gt;CC: Cache this result? (Respect CacheMode in config) CC--&gt;&gt;AWC: OK, cached. AWC--&gt;&gt;CR: Package new result. AWC--&gt;&gt;U: Here is the CrawlResult end . Simplified Steps: . | Receive Request: The AsyncWebCrawler gets the URL and configuration from your arun call. | Check Cache: It checks if a valid result for this URL is already saved (cached) and if the CacheMode allows using it. (See Chapter 9). | Fetch (if needed): If no valid cached result exists or caching is bypassed, it asks the configured AsyncCrawlerStrategy (e.g., Playwright or HTTP) to fetch the raw page content. | Process Content: It takes the raw HTML and passes it through various processing steps based on the configuration: . | Scraping: Cleaning up HTML, extracting basic structure using a ContentScrapingStrategy. | Filtering: Optionally filtering content for relevance using a RelevantContentFilter. | Extraction: Optionally extracting specific structured data using an ExtractionStrategy. | . | Cache Result (if needed): If caching is enabled for writing, it saves the final processed result. | Return Result: It bundles everything into a CrawlResult object and returns it to you. | . ",
    "url": "/Crawl4AI/02_asyncwebcrawler.html#what-happens-when-you-call-arun-the-flow",
    
    "relUrl": "/Crawl4AI/02_asyncwebcrawler.html#what-happens-when-you-call-arun-the-flow"
  },"50": {
    "doc": "AsyncWebCrawler",
    "title": "Crawling Many Pages: arun_many",
    "content": "What if you have a whole list of URLs to crawl? Calling arun in a loop works, but it might not be the most efficient way. AsyncWebCrawler provides the arun_many method designed for this. # chapter2_example_3.py import asyncio from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode async def main(): async with AsyncWebCrawler() as crawler: urls_to_crawl = [ \"https://httpbin.org/html\", \"https://httpbin.org/links/10/0\", \"https://httpbin.org/robots.txt\" ] print(f\"Asking crawler to fetch {len(urls_to_crawl)} URLs.\") # Use arun_many for multiple URLs # We can still pass a config that applies to all URLs in the batch config = CrawlerRunConfig(cache_mode=CacheMode.BYPASS) results = await crawler.arun_many(urls=urls_to_crawl, config=config) print(f\"\\nFinished crawling! Got {len(results)} results.\") for result in results: status = \"Success\" if result.success else \"Failed\" url_short = result.url.split('/')[-1] # Get last part of URL print(f\"- URL: {url_short:&lt;10} | Status: {status:&lt;7} | Title: {result.metadata.get('title', 'N/A')}\") if __name__ == \"__main__\": asyncio.run(main()) . Explanation: . | urls_to_crawl = [...]: We define a list of URLs. | await crawler.arun_many(urls=urls_to_crawl, config=config): We call arun_many, passing the list of URLs. It handles crawling them concurrently (like dispatching multiple delivery trucks or drones efficiently). | results: arun_many returns a list where each item is a CrawlResult object corresponding to one of the input URLs. | . arun_many is much more efficient for batch processing as it leverages asyncio to handle multiple fetches and processing tasks concurrently. It uses a BaseDispatcher internally to manage this concurrency. ",
    "url": "/Crawl4AI/02_asyncwebcrawler.html#crawling-many-pages-arun_many",
    
    "relUrl": "/Crawl4AI/02_asyncwebcrawler.html#crawling-many-pages-arun_many"
  },"51": {
    "doc": "AsyncWebCrawler",
    "title": "Under the Hood (A Peek at the Code)",
    "content": "You don’t need to know the internal details to use AsyncWebCrawler, but seeing the structure can help. Inside the crawl4ai library, the file async_webcrawler.py defines this class. # Simplified from async_webcrawler.py # ... imports ... from .async_crawler_strategy import AsyncCrawlerStrategy, AsyncPlaywrightCrawlerStrategy from .async_configs import BrowserConfig, CrawlerRunConfig from .models import CrawlResult from .cache_context import CacheContext, CacheMode # ... other strategy imports ... class AsyncWebCrawler: def __init__( self, crawler_strategy: AsyncCrawlerStrategy = None, # You can provide a strategy... config: BrowserConfig = None, # Configuration for the browser # ... other parameters like logger, base_directory ... ): # If no strategy is given, it defaults to Playwright (the 'truck') self.crawler_strategy = crawler_strategy or AsyncPlaywrightCrawlerStrategy(...) self.browser_config = config or BrowserConfig() # ... setup logger, directories, etc... self.ready = False # Flag to track if setup is complete async def __aenter__(self): # This is called when you use 'async with'. It starts the strategy. await self.crawler_strategy.__aenter__() await self.awarmup() # Perform internal setup self.ready = True return self async def __aexit__(self, exc_type, exc_val, exc_tb): # This is called when exiting 'async with'. It cleans up. await self.crawler_strategy.__aexit__(exc_type, exc_val, exc_tb) self.ready = False async def arun(self, url: str, config: CrawlerRunConfig = None) -&gt; CrawlResult: # 1. Ensure config exists, set defaults (like CacheMode.ENABLED) crawler_config = config or CrawlerRunConfig() if crawler_config.cache_mode is None: crawler_config.cache_mode = CacheMode.ENABLED # 2. Create CacheContext to manage caching logic cache_context = CacheContext(url, crawler_config.cache_mode) # 3. Try reading from cache if allowed cached_result = None if cache_context.should_read(): cached_result = await async_db_manager.aget_cached_url(url) # 4. If cache hit and valid, return cached result if cached_result and self._is_cache_valid(cached_result, crawler_config): # ... log cache hit ... return cached_result # 5. If no cache hit or cache invalid/bypassed: Fetch fresh content # Delegate to the configured AsyncCrawlerStrategy async_response = await self.crawler_strategy.crawl(url, config=crawler_config) # 6. Process the HTML (scrape, filter, extract) # This involves calling other strategies based on config crawl_result = await self.aprocess_html( url=url, html=async_response.html, config=crawler_config, # ... other details from async_response ... ) # 7. Write to cache if allowed if cache_context.should_write(): await async_db_manager.acache_url(crawl_result) # 8. Return the final CrawlResult return crawl_result async def aprocess_html(self, url: str, html: str, config: CrawlerRunConfig, ...) -&gt; CrawlResult: # This internal method handles: # - Getting the configured ContentScrapingStrategy # - Calling its 'scrap' method # - Getting the configured MarkdownGenerationStrategy # - Calling its 'generate_markdown' method # - Getting the configured ExtractionStrategy (if any) # - Calling its 'run' method # - Packaging everything into a CrawlResult # ... implementation details ... pass # Simplified async def arun_many(self, urls: List[str], config: Optional[CrawlerRunConfig] = None, ...) -&gt; List[CrawlResult]: # Uses a Dispatcher (like MemoryAdaptiveDispatcher) # to run self.arun for each URL concurrently. # ... implementation details using a dispatcher ... pass # Simplified # ... other methods like awarmup, close, caching helpers ... The key takeaway is that AsyncWebCrawler doesn’t do the fetching or detailed processing itself. It acts as the central hub, coordinating calls to the various specialized Strategy classes based on the provided configuration. ",
    "url": "/Crawl4AI/02_asyncwebcrawler.html#under-the-hood-a-peek-at-the-code",
    
    "relUrl": "/Crawl4AI/02_asyncwebcrawler.html#under-the-hood-a-peek-at-the-code"
  },"52": {
    "doc": "AsyncWebCrawler",
    "title": "Conclusion",
    "content": "You’ve met the General Manager: AsyncWebCrawler! . | It’s the main entry point for using Crawl4AI. | It coordinates all the steps: fetching, caching, scraping, extracting. | You primarily interact with it using async with and the arun() (single URL) or arun_many() (multiple URLs) methods. | It takes a URL and an optional CrawlerRunConfig object to customize the crawl. | It returns a comprehensive CrawlResult object. | . Now that you understand the central role of AsyncWebCrawler, let’s explore how to give it detailed instructions for each crawling job. Next: Let’s dive into the specifics of configuration with Chapter 3: Giving Instructions - CrawlerRunConfig. Generated by AI Codebase Knowledge Builder . ",
    "url": "/Crawl4AI/02_asyncwebcrawler.html#conclusion",
    
    "relUrl": "/Crawl4AI/02_asyncwebcrawler.html#conclusion"
  },"53": {
    "doc": "AsyncWebCrawler",
    "title": "AsyncWebCrawler",
    "content": " ",
    "url": "/Crawl4AI/02_asyncwebcrawler.html",
    
    "relUrl": "/Crawl4AI/02_asyncwebcrawler.html"
  },"54": {
    "doc": "Chapter 2: AsyncWebCrawler",
    "title": "Chapter 2: AsyncWebCrawler",
    "content": "In Chapter 1: Configuration System, we learned how to set up the configuration for our web crawler. Now, let’s put those configurations to work with the AsyncWebCrawler, the main engine of the crawl4ai library! . ",
    "url": "/Crawl4AI/02_asyncwebcrawler_.html",
    
    "relUrl": "/Crawl4AI/02_asyncwebcrawler_.html"
  },"55": {
    "doc": "Chapter 2: AsyncWebCrawler",
    "title": "What is AsyncWebCrawler?",
    "content": "Imagine you want to automatically collect information from websites. You could manually visit each page, copy the text, save images, and organize everything—but that would take forever! The AsyncWebCrawler is like your personal assistant that does all this automatically and efficiently. The “Async” part means it can work on multiple tasks simultaneously without waiting for each one to finish before starting the next, similar to how you might cook multiple dishes at once rather than one after another. ",
    "url": "/Crawl4AI/02_asyncwebcrawler_.html#what-is-asyncwebcrawler",
    
    "relUrl": "/Crawl4AI/02_asyncwebcrawler_.html#what-is-asyncwebcrawler"
  },"56": {
    "doc": "Chapter 2: AsyncWebCrawler",
    "title": "Basic Usage: Crawling a Single Webpage",
    "content": "Let’s start with a simple example. Say you want to extract content from a webpage and convert it to markdown format: . import asyncio from crawl4ai import AsyncWebCrawler async def simple_crawl(): async with AsyncWebCrawler() as crawler: result = await crawler.arun(url=\"https://example.com\") print(result.markdown) # Print the extracted markdown content # Run the async function asyncio.run(simple_crawl()) . This code: . | Creates an AsyncWebCrawler using a context manager (async with) | Crawls the URL “https://example.com” | Prints the extracted markdown content | . The async with statement ensures the crawler is properly started and closed, even if errors occur. ",
    "url": "/Crawl4AI/02_asyncwebcrawler_.html#basic-usage-crawling-a-single-webpage",
    
    "relUrl": "/Crawl4AI/02_asyncwebcrawler_.html#basic-usage-crawling-a-single-webpage"
  },"57": {
    "doc": "Chapter 2: AsyncWebCrawler",
    "title": "Using Configuration with AsyncWebCrawler",
    "content": "Remember the configurations we learned about in Chapter 1? Let’s use them with our crawler: . from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig from crawl4ai.cache_context import CacheMode async def configured_crawl(): # Browser configuration browser_config = BrowserConfig(headless=True, browser_type=\"chromium\") # Crawler run configuration run_config = CrawlerRunConfig( cache_mode=CacheMode.ENABLED, screenshot=True ) async with AsyncWebCrawler(config=browser_config) as crawler: result = await crawler.arun(url=\"https://example.com\", config=run_config) return result . Here, we: . | Create a BrowserConfig for how the browser behaves | Create a CrawlerRunConfig for how the crawler processes content | Pass the browser config when creating the crawler | Pass the run config when crawling the URL | . ",
    "url": "/Crawl4AI/02_asyncwebcrawler_.html#using-configuration-with-asyncwebcrawler",
    
    "relUrl": "/Crawl4AI/02_asyncwebcrawler_.html#using-configuration-with-asyncwebcrawler"
  },"58": {
    "doc": "Chapter 2: AsyncWebCrawler",
    "title": "Understanding Crawler Results",
    "content": "When you call crawler.arun(), it returns a CrawlResult object that contains all the extracted data: . async def examine_result(): async with AsyncWebCrawler() as crawler: result = await crawler.arun(url=\"https://example.com\") # Access different parts of the result print(f\"Markdown content: {result.markdown}\") print(f\"HTML content: {result.html}\") print(f\"Was crawl successful? {result.success}\") # If you took a screenshot if result.screenshot: with open(\"screenshot.png\", \"wb\") as f: f.write(result.screenshot) . The result object has many useful attributes: . | markdown: The page content converted to markdown format | html: The original HTML of the page | success: Whether the crawl was successful | screenshot: Screenshot data (if configured) | links: Dictionary of internal and external links found | media: Dictionary of images, videos, and other media found | . ",
    "url": "/Crawl4AI/02_asyncwebcrawler_.html#understanding-crawler-results",
    
    "relUrl": "/Crawl4AI/02_asyncwebcrawler_.html#understanding-crawler-results"
  },"59": {
    "doc": "Chapter 2: AsyncWebCrawler",
    "title": "Crawling Multiple Webpages",
    "content": "Often, you’ll want to crawl multiple pages. The AsyncWebCrawler makes this efficient with the arun_many method: . async def crawl_multiple(): urls = [ \"https://example.com\", \"https://example.org\", \"https://example.net\" ] async with AsyncWebCrawler() as crawler: results = await crawler.arun_many(urls=urls) for result in results: print(f\"{result.url}: {len(result.markdown)} chars\") . This crawls all three URLs concurrently, which is much faster than doing them one by one! . ",
    "url": "/Crawl4AI/02_asyncwebcrawler_.html#crawling-multiple-webpages",
    
    "relUrl": "/Crawl4AI/02_asyncwebcrawler_.html#crawling-multiple-webpages"
  },"60": {
    "doc": "Chapter 2: AsyncWebCrawler",
    "title": "Managing Crawler Lifecycle",
    "content": "While the context manager (async with) is recommended, you can also manage the crawler’s lifecycle explicitly: . async def explicit_lifecycle(): crawler = AsyncWebCrawler() await crawler.start() # Explicitly start the crawler # Use the crawler result = await crawler.arun(url=\"https://example.com\") # Close when done await crawler.close() return result . This approach is useful for long-running applications where you want more control. ",
    "url": "/Crawl4AI/02_asyncwebcrawler_.html#managing-crawler-lifecycle",
    
    "relUrl": "/Crawl4AI/02_asyncwebcrawler_.html#managing-crawler-lifecycle"
  },"61": {
    "doc": "Chapter 2: AsyncWebCrawler",
    "title": "Advanced Feature: Taking Screenshots",
    "content": "Want to capture what the webpage looks like? Just enable screenshots in your configuration: . async def take_screenshot(): run_config = CrawlerRunConfig(screenshot=True) async with AsyncWebCrawler() as crawler: result = await crawler.arun( url=\"https://example.com\", config=run_config ) # Save the screenshot if result.screenshot: with open(\"example_screenshot.png\", \"wb\") as f: f.write(result.screenshot) . This will save a PNG image of how the webpage looked during crawling. ",
    "url": "/Crawl4AI/02_asyncwebcrawler_.html#advanced-feature-taking-screenshots",
    
    "relUrl": "/Crawl4AI/02_asyncwebcrawler_.html#advanced-feature-taking-screenshots"
  },"62": {
    "doc": "Chapter 2: AsyncWebCrawler",
    "title": "What Happens Under the Hood",
    "content": "When you use AsyncWebCrawler, a lot happens behind the scenes. Here’s a simplified view: . sequenceDiagram participant User as Your Code participant C as AsyncWebCrawler participant B as Browser participant W as Website participant P as Processor User-&gt;&gt;C: Create crawler User-&gt;&gt;C: arun(\"https://example.com\") C-&gt;&gt;B: Launch browser B-&gt;&gt;W: Request page W-&gt;&gt;B: Return HTML B-&gt;&gt;C: Return HTML &amp; screenshot C-&gt;&gt;P: Process content P-&gt;&gt;C: Return markdown &amp; extracted data C-&gt;&gt;User: Return CrawlResult . | When you call arun(), the crawler first checks if the URL is in the cache (if caching is enabled) | If not cached, it uses the Playwright library to launch a browser | The browser navigates to the URL and waits for the page to load | If configured, it takes a screenshot of the page | The HTML content is extracted and processed to generate markdown | All the extracted data is bundled into a CrawlResult and returned to you | . ",
    "url": "/Crawl4AI/02_asyncwebcrawler_.html#what-happens-under-the-hood",
    
    "relUrl": "/Crawl4AI/02_asyncwebcrawler_.html#what-happens-under-the-hood"
  },"63": {
    "doc": "Chapter 2: AsyncWebCrawler",
    "title": "Implementation Details",
    "content": "Let’s peek at some implementation details from the code you provided: . # From async_webcrawler.py async def arun(self, url: str, config: CrawlerRunConfig = None, **kwargs): # Auto-start if not ready if not self.ready: await self.start() config = config or CrawlerRunConfig() # ... [cache checking logic] ... # Fetch fresh content if needed if not cached_result: async_response = await self.crawler_strategy.crawl( url, config=config ) # Process the HTML content crawl_result = await self.aprocess_html( url=url, html=async_response.html, screenshot_data=async_response.screenshot, # ... [other parameters] ... ) return CrawlResultContainer(crawl_result) . The arun method is the core of the crawler. It: . | Makes sure the crawler is ready | Uses the configuration (or creates a default one) | Checks the cache for existing results | If needed, uses a crawler strategy to fetch content | Processes the HTML into a structured result | Returns everything wrapped in a container | . The actual browser interaction is handled by the crawler_strategy, typically an AsyncPlaywrightCrawlerStrategy instance that uses the Playwright library to control a real browser. ",
    "url": "/Crawl4AI/02_asyncwebcrawler_.html#implementation-details",
    
    "relUrl": "/Crawl4AI/02_asyncwebcrawler_.html#implementation-details"
  },"64": {
    "doc": "Chapter 2: AsyncWebCrawler",
    "title": "Conclusion",
    "content": "Now you understand the AsyncWebCrawler, the workhorse of the crawl4ai library! You’ve learned how to: . | Create and use an AsyncWebCrawler | Configure its behavior | Extract content from a single webpage | Process multiple webpages efficiently | Handle the results | . The AsyncWebCrawler provides a high-level interface that hides the complexity of browser automation, network requests, and content extraction. It’s like having a robot that visits websites for you and brings back organized information! . In the next chapter, Content Extraction Pipeline, we’ll dive deeper into how the crawler extracts and processes the content it collects. You’ll learn how to customize this process to get exactly the content you need. Generated by AI Codebase Knowledge Builder . ",
    "url": "/Crawl4AI/02_asyncwebcrawler_.html#conclusion",
    
    "relUrl": "/Crawl4AI/02_asyncwebcrawler_.html#conclusion"
  },"65": {
    "doc": "Chapter 2: DimList and Dim",
    "title": "Chapter 2: Defining Dimensions - Dim and DimList",
    "content": "In Chapter 1: Meet UnsafePointer, we learned that NDBuffer uses an UnsafePointer (its data field) to know the starting memory address of its data. This tells NDBuffer where its numbers, pixels, or other elements begin in the computer’s memory. But just knowing where data starts isn’t enough. If you have a block of memory, how do you know if it represents: . | A simple list of 10 numbers? ([1, 2, ..., 10]) | A 2x5 grid of numbers? 1, 2, 3, 4, 5 6, 7, 8, 9, 10 . | Or even a 3D cube of data? | . To make sense of the raw data pointed to by UnsafePointer, NDBuffer needs to understand its structure or shape. It also needs to know how elements are laid out in memory, which involves something called strides. For these crucial pieces of information, Mojo uses two important building blocks: Dim and DimList. As the project description states: . DimList is a compile-time list that holds dimension-related information, primarily used for an NDBuffer’s shape and strides. Each element in a DimList is a Dim. A Dim object can represent either a statically known integer value (e.g., a dimension of size 3, known at compile time) or an unknown value (dynamic size) that will only be determined at runtime. This design allows NDBuffer to be flexible, supporting both fully static and partially or fully dynamic tensor shapes and strides. Let’s break these down. ",
    "url": "/mojo-v2/02_dimlist_and_dim_.html#chapter-2-defining-dimensions---dim-and-dimlist",
    
    "relUrl": "/mojo-v2/02_dimlist_and_dim_.html#chapter-2-defining-dimensions---dim-and-dimlist"
  },"66": {
    "doc": "Chapter 2: DimList and Dim",
    "title": "Dim: A Single Dimension’s Story",
    "content": "Imagine you’re describing one side of a box. That side has a length. A Dim in Mojo represents the size of a single dimension. This “size” can be: . | Statically Known: The size is a specific, fixed number known when you write your code and when Mojo compiles it. For example, a dimension of size 5. | Dynamically Known: The size isn’t known until your program is actually running. It might depend on user input, a file being read, or calculations done earlier in the program. Mojo uses a special internal marker (a _sentinel value, which is -31337 in the stdlib/src/buffer/dimlist.mojo code) to represent this “unknown for now” state. | . Think of it like this: . | Dim(5) means “This dimension definitely has a size of 5.” | Dim() means “The size of this dimension is currently unknown; we’ll find out at runtime.” | . Creating and Using Dim . Let’s look at how you work with Dim objects. The code for Dim can be found in stdlib/src/buffer/dimlist.mojo. // First, you'd typically import Dim if you're using it directly // (though often it's used implicitly through DimList and NDBuffer) // from buffer import Dim // We'll assume this for standalone examples // Creating Dims let static_dim = Dim(5) // A dimension known to be 5 at compile time let another_static_dim: Dim = 10 // You can also assign integers directly let dynamic_dim = Dim() // A dimension whose size is unknown at compile time // Printing Dims print(\"Static Dim:\", static_dim) // Output: Static Dim: 5 print(\"Dynamic Dim:\", dynamic_dim) // Output: Dynamic Dim: ? // Checking if a Dim has a static value if static_dim.has_value(): print(\"static_dim has a value.\") // This will print else: print(\"static_dim is dynamic.\") if dynamic_dim.has_value(): print(\"dynamic_dim has a value.\") else: print(\"dynamic_dim is dynamic.\") // This will print // You can also use a Dim directly in a boolean context if static_dim: // This is true if static_dim.has_value() is true print(\"static_dim evaluates to true (it has a value).\") if dynamic_dim: print(\"dynamic_dim evaluates to true.\") else: print(\"dynamic_dim evaluates to false (it's dynamic).\") // Getting the static value if static_dim: print(\"Value of static_dim:\", static_dim.get()) // Output: Value of static_dim: 5 // What if you try to .get() a dynamic dimension? // print(dynamic_dim.get()) // This would give the internal sentinel value (-31337). // It's safer to check first, or use or_else: let dynamic_val_or_default = dynamic_dim.or_else(1) // If dynamic_dim is dynamic, use 1 print(\"Dynamic value or default:\", dynamic_val_or_default) // Output: Dynamic value or default: 1 // Checking if a dimension is dynamic if dynamic_dim.is_dynamic(): print(\"dynamic_dim is indeed dynamic!\") // This will print . Key Dim features to remember: . | Creation: Dim(number) for static, Dim() for dynamic. | Checking: has_value() (or just if my_dim:) tells you if it’s static. is_dynamic() tells you if it’s dynamic. | Getting Value: get() retrieves the static value. Be cautious and check has_value() first, or use or_else(default_value). | String Representation: Prints as the number if static, or ? if dynamic. | . The Dim struct (defined with @value struct Dim(...) in the source code) is designed to be very efficient, especially when its value is known at compile time. ",
    "url": "/mojo-v2/02_dimlist_and_dim_.html#dim-a-single-dimensions-story",
    
    "relUrl": "/mojo-v2/02_dimlist_and_dim_.html#dim-a-single-dimensions-story"
  },"67": {
    "doc": "Chapter 2: DimList and Dim",
    "title": "DimList: A Collection of Dimensions",
    "content": "Now that we understand a single Dim, DimList is straightforward: it’s a list of Dim objects. Imagine DimList as the blueprint for the dimensions of a container. Each Dim in the list specifies a particular dimension’s size, like length, width, or height. Some of these might be fixed values written directly on the blueprint (e.g., “length is 5 units”), while others might be placeholders (e.g., “width is N units, to be measured on-site”). DimList collects all these specifications. For an NDBuffer, DimList is used for two main things: . | Shape: Describes the size of the NDBuffer in each dimension (e.g., number of rows, number of columns, number of channels). | Strides: Describes how many elements you need to “skip” in memory to move from one element to the next along a particular dimension. (We’ll cover strides in more detail in a later chapter). | . Creating and Using DimList . Let’s see how to create and use DimList. The code for DimList is also in stdlib/src/buffer/dimlist.mojo. // from buffer import Dim, DimList // Assuming imports // --- Creating DimLists --- // For a 2D shape like 3 rows, 4 columns (all static) let static_shape_A = DimList(3, 4) print(\"Static Shape A:\", static_shape_A) // Output: Static Shape A: [3, 4] // You can also be explicit with Dims let static_shape_B = DimList(Dim(3), Dim(4)) print(\"Static Shape B:\", static_shape_B) // Output: Static Shape B: [3, 4] // For a 3D shape: 10 depth, dynamic height, 5 width let mixed_shape = DimList(10, Dim(), 5) print(\"Mixed Shape:\", mixed_shape) // Output: Mixed Shape: [10, ?, 5] // For a 2D shape where both dimensions are dynamic. // Let's say the rank (number of dimensions) is 2. let dynamic_shape_rank2 = DimList.create_unknown[2]() print(\"Dynamic Shape (Rank 2):\", dynamic_shape_rank2) // Output: Dynamic Shape (Rank 2): [?, ?] // --- Using DimLists --- // Getting the length (number of dimensions, or rank) print(\"Rank of static_shape_A:\", len(static_shape_A)) // Output: Rank of static_shape_A: 2 print(\"Rank of mixed_shape:\", len(mixed_shape)) // Output: Rank of mixed_shape: 3 // Accessing a specific Dim at an index (0-based) let first_dim_of_mixed = mixed_shape.at[0]() print(\"First Dim of mixed_shape:\", first_dim_of_mixed) // Output: First Dim of mixed_shape: 10 let second_dim_of_mixed = mixed_shape.at[1]() print(\"Second Dim of mixed_shape:\", second_dim_of_mixed) // Output: Second Dim of mixed_shape: ? // Checking if a Dim at an index has a static value print(\"Is 1st dim of mixed_shape static?\", mixed_shape.has_value[0]()) // Output: true print(\"Is 2nd dim of mixed_shape static?\", mixed_shape.has_value[1]()) // Output: false // Getting the static value of a Dim at an index (if it's static) if mixed_shape.has_value[0](): print(\"Value of 1st dim of mixed_shape:\", mixed_shape.get[0]()) // Output: 10 // Checking if ALL dimensions in a DimList are statically known if static_shape_A.all_known[len(static_shape_A)](): // or simply static_shape_A.all_known() in newer Mojo print(\"static_shape_A has all known dimensions.\") // This will print if mixed_shape.all_known[len(mixed_shape)](): print(\"mixed_shape has all known dimensions.\") else: print(\"mixed_shape has at least one dynamic dimension.\") // This will print // Calculating the product of dimensions (e.g., total number of elements) let product_static = static_shape_A.product() print(\"Product of static_shape_A:\", product_static) // Output: Product of static_shape_A: 12 (Dim(12)) if product_static: print(\"Value:\", product_static.get()) // Output: Value: 12 let product_mixed = mixed_shape.product() print(\"Product of mixed_shape:\", product_mixed) // Output: Product of mixed_shape: ? (Dim()) if product_mixed.is_dynamic(): print(\"Product is dynamic because at least one dimension was dynamic.\") . Key DimList features to remember: . | Stores Dims: It’s a specialized list for holding dimension information. | Static &amp; Dynamic Mix: Can hold any combination of static and dynamic Dims. | DimList.create_unknown[RANK](): A handy way to make a DimList where all dimensions are dynamic for a given RANK. | at[INDEX](): Gets the Dim object at a specific index. | get[INDEX](): Gets the value of the Dim at that index (if static). | product(): Computes the product of all dimensions. If any Dim in the list is dynamic, the resulting product Dim will also be dynamic (represented as ?). | . ",
    "url": "/mojo-v2/02_dimlist_and_dim_.html#dimlist-a-collection-of-dimensions",
    
    "relUrl": "/mojo-v2/02_dimlist_and_dim_.html#dimlist-a-collection-of-dimensions"
  },"68": {
    "doc": "Chapter 2: DimList and Dim",
    "title": "DimList in Action: NDBuffer’s Shape and Strides",
    "content": "Now, let’s see how DimList and Dim are fundamental to NDBuffer. If we look at the definition of NDBuffer in stdlib/src/buffer/buffer.mojo (simplified): . // Simplified NDBuffer definition from stdlib/src/buffer/buffer.mojo struct NDBuffer[ mut: Bool, type: DType, rank: Int, origin: Origin[mut], // Here they are! shape: DimList = DimList.create_unknown[rank](), strides: DimList = DimList.create_unknown[rank](), *, // ... other parameters ... ]{ var data: UnsafePointer[...]; var dynamic_shape: IndexList[rank, ...]; var dynamic_stride: IndexList[rank, ...]; // ... } . Notice the shape: DimList and strides: DimList parameters in the NDBuffer definition. | shape: DimList: This DimList tells the NDBuffer its size in each of its rank dimensions. | strides: DimList: This DimList tells the NDBuffer how to navigate its data in memory. (More on this in a future chapter!) | . By default, if you don’t specify them, shape and strides are DimList.create_unknown[rank](), meaning all dimensions and strides are initially considered dynamic. When you create or work with an NDBuffer, you might provide these DimLists: . | If shape has all static Dims (e.g., DimList(100, 200) for a 2D buffer), Mojo knows the exact size of the buffer at compile time. | If shape has some dynamic Dims (e.g., DimList(Dim(100), Dim())), Mojo knows some dimensions statically and will figure out others at runtime. The actual runtime values for dynamic dimensions are stored in fields like dynamic_shape inside the NDBuffer. | . ",
    "url": "/mojo-v2/02_dimlist_and_dim_.html#dimlist-in-action-ndbuffers-shape-and-strides",
    
    "relUrl": "/mojo-v2/02_dimlist_and_dim_.html#dimlist-in-action-ndbuffers-shape-and-strides"
  },"69": {
    "doc": "Chapter 2: DimList and Dim",
    "title": "Why Static vs. Dynamic Matters So Much",
    "content": "This ability to mix static and dynamic dimensions is a superpower of Mojo: . | Flexibility: You can write code that works with data of varying sizes, where those sizes aren’t known until your program runs. This is essential for real-world applications. | Performance: When dimension sizes are known statically (at compile time), Mojo can perform powerful optimizations. It can unroll loops, pre-calculate memory offsets, and generate highly specialized machine code. This often leads to significantly faster execution compared to situations where all sizes are only known dynamically. | . Dim and DimList are the mechanisms that allow Mojo developers to communicate this static-or-dynamic information to the compiler, enabling this blend of flexibility and performance. ",
    "url": "/mojo-v2/02_dimlist_and_dim_.html#why-static-vs-dynamic-matters-so-much",
    
    "relUrl": "/mojo-v2/02_dimlist_and_dim_.html#why-static-vs-dynamic-matters-so-much"
  },"70": {
    "doc": "Chapter 2: DimList and Dim",
    "title": "Key Takeaways for Chapter 2",
    "content": ". | A Dim represents a single dimension’s size, which can be statically known (a fixed number) or dynamic (unknown until runtime, represented as ?). | DimList is a list of Dims, used by NDBuffer to define its shape (size in each dimension) and strides (memory layout). | You can create DimLists with all static dimensions (DimList(2,3,4)), all dynamic dimensions (DimList.create_unknown[3]()), or a mix (DimList(2, Dim(), 4)). | If any Dim in a DimList is dynamic, operations like product() on that DimList will result in a dynamic Dim (?). | This static/dynamic system allows NDBuffer (and Mojo in general) to be both flexible (handling runtime-determined sizes) and performant (optimizing heavily when sizes are compile-time known). | . ",
    "url": "/mojo-v2/02_dimlist_and_dim_.html#key-takeaways-for-chapter-2",
    
    "relUrl": "/mojo-v2/02_dimlist_and_dim_.html#key-takeaways-for-chapter-2"
  },"71": {
    "doc": "Chapter 2: DimList and Dim",
    "title": "What’s Next?",
    "content": "We’ve now seen how NDBuffer knows where its data starts (UnsafePointer) and how it understands the intended structure of that data (DimList for shape). In the next chapter, Chapter 3: NDBuffer, we’ll take a closer look at the NDBuffer struct itself, how it’s initialized with these components, and how you start actually using it to access and manipulate N-dimensional data. Navigation . | UnsafePointer (as used by NDBuffer) | DimList and Dim (You are here) | NDBuffer | Strides and Offset Computation | SIMD Data Access ``` | . Generated by AI Codebase Knowledge Builder . ",
    "url": "/mojo-v2/02_dimlist_and_dim_.html#whats-next",
    
    "relUrl": "/mojo-v2/02_dimlist_and_dim_.html#whats-next"
  },"72": {
    "doc": "Chapter 2: DimList and Dim",
    "title": "Chapter 2: DimList and Dim",
    "content": "Okay, here’s Chapter 2 of the Mojo tutorial, focusing on DimList and Dim, designed to be very beginner-friendly. ",
    "url": "/mojo-v2/02_dimlist_and_dim_.html",
    
    "relUrl": "/mojo-v2/02_dimlist_and_dim_.html"
  },"73": {
    "doc": "Chapter 2: Serving API Layer",
    "title": "Chapter 2: Serving API Layer (FastAPI App &amp; Routers)",
    "content": "Welcome back! In Chapter 1: Settings (Settings class), we explored how the modular application is configured using the Settings class. We learned how to adjust things like the server port or which API types to enable. Now, let’s see how modular actually uses these settings to listen for and understand requests from the outside world. This chapter focuses on the Serving API Layer, the very front door of our application. ",
    "url": "/modular_max/02_serving_api_layer__fastapi_app___routers__.html#chapter-2-serving-api-layer-fastapi-app--routers",
    
    "relUrl": "/modular_max/02_serving_api_layer__fastapi_app___routers__.html#chapter-2-serving-api-layer-fastapi-app--routers"
  },"74": {
    "doc": "Chapter 2: Serving API Layer",
    "title": "What Problem Does the API Layer Solve?",
    "content": "Imagine you want modular to generate some text based on a prompt, or you need it to calculate embeddings for a sentence. How do you tell modular what you want? You can’t just shout at your computer! You need a structured way to send your request and get a response. This is where the Serving API Layer comes in. It’s like the reception desk and main switchboard operator in a large office building: . | It listens for incoming “calls” (which are HTTP requests from users or other programs). | It understands what the caller wants by looking at the “extension” they dialed (the URL path, e.g., /v1/chat/completions). | It directs the call to the correct department or person (an internal function that can handle the request). | Finally, it sends back a reply (an HTTP response, like the generated text). | . The modular API layer is built using a popular Python framework called FastAPI. It’s designed to be flexible and can speak different “languages” or API styles, such as those used by OpenAI, KServe, or AWS SageMaker. This means you can often use existing tools and client libraries that already know how to talk in these styles. ",
    "url": "/modular_max/02_serving_api_layer__fastapi_app___routers__.html#what-problem-does-the-api-layer-solve",
    
    "relUrl": "/modular_max/02_serving_api_layer__fastapi_app___routers__.html#what-problem-does-the-api-layer-solve"
  },"75": {
    "doc": "Chapter 2: Serving API Layer",
    "title": "Key Components of the API Layer",
    "content": "Let’s break down the main parts of this “reception and switchboard” system: . 1. FastAPI: The Super-Efficient Receptionist . FastAPI is a modern, high-performance web framework for building APIs with Python. Think of it as a highly skilled and efficient receptionist. It handles many of the complex, low-level details of web communication for us, such as: . | Understanding incoming HTTP requests (e.g., GET, POST). | Validating incoming data (e.g., ensuring a required field is present). | Converting Python data into JSON to send back in responses. | Generating interactive API documentation automatically. | . modular uses FastAPI to build its entire API surface. 2. The Main app: The Central Hub . At the heart of the API layer is the main FastAPI application instance, often referred to simply as app. This is like the central hub of the reception area. All incoming requests arrive here first. In modular, this app object is created and configured primarily in src/max/serve/api_server.py. It’s where various settings (from Chapter 1: Settings (Settings class)) are applied, middleware (special request/response handlers) are added, and different API “wings” (Routers) are connected. # Simplified from: src/max/serve/api_server.py from fastapi import FastAPI from max.serve.config import Settings # We learned about this in Chapter 1! # This function sets up and returns the main FastAPI app def fastapi_app( settings: Settings, # ... other parameters for pipeline setup ... ) -&gt; FastAPI: # Create the main application instance app = FastAPI(title=\"MAX Serve\") # ... (other setup like middleware, /version endpoint) ... # We'll see how routers are added soon! return app . This code snippet shows the basic creation of the FastAPI app. This app object will then be “run” by a web server like Uvicorn to start listening for requests. 3. Routers: The Departmental Switchboards . Imagine our office building has different departments, each specializing in a certain type of work: one for OpenAI-style requests, one for KServe, and another for SageMaker. In FastAPI, APIRouter objects (or simply “routers”) help organize these different “departments.” . A router groups together related API endpoints (specific tasks or functions). For example, all endpoints related to the OpenAI API (like /v1/chat/completions or /v1/embeddings) would be managed by an OpenAI-specific router. modular uses routers to support multiple API standards. The Settings (Settings class) we discussed in Chapter 1 determine which of these routers are activated. If you configure api_types to include OPENAI, then the OpenAI router will be wired into the main app. # Simplified from: src/max/serve/api_server.py # (inside the fastapi_app function) from max.serve.config import APIType # From Settings from max.serve.router import kserve_routes, openai_routes, sagemaker_routes # A mapping from API type to its router module ROUTES = { APIType.KSERVE: kserve_routes, APIType.OPENAI: openai_routes, APIType.SAGEMAKER: sagemaker_routes, } # Inside the fastapi_app function: # for api_type_enabled in settings.api_types: # router_module = ROUTES[api_type_enabled] # app.include_router(router_module.router) # Adds the router to the app . This shows how, based on your configuration, the appropriate routers (like openai_routes.router) are “plugged into” the main app. 4. Endpoints (Paths &amp; Operations): The Specific Service Desks . Within each router (department), there are specific “desks” or “services” offered. These are called endpoints. An endpoint is defined by: . | A Path: The specific URL part after the main address (e.g., /chat/completions). | An HTTP Method (or Operation): What you want to do (e.g., POST to send data, GET to retrieve data). | . For example, to ask modular for a chat completion using the OpenAI API style, you would send a POST request to the path /v1/chat/completions. The API layer uses the path and method to find the exact Python function that should handle this request. Here’s how an endpoint might look within a router (e.g., src/max/serve/router/openai_routes.py): . # Simplified from: src/max/serve/router/openai_routes.py from fastapi import APIRouter, Request # This router handles all paths starting with /v1 router = APIRouter(prefix=\"/v1\") # This function handles POST requests to /v1/chat/completions @router.post(\"/chat/completions\") async def openai_create_chat_completion(request: Request): # Logic to handle the chat completion request goes here # For example, get data from 'request', call the model, format response # response_data = {\"id\": \"chatcmpl-123\", \"choices\": [...]} # return response_data print(f\"Handling chat completion for request ID: {request.state.request_id}\") return {\"message\": \"Chat completion response (simplified)\"} . The @router.post(\"/chat/completions\") part is a decorator. It tells FastAPI that the openai_create_chat_completion function should be called whenever a POST request arrives for the /v1/chat/completions path. ",
    "url": "/modular_max/02_serving_api_layer__fastapi_app___routers__.html#key-components-of-the-api-layer",
    
    "relUrl": "/modular_max/02_serving_api_layer__fastapi_app___routers__.html#key-components-of-the-api-layer"
  },"76": {
    "doc": "Chapter 2: Serving API Layer",
    "title": "Example: Requesting a Chat Completion",
    "content": "Let’s walk through what happens when you (or an application) want to generate text using modular, assuming it’s configured to serve an OpenAI-compatible API: . | You send a request: You might use a tool like curl or a Python script to send an HTTP POST request to http://&lt;server_address&gt;:&lt;port&gt;/v1/chat/completions. This request includes your prompt and other parameters in a JSON format. # Example using curl (if the server is running on localhost:8000) curl -X POST http://localhost:8000/v1/chat/completions \\ -H \"Content-Type: application/json\" \\ -d '{ \"model\": \"my-model-name\", \"messages\": [{\"role\": \"user\", \"content\": \"Hello, world!\"}] }' . If you run this (and modular is running with a model), you’d get a JSON response back. | The API Layer receives it: The modular server, listening on the configured host and port (thanks to Settings (Settings class)), receives this HTTP request. | FastAPI takes over: The main FastAPI app instance gets the request. | Routing to the right department: . | FastAPI sees the path starts with /v1. It knows the openai_routes router handles /v1 paths. | The openai_routes router then looks at the rest of the path: /chat/completions and the method POST. | . | Calling the right function: It finds the openai_create_chat_completion function (or a similar one) that’s decorated to handle this specific path and method. | Processing the request: The openai_create_chat_completion function executes. | It extracts the data (your prompt) from the request. | It then typically calls another major component, the LLM Pipeline Orchestrator (TokenGeneratorPipeline), to do the actual text generation (we’ll learn about this in the next chapter!). | . | Sending the response: Once the LLM Pipeline Orchestrator (TokenGeneratorPipeline) returns the generated text, the openai_create_chat_completion function formats it into an OpenAI-compatible JSON response and returns it. FastAPI then sends this HTTP response back to you. | . ",
    "url": "/modular_max/02_serving_api_layer__fastapi_app___routers__.html#example-requesting-a-chat-completion",
    
    "relUrl": "/modular_max/02_serving_api_layer__fastapi_app___routers__.html#example-requesting-a-chat-completion"
  },"77": {
    "doc": "Chapter 2: Serving API Layer",
    "title": "Under the Hood: The Journey of a Request",
    "content": "Let’s visualize the path of a request: . sequenceDiagram participant User participant WebServer as Uvicorn Web Server participant FastAPIApp as FastAPI App participant APIRouter as API Router (e.g., OpenAI) participant PathHandler as Path Operation Function participant Pipeline as LLM Pipeline (Chapter 3) User-&gt;&gt;WebServer: POST /v1/chat/completions (HTTP Request) WebServer-&gt;&gt;FastAPIApp: Forwards request Note over FastAPIApp: Request ID might be added here (via middleware from `src/max/serve/request.py`) FastAPIApp-&gt;&gt;APIRouter: Route based on path prefix /v1 APIRouter-&gt;&gt;PathHandler: Invoke `openai_create_chat_completion()` PathHandler-&gt;&gt;Pipeline: Ask to generate text based on request data Pipeline--&gt;&gt;PathHandler: Generated text/result PathHandler--&gt;&gt;FastAPIApp: Formatted JSON Response FastAPIApp--&gt;&gt;WebServer: HTTP Response WebServer--&gt;&gt;User: Sends HTTP Response back . Key steps in code: . | Server Setup (src/max/serve/api_server.py or src/max/entrypoints/cli/serve.py): The FastAPI app needs to be run by an ASGI server like Uvicorn. The Settings (Settings class) provide the host and port. # Simplified from src/max/serve/api_server.py from uvicorn import Config, Server # from max.serve.config import Settings # Defined in Chapter 1 # from max.serve.api_server import fastapi_app # Our app factory # settings = Settings() # Load configuration # app_instance = fastapi_app(settings, ...) # Create the FastAPI app # Create Uvicorn config # config = Config(app=app_instance, host=settings.host, port=settings.port) # server = Server(config) # # server.serve() # This starts listening for requests! . This Uvicorn server is what makes your FastAPI application accessible over the network. | Application Lifespan (src/max/serve/api_server.py): FastAPI has a concept called “lifespan” events, which are functions that run at startup and shutdown. modular uses this to initialize critical components, like the LLM Pipeline Orchestrator (TokenGeneratorPipeline), and make them available to request handlers. # Simplified from: src/max/serve/api_server.py from contextlib import asynccontextmanager from fastapi import FastAPI # from max.serve.pipelines.llm import TokenGeneratorPipeline @asynccontextmanager async def lifespan(app: FastAPI, settings, serving_settings): # ... (setup telemetry, workers etc.) ... # pipeline = TokenGeneratorPipeline(...) # Initialize the pipeline # app.state.pipeline = pipeline # Make pipeline accessible in requests print(\"Server starting up, pipeline getting ready...\") yield # The app runs while yield is active print(\"Server shutting down...\") # When creating the app: # app = FastAPI(lifespan=partial(lifespan, ...)) . The app.state.pipeline makes the pipeline object (which does the heavy lifting for text generation) easily accessible within your endpoint functions like openai_create_chat_completion. | Request Handling in an Endpoint (src/max/serve/router/openai_routes.py): Once a request reaches an endpoint function, it can access shared state (like the pipeline) and process the request. # Simplified from: src/max/serve/router/openai_routes.py from fastapi import Request # from max.serve.pipelines.llm import TokenGeneratorPipeline # @router.post(\"/chat/completions\") async def openai_create_chat_completion(request: Request): # request_data = await request.json() # Get JSON from POST body # pipeline: TokenGeneratorPipeline = request.app.state.pipeline # prompt = request_data[\"messages\"][0][\"content\"] # generated_text = await pipeline.generate_text(prompt) # Imaginary method # return {\"choices\": [{\"message\": {\"content\": generated_text}}]} # Actual logic is more complex, involves TokenGeneratorRequest, etc. return {\"message\": \"Simplified response from chat completion\"} . This shows the basic flow: get data from the request, use the pipeline from request.app.state, and return a response. The actual implementation in modular is more detailed to handle streaming, various parameters, and error conditions. | . ",
    "url": "/modular_max/02_serving_api_layer__fastapi_app___routers__.html#under-the-hood-the-journey-of-a-request",
    
    "relUrl": "/modular_max/02_serving_api_layer__fastapi_app___routers__.html#under-the-hood-the-journey-of-a-request"
  },"78": {
    "doc": "Chapter 2: Serving API Layer",
    "title": "Conclusion",
    "content": "You’ve now seen how the Serving API Layer acts as the front door for modular. Using FastAPI, it defines clear entry points (endpoints like /v1/chat/completions) and uses routers to organize these endpoints based on API styles (OpenAI, KServe, etc.). This layer takes the configuration from the Settings (Settings class) to decide which APIs to expose and on which host/port to listen. Essentially, this layer is responsible for: . | Receiving HTTP requests. | Understanding what the request is for (based on URL and HTTP method). | Directing the request to the correct internal handler function. | Sending back an HTTP response. | . But what happens inside those handler functions? How does modular actually process a request to generate text or embeddings once the API layer has handed it off? That’s the job of the LLM Pipeline Orchestrator (TokenGeneratorPipeline), which we’ll dive into in the next chapter! . Generated by AI Codebase Knowledge Builder . ",
    "url": "/modular_max/02_serving_api_layer__fastapi_app___routers__.html#conclusion",
    
    "relUrl": "/modular_max/02_serving_api_layer__fastapi_app___routers__.html#conclusion"
  },"79": {
    "doc": "Chapter 2: Serving API Layer",
    "title": "Chapter 2: Serving API Layer",
    "content": " ",
    "url": "/modular_max/02_serving_api_layer__fastapi_app___routers__.html",
    
    "relUrl": "/modular_max/02_serving_api_layer__fastapi_app___routers__.html"
  },"80": {
    "doc": "Chapter 2: UnsafePointer",
    "title": "Chapter 2: UnsafePointer",
    "content": "Welcome back! In Chapter 1: Understanding Memory Neighborhoods with AddressSpace, we learned how Mojo categorizes memory into different “neighborhoods” using AddressSpace. This helps Mojo optimize how data is handled, especially with hardware like GPUs. Now, we’re going to zoom in and look at how Mojo can directly refer to a specific spot in memory. Get ready to meet UnsafePointer! . ",
    "url": "/mojo-v1/02_unsafepointer_.html",
    
    "relUrl": "/mojo-v1/02_unsafepointer_.html"
  },"81": {
    "doc": "Chapter 2: UnsafePointer",
    "title": "What’s a Pointer Anyway?",
    "content": "Imagine you have a giant warehouse full of mailboxes (this is your computer’s memory). Each mailbox has a unique address. A pointer is like a slip of paper that holds the address of one specific mailbox. It doesn’t hold the mail (the data) itself, but it tells you where to find the mail. An UnsafePointer in Mojo is exactly this: a raw, direct reference to a location in memory. It stores the memory address where some data begins. The NDBuffer type we’ll explore later uses an UnsafePointer internally (in its data field) to know the starting memory address of the data it manages. ",
    "url": "/mojo-v1/02_unsafepointer_.html#whats-a-pointer-anyway",
    
    "relUrl": "/mojo-v1/02_unsafepointer_.html#whats-a-pointer-anyway"
  },"82": {
    "doc": "Chapter 2: UnsafePointer",
    "title": "Why “Unsafe”?",
    "content": "The “unsafe” part of UnsafePointer is a bit like having a very precise GPS coordinate but no map, no road signs, and no safety barriers. | No Automatic Memory Management: If you ask for memory using an UnsafePointer (like reserving a mailbox), Mojo won’t automatically clean it up for you when you’re done. You have to do it manually. Forget, and you get a “memory leak” (mailboxes that are reserved but unused, eventually running out of space). | No Automatic Bounds Checking: If you have a pointer to a mailbox and you try to access the mailbox 10 doors down, but you only reserved the first one, UnsafePointer won’t stop you. This can lead to reading garbage data or corrupting other data, often causing your program to crash or behave strangely. | No Automatic Initialization: When you get memory, it’s just raw space. An UnsafePointer won’t ensure it contains meaningful data until you explicitly put something there. | . UnsafePointer gives you maximum control and performance by stripping away these safety nets. This is powerful but means you, the programmer, are responsible for using it correctly. ",
    "url": "/mojo-v1/02_unsafepointer_.html#why-unsafe",
    
    "relUrl": "/mojo-v1/02_unsafepointer_.html#why-unsafe"
  },"83": {
    "doc": "Chapter 2: UnsafePointer",
    "title": "Meet UnsafePointer[T]",
    "content": "In Mojo, an UnsafePointer is always tied to a specific data type. You’d write UnsafePointer[Int] for a pointer to an integer, or UnsafePointer[String] for a pointer to a string. The T in UnsafePointer[T] tells Mojo what kind of data to expect at that memory address. Let’s look at its definition from stdlib/src/memory/unsafe_pointer.mojo (simplified): . @register_passable(\"trivial\") struct UnsafePointer[ type: AnyType, // The type of data it points to (e.g., Int, Float32) *, address_space: AddressSpace = AddressSpace.GENERIC, // Where is this memory? (from Ch 1) alignment: Int = _default_alignment[type](), // How data is arranged for efficiency mut: Bool = True, // Can we change the data via this pointer? origin: Origin[mut] = Origin[mut].cast_from[MutableAnyOrigin].result, // Advanced: Tracks memory validity ](...) . Don’t worry about all the details yet! The key things are: . | type: What kind of data this pointer “points to.” | address_space: This links back to Chapter 1! It tells Mojo if this memory is in the CPU’s main area (AddressSpace.GENERIC), on a GPU, etc. | alignment: A performance detail about how data should be positioned in memory. Usually handled by default. | mut: Short for “mutable.” If True, you can change the data the pointer points to. | origin: An advanced feature for tracking where the memory came from and if it’s still valid. | . Internally, an UnsafePointer just holds the numerical memory address. ",
    "url": "/mojo-v1/02_unsafepointer_.html#meet-unsafepointert",
    
    "relUrl": "/mojo-v1/02_unsafepointer_.html#meet-unsafepointert"
  },"84": {
    "doc": "Chapter 2: UnsafePointer",
    "title": "Key Operations with UnsafePointer",
    "content": "Let’s see how you can use UnsafePointer. We’ll use examples inspired by stdlib/test/memory/test_unsafepointer.mojo. 1. Getting Memory: Allocation . To point to new memory, you first need to “allocate” it. This is like reserving one or more mailboxes. The alloc() method is used for this. It needs to know how many items of type you want space for. from memory import UnsafePointer // Allocate memory for 1 integer var p_int = UnsafePointer[Int].alloc(1) // Allocate memory for 5 Float32 values var p_floats = UnsafePointer[Float32].alloc(5) . After alloc(), p_int holds the starting address of a memory block big enough for one Int. p_floats holds the starting address for five Float32s. Important: This memory is uninitialized. It contains garbage data. 2. Initializing and Writing to Memory . Once you have allocated memory, you need to put meaningful data into it. | For simple types (like Int, Float32): You can often use the subscript operator [] to write values, similar to how you’d use it with an array. ptr[0] refers to the first item, ptr[1] to the second, and so on. ptr[] is a shorthand for ptr[0]. // Continuing from above: p_int[0] = 100 // Put 100 into the memory p_int points to // or p_int[] = 100 p_floats[0] = 1.0 p_floats[1] = 2.5 // ... and so on for p_floats[2] through p_floats[4] . | For more complex types (structs, objects): Mojo provides special methods to initialize the memory location (the “pointee” - what’s being pointed to): . | init_pointee_move(value): Moves value into the memory. The original value is no longer valid. This is for types that are Movable. | init_pointee_copy(value): Copies value into the memory. The original value is still valid. This is for types that are Copyable. | init_pointee_explicit_copy(value): Similar to copy, for types that require an explicit .copy() call. | . Here’s an example with a List[String] (which is a more complex type) from stdlib/test/memory/test_unsafepointer.mojo: . from memory import UnsafePointer // Assuming List and String are available from collections import List // For List // In a real scenario, ensure String is also imported if not built-in for this context fn demo_complex_type_init(): var list_ptr = UnsafePointer[List[String]].alloc(1) // Create an empty List[String] and move it into the memory list_ptr points to list_ptr.init_pointee_move(List[String]()) // Now list_ptr[0] is an initialized, empty List[String] list_ptr[0].append(\"Hello\") print(list_ptr[0][0]) // Prints \"Hello\" // Proper cleanup for complex types: var taken_list = list_ptr.take_pointee() // Move the list out list_ptr.free() // Free the raw memory slot // taken_list will be destroyed when it goes out of scope, or manage it further. For beginners, knowing these init_pointee_* methods exist is important, especially if you work with types beyond simple numbers. The crucial part is that uninitialized memory from alloc() must be initialized before it’s reliably read. | . 3. Reading from Memory . To read the value stored at the memory location, you also use the [] operator: . var my_val = p_int[0] // my_val will be 100, assuming p_int[0] was set to 100 print(p_int[]) // Prints 100 var first_float = p_floats[0] // first_float will be 1.0, assuming p_floats[0] was set to 1.0 print(p_floats[1]) // Prints 2.5, assuming p_floats[1] was set to 2.5 . 4. Pointer Arithmetic: Moving Around . You can perform arithmetic on pointers to access adjacent memory locations. | ptr + offset: Gives a new pointer offset elements away from ptr. | ptr - offset: Gives a new pointer offset elements before ptr. | ptr.offset(idx): Another way to get an offset pointer. | . // Assuming p_floats points to memory for 5 Float32s, initialized earlier var p_item1 = p_floats + 1 // p_item1 now points to p_floats[1] print(p_item1[0]) // This is equivalent to p_floats[1] // You can iterate through allocated memory: for i in range(5): p_floats[i] = Float32(i * 10) // Initialize for i in range(5): print((p_floats + i)[0]) // Read using pointer arithmetic and dereference // or simply: print(p_floats[i]) . 5. Getting a Pointer to Existing Data . Sometimes, you want a pointer to a variable that already exists. You can use the UnsafePointer(to=...) constructor. var my_local_var: Int = 42 var ptr_to_local = UnsafePointer[Int](to=my_local_var) print(ptr_to_local[]) // Prints 42 // You can modify the original variable through the pointer (if mut is True) ptr_to_local[] = 50 print(my_local_var) // Prints 50! . Big Safety Note: This is very “unsafe”! If my_local_var goes out of scope (e.g., the function ends) but you still have ptr_to_local somewhere, ptr_to_local becomes a “dangling pointer.” Using it will lead to undefined behavior. The UnsafePointer does not keep my_local_var alive. 6. Null Pointers . A null pointer is a pointer that doesn’t point to any valid memory location. It’s like an address slip that’s intentionally blank or points to an invalid address (e.g., address 0). You can create a null UnsafePointer: . var null_ptr = UnsafePointer[Int]() // Creates a null pointer print(null_ptr) // Usually prints something like 0x0 . You can check if a pointer is null. An UnsafePointer behaves like False in a boolean context if it’s null, and True if it’s not. if null_ptr: print(\"This won't print, null_ptr is null.\") else: print(\"null_ptr is indeed null.\") // Assuming p_int was allocated: if p_int: print(\"p_int is not null.\") . Attempting to read or write through a null pointer will usually crash your program. 7. Releasing Memory: free() . When you’re done with memory you allocated with alloc(), you must release it using the free() method. This returns the “mailboxes” to the system so they can be reused. // After you are finished using p_int and p_floats: p_int.free() p_floats.free() . If you allocated memory for complex objects (those that might have special cleanup code called a destructor, like List[String] in our earlier example), you need to ensure those objects are properly destroyed before freeing the raw memory. | ptr.take_pointee(): Moves the value out of the pointer’s location, leaving the memory uninitialized. The moved-out value can then be managed or destroyed according to its own rules. | ptr.destroy_pointee(): Directly calls the destructor of the object at the pointer’s location. This is used when you don’t need the value anymore. | . For simple types like Int or Float32, just free()-ing the pointer is generally enough as they don’t have complex destructors. // For the List[String] example (see full version in section 2): // After using list_ptr: // var taken_list = list_ptr.take_pointee() // Or list_ptr.destroy_pointee() if not needed // list_ptr.free() . ",
    "url": "/mojo-v1/02_unsafepointer_.html#key-operations-with-unsafepointer",
    
    "relUrl": "/mojo-v1/02_unsafepointer_.html#key-operations-with-unsafepointer"
  },"85": {
    "doc": "Chapter 2: UnsafePointer",
    "title": "The Programmer’s Responsibilities (The “Unsafe” Contract)",
    "content": "Using UnsafePointer means you agree to: . | Manage Lifetime: You alloc, you free. If you get a pointer to existing data, ensure the data outlives the pointer. | Stay In Bounds: Only access memory you’ve actually allocated. ptr[10] is bad if you only alloc(5). | Initialize Before Use: Don’t read from alloc‘d memory until you’ve put something valid there. | Handle Null Pointers: Always check if a pointer could be null before using it, if its validity isn’t guaranteed. | Correctly Destroy Complex Objects: If pointing to objects with destructors, ensure they are destroyed (e.g., via take_pointee() or destroy_pointee()) before freeing their memory. | . Breaking this contract leads to bugs that can be hard to find: crashes, corrupted data, or security vulnerabilities. ",
    "url": "/mojo-v1/02_unsafepointer_.html#the-programmers-responsibilities-the-unsafe-contract",
    
    "relUrl": "/mojo-v1/02_unsafepointer_.html#the-programmers-responsibilities-the-unsafe-contract"
  },"86": {
    "doc": "Chapter 2: UnsafePointer",
    "title": "A Complete, Simple Example",
    "content": "Let’s tie some of this together with simple integers: . from memory import UnsafePointer fn main(): print(\"Chapter 2: UnsafePointer Simple Example\") # 1. Allocate memory for 3 integers # This memory is uninitialized. var count = 3 var int_ptr = UnsafePointer[Int].alloc(count) print(\"Allocated memory for\", count, \"integers at address:\", int_ptr) # 2. Initialize the memory locations (pointees) # For simple types like Int, we can use direct assignment. print(\"Initializing values...\") int_ptr[0] = 100 int_ptr[1] = 200 int_ptr[2] = 300 # 3. Read and print the values print(\"Reading values:\") for i in range(count): print(\"Value at index\", i, \":\", int_ptr[i]) # 4. Modify a value print(\"Modifying value at index 1...\") int_ptr[1] = 250 # 5. Read and print again print(\"Reading values after modification:\") for i in range(count): print(\"Value at index\", i, \":\", int_ptr[i]) # 6. Free the allocated memory # This is crucial! print(\"Freeing memory...\") int_ptr.free() print(\"Memory freed.\") # Accessing int_ptr[0] now would be undefined behavior. # For example, this might crash or print garbage: # print(\"Trying to access freed memory (DANGEROUS):\", int_ptr[0]) . Run this example (you might need to call main() if it’s not in a context where main runs automatically), and you’ll see how you can directly manipulate memory. ",
    "url": "/mojo-v1/02_unsafepointer_.html#a-complete-simple-example",
    
    "relUrl": "/mojo-v1/02_unsafepointer_.html#a-complete-simple-example"
  },"87": {
    "doc": "Chapter 2: UnsafePointer",
    "title": "Why Use UnsafePointer?",
    "content": "If it’s so “unsafe,” why does UnsafePointer exist? . | Building Blocks: It’s a fundamental tool for building safer, higher-level memory abstractions (like List, Dict, and even NDBuffer which uses it internally for its data field). | Interfacing with C/C++: When calling code written in languages like C or C++, you often deal with raw pointers. | Ultimate Performance: In rare, performance-critical situations, direct memory control can eke out the last bits of speed, but only if you know exactly what you’re doing. | . For most everyday Mojo programming, you’ll use safer abstractions. But understanding UnsafePointer gives you insight into what’s happening “under the hood.” . ",
    "url": "/mojo-v1/02_unsafepointer_.html#why-use-unsafepointer",
    
    "relUrl": "/mojo-v1/02_unsafepointer_.html#why-use-unsafepointer"
  },"88": {
    "doc": "Chapter 2: UnsafePointer",
    "title": "Summary",
    "content": ". | UnsafePointer[T] gives you a direct, raw memory address for data of type T. | It’s “unsafe” because it bypasses automatic memory management and bounds checking, making you responsible for correctness. | Key operations include alloc (get memory), [] (read/write), init_pointee_* (initialize complex types), take_pointee/destroy_pointee (handle complex type cleanup), and free (release memory). | It’s crucial for low-level programming, interfacing with C, and building higher-level types. | . ",
    "url": "/mojo-v1/02_unsafepointer_.html#summary",
    
    "relUrl": "/mojo-v1/02_unsafepointer_.html#summary"
  },"89": {
    "doc": "Chapter 2: UnsafePointer",
    "title": "Next Steps",
    "content": "We’ve seen how AddressSpace tells us about memory “neighborhoods” and UnsafePointer gives us a specific “street address.” These are low-level concepts. Next, we’ll start looking at how Mojo builds more structured and often safer ways to handle collections of data, which often use these fundamental tools internally. We’ll begin by exploring IndexList, a helper for working with multi-dimensional indices, essential for understanding types like NDBuffer. | Chapter 1: Understanding Memory Neighborhoods with AddressSpace | Chapter 2: Peeking into Memory with UnsafePointer (You are here) | Chapter 3: Working with Multiple Dimensions: IndexList | Chapter 4: Describing Dimensions: DimList | Chapter 5: The N-Dimensional Buffer: NDBuffer | Chapter 6: N-D to 1D Indexing Logic (Strided Memory Access) | . Generated by AI Codebase Knowledge Builder . ",
    "url": "/mojo-v1/02_unsafepointer_.html#next-steps",
    
    "relUrl": "/mojo-v1/02_unsafepointer_.html#next-steps"
  },"90": {
    "doc": "Chapter 3: Content Extraction Pipeline",
    "title": "Chapter 3: Content Extraction Pipeline",
    "content": "In Chapter 2: AsyncWebCrawler, we learned how to use the AsyncWebCrawler to fetch web content. But what happens to that content after it’s downloaded? How does it go from raw HTML to structured, usable information? Let’s find out! . ",
    "url": "/Crawl4AI/03_content_extraction_pipeline_.html",
    
    "relUrl": "/Crawl4AI/03_content_extraction_pipeline_.html"
  },"91": {
    "doc": "Chapter 3: Content Extraction Pipeline",
    "title": "What is the Content Extraction Pipeline?",
    "content": "Imagine you’re a gold miner. You don’t just find a chunk of earth and call it a day - you need to extract the gold! You’ll wash the dirt away, break down larger rocks, separate the gold from other minerals, and finally polish it into something valuable. The Content Extraction Pipeline works the same way, but for web content. It’s a series of steps that transform messy HTML into clean, structured content that’s ready to use: . flowchart LR A[Raw HTML] --&gt; B[Clean HTML] B --&gt; C[Content Chunks] C --&gt; D[Extracted Information] D --&gt; E[Markdown] style A fill:#f9d5e5 style E fill:#ade8f4 . ",
    "url": "/Crawl4AI/03_content_extraction_pipeline_.html#what-is-the-content-extraction-pipeline",
    
    "relUrl": "/Crawl4AI/03_content_extraction_pipeline_.html#what-is-the-content-extraction-pipeline"
  },"92": {
    "doc": "Chapter 3: Content Extraction Pipeline",
    "title": "A Simple Example",
    "content": "Let’s start with a simple use case: extracting the main article content from a news website and converting it to readable markdown. from crawl4ai import AsyncWebCrawler, CrawlerRunConfig from crawl4ai.content_scraping_strategy import WebScrapingStrategy async def extract_article(): # Set up a configuration with our pipeline components config = CrawlerRunConfig( scraping_strategy=WebScrapingStrategy() ) # Use the crawler with our config async with AsyncWebCrawler() as crawler: result = await crawler.arun( url=\"https://example.com/article\", config=config ) print(result.markdown) . In this code, we’re using the WebScrapingStrategy to clean the HTML, and the default settings for the other pipeline steps. The crawler will return our article as clean markdown! . ",
    "url": "/Crawl4AI/03_content_extraction_pipeline_.html#a-simple-example",
    
    "relUrl": "/Crawl4AI/03_content_extraction_pipeline_.html#a-simple-example"
  },"93": {
    "doc": "Chapter 3: Content Extraction Pipeline",
    "title": "The Four Key Components",
    "content": "The Content Extraction Pipeline has four main components, each handling a specific part of the transformation process: . 1. Content Scraping Strategy . This strategy cleans up the raw HTML by removing ads, navigation menus, footers, and other non-content elements. from crawl4ai.content_scraping_strategy import WebScrapingStrategy # Create a simple scraping strategy scraping_strategy = WebScrapingStrategy() # OR customize it to focus on specific parts of the page custom_strategy = WebScrapingStrategy( css_selector=\"article.main-content\" ) . The scraping strategy is like a filter that separates the gold (main content) from the dirt (ads, menus, etc.). 2. Chunking Strategy . This strategy splits the content into manageable pieces (chunks) that can be processed individually. from crawl4ai.chunking_strategy import RegexChunking # Create a strategy that splits content by paragraphs chunking_strategy = RegexChunking(patterns=[r\"\\n\\n\"]) # OR create a strategy for fixed-length chunks from crawl4ai.chunking_strategy import FixedLengthWordChunking fixed_chunks = FixedLengthWordChunking(chunk_size=100) . Think of the chunking strategy as cutting a large gold nugget into smaller pieces that are easier to process. 3. Extraction Strategy . This strategy extracts specific information from the chunks, like article titles, author names, dates, or any other structured data. from crawl4ai.extraction_strategy import RegexExtractionStrategy # Create a strategy that extracts dates using regex extraction_strategy = RegexExtractionStrategy( patterns={\"date\": r\"\\d{1,2}/\\d{1,2}/\\d{4}\"} ) . The extraction strategy is like identifying and separating different types of precious metals from your raw materials. 4. Markdown Generation Strategy . This strategy converts the processed content into readable markdown format. from crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator # Create a markdown generator with citations markdown_generator = DefaultMarkdownGenerator(citations=True) . The markdown generator is like the final polish that turns your raw gold into a beautiful, finished product. ",
    "url": "/Crawl4AI/03_content_extraction_pipeline_.html#the-four-key-components",
    
    "relUrl": "/Crawl4AI/03_content_extraction_pipeline_.html#the-four-key-components"
  },"94": {
    "doc": "Chapter 3: Content Extraction Pipeline",
    "title": "Putting It All Together",
    "content": "Now let’s see how to use all four components together in a complete pipeline: . from crawl4ai import AsyncWebCrawler, CrawlerRunConfig from crawl4ai.content_scraping_strategy import WebScrapingStrategy from crawl4ai.chunking_strategy import RegexChunking from crawl4ai.extraction_strategy import JsonCssExtractionStrategy from crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator async def extract_article_data(): # 1. Set up each component of our pipeline scraping = WebScrapingStrategy(css_selector=\"article\") chunking = RegexChunking() extraction = JsonCssExtractionStrategy({ \"title\": \"h1\", \"author\": \".author-name\", \"content\": \"article p\" }) markdown_gen = DefaultMarkdownGenerator() # 2. Create a config with our pipeline config = CrawlerRunConfig( scraping_strategy=scraping, chunking_strategy=chunking, extraction_strategy=extraction, markdown_generator=markdown_gen ) # 3. Run the crawler with our config async with AsyncWebCrawler() as crawler: result = await crawler.arun( url=\"https://example.com/article\", config=config ) # 4. Access the extracted content print(f\"Title: {result.extracted_content['title']}\") print(f\"Markdown: {result.markdown}\") . In this example: . | We create each component with specific settings | We build a configuration that includes all our components | We run the crawler with our custom pipeline | We access both the structured data and the markdown output | . ",
    "url": "/Crawl4AI/03_content_extraction_pipeline_.html#putting-it-all-together",
    
    "relUrl": "/Crawl4AI/03_content_extraction_pipeline_.html#putting-it-all-together"
  },"95": {
    "doc": "Chapter 3: Content Extraction Pipeline",
    "title": "Understanding the Default Pipeline",
    "content": "If you don’t specify custom components, crawl4ai uses sensible defaults: . from crawl4ai import AsyncWebCrawler, CrawlerRunConfig async def simple_extraction(): # Using all default pipeline components config = CrawlerRunConfig() async with AsyncWebCrawler() as crawler: result = await crawler.arun( url=\"https://example.com/article\", config=config ) print(result.markdown) . By default: . | It uses WebScrapingStrategy to extract the main content | It uses RegexChunking to split by paragraphs | It doesn’t use an extraction strategy (no structured data) | It uses DefaultMarkdownGenerator for markdown conversion | . ",
    "url": "/Crawl4AI/03_content_extraction_pipeline_.html#understanding-the-default-pipeline",
    
    "relUrl": "/Crawl4AI/03_content_extraction_pipeline_.html#understanding-the-default-pipeline"
  },"96": {
    "doc": "Chapter 3: Content Extraction Pipeline",
    "title": "What Happens Under the Hood",
    "content": "When you run a crawl with a Content Extraction Pipeline, here’s what’s happening step by step: . sequenceDiagram participant User as Your Code participant Crawler as WebCrawler participant ScrapingS as Scraping Strategy participant ES as Extraction Strategy participant MG as Markdown Generator User-&gt;&gt;Crawler: arun(url, config) Crawler-&gt;&gt;Crawler: Fetch HTML from URL Crawler-&gt;&gt;ScrapingS: scrap(url, html) ScrapingS-&gt;&gt;Crawler: Return cleaned HTML Crawler-&gt;&gt;ES: run(url, chunks) ES-&gt;&gt;Crawler: Return structured data Crawler-&gt;&gt;MG: generate_markdown(html) MG-&gt;&gt;Crawler: Return markdown Crawler-&gt;&gt;User: Return CrawlResult . Let’s look at some of the actual implementation: . # From AsyncWebCrawler.aprocess_html method scraping_strategy = config.scraping_strategy result: ScrapingResult = scraping_strategy.scrap(url, html, **params) # Later in the same method if config.extraction_strategy: chunking = config.chunking_strategy sections = chunking.chunk(content) extracted_content = config.extraction_strategy.run(url, sections) . Each strategy is called in sequence, passing the results from one step to the next. ",
    "url": "/Crawl4AI/03_content_extraction_pipeline_.html#what-happens-under-the-hood",
    
    "relUrl": "/Crawl4AI/03_content_extraction_pipeline_.html#what-happens-under-the-hood"
  },"97": {
    "doc": "Chapter 3: Content Extraction Pipeline",
    "title": "Customizing Each Component",
    "content": "Let’s look at how to customize each component in more detail: . Advanced Content Scraping . The WebScrapingStrategy has many options for fine-tuning: . from crawl4ai.content_scraping_strategy import WebScrapingStrategy advanced_scraping = WebScrapingStrategy( css_selector=\"article\", # Focus on specific element min_text_length=100, # Min text length to keep keep_images=True, # Keep images in content extract_metadata=True # Extract page metadata ) . This creates a scraping strategy that focuses on article elements, ignores sections with less than 100 characters, keeps images, and extracts metadata like title and description. Advanced Chunking . For more complex chunking needs: . from crawl4ai.chunking_strategy import SlidingWindowChunking # Create overlapping chunks for better context sliding_chunks = SlidingWindowChunking( window_size=200, # Words per chunk step=100 # Words to move forward ) . This creates chunks that overlap, which can be useful when analyzing text where context matters. Advanced Extraction . For extracting complex structured data: . from crawl4ai.extraction_strategy import JsonLxmlExtractionStrategy # Extract product information from an e-commerce page product_extractor = JsonLxmlExtractionStrategy({ \"name\": \"//h1[@class='product-title']/text()\", \"price\": \"//span[@class='price']/text()\", \"description\": \"//div[@id='description']//text()\", \"specs\": \"//table[@class='specs']\" }) . This uses XPath expressions to extract specific product data from an e-commerce page. Advanced Markdown Generation . For customizing the markdown output: . from crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator from crawl4ai.content_filter_strategy import PruningContentFilter # Generate markdown with citations and content filtering markdown_gen = DefaultMarkdownGenerator( content_filter=PruningContentFilter(), # Remove less relevant content citations=True, # Include citations content_source=\"cleaned_html\" # Use cleaned HTML as source ) . This creates a markdown generator that filters out less relevant content and includes citations for links. ",
    "url": "/Crawl4AI/03_content_extraction_pipeline_.html#customizing-each-component",
    
    "relUrl": "/Crawl4AI/03_content_extraction_pipeline_.html#customizing-each-component"
  },"98": {
    "doc": "Chapter 3: Content Extraction Pipeline",
    "title": "How the Pipeline Processes a Real Web Page",
    "content": "Let’s walk through what happens when you process a typical news article: . | Raw HTML: The crawler fetches the complete HTML of the article page, which includes navigation menus, ads, comments, footers, etc. | Content Scraping: The WebScrapingStrategy analyzes the page and identifies the main article content, removing everything else. Before: &lt;html&gt;...&lt;header&gt;...&lt;/header&gt;&lt;main&gt;&lt;article&gt;Real content&lt;/article&gt;&lt;/main&gt;&lt;footer&gt;...&lt;/footer&gt;...&lt;/html&gt; . After: &lt;article&gt;Real content&lt;/article&gt; . | Chunking: The RegexChunking strategy splits the content into paragraphs. Before: &lt;article&gt;&lt;p&gt;Paragraph 1&lt;/p&gt;&lt;p&gt;Paragraph 2&lt;/p&gt;&lt;/article&gt; . After: [\"&lt;p&gt;Paragraph 1&lt;/p&gt;\", \"&lt;p&gt;Paragraph 2&lt;/p&gt;\"] . | Extraction: The JsonCssExtractionStrategy extracts specific information using CSS selectors. Input: [\"&lt;p&gt;Paragraph 1&lt;/p&gt;\", \"&lt;p&gt;Paragraph 2&lt;/p&gt;\"] . Output: {\"content\": [\"Paragraph 1\", \"Paragraph 2\"]} . | Markdown Generation: The DefaultMarkdownGenerator converts the HTML to markdown. Input: &lt;article&gt;&lt;p&gt;Paragraph 1&lt;/p&gt;&lt;p&gt;Paragraph 2&lt;/p&gt;&lt;/article&gt; . Output: Paragraph 1\\n\\nParagraph 2 . | . The final CrawlResult includes both the markdown output and any structured data that was extracted. ",
    "url": "/Crawl4AI/03_content_extraction_pipeline_.html#how-the-pipeline-processes-a-real-web-page",
    
    "relUrl": "/Crawl4AI/03_content_extraction_pipeline_.html#how-the-pipeline-processes-a-real-web-page"
  },"99": {
    "doc": "Chapter 3: Content Extraction Pipeline",
    "title": "Implementation Details",
    "content": "The Content Extraction Pipeline is implemented through a series of strategy classes. Let’s look at some of the implementation details: . ContentScrapingStrategy . The base class for scraping strategies: . # From content_scraping_strategy.py class ContentScrapingStrategy(ABC): @abstractmethod def scrap(self, url: str, html: str, **kwargs) -&gt; ScrapingResult: \"\"\"Scrape content from HTML.\"\"\" pass . Different implementations use various techniques: . | WebScrapingStrategy uses readability algorithms | LXMLWebScrapingStrategy uses LXML for HTML parsing | . ChunkingStrategy . The base class for chunking strategies: . # From chunking_strategy.py class ChunkingStrategy(ABC): @abstractmethod def chunk(self, text: str) -&gt; list: \"\"\"Chunk the given text.\"\"\" pass . The implementation shown in the code you provided includes: . | RegexChunking for pattern-based splitting | FixedLengthWordChunking for fixed-size chunks | SlidingWindowChunking for overlapping chunks | . ExtractionStrategy . The base class for extraction strategies: . # Hypothetical from extraction_strategy.py class ExtractionStrategy(ABC): @abstractmethod def run(self, url: str, chunks: list) -&gt; dict: \"\"\"Extract structured data from chunks.\"\"\" pass . Different implementations support different methods: . | RegexExtractionStrategy for pattern matching | JsonCssExtractionStrategy for CSS selector-based extraction | LLMExtractionStrategy for using language models | . MarkdownGenerationStrategy . The base class for markdown generators: . # From markdown_generation_strategy.py class MarkdownGenerationStrategy(ABC): @abstractmethod def generate_markdown(self, input_html: str, **kwargs) -&gt; MarkdownGenerationResult: \"\"\"Generate markdown from HTML.\"\"\" pass . The DefaultMarkdownGenerator implementation handles: . | Converting HTML to markdown | Adding citations for links | Filtering content if a content filter is provided | . ",
    "url": "/Crawl4AI/03_content_extraction_pipeline_.html#implementation-details",
    
    "relUrl": "/Crawl4AI/03_content_extraction_pipeline_.html#implementation-details"
  },"100": {
    "doc": "Chapter 3: Content Extraction Pipeline",
    "title": "Conclusion",
    "content": "The Content Extraction Pipeline is a powerful system that transforms raw HTML into useful content through a series of specialized steps. By understanding and customizing each component, you can extract exactly the information you need from any web page. In this chapter, we’ve learned: . | How the pipeline works as a sequence of processing steps | The four key components: scraping, chunking, extraction, and markdown generation | How to customize each component for specific needs | How to use the complete pipeline in your crawl4ai applications | . In the next chapter, URL Filtering &amp; Scoring, we’ll explore how to decide which URLs to crawl and how to prioritize them, which is essential for building efficient web crawlers. Generated by AI Codebase Knowledge Builder . ",
    "url": "/Crawl4AI/03_content_extraction_pipeline_.html#conclusion",
    
    "relUrl": "/Crawl4AI/03_content_extraction_pipeline_.html#conclusion"
  },"101": {
    "doc": "CrawlerRunConfig",
    "title": "Chapter 3: Giving Instructions - CrawlerRunConfig",
    "content": "In Chapter 2: Meet the General Manager - AsyncWebCrawler, we met the AsyncWebCrawler, the central coordinator for our web crawling tasks. We saw how to tell it what URL to crawl using the arun method. But what if we want to tell the crawler how to crawl that URL? Maybe we want it to take a picture (screenshot) of the page? Or perhaps we only care about a specific section of the page? Or maybe we want to ignore the cache and get the very latest version? . Passing all these different instructions individually every time we call arun could get complicated and messy. # Imagine doing this every time - it gets long! # result = await crawler.arun( # url=\"https://example.com\", # take_screenshot=True, # ignore_cache=True, # only_look_at_this_part=\"#main-content\", # wait_for_this_element=\"#data-table\", # # ... maybe many more settings ... # ) . That’s where CrawlerRunConfig comes in! . ",
    "url": "/Crawl4AI/03_crawlerrunconfig.html#chapter-3-giving-instructions---crawlerrunconfig",
    
    "relUrl": "/Crawl4AI/03_crawlerrunconfig.html#chapter-3-giving-instructions---crawlerrunconfig"
  },"102": {
    "doc": "CrawlerRunConfig",
    "title": "What Problem Does CrawlerRunConfig Solve?",
    "content": "Think of CrawlerRunConfig as the Instruction Manual for a specific crawl job. Instead of giving the AsyncWebCrawler manager lots of separate instructions each time, you bundle them all neatly into a single CrawlerRunConfig object. This object tells the AsyncWebCrawler exactly how to handle a particular URL or set of URLs for that specific run. It makes your code cleaner and easier to manage. ",
    "url": "/Crawl4AI/03_crawlerrunconfig.html#what-problem-does-crawlerrunconfig-solve",
    
    "relUrl": "/Crawl4AI/03_crawlerrunconfig.html#what-problem-does-crawlerrunconfig-solve"
  },"103": {
    "doc": "CrawlerRunConfig",
    "title": "What is CrawlerRunConfig?",
    "content": "CrawlerRunConfig is a configuration class that holds all the settings for a single crawl operation initiated by AsyncWebCrawler.arun() or arun_many(). It allows you to customize various aspects of the crawl, such as: . | Taking Screenshots: Should the crawler capture an image of the page? (screenshot) | Waiting: How long should the crawler wait for the page or specific elements to load? (page_timeout, wait_for) | Focusing Content: Should the crawler only process a specific part of the page? (css_selector) | Extracting Data: Should the crawler use a specific method to pull out structured data? (ExtractionStrategy) | Caching: How should the crawler interact with previously saved results? (CacheMode) | And much more! (like handling JavaScript, filtering links, etc.) | . ",
    "url": "/Crawl4AI/03_crawlerrunconfig.html#what-is-crawlerrunconfig",
    
    "relUrl": "/Crawl4AI/03_crawlerrunconfig.html#what-is-crawlerrunconfig"
  },"104": {
    "doc": "CrawlerRunConfig",
    "title": "Using CrawlerRunConfig",
    "content": "Let’s see how to use it. Remember our basic crawl from Chapter 2? . # chapter3_example_1.py import asyncio from crawl4ai import AsyncWebCrawler async def main(): async with AsyncWebCrawler() as crawler: url_to_crawl = \"https://httpbin.org/html\" print(f\"Crawling {url_to_crawl} with default settings...\") # This uses the default behavior (no specific config) result = await crawler.arun(url=url_to_crawl) if result.success: print(\"Success! Got the content.\") print(f\"Screenshot taken? {'Yes' if result.screenshot else 'No'}\") # Likely No # We'll learn about CacheMode later, but it defaults to using the cache else: print(f\"Failed: {result.error_message}\") if __name__ == \"__main__\": asyncio.run(main()) . Now, let’s say for this specific crawl, we want to bypass the cache (fetch fresh) and also take a screenshot. We create a CrawlerRunConfig instance and pass it to arun: . # chapter3_example_2.py import asyncio from crawl4ai import AsyncWebCrawler from crawl4ai import CrawlerRunConfig # 1. Import the config class from crawl4ai import CacheMode # Import cache options async def main(): async with AsyncWebCrawler() as crawler: url_to_crawl = \"https://httpbin.org/html\" print(f\"Crawling {url_to_crawl} with custom settings...\") # 2. Create an instance of CrawlerRunConfig with our desired settings my_instructions = CrawlerRunConfig( cache_mode=CacheMode.BYPASS, # Don't use the cache, fetch fresh screenshot=True # Take a screenshot ) print(\"Instructions: Bypass cache, take screenshot.\") # 3. Pass the config object to arun() result = await crawler.arun( url=url_to_crawl, config=my_instructions # Pass our instruction manual ) if result.success: print(\"\\nSuccess! Got the content with custom config.\") print(f\"Screenshot taken? {'Yes' if result.screenshot else 'No'}\") # Should be Yes # Check if the screenshot file path exists in result.screenshot if result.screenshot: print(f\"Screenshot saved to: {result.screenshot}\") else: print(f\"\\nFailed: {result.error_message}\") if __name__ == \"__main__\": asyncio.run(main()) . Explanation: . | Import: We import CrawlerRunConfig and CacheMode. | Create Config: We create an instance: my_instructions = CrawlerRunConfig(...). We set cache_mode to CacheMode.BYPASS and screenshot to True. All other settings remain at their defaults. | Pass Config: We pass this my_instructions object to crawler.arun using the config= parameter. | . Now, when AsyncWebCrawler runs this job, it will look inside my_instructions and follow those specific settings for this run only. ",
    "url": "/Crawl4AI/03_crawlerrunconfig.html#using-crawlerrunconfig",
    
    "relUrl": "/Crawl4AI/03_crawlerrunconfig.html#using-crawlerrunconfig"
  },"105": {
    "doc": "CrawlerRunConfig",
    "title": "Some Common CrawlerRunConfig Parameters",
    "content": "CrawlerRunConfig has many options, but here are a few common ones you might use: . | cache_mode: Controls caching behavior. | CacheMode.ENABLED (Default): Use the cache if available, otherwise fetch and save. | CacheMode.BYPASS: Always fetch fresh, ignoring any cached version (but still save the new result). | CacheMode.DISABLED: Never read from or write to the cache. | (More details in Chapter 9: Smart Fetching with Caching - CacheContext / CacheMode) | . | screenshot (bool): If True, takes a screenshot of the fully rendered page. The path to the screenshot file will be in CrawlResult.screenshot. Default: False. | pdf (bool): If True, generates a PDF of the page. The path to the PDF file will be in CrawlResult.pdf. Default: False. | css_selector (str): If provided (e.g., \"#main-content\" or .article-body), the crawler will try to extract only the HTML content within the element(s) matching this CSS selector. This is great for focusing on the important part of a page. Default: None (process the whole page). | wait_for (str): A CSS selector (e.g., \"#data-loaded-indicator\"). The crawler will wait until an element matching this selector appears on the page before proceeding. Useful for pages that load content dynamically with JavaScript. Default: None. | page_timeout (int): Maximum time in milliseconds to wait for page navigation or certain operations. Default: 60000 (60 seconds). | extraction_strategy: An object that defines how to extract specific, structured data (like product names and prices) from the page. Default: None. (See Chapter 6: Getting Specific Data - ExtractionStrategy) | scraping_strategy: An object defining how the raw HTML is cleaned and basic content (like text and links) is extracted. Default: WebScrapingStrategy(). (See Chapter 4: Cleaning Up the Mess - ContentScrapingStrategy) | . Let’s try combining a few: focus on a specific part of the page and wait for something to appear. # chapter3_example_3.py import asyncio from crawl4ai import AsyncWebCrawler, CrawlerRunConfig async def main(): # This example site has a heading 'H1' inside a 'body' tag. url_to_crawl = \"https://httpbin.org/html\" async with AsyncWebCrawler() as crawler: print(f\"Crawling {url_to_crawl}, focusing on the H1 tag...\") # Instructions: Only get the H1 tag, wait max 10s for it specific_config = CrawlerRunConfig( css_selector=\"h1\", # Only grab content inside &lt;h1&gt; tags page_timeout=10000 # Set page timeout to 10 seconds # We could also add wait_for=\"h1\" if needed for dynamic loading ) result = await crawler.arun(url=url_to_crawl, config=specific_config) if result.success: print(\"\\nSuccess! Focused crawl completed.\") # The markdown should now ONLY contain the H1 content print(f\"Markdown content:\\n---\\n{result.markdown.raw_markdown.strip()}\\n---\") else: print(f\"\\nFailed: {result.error_message}\") if __name__ == \"__main__\": asyncio.run(main()) . This time, the result.markdown should only contain the text from the &lt;h1&gt; tag on that page, because we used css_selector=\"h1\" in our CrawlerRunConfig. ",
    "url": "/Crawl4AI/03_crawlerrunconfig.html#some-common-crawlerrunconfig-parameters",
    
    "relUrl": "/Crawl4AI/03_crawlerrunconfig.html#some-common-crawlerrunconfig-parameters"
  },"106": {
    "doc": "CrawlerRunConfig",
    "title": "How AsyncWebCrawler Uses the Config (Under the Hood)",
    "content": "You don’t need to know the exact internal code, but it helps to understand the flow. When you call crawler.arun(url, config=my_config), the AsyncWebCrawler essentially does this: . | Receives the url and the my_config object. | Before fetching, it checks my_config.cache_mode to see if it should look in the cache first. | If fetching is needed, it passes my_config to the underlying AsyncCrawlerStrategy. | The strategy uses settings from my_config like page_timeout, wait_for, and whether to take a screenshot. | After getting the raw HTML, AsyncWebCrawler uses the my_config.scraping_strategy and my_config.css_selector to process the content. | If my_config.extraction_strategy is set, it uses that to extract structured data. | Finally, it bundles everything into a CrawlResult and returns it. | . Here’s a simplified view: . sequenceDiagram participant User participant AWC as AsyncWebCrawler participant Config as CrawlerRunConfig participant Fetcher as AsyncCrawlerStrategy participant Processor as Scraping/Extraction User-&gt;&gt;AWC: arun(url, config=my_config) AWC-&gt;&gt;Config: Check my_config.cache_mode alt Need to Fetch AWC-&gt;&gt;Fetcher: crawl(url, config=my_config) Note over Fetcher: Uses my_config settings (timeout, wait_for, screenshot...) Fetcher--&gt;&gt;AWC: Raw Response (HTML, screenshot?) AWC-&gt;&gt;Processor: Process HTML (using my_config.css_selector, my_config.extraction_strategy...) Processor--&gt;&gt;AWC: Processed Data else Use Cache AWC-&gt;&gt;AWC: Retrieve from Cache end AWC--&gt;&gt;User: Return CrawlResult . The CrawlerRunConfig acts as a messenger carrying your specific instructions throughout the crawling process. Inside the crawl4ai library, in the file async_configs.py, you’ll find the definition of the CrawlerRunConfig class. It looks something like this (simplified): . # Simplified from crawl4ai/async_configs.py from .cache_context import CacheMode from .extraction_strategy import ExtractionStrategy from .content_scraping_strategy import ContentScrapingStrategy, WebScrapingStrategy # ... other imports ... class CrawlerRunConfig(): \"\"\" Configuration class for controlling how the crawler runs each crawl operation. \"\"\" def __init__( self, # Caching cache_mode: CacheMode = CacheMode.BYPASS, # Default behavior if not specified # Content Selection / Waiting css_selector: str = None, wait_for: str = None, page_timeout: int = 60000, # 60 seconds # Media screenshot: bool = False, pdf: bool = False, # Processing Strategies scraping_strategy: ContentScrapingStrategy = None, # Defaults internally if None extraction_strategy: ExtractionStrategy = None, # ... many other parameters omitted for clarity ... **kwargs # Allows for flexibility ): self.cache_mode = cache_mode self.css_selector = css_selector self.wait_for = wait_for self.page_timeout = page_timeout self.screenshot = screenshot self.pdf = pdf # Assign scraping strategy, ensuring a default if None is provided self.scraping_strategy = scraping_strategy or WebScrapingStrategy() self.extraction_strategy = extraction_strategy # ... initialize other attributes ... # Helper methods like 'clone', 'to_dict', 'from_kwargs' might exist too # ... The key idea is that it’s a class designed to hold various settings together. When you create an instance CrawlerRunConfig(...), you’re essentially creating an object that stores your choices for these parameters. ",
    "url": "/Crawl4AI/03_crawlerrunconfig.html#how-asyncwebcrawler-uses-the-config-under-the-hood",
    
    "relUrl": "/Crawl4AI/03_crawlerrunconfig.html#how-asyncwebcrawler-uses-the-config-under-the-hood"
  },"107": {
    "doc": "CrawlerRunConfig",
    "title": "Conclusion",
    "content": "You’ve learned about CrawlerRunConfig, the “Instruction Manual” for individual crawl jobs in Crawl4AI! . | It solves the problem of passing many settings individually to AsyncWebCrawler. | You create an instance of CrawlerRunConfig and set the parameters you want to customize (like cache_mode, screenshot, css_selector, wait_for). | You pass this config object to crawler.arun(url, config=your_config). | This makes your code cleaner and gives you fine-grained control over how each crawl is performed. | . Now that we know how to fetch content (AsyncCrawlerStrategy), manage the overall process (AsyncWebCrawler), and give specific instructions (CrawlerRunConfig), let’s look at how the raw, messy HTML fetched from the web is initially cleaned up and processed. Next: Let’s explore Chapter 4: Cleaning Up the Mess - ContentScrapingStrategy. Generated by AI Codebase Knowledge Builder . ",
    "url": "/Crawl4AI/03_crawlerrunconfig.html#conclusion",
    
    "relUrl": "/Crawl4AI/03_crawlerrunconfig.html#conclusion"
  },"108": {
    "doc": "CrawlerRunConfig",
    "title": "CrawlerRunConfig",
    "content": " ",
    "url": "/Crawl4AI/03_crawlerrunconfig.html",
    
    "relUrl": "/Crawl4AI/03_crawlerrunconfig.html"
  },"109": {
    "doc": "Chapter 3: IndexList",
    "title": "Chapter 3: Working with Multiple Dimensions: IndexList",
    "content": "In the previous chapters, we explored some of Mojo’s fundamental building blocks for memory management, like AddressSpace for understanding different memory regions and UnsafePointer for directly accessing memory locations. Now, we’re going to shift our focus to a common task in programming, especially in fields like graphics, scientific computing, and AI: working with multi-dimensional data. Imagine you’re working with an image. An image is 2-dimensional (it has a width and a height). To specify a single pixel in that image, you need two numbers: a row and a column. Or, think about a 3D model; you’d need three coordinates (x, y, z) to define a point in space. Mojo provides a handy tool for exactly this: IndexList. ",
    "url": "/mojo-v1/03_indexlist_.html#chapter-3-working-with-multiple-dimensions-indexlist",
    
    "relUrl": "/mojo-v1/03_indexlist_.html#chapter-3-working-with-multiple-dimensions-indexlist"
  },"110": {
    "doc": "Chapter 3: IndexList",
    "title": "What is IndexList?",
    "content": "IndexList is a fixed-size list, much like a tuple in Python. Its primary job is to represent multi-dimensional coordinates (indices) or the shape of an N-dimensional array. Think of it like this: . | Coordinates on a map: If you have a 2D map (like an image), an IndexList could hold (row, column) to pinpoint a specific location (a pixel). | Dimensions of a box: If you have a 3D box (like a 3D array of data), an IndexList could hold (depth, height, width) to describe its size. | . It’s a simple but powerful structure that helps us talk about locations and sizes in multi-dimensional spaces. As we’ll see in later chapters, IndexList is used internally by more complex types like NDBuffer (Mojo’s N-dimensional buffer, similar to a NumPy array or a tensor) to store its dimensions and the “strides” that help calculate memory offsets. ",
    "url": "/mojo-v1/03_indexlist_.html#what-is-indexlist",
    
    "relUrl": "/mojo-v1/03_indexlist_.html#what-is-indexlist"
  },"111": {
    "doc": "Chapter 3: IndexList",
    "title": "Why Do We Need IndexList?",
    "content": "You might wonder, “Can’t I just use a regular tuple of integers?” While you could, IndexList offers several advantages: . | Clarity and Type Safety: Using IndexList makes your code more explicit about its intent. When you see an IndexList, you know it represents a set of indices or a shape. This is clearer than a generic tuple. Mojo’s type system can also leverage this specificity. | Fixed Size at Compile Time: A key feature of IndexList is that its size (the number of dimensions it holds) is known when your Mojo program is compiled. This allows Mojo to perform many optimizations for better performance. | Specialized Operations: IndexList is designed for working with indices and comes with useful built-in functionalities like element-wise arithmetic and type casting. | . ",
    "url": "/mojo-v1/03_indexlist_.html#why-do-we-need-indexlist",
    
    "relUrl": "/mojo-v1/03_indexlist_.html#why-do-we-need-indexlist"
  },"112": {
    "doc": "Chapter 3: IndexList",
    "title": "Creating an IndexList",
    "content": "There are a couple of common ways to create an IndexList. 1. Direct Initialization: IndexList[size](...) . You can directly create an IndexList by specifying its size (the number of elements it will hold) as a compile-time parameter, and then providing the values. from utils import IndexList from builtin import DType // For specifying element types fn main_direct_init(): // A 2D coordinate (e.g., row, column) // Size is 2. Elements are (by default) 64-bit integers. var coord2d = IndexList[2](10, 25) print(\"2D Coordinate:\", coord2d) // Output: (10, 25) // A 3D shape (e.g., depth, height, width) // Size is 3. var shape3d = IndexList[3](3, 64, 128) print(\"3D Shape:\", shape3d) // Output: (3, 64, 128) // You can also specify the element type. // Here, we use 32-bit integers. var coord2d_int32 = IndexList[2, element_type=DType.int32](5, 15) print(\"2D Coordinate (Int32):\", coord2d_int32) // Output: (5, 15) . | IndexList[size]: size is a compile-time constant telling Mojo how many elements this list will hold. | element_type: This optional parameter specifies the data type of the numbers within the IndexList. If you don’t specify it, it defaults to DType.int64 (a 64-bit integer). IndexList is designed to hold integral types. | . 2. Using the Index() Factory Function . Mojo also provides a convenient Index() factory function (which you import from utils) that often feels more natural, especially if you’re used to Python. It infers the size from the number of arguments you provide. from utils import Index, IndexList // IndexList is often good to have for type annotations from builtin import DType fn main_factory_func(): // The Index() function infers the size var point_a = Index(100, 200) // Creates an IndexList[2] print(\"Point A:\", point_a) // Output: (100, 200) var dimensions = Index(800, 600, 3) // Creates an IndexList[3] print(\"Dimensions:\", dimensions) // Output: (800, 600, 3) // You can also specify the element type with Index() var point_b_int32 = Index[dtype=DType.int32](50, 75) print(\"Point B (Int32):\", point_b_int32) // Output: (50, 75) . The Index() function is often a more concise way to create IndexList instances. ",
    "url": "/mojo-v1/03_indexlist_.html#creating-an-indexlist",
    
    "relUrl": "/mojo-v1/03_indexlist_.html#creating-an-indexlist"
  },"113": {
    "doc": "Chapter 3: IndexList",
    "title": "Working with IndexList",
    "content": "Once you have an IndexList, here are some common things you can do: . Accessing Elements . You can access individual elements of an IndexList using the square bracket [] operator, just like with Python lists or tuples. Indexing starts at 0. from utils import Index fn main_access_elements(): var my_coords = Index(15, 30, 45) // An IndexList[3] var x = my_coords[0] // Get the first element var y = my_coords[1] // Get the second element var z = my_coords[2] // Get the third element print(\"X:\", x) // Output: X: 15 print(\"Y:\", y) // Output: Y: 30 print(\"Z:\", z) // Output: Z: 45 // You can also modify elements if the IndexList is mutable (declared with `var`) my_coords[0] = 16 print(\"Modified X:\", my_coords[0]) // Output: Modified X: 16 . Getting the Length (Size) . The len() function tells you how many elements are in the IndexList (which matches the size parameter it was created with). from utils import Index fn main_get_length(): var coord2d = Index(10, 20) var shape3d = Index(5, 8, 12) print(\"Length of coord2d:\", len(coord2d)) // Output: Length of coord2d: 2 print(\"Length of shape3d:\", len(shape3d)) // Output: Length of shape3d: 3 . Converting to String (Printing) . IndexList knows how to represent itself as a string, so you can easily print it. from utils import Index, IndexList fn main_to_string(): var il_multi = Index(1, 2, 3) print(il_multi) // Output: (1, 2, 3) // For single-element IndexLists, it prints with a trailing comma, // just like single-element tuples in Python. var il_single = IndexList[1](42) print(il_single) // Output: (42,) . Comparing IndexLists . You can compare two IndexLists for equality (==) or inequality (!=). The comparison is element-wise: two IndexLists are equal if they have the same size and all their corresponding elements are equal. from utils import Index fn main_compare(): var p1 = Index(10, 20) var p2 = Index(10, 20) var p3 = Index(10, 30) if p1 == p2: print(\"p1 is equal to p2\") // This will print else: print(\"p1 is not equal to p2\") if p1 == p3: print(\"p1 is equal to p3\") else: print(\"p1 is not equal to p3\") // This will print . IndexList also supports other comparison operators like &lt;, &lt;=, &gt;, &gt;=. These also perform element-wise comparisons, meaning all elements must satisfy the condition for the overall result to be true (this is different from a lexicographical comparison you might see in Python tuples). Casting Element Types . Sometimes, you might have an IndexList with one integer type (e.g., DType.int64) but need to convert it to another (e.g., DType.int32). The cast[NewDType]() method lets you do this. from utils import Index from builtin import DType fn main_casting(): var original_indices = Index(100, 200, -50) // Default DType.int64 print(\"Original:\", original_indices) // Cast to DType.int32 var casted_indices_i32 = original_indices.cast[DType.int32]() print(\"Casted to Int32:\", casted_indices_i32) // Note: The print output looks the same, but internally // the elements are now 32-bit integers. // Cast to DType.uint64 (unsigned 64-bit integer) // Be careful with signed to unsigned casts if there are negative numbers! // For positive numbers, it's usually fine. var positive_indices = Index(5, 10) var casted_indices_u64 = positive_indices.cast[DType.uint64]() print(\"Casted to UInt64:\", casted_indices_u64) . Casting is useful when interfacing with functions or data structures that expect indices of a particular integer type. Arithmetic Operations . IndexList supports element-wise arithmetic operations like addition (+), subtraction (-), multiplication (*), and floor division (//). from utils import Index fn main_arithmetic(): var v1 = Index(10, 20, 30) var v2 = Index(2, 3, 4) var sum_v = v1 + v2 print(\"v1 + v2 =\", sum_v) // Output: v1 + v2 = (12, 23, 34) var diff_v = v1 - v2 print(\"v1 - v2 =\", diff_v) // Output: v1 - v2 = (8, 17, 26) var prod_v = v1 * v2 print(\"v1 * v2 =\", prod_v) // Output: v1 * v2 = (20, 60, 120) var div_v = v1 // v2 print(\"v1 // v2 =\", div_v) // Output: v1 // v2 = (5, 6, 7) . These operations create a new IndexList containing the results. ",
    "url": "/mojo-v1/03_indexlist_.html#working-with-indexlist",
    
    "relUrl": "/mojo-v1/03_indexlist_.html#working-with-indexlist"
  },"114": {
    "doc": "Chapter 3: IndexList",
    "title": "A Practical Example",
    "content": "Let’s put some of these concepts together. Imagine we’re working with a 2D grid. from utils import Index, IndexList from builtin import DType fn main(): print(\"--- IndexList Practical Example ---\") # Define the starting coordinates of an object on a grid var start_pos = Index(5, 10) # An IndexList[2] of Int64 print(\"Starting position:\", start_pos) # Define a movement vector var move_vector = Index(3, -2) print(\"Movement vector:\", move_vector) # Calculate the new position by adding the start_pos and move_vector var end_pos = start_pos + move_vector print(\"Ending position:\", end_pos) # Expected: (5+3, 10-2) = (8, 8) # Define the dimensions of a small tile var tile_shape = IndexList[2, element_type=DType.int32](16, 16) print(\"Tile shape (Int32):\", tile_shape) # Access elements print(\"Starting X:\", start_pos[0]) print(\"Starting Y:\", start_pos[1]) print(\"Tile width:\", tile_shape[0]) # Check length print(\"Dimensions in start_pos:\", len(start_pos)) print(\"--- End of Practical Example ---\") . ",
    "url": "/mojo-v1/03_indexlist_.html#a-practical-example",
    
    "relUrl": "/mojo-v1/03_indexlist_.html#a-practical-example"
  },"115": {
    "doc": "Chapter 3: IndexList",
    "title": "Key Takeaways",
    "content": ". | IndexList is a fixed-size list (like a tuple) used for representing multi-dimensional indices or shapes. | Its size is a compile-time parameter, enabling optimizations: IndexList[size](...). | The Index(...) factory function is a convenient way to create them, inferring the size. | Elements are accessed using [] (e.g., my_list[0]). | Supports common operations: len(), string conversion, comparisons (==, !=, etc.), type casting (.cast[DType]()), and element-wise arithmetic (+, -, *, //). | It plays a crucial role in defining and accessing data in multi-dimensional structures like NDBuffer. | . ",
    "url": "/mojo-v1/03_indexlist_.html#key-takeaways",
    
    "relUrl": "/mojo-v1/03_indexlist_.html#key-takeaways"
  },"116": {
    "doc": "Chapter 3: IndexList",
    "title": "What’s Next?",
    "content": "Understanding IndexList is a stepping stone. It helps us define how many dimensions something has and what its size is in each of those dimensions. In the upcoming chapters, we’ll see how types like DimList (another way to describe dimensions, often with more static information) and ultimately NDBuffer build upon these concepts to provide powerful ways to work with N-dimensional data in Mojo. Index of Chapters (So Far): . | Chapter 1: Understanding Memory Neighborhoods with AddressSpace | Chapter 2: Peeking into Memory with UnsafePointer | Chapter 3: Working with Multiple Dimensions: IndexList (You are here) | Coming Soon: Chapter 4: Describing Dimensions: DimList | Coming Soon: Chapter 5: The N-Dimensional Buffer: NDBuffer | Coming Soon: Chapter 6: N-D to 1D Indexing Logic (Strided Memory Access) ``` | . Generated by AI Codebase Knowledge Builder . ",
    "url": "/mojo-v1/03_indexlist_.html#whats-next",
    
    "relUrl": "/mojo-v1/03_indexlist_.html#whats-next"
  },"117": {
    "doc": "Chapter 3: IndexList",
    "title": "Chapter 3: IndexList",
    "content": " ",
    "url": "/mojo-v1/03_indexlist_.html",
    
    "relUrl": "/mojo-v1/03_indexlist_.html"
  },"118": {
    "doc": "Chapter 3: LLM Pipeline Orchestrator",
    "title": "Chapter 3: LLM Pipeline Orchestrator (TokenGeneratorPipeline)",
    "content": "In Chapter 2: Serving API Layer (FastAPI App &amp; Routers), we saw how modular receives requests from the outside world, like a receptionist directing calls. But once a request like “tell me a story about a brave knight” arrives, who actually takes that request, gets it ready for the language model, and makes sure the story comes back? . That’s where the TokenGeneratorPipeline comes in! It’s the general manager for a specific language model. ",
    "url": "/modular_max/03_llm_pipeline_orchestrator___tokengeneratorpipeline___.html#chapter-3-llm-pipeline-orchestrator-tokengeneratorpipeline",
    
    "relUrl": "/modular_max/03_llm_pipeline_orchestrator___tokengeneratorpipeline___.html#chapter-3-llm-pipeline-orchestrator-tokengeneratorpipeline"
  },"119": {
    "doc": "Chapter 3: LLM Pipeline Orchestrator",
    "title": "What Problem Does the TokenGeneratorPipeline Solve?",
    "content": "Imagine you’ve asked modular to write that story. The API Layer has accepted your request. Now what? . | Your request (“tell me a story…”) is in human language. The AI model doesn’t understand English directly; it understands “tokens” (like special code words or numbers). | The AI model itself is a powerful but specialized piece of machinery (the Model Worker, which we’ll cover later). It needs to be told exactly what to do and how. | Once the model starts generating the story, it might produce it word by word (or token by token). Someone needs to collect these pieces and send them back to you, either as they come or all at once. | . The TokenGeneratorPipeline acts like an assembly line manager for this whole process. It takes the raw material (your prompt), ensures it’s correctly prepared, sends it to the machinery (the model worker), and then makes sure the finished product (the generated story) is delivered efficiently. It handles: . | Preparation: Using a “tokenizer” to translate your human-language prompt into model-understandable tokens. | Coordination: Sending this prepared request to the correct Model Worker via an EngineQueue (think of it as a high-tech conveyor belt). | Delivery: Streaming the model’s answer back token by token, or as a complete response. | . ",
    "url": "/modular_max/03_llm_pipeline_orchestrator___tokengeneratorpipeline___.html#what-problem-does-the-tokengeneratorpipeline-solve",
    
    "relUrl": "/modular_max/03_llm_pipeline_orchestrator___tokengeneratorpipeline___.html#what-problem-does-the-tokengeneratorpipeline-solve"
  },"120": {
    "doc": "Chapter 3: LLM Pipeline Orchestrator",
    "title": "Meet the TokenGeneratorPipeline",
    "content": "The TokenGeneratorPipeline is the central orchestrator for getting text from a Large Language Model (LLM). Let’s look at its key collaborators: . | Tokenizer (PipelineTokenizer): This is like a specialized translator. It converts human-readable text (your prompt) into a sequence of numbers (tokens) that the LLM can understand. It also does the reverse: converts tokens from the LLM back into human-readable text. | EngineQueue: This is a communication channel. The TokenGeneratorPipeline places the tokenized request into this queue, and the Model Worker picks it up from there. It’s also used to send generated tokens back from the worker. | Model Worker: This is the component that actually runs the LLM to generate new tokens based on the input. We’ll explore this in Chapter 4: Model Worker. | . How It’s Set Up . In Chapter 2: Serving API Layer (FastAPI App &amp; Routers), we saw that the main FastAPI app has a “lifespan” function. This is where the TokenGeneratorPipeline is typically created and made available to the API endpoint handlers. # Simplified from: src/max/serve/api_server.py # (inside the `lifespan` async context manager) # ... pipeline: TokenGeneratorPipeline = TokenGeneratorPipeline( model_name=\"my-cool-model\", # Name of the model being served tokenizer=my_tokenizer, # The translator we talked about engine_queue=engine_queue, # The conveyor belt to the Model Worker ) app.state.pipeline = pipeline # Make it accessible to API endpoints # ... This means that when an API endpoint (like /v1/chat/completions) needs to generate text, it can access this pipeline object. Receiving a Request and Generating Text . When an API endpoint (like openai_create_chat_completion from src/max/serve/router/openai_routes.py) receives a user’s request, it prepares a TokenGeneratorRequest object. This object is like a work order for the pipeline. Let’s look at what’s in a TokenGeneratorRequest: . # Simplified from: src/max/pipelines/core/interfaces/text_generation.py @dataclass(frozen=True) class TokenGeneratorRequest: id: str # A unique ID for this specific request model_name: str # Which model to use messages: Optional[list[TokenGeneratorRequestMessage]] = None # For chat prompts prompt: Union[str, Sequence[int], None] = None # For simple prompts max_new_tokens: Optional[int] = None # How many tokens to generate at most # ... other fields like timestamp, tools, etc. This TokenGeneratorRequest contains all the information the pipeline needs. The API endpoint then calls a method on the TokenGeneratorPipeline, usually next_token() for streaming responses or all_tokens() for a complete response. # Simplified example of how an API route might use the pipeline # (Inspired by src/max/serve/router/openai_routes.py) # async def openai_create_chat_completion(request: Request): # ... (code to parse user's HTTP request into `my_chat_request_data`) ... # pipeline is taken from request.app.state.pipeline # pipeline: TokenGeneratorPipeline = request.app.state.pipeline # 1. Create the work order (TokenGeneratorRequest) token_gen_request = TokenGeneratorRequest( id=\"unique_req_abc123\", model_name=pipeline.model_name, messages=[{\"role\": \"user\", \"content\": \"Tell me a short story.\"}], max_new_tokens=50 ) # 2. Ask the pipeline to generate text (streaming token by token) # response_stream = pipeline.next_token(token_gen_request) # async for generated_output_chunk in response_stream: # # generated_output_chunk is a TokenGeneratorOutput # print(generated_output_chunk.decoded_token, end=\"\") # # In a real server, this would be sent back to the user over HTTP # Output (example, token by token): # \"Once \" # \"upon \" # \"a \" # \"time\" # \", \" # \"in \" # \"a \" # # ... and so on . The pipeline.next_token() method returns an “async generator.” This means it yields TokenGeneratorOutput objects one by one as the model produces them. Let’s see what a TokenGeneratorOutput looks like: . # Simplified from: src/max/serve/pipelines/llm.py @dataclass(frozen=True) class TokenGeneratorOutput: decoded_token: str # The actual text part generated in this step # ... other fields like log probabilities, token counts, etc. Each TokenGeneratorOutput contains a piece of the generated text (decoded_token). ",
    "url": "/modular_max/03_llm_pipeline_orchestrator___tokengeneratorpipeline___.html#meet-the-tokengeneratorpipeline",
    
    "relUrl": "/modular_max/03_llm_pipeline_orchestrator___tokengeneratorpipeline___.html#meet-the-tokengeneratorpipeline"
  },"121": {
    "doc": "Chapter 3: LLM Pipeline Orchestrator",
    "title": "Under the Hood: The Journey of a Prompt",
    "content": "So, what happens inside the TokenGeneratorPipeline when its next_token() method is called? . | Context Creation: The pipeline first uses its tokenizer to create a “context” from the TokenGeneratorRequest. This involves converting the prompt messages or text into the initial set of tokens the model needs. This step uses the tokenizer.new_context() method. | Analogy: The assembly line manager gives the raw materials (your prompt) to a specialist (tokenizer) who prepares them (converts to tokens) for the main machine. | . | Sending to EngineQueue: The request (now in a model-understandable format, often part of the “context”) is put into the EngineQueue. This queue acts as a buffer and a communication channel to the Model Worker. | Analogy: The prepared materials are placed on a conveyor belt (EngineQueue) leading to the main processing machinery (Model Worker). | . | Model Processing (by Model Worker): The Model Worker (running in a separate process or thread) picks up the request from the EngineQueue. It runs the LLM to generate the next token(s). | Analogy: The machinery processes the materials and produces a part of the finished product. | . | Receiving from EngineQueue: The generated token(s) are sent back by the Model Worker through the EngineQueue to the TokenGeneratorPipeline. | Analogy: The finished part comes back on another conveyor belt. | . | Decoding: The TokenGeneratorPipeline receives these raw generated tokens. It uses its tokenizer again, this time to decode() the tokens back into human-readable text. | Analogy: The manager takes the finished part and asks the translator (tokenizer) to convert it back into something the customer understands. | . | Yielding Output: The decoded text (and other info) is packaged into a TokenGeneratorOutput object and “yielded” back to the caller (the API endpoint). If it’s a streaming request, this happens for each new piece of text generated. | Repetition: Steps 3-6 repeat until the model finishes generating (e.g., it reaches max_new_tokens, generates an “end-of-sequence” token, or a stop phrase is detected). | . Here’s a simplified diagram of this flow: . sequenceDiagram participant APIEndpoint as API Endpoint Handler participant TGPipeline as TokenGeneratorPipeline participant Tokenizer as PipelineTokenizer participant EngQueue as EngineQueue participant MWorker as Model Worker APIEndpoint-&gt;&gt;TGPipeline: Call next_token(request) TGPipeline-&gt;&gt;Tokenizer: new_context(request) [Encode Prompt] Tokenizer--&gt;&gt;TGPipeline: Model-ready context TGPipeline-&gt;&gt;EngQueue: stream(request.id, context) EngQueue-&gt;&gt;MWorker: Forward request data loop Generation Loop MWorker-&gt;&gt;MWorker: Generate next token(s) MWorker-&gt;&gt;EngQueue: Send generated token(s) back EngQueue--&gt;&gt;TGPipeline: Receive generated token(s) TGPipeline-&gt;&gt;Tokenizer: decode(token(s)) Tokenizer--&gt;&gt;TGPipeline: Decoded text chunk TGPipeline--&gt;&gt;APIEndpoint: Yield TokenGeneratorOutput (chunk) end APIEndpoint--&gt;&gt;User: Streams text chunks . Diving into the Code (src/max/serve/pipelines/llm.py) . Let’s look at a very simplified structure of the TokenGeneratorPipeline and its next_token method: . # Simplified from: src/max/serve/pipelines/llm.py class TokenGeneratorPipeline(Generic[TokenGeneratorContext]): def __init__( self, model_name: str, tokenizer: PipelineTokenizer, # Our \"translator\" engine_queue: EngineQueue, # Our \"conveyor belt\" ): self.model_name = model_name self.tokenizer = tokenizer self.engine_queue = engine_queue # ... other initializations ... async def next_token( self, request: TokenGeneratorRequest ) -&gt; AsyncGenerator[TokenGeneratorOutput, None]: # 1. Create context using the tokenizer context = await self.tokenizer.new_context(request) # 2. Stream responses from the engine_queue (which talks to Model Worker) async for response_from_worker in self.engine_queue.stream( request.id, context ): # `response_from_worker` contains raw tokens from the model # 5. Decode the raw token from worker into text decoded_text_chunk = await self.tokenizer.decode( context, response_from_worker.next_token, # The raw token # ... other args like skip_special_tokens ... ) # 6. Yield the human-readable output yield TokenGeneratorOutput(decoded_token=decoded_text_chunk) # ... (error handling and cleanup would be here) ... In this snippet: . | The __init__ method shows how the pipeline is set up with its crucial components: the tokenizer and the engine_queue. | The next_token method: . | Calls self.tokenizer.new_context() to prepare the initial input for the model. | Uses self.engine_queue.stream() to send the request to the Model Worker (via the queue) and get back a stream of raw responses. | For each raw response from the worker, it calls self.tokenizer.decode() to convert the model’s output token back into text. | It then yields a TokenGeneratorOutput containing this piece of decoded text. | . | . The PipelineTokenizer itself is an interface (defined in src/max/pipelines/core/interfaces/text_generation.py). A concrete implementation of this tokenizer would know the specific vocabulary and rules for the particular LLM being used. Key methods of PipelineTokenizer: . # Simplified from: src/max/pipelines/core/interfaces/text_generation.py class PipelineTokenizer(Protocol): # ... (properties like eos token) ... async def new_context(self, request: TokenGeneratorRequest) -&gt; TokenGeneratorContext: # Prepares the initial state for the model from the user's request. # This often involves encoding the prompt messages. # ... pass async def encode(self, prompt: str, ...) -&gt; TokenizerEncoded: # Converts a string prompt into a sequence of token IDs. # ... pass async def decode(self, context: TokenGeneratorContext, encoded_tokens, ...) -&gt; str: # Converts a sequence of token IDs back into a string. # ... pass . The TokenGeneratorPipeline acts as the skilled manager ensuring that your request smoothly goes through all these steps, from human language to model tokens, through the model, back to tokens, and finally back to human language for you to read. ",
    "url": "/modular_max/03_llm_pipeline_orchestrator___tokengeneratorpipeline___.html#under-the-hood-the-journey-of-a-prompt",
    
    "relUrl": "/modular_max/03_llm_pipeline_orchestrator___tokengeneratorpipeline___.html#under-the-hood-the-journey-of-a-prompt"
  },"122": {
    "doc": "Chapter 3: LLM Pipeline Orchestrator",
    "title": "Conclusion",
    "content": "The TokenGeneratorPipeline is the heart of request processing in modular for language models. It’s the “general manager” that: . | Takes a TokenGeneratorRequest (the work order). | Uses a PipelineTokenizer (the translator) to convert human language to model-understandable tokens and back. | Communicates with the Model Worker (the machinery) through an EngineQueue (the conveyor belt). | Delivers the generated text, often streaming it token by token as TokenGeneratorOutput. | . It efficiently orchestrates the entire journey of your prompt to get you the response you need from the LLM. Now that we understand how requests are managed and prepared, what about the component that actually runs the AI model? In the next chapter, we’ll delve into the Model Worker. Generated by AI Codebase Knowledge Builder . ",
    "url": "/modular_max/03_llm_pipeline_orchestrator___tokengeneratorpipeline___.html#conclusion",
    
    "relUrl": "/modular_max/03_llm_pipeline_orchestrator___tokengeneratorpipeline___.html#conclusion"
  },"123": {
    "doc": "Chapter 3: LLM Pipeline Orchestrator",
    "title": "Chapter 3: LLM Pipeline Orchestrator",
    "content": " ",
    "url": "/modular_max/03_llm_pipeline_orchestrator___tokengeneratorpipeline___.html",
    
    "relUrl": "/modular_max/03_llm_pipeline_orchestrator___tokengeneratorpipeline___.html"
  },"124": {
    "doc": "Chapter 3: NDBuffer",
    "title": "Chapter 3: NDBuffer",
    "content": "Welcome back! In our Mojo journey so far, we’ve explored some fundamental building blocks: . | In Chapter 1: Meet UnsafePointer, we learned that an UnsafePointer is like a direct street address to a location in your computer’s memory where data can be stored. | In Chapter 2: Defining Dimensions - Dim and DimList, we saw how Dim represents the size of a single dimension (which can be known at compile-time or runtime) and DimList collects these Dims to describe the overall shape of multi-dimensional data. | . Now, it’s time to bring these concepts together and meet the star of our show: NDBuffer. The NDBuffer struct is central to handling multi-dimensional arrays (like vectors, matrices, or tensors) in Mojo. Think of it as a sophisticated “lens” or a “view” that you place over a raw block of memory. It doesn’t own the memory itself, but it tells Mojo how to interpret that memory as an N-dimensional structure. As described in the project: . The NDBuffer struct is a non-owning, multi-dimensional view into a block of memory. It describes how to interpret a (potentially flat) piece of memory as an N-dimensional array (like a tensor or matrix). It achieves this by specifying the data type of elements (type), the number of dimensions (rank), the size of each dimension (shape), how to step through memory for each dimension (strides), the memory’s origin (related to aliasing and lifetime), and its address_space (e.g., CPU, GPU shared memory). NDBuffer itself doesn’t own the memory; it’s a descriptor or a “lens” for existing memory. Let’s dive into what makes an NDBuffer tick! . ",
    "url": "/mojo-v2/03_ndbuffer_.html",
    
    "relUrl": "/mojo-v2/03_ndbuffer_.html"
  },"125": {
    "doc": "Chapter 3: NDBuffer",
    "title": "What Exactly is an NDBuffer?",
    "content": "Imagine you have a large, flat sheet of graph paper, and each square on the paper can hold a single number. This sheet of graph paper is like a block of memory. An NDBuffer is like an adjustable stencil or a set of instructions you lay over this graph paper. This stencil tells you: . | What kind of numbers are in the squares (e.g., integers, decimal numbers). | How many rows and columns (or even more dimensions, like layers) you should consider. | The size of each row and column. | How many squares to jump to get from one number to the next, either across a row or down to the next row. | . Crucially, the NDBuffer is just the stencil/instructions. It doesn’t contain the graph paper (the memory) itself. It just knows how to look at it. This is why it’s called a “non-owning view.” . ",
    "url": "/mojo-v2/03_ndbuffer_.html#what-exactly-is-an-ndbuffer",
    
    "relUrl": "/mojo-v2/03_ndbuffer_.html#what-exactly-is-an-ndbuffer"
  },"126": {
    "doc": "Chapter 3: NDBuffer",
    "title": "The Anatomy of an NDBuffer",
    "content": "Let’s peek inside the NDBuffer struct as defined in stdlib/src/buffer/buffer.mojo. We won’t look at every line, but we’ll focus on the main parts a beginner needs to understand. An NDBuffer is defined as a struct. When you create an NDBuffer, you’re creating an instance of this struct. It has several important parameters (information you provide when creating it) and internal fields (where it stores its state). Key Parameters (Compile-Time Information) . These are specified when you declare or create an NDBuffer. They help Mojo understand the intended structure and properties, often allowing for compile-time optimizations. // Simplified NDBuffer definition from stdlib/src/buffer/buffer.mojo struct NDBuffer[ mut: Bool, // Can the data be modified through this NDBuffer? type: DType, // What's the data type of each element? (e.g., DType.float32) rank: Int, // How many dimensions? (e.g., 2 for a matrix) origin: Origin[mut], // Related to memory safety and lifetime tracking // These use DimList from Chapter 2! shape: DimList = DimList.create_unknown[rank](), // Compile-time description of dimension sizes strides: DimList = DimList.create_unknown[rank](), // Compile-time description of memory layout *, // Indicates subsequent parameters are keyword-only alignment: Int = 1, // Preferred memory alignment for performance address_space: AddressSpace = AddressSpace.GENERIC, // Where is the memory? (CPU, GPU) exclusive: Bool = True // Is this NDBuffer the only way to access the memory? ] { // ... Internal fields and methods ... } . Let’s break these down: . | mut: Bool: Short for “mutable.” If True, you can change the data elements in the NDBuffer. If False, it’s read-only. | type: DType: Specifies the data type of the elements stored. DType is Mojo’s way of representing types like float32, int64, etc. | rank: Int: The number of dimensions. A 1D array (vector) has rank=1. A 2D array (matrix) has rank=2. A 3D array (like a cube of data) has rank=3, and so on. | origin: Origin[mut]: An advanced Mojo concept related to memory ownership and borrowing rules. It helps Mojo track where the permission to access the memory came from, enhancing safety. It’s often tied to the UnsafePointer that the NDBuffer will use. | shape: DimList: This is where DimList (from Chapter 2) comes in! It’s a compile-time list of Dims describing the intended size of each dimension. Some Dims can be static (e.g., Dim(10)), and some can be dynamic (Dim(), meaning the size will be known at runtime). By default, it’s DimList.create_unknown[rank](), meaning all dimension sizes are initially considered dynamic. | strides: DimList: Another DimList! This describes the strides of the buffer at compile time. Strides tell Mojo how many elements to “skip” in memory to move to the next element along a particular dimension. We’ll cover strides in detail in the next chapter. Like shape, it defaults to all dynamic. | alignment: Int: For performance, data in memory is sometimes “aligned” to start at addresses that are multiples of a certain number (e.g., 16 bytes, 64 bytes). This parameter specifies the desired alignment. | address_space: AddressSpace: Tells Mojo where the memory this NDBuffer describes is located (e.g., general CPU memory (AddressSpace.GENERIC), GPU shared memory, etc.). | exclusive: Bool: A more advanced flag. If True, it suggests that this NDBuffer (and its underlying pointer) is the only way the program is supposed to access this specific block of memory. This can enable certain optimizations. | . Core Internal Fields (Runtime Information) . Inside every NDBuffer instance, there are a few crucial fields that store its runtime state: . // Inside the NDBuffer struct definition... // ... var data: UnsafePointer[ Scalar[type], // Points to elements of the NDBuffer's 'type' address_space=address_space, // Inherits from NDBuffer's parameter mut=mut, // Inherits from NDBuffer's parameter origin=origin // Inherits from NDBuffer's parameter ]; // These store the *actual* runtime dimension sizes and strides var dynamic_shape: IndexList[rank, element_type = DType.uint64]; var dynamic_stride: IndexList[rank, element_type = DType.uint64]; // ... Let’s understand these: . | data: UnsafePointer[...]: This is the direct link to the memory! It’s an UnsafePointer (from Chapter 1) that holds the memory address where the first element of the NDBuffer’s data begins. | Notice Scalar[type]: If NDBuffer.type is DType.float32, then Scalar[type] becomes Float32. The UnsafePointer points to individual elements of this type. | Its address_space, mut (mutability), and origin are typically inherited from the NDBuffer’s own parameters. | . | dynamic_shape: IndexList[rank, ... ]: This field holds the actual, runtime size of each of the NDBuffer’s rank dimensions. | An IndexList is a list of integers. | Even if you provided static dimensions in the shape: DimList parameter (e.g., DimList(Dim(3), Dim(4))), those values get stored here (e.g., IndexList(3, 4)). | If your shape: DimList had dynamic dimensions (e.g., DimList(Dim(3), Dim())), then dynamic_shape would be filled in when the NDBuffer is fully initialized with the actual runtime size for that dynamic dimension. | . | dynamic_stride: IndexList[rank, ... ]: Similar to dynamic_shape, this field holds the actual, runtime stride for each dimension. Strides are crucial for navigating the N-dimensional data in the flat 1D memory. We’ll dedicate the next chapter to understanding them. | . So, the NDBuffer uses its data pointer to find the start of the memory, and then uses dynamic_shape and dynamic_stride to interpret that flat memory as a structured N-dimensional array. ",
    "url": "/mojo-v2/03_ndbuffer_.html#the-anatomy-of-an-ndbuffer",
    
    "relUrl": "/mojo-v2/03_ndbuffer_.html#the-anatomy-of-an-ndbuffer"
  },"127": {
    "doc": "Chapter 3: NDBuffer",
    "title": "Bringing It All Together: Initializing an NDBuffer",
    "content": "NDBuffer has many ways to be initialized (many __init__ methods). We won’t cover them all, but let’s look at the general idea with a couple of simplified examples. Example 1: A 2D NDBuffer with Statically Known Shape . Imagine you have an UnsafePointer called my_ptr that points to a block of Float32 data in memory. You want to view this data as a 2x3 matrix (2 rows, 3 columns). from memory import UnsafePointer, DType, Origin from buffer import NDBuffer, DimList, Dim fn main() raises: // Assume my_ptr is a valid UnsafePointer to enough Float32 data // For this example, we'll just create a dummy one. // In a real scenario, this memory would be allocated. var my_ptr = UnsafePointer[Float32](); // Dummy pointer // Define the NDBuffer's properties alias MyOrigin = Origin[True].init() // A dummy origin for the example // Create an NDBuffer for a 2x3 matrix of Float32s // NDBuffer[mut, type, rank, origin, shape_dimlist](pointer_to_data) let matrix_view = NDBuffer[ True, // mut = True (mutable) DType.float32, // type = DType.float32 2, // rank = 2 (2D) MyOrigin, // origin (memory tracking) DimList(Dim(2), Dim(3)) // shape = 2 rows, 3 columns (all static) ](my_ptr); // What happens inside matrix_view? // - matrix_view.data will hold my_ptr // - matrix_view.dynamic_shape will be an IndexList like (2, 3) // - matrix_view.dynamic_stride will be calculated for a contiguous 2x3 layout // (e.g., (3, 1) meaning jump 3 elements for next row, 1 for next column) print(matrix_view.get_shape()) . Output (conceptual, depends on actual initialization context for dummy pointer): . (2, 3) . In this case, because the shape DimList(Dim(2), Dim(3)) is fully static, Mojo knows the exact dimensions at compile time. The NDBuffer then stores this pointer and shape information. It also computes default strides assuming the data is laid out contiguously (like words in a book, one row after another). Example 2: A 1D NDBuffer with Dynamically Known Shape . Now, let’s say my_ptr points to Int data, but the number of integers isn’t known until runtime. from memory import UnsafePointer, DType, Origin, IndexList from buffer import NDBuffer, DimList fn main() raises: var my_ptr = UnsafePointer[Int](); // Dummy pointer alias MyOrigin = Origin[True].init() let num_elements_runtime: Int = 10; // Determined at runtime // Create an NDBuffer for a 1D array. // The shape DimList parameter defaults to DimList.create_unknown[rank](), // so we pass the dynamic shape directly. // NDBuffer[mut, type, rank, origin](pointer_to_data, dynamic_shape_indexlist) let vector_view = NDBuffer[ True, // mut DType.int64, // type 1, // rank = 1 (1D) MyOrigin // origin ](my_ptr, IndexList[1](num_elements_runtime)); // Inside vector_view: // - vector_view.data holds my_ptr // - vector_view.dynamic_shape will be an IndexList like (10) // - vector_view.dynamic_stride will be (1) for a contiguous 1D array. print(vector_view.get_shape()) . Output (conceptual): . (10) . Here, the NDBuffer is initialized with my_ptr and an IndexList containing the runtime size num_elements_runtime. The dynamic_shape field inside vector_view will store this runtime size. These examples show the core idea: NDBuffer combines a pointer to raw memory (data) with descriptions of its structure (dynamic_shape, dynamic_stride) to provide a meaningful N-dimensional view. ",
    "url": "/mojo-v2/03_ndbuffer_.html#bringing-it-all-together-initializing-an-ndbuffer",
    
    "relUrl": "/mojo-v2/03_ndbuffer_.html#bringing-it-all-together-initializing-an-ndbuffer"
  },"128": {
    "doc": "Chapter 3: NDBuffer",
    "title": "What Can You Do With an NDBuffer? (A Sneak Peek)",
    "content": "Once you have an NDBuffer, you can perform many operations, such as: . | Get its rank: let r = my_ndbuffer.get_rank() | Get its shape (runtime): let s = my_ndbuffer.get_shape() (returns an IndexList) | Get its strides (runtime): let st = my_ndbuffer.get_strides() (returns an IndexList) | Get the total number of elements: let count = my_ndbuffer.num_elements() or len(my_ndbuffer) | Access an element: let element = my_ndbuffer[idx0, idx1, ...] (e.g., matrix_view[0, 1] for row 0, col 1). This is done via the __getitem__ method. | Set an element (if mutable): my_ndbuffer[idx0, idx1, ...] = new_value. This is done via the __setitem__ method. | Load/Store SIMD vectors: For performance, you can load or store multiple elements at once using SIMD operations (e.g., my_ndbuffer.load[width=4](...)). | . We’ll explore these operations, especially element access and the role of strides, in more detail in upcoming chapters. ",
    "url": "/mojo-v2/03_ndbuffer_.html#what-can-you-do-with-an-ndbuffer-a-sneak-peek",
    
    "relUrl": "/mojo-v2/03_ndbuffer_.html#what-can-you-do-with-an-ndbuffer-a-sneak-peek"
  },"129": {
    "doc": "Chapter 3: NDBuffer",
    "title": "Key Takeaways for Chapter 3",
    "content": ". | NDBuffer is a non-owning view or “lens” that interprets a block of memory as an N-dimensional array. | It combines an UnsafePointer (to the start of the data) with shape and stride information. | Key Parameters (compile-time info): mut, type, rank, origin, shape: DimList, strides: DimList, alignment, address_space. | Core Internal Fields (runtime info): data: UnsafePointer, dynamic_shape: IndexList, dynamic_stride: IndexList. | The shape: DimList parameter allows specifying static or dynamic dimension sizes at compile time, while dynamic_shape: IndexList stores the actual runtime sizes. | NDBuffer enables Mojo to handle multi-dimensional data flexibly and efficiently. | . ",
    "url": "/mojo-v2/03_ndbuffer_.html#key-takeaways-for-chapter-3",
    
    "relUrl": "/mojo-v2/03_ndbuffer_.html#key-takeaways-for-chapter-3"
  },"130": {
    "doc": "Chapter 3: NDBuffer",
    "title": "What’s Next?",
    "content": "We’ve seen that NDBuffer uses data, dynamic_shape, and dynamic_stride to make sense of memory. But how exactly does it use dynamic_stride to find, for example, the element at [row_index, column_index] in a 2D matrix? This involves understanding strides and offset computation. That’s precisely what we’ll unravel in Chapter 4: Strides and Offset Computation! . Navigation . | UnsafePointer (as used by NDBuffer) | DimList and Dim | NDBuffer (You are here) | Strides and Offset Computation | SIMD Data Access ``` | . Generated by AI Codebase Knowledge Builder . ",
    "url": "/mojo-v2/03_ndbuffer_.html#whats-next",
    
    "relUrl": "/mojo-v2/03_ndbuffer_.html#whats-next"
  },"131": {
    "doc": "ContentScrapingStrategy",
    "title": "Chapter 4: Cleaning Up the Mess - ContentScrapingStrategy",
    "content": "In Chapter 3: Giving Instructions - CrawlerRunConfig, we learned how to give specific instructions to our AsyncWebCrawler using CrawlerRunConfig. This included telling it how to fetch the page and potentially take screenshots or PDFs. Now, imagine the crawler has successfully fetched the raw HTML content of a webpage. What’s next? Raw HTML is often messy! It contains not just the main article or product description you might care about, but also: . | Navigation menus | Advertisements | Headers and footers | Hidden code like JavaScript (&lt;script&gt;) and styling information (&lt;style&gt;) | Comments left by developers | . Before we can really understand the meaning of the page or extract specific important information, we need to clean up this mess and get a basic understanding of its structure. ",
    "url": "/Crawl4AI/04_contentscrapingstrategy.html#chapter-4-cleaning-up-the-mess---contentscrapingstrategy",
    
    "relUrl": "/Crawl4AI/04_contentscrapingstrategy.html#chapter-4-cleaning-up-the-mess---contentscrapingstrategy"
  },"132": {
    "doc": "ContentScrapingStrategy",
    "title": "What Problem Does ContentScrapingStrategy Solve?",
    "content": "Think of the raw HTML fetched by the crawler as a very rough first draft of a book manuscript. It has the core story, but it’s full of editor’s notes, coffee stains, layout instructions for the printer, and maybe even doodles in the margins. Before the main editor (who focuses on plot and character) can work on it, someone needs to do an initial cleanup. This “First Pass Editor” would: . | Remove the coffee stains and doodles (irrelevant stuff like ads, scripts, styles). | Identify the basic structure: chapter headings (like the page title), paragraph text, image captions (image alt text), and maybe a list of illustrations (links). | Produce a tidier version of the manuscript, ready for more detailed analysis. | . In Crawl4AI, the ContentScrapingStrategy acts as this First Pass Editor. It takes the raw HTML and performs an initial cleanup and structure extraction. Its job is to transform the messy HTML into a more manageable format, identifying key elements like text content, links, images, and basic page metadata (like the title). ",
    "url": "/Crawl4AI/04_contentscrapingstrategy.html#what-problem-does-contentscrapingstrategy-solve",
    
    "relUrl": "/Crawl4AI/04_contentscrapingstrategy.html#what-problem-does-contentscrapingstrategy-solve"
  },"133": {
    "doc": "ContentScrapingStrategy",
    "title": "What is ContentScrapingStrategy?",
    "content": "ContentScrapingStrategy is an abstract concept (like a job description) in Crawl4AI that defines how the initial processing of raw HTML should happen. It specifies that we need a method to clean HTML and extract basic structure, but the specific tools and techniques used can vary. This allows Crawl4AI to be flexible. Different strategies might use different underlying libraries or have different performance characteristics. ",
    "url": "/Crawl4AI/04_contentscrapingstrategy.html#what-is-contentscrapingstrategy",
    
    "relUrl": "/Crawl4AI/04_contentscrapingstrategy.html#what-is-contentscrapingstrategy"
  },"134": {
    "doc": "ContentScrapingStrategy",
    "title": "The Implementations: Meet the Editors",
    "content": "Crawl4AI provides concrete implementations (the actual editors doing the work) of this strategy: . | WebScrapingStrategy (The Default Editor): . | This is the strategy used by default if you don’t specify otherwise. | It uses a popular Python library called BeautifulSoup behind the scenes to parse and manipulate the HTML. | It’s generally robust and good at handling imperfect HTML. | Think of it as a reliable, experienced editor who does a thorough job. | . | LXMLWebScrapingStrategy (The Speedy Editor): . | This strategy uses another powerful library called lxml. | lxml is often faster than BeautifulSoup, especially on large or complex pages. | Think of it as a very fast editor who might be slightly stricter about the manuscript’s format but gets the job done quickly. | . | . For most beginners, the default WebScrapingStrategy works perfectly fine! You usually don’t need to worry about switching unless you encounter performance issues on very large-scale crawls (which is a more advanced topic). ",
    "url": "/Crawl4AI/04_contentscrapingstrategy.html#the-implementations-meet-the-editors",
    
    "relUrl": "/Crawl4AI/04_contentscrapingstrategy.html#the-implementations-meet-the-editors"
  },"135": {
    "doc": "ContentScrapingStrategy",
    "title": "How It Works Conceptually",
    "content": "Here’s the flow: . | The AsyncWebCrawler receives the raw HTML from the AsyncCrawlerStrategy (the fetcher). | It looks at the CrawlerRunConfig to see which ContentScrapingStrategy to use (defaulting to WebScrapingStrategy if none is specified). | It hands the raw HTML over to the chosen strategy’s scrap method. | The strategy parses the HTML, removes unwanted tags (like &lt;script&gt;, &lt;style&gt;, &lt;nav&gt;, &lt;aside&gt;, etc., based on its internal rules), extracts all links (&lt;a&gt; tags), images (&lt;img&gt; tags with their alt text), and metadata (like the &lt;title&gt; tag). | It returns the results packaged in a ScrapingResult object, containing the cleaned HTML, lists of links and media items, and extracted metadata. | The AsyncWebCrawler then takes this ScrapingResult and uses its contents (along with other info) to build the final CrawlResult. | . sequenceDiagram participant AWC as AsyncWebCrawler (Manager) participant Fetcher as AsyncCrawlerStrategy participant HTML as Raw HTML participant CSS as ContentScrapingStrategy (Editor) participant SR as ScrapingResult (Cleaned Draft) participant CR as CrawlResult (Final Report) AWC-&gt;&gt;Fetcher: Fetch(\"https://example.com\") Fetcher--&gt;&gt;AWC: Here's the Raw HTML AWC-&gt;&gt;CSS: Please scrap this Raw HTML (using config) Note over CSS: Parsing HTML... Removing scripts, styles, ads... Extracting links, images, title... CSS--&gt;&gt;AWC: Here's the ScrapingResult (Cleaned HTML, Links, Media, Metadata) AWC-&gt;&gt;CR: Combine ScrapingResult with other info AWC--&gt;&gt;User: Return final CrawlResult . ",
    "url": "/Crawl4AI/04_contentscrapingstrategy.html#how-it-works-conceptually",
    
    "relUrl": "/Crawl4AI/04_contentscrapingstrategy.html#how-it-works-conceptually"
  },"136": {
    "doc": "ContentScrapingStrategy",
    "title": "Using the Default Strategy (WebScrapingStrategy)",
    "content": "You’re likely already using it without realizing it! When you run a basic crawl, AsyncWebCrawler automatically employs WebScrapingStrategy. # chapter4_example_1.py import asyncio from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode async def main(): # Uses the default AsyncPlaywrightCrawlerStrategy (fetching) # AND the default WebScrapingStrategy (scraping/cleaning) async with AsyncWebCrawler() as crawler: url_to_crawl = \"https://httpbin.org/html\" # A very simple HTML page # We don't specify a scraping_strategy in the config, so it uses the default config = CrawlerRunConfig(cache_mode=CacheMode.BYPASS) # Fetch fresh print(f\"Crawling {url_to_crawl} using default scraping strategy...\") result = await crawler.arun(url=url_to_crawl, config=config) if result.success: print(\"\\nSuccess! Content fetched and scraped.\") # The 'result' object now contains info processed by WebScrapingStrategy # 1. Metadata extracted (e.g., page title) print(f\"Page Title: {result.metadata.get('title', 'N/A')}\") # 2. Links extracted print(f\"Found {len(result.links.internal)} internal links and {len(result.links.external)} external links.\") # Example: print first external link if exists if result.links.external: print(f\" Example external link: {result.links.external[0].href}\") # 3. Media extracted (images, videos, etc.) print(f\"Found {len(result.media.images)} images.\") # Example: print first image alt text if exists if result.media.images: print(f\" Example image alt text: '{result.media.images[0].alt}'\") # 4. Cleaned HTML (scripts, styles etc. removed) - might still be complex # print(f\"\\nCleaned HTML snippet:\\n---\\n{result.cleaned_html[:200]}...\\n---\") # 5. Markdown representation (generated AFTER scraping) print(f\"\\nMarkdown snippet:\\n---\\n{result.markdown.raw_markdown[:200]}...\\n---\") else: print(f\"\\nFailed: {result.error_message}\") if __name__ == \"__main__\": asyncio.run(main()) . Explanation: . | We create AsyncWebCrawler and CrawlerRunConfig as usual. | We don’t set the scraping_strategy parameter in CrawlerRunConfig. Crawl4AI automatically picks WebScrapingStrategy. | When crawler.arun executes, after fetching the HTML, it internally calls WebScrapingStrategy.scrap(). | The result (a CrawlResult object) contains fields populated by the scraping strategy: . | result.metadata: Contains things like the page title found in &lt;title&gt; tags. | result.links: Contains lists of internal and external links found (&lt;a&gt; tags). | result.media: Contains lists of images (&lt;img&gt;), videos (&lt;video&gt;), etc. | result.cleaned_html: The HTML after the strategy removed unwanted tags and attributes (this is then used to generate the Markdown). | result.markdown: While not directly created by the scraping strategy, the cleaned HTML it produces is the input for generating the Markdown representation. | . | . ",
    "url": "/Crawl4AI/04_contentscrapingstrategy.html#using-the-default-strategy-webscrapingstrategy",
    
    "relUrl": "/Crawl4AI/04_contentscrapingstrategy.html#using-the-default-strategy-webscrapingstrategy"
  },"137": {
    "doc": "ContentScrapingStrategy",
    "title": "Explicitly Choosing a Strategy (e.g., LXMLWebScrapingStrategy)",
    "content": "What if you want to try the potentially faster LXMLWebScrapingStrategy? You can specify it in the CrawlerRunConfig. # chapter4_example_2.py import asyncio from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode # 1. Import the specific strategy you want to use from crawl4ai import LXMLWebScrapingStrategy async def main(): # 2. Create an instance of the desired scraping strategy lxml_editor = LXMLWebScrapingStrategy() print(f\"Using scraper: {lxml_editor.__class__.__name__}\") async with AsyncWebCrawler() as crawler: url_to_crawl = \"https://httpbin.org/html\" # 3. Create a CrawlerRunConfig and pass the strategy instance config = CrawlerRunConfig( cache_mode=CacheMode.BYPASS, scraping_strategy=lxml_editor # Tell the config which strategy to use ) print(f\"Crawling {url_to_crawl} with explicit LXML scraping strategy...\") result = await crawler.arun(url=url_to_crawl, config=config) if result.success: print(\"\\nSuccess! Content fetched and scraped using LXML.\") print(f\"Page Title: {result.metadata.get('title', 'N/A')}\") print(f\"Found {len(result.links.external)} external links.\") # Output should be largely the same as the default strategy for simple pages else: print(f\"\\nFailed: {result.error_message}\") if __name__ == \"__main__\": asyncio.run(main()) . Explanation: . | Import: We import LXMLWebScrapingStrategy alongside the other classes. | Instantiate: We create an instance: lxml_editor = LXMLWebScrapingStrategy(). | Configure: We create CrawlerRunConfig and pass our instance to the scraping_strategy parameter: CrawlerRunConfig(..., scraping_strategy=lxml_editor). | Run: Now, when crawler.arun is called with this config, it will use LXMLWebScrapingStrategy instead of the default WebScrapingStrategy for the initial HTML processing step. | . For simple pages, the results from both strategies will often be very similar. The choice typically comes down to performance considerations in more advanced scenarios. ",
    "url": "/Crawl4AI/04_contentscrapingstrategy.html#explicitly-choosing-a-strategy-eg-lxmlwebscrapingstrategy",
    
    "relUrl": "/Crawl4AI/04_contentscrapingstrategy.html#explicitly-choosing-a-strategy-eg-lxmlwebscrapingstrategy"
  },"138": {
    "doc": "ContentScrapingStrategy",
    "title": "A Glimpse Under the Hood",
    "content": "Inside the crawl4ai library, the file content_scraping_strategy.py defines the blueprint and the implementations. The Blueprint (Abstract Base Class): . # Simplified from crawl4ai/content_scraping_strategy.py from abc import ABC, abstractmethod from .models import ScrapingResult # Defines the structure of the result class ContentScrapingStrategy(ABC): \"\"\"Abstract base class for content scraping strategies.\"\"\" @abstractmethod def scrap(self, url: str, html: str, **kwargs) -&gt; ScrapingResult: \"\"\" Synchronous method to scrape content. Takes raw HTML, returns structured ScrapingResult. \"\"\" pass @abstractmethod async def ascrap(self, url: str, html: str, **kwargs) -&gt; ScrapingResult: \"\"\" Asynchronous method to scrape content. Takes raw HTML, returns structured ScrapingResult. \"\"\" pass . The Implementations: . # Simplified from crawl4ai/content_scraping_strategy.py from bs4 import BeautifulSoup # Library used by WebScrapingStrategy # ... other imports like models ... class WebScrapingStrategy(ContentScrapingStrategy): def __init__(self, logger=None): self.logger = logger # ... potentially other setup ... def scrap(self, url: str, html: str, **kwargs) -&gt; ScrapingResult: # 1. Parse HTML using BeautifulSoup soup = BeautifulSoup(html, 'lxml') # Or another parser # 2. Find the main content area (maybe using kwargs['css_selector']) # 3. Remove unwanted tags (scripts, styles, nav, footer, ads...) # 4. Extract metadata (title, description...) # 5. Extract all links (&lt;a&gt; tags) # 6. Extract all images (&lt;img&gt; tags) and other media # 7. Get the remaining cleaned HTML text content # ... complex cleaning and extraction logic using BeautifulSoup methods ... # 8. Package results into a ScrapingResult object cleaned_html_content = \"&lt;html&gt;&lt;body&gt;Cleaned content...&lt;/body&gt;&lt;/html&gt;\" # Placeholder links_data = Links(...) media_data = Media(...) metadata_dict = {\"title\": \"Page Title\"} return ScrapingResult( cleaned_html=cleaned_html_content, links=links_data, media=media_data, metadata=metadata_dict, success=True ) async def ascrap(self, url: str, html: str, **kwargs) -&gt; ScrapingResult: # Often delegates to the synchronous version for CPU-bound tasks return await asyncio.to_thread(self.scrap, url, html, **kwargs) . # Simplified from crawl4ai/content_scraping_strategy.py from lxml import html as lhtml # Library used by LXMLWebScrapingStrategy # ... other imports like models ... class LXMLWebScrapingStrategy(WebScrapingStrategy): # Often inherits for shared logic def __init__(self, logger=None): super().__init__(logger) # ... potentially LXML specific setup ... def scrap(self, url: str, html: str, **kwargs) -&gt; ScrapingResult: # 1. Parse HTML using lxml doc = lhtml.document_fromstring(html) # 2. Find main content, remove unwanted tags, extract info # ... complex cleaning and extraction logic using lxml's XPath or CSS selectors ... # 3. Package results into a ScrapingResult object cleaned_html_content = \"&lt;html&gt;&lt;body&gt;Cleaned LXML content...&lt;/body&gt;&lt;/html&gt;\" # Placeholder links_data = Links(...) media_data = Media(...) metadata_dict = {\"title\": \"Page Title LXML\"} return ScrapingResult( cleaned_html=cleaned_html_content, links=links_data, media=media_data, metadata=metadata_dict, success=True ) # ascrap might also delegate or have specific async optimizations . The key takeaway is that both strategies implement the scrap (and ascrap) method, taking raw HTML and returning a structured ScrapingResult. The AsyncWebCrawler can use either one thanks to this common interface. ",
    "url": "/Crawl4AI/04_contentscrapingstrategy.html#a-glimpse-under-the-hood",
    
    "relUrl": "/Crawl4AI/04_contentscrapingstrategy.html#a-glimpse-under-the-hood"
  },"139": {
    "doc": "ContentScrapingStrategy",
    "title": "Conclusion",
    "content": "You’ve learned about ContentScrapingStrategy, Crawl4AI’s “First Pass Editor” for raw HTML. | It tackles the problem of messy HTML by cleaning it and extracting basic structure. | It acts as a blueprint, with WebScrapingStrategy (default, using BeautifulSoup) and LXMLWebScrapingStrategy (using lxml) as concrete implementations. | It’s used automatically by AsyncWebCrawler after fetching content. | You can specify which strategy to use via CrawlerRunConfig. | Its output (cleaned HTML, links, media, metadata) is packaged into a ScrapingResult and contributes significantly to the final CrawlResult. | . Now that we have this initially cleaned and structured content, we might want to further filter it. What if we only care about the parts of the page that are relevant to a specific topic? . Next: Let’s explore how to filter content for relevance with Chapter 5: Focusing on What Matters - RelevantContentFilter. Generated by AI Codebase Knowledge Builder . ",
    "url": "/Crawl4AI/04_contentscrapingstrategy.html#conclusion",
    
    "relUrl": "/Crawl4AI/04_contentscrapingstrategy.html#conclusion"
  },"140": {
    "doc": "ContentScrapingStrategy",
    "title": "ContentScrapingStrategy",
    "content": " ",
    "url": "/Crawl4AI/04_contentscrapingstrategy.html",
    
    "relUrl": "/Crawl4AI/04_contentscrapingstrategy.html"
  },"141": {
    "doc": "Chapter 4: DimList",
    "title": "Chapter 4: DimList",
    "content": "Okay, here is Chapter 4 on DimList in Markdown format, designed to be very beginner-friendly. # Chapter 4: Describing Dimensions with `Dim` and `DimList` In [Chapter 3: Working with Multiple Dimensions: `IndexList`](03_indexlist_.md), we learned about `IndexList`, a handy tool for representing coordinates or shapes with a fixed number of dimensions, where each dimension's size was a concrete number. Now, we're going to explore a more specialized way to describe dimensions, particularly for `NDBuffer` (which we'll cover in the next chapter). Meet `Dim` and `DimList`! The official description puts it well: &gt; `DimList` is a specialized list used by `NDBuffer` to define its structural properties, specifically its shape (the size of each dimension) and strides (the memory step size for each dimension). Imagine building with LEGOs; `DimList` would be like the part of the instructions telling you the length, width, and height of a particular block (shape) and how many studs to count in the flat array of all LEGO pieces to find the next piece in each respective direction (strides). `DimList` can handle both compile-time known (static) dimensions and runtime-defined (dynamic) dimensions, offering flexibility in defining tensor structures. This means `DimList` is super useful because sometimes we know the exact size of our data when we write our code (static), and sometimes we only find out when the program runs (dynamic), like when loading an image file. ## The Building Block: `Dim` Before we jump into `DimList`, let's understand its fundamental component: `Dim`. A `Dim` represents a **single dimension's size**. The cool part is that this size can either be: 1. **Statically Known**: The size is a specific number known when you compile your code (e.g., a dimension of size 5). 2. **Dynamic (Unknown at Compile Time)**: The size isn't known until the program is running (e.g., the width of an image loaded from a file). You'll need to import `Dim` (and `DimList`) from the `buffer.dimlist` module: ```mojo from buffer.dimlist import Dim, DimList . Creating Dims . | Dynamic Dim: If a dimension’s size is unknown at compile time, you create a Dim without an argument: var dynamic_dim = Dim() print(dynamic_dim) # Output: ? . The ? tells us this dimension’s size is dynamic. | Static Dim: If the size is known, you provide it as an argument: var static_dim = Dim(8) print(static_dim) # Output: 8 . | . Checking Dim Properties . Dim has helpful methods to tell you about its nature: . | has_value() -&gt; Bool: Returns True if the Dim has a static value, False if it’s dynamic. | is_dynamic() -&gt; Bool: Returns True if the Dim is dynamic, False if it has a static value. (This is the opposite of has_value()). | get() -&gt; Int: If the Dim has a static value (has_value() is True), get() returns that integer value. Caution: Calling get() on a dynamic Dim might lead to unexpected results or errors; it’s best to check has_value() first. | . Let’s see them in action: . fn demo_dim_properties(): var d_known = Dim(42) var d_unknown = Dim() print(\"d_known:\") print(\" Value:\", d_known) # Output: 42 print(\" Has value?\", d_known.has_value()) # Output: True print(\" Is dynamic?\", d_known.is_dynamic())# Output: False if d_known.has_value(): print(\" Actual value:\", d_known.get())# Output: 42 print(\"d_unknown:\") print(\" Value:\", d_unknown) # Output: ? print(\" Has value?\", d_unknown.has_value()) # Output: False print(\" Is dynamic?\", d_unknown.is_dynamic())# Output: True if d_unknown.has_value(): # This block won't execute print(\" Actual value:\", d_unknown.get()) else: print(\" Actual value: Not available (dynamic)\") # To run the demo: # demo_dim_properties() . Dim Operations . Dim objects can also be part of simple arithmetic: . fn demo_dim_ops(): var dim8 = Dim(8) var dim2 = Dim(2) var dim_unknown = Dim() var dim_product = dim8 * dim2 print(dim_product) # Output: 16 (because 8 * 2 = 16) var dim_product_with_unknown = dim8 * dim_unknown print(dim_product_with_unknown) # Output: ? (if one is unknown, result is unknown) var dim_div = dim8 // dim2 print(dim_div) # Output: 4 (because 8 // 2 = 4) print(dim_div.get()) # Output: 4 . If any Dim in an operation is dynamic (?), the result is usually also dynamic. ",
    "url": "/mojo-v1/04_dimlist_.html",
    
    "relUrl": "/mojo-v1/04_dimlist_.html"
  },"142": {
    "doc": "Chapter 4: DimList",
    "title": "The List of Dimensions: DimList",
    "content": "Now that we understand Dim, a DimList is simply a list of Dim objects. It’s used to represent a collection of dimension sizes, typically for the shape (e.g., height, width, channels of an image) or strides (how many steps to take in memory to get to the next element in each dimension) of an N-dimensional piece of data. Creating DimLists . You can create a DimList by providing Dim objects or integers (which will be converted to static Dims) as arguments: . fn demo_dimlist_creation(): // All dimensions known var lst0 = DimList(1, 2, 3, 4) print(\"lst0:\", lst0) # Output: lst0: [1, 2, 3, 4] // Some dimensions dynamic var lst1 = DimList(Dim(), 2, Dim(3), 4) // Dim() is dynamic, Dim(3) is static print(\"lst1:\", lst1) # Output: lst1: [?, 2, 3, 4] // You can also create a DimList with all dimensions dynamic using `create_unknown`. // The number in the square brackets `[N]` must be known at compile time. // It tells Mojo how many dynamic dimensions to create. var lst_unknown = DimList.create_unknown[3]() print(\"lst_unknown:\", lst_unknown) # Output: lst_unknown: [?, ?, ?] . DimList Properties and Operations . DimList offers several ways to inspect and work with its dimensions: . | Length: len(my_dimlist) gives you the number of dimensions (the rank). var dl = DimList(5, 10, Dim()) print(\"Length of dl:\", len(dl)) # Output: Length of dl: 3 . | Accessing Dims: .at[index]() returns the Dim object at a specific index. The index must be known at compile time. var dl = DimList(10, Dim(), 30) var first_dim = dl.at[0]() var second_dim = dl.at[1]() print(\"First Dim:\", first_dim) # Output: First Dim: 10 print(\"Second Dim:\", second_dim) # Output: Second Dim: ? . | Accessing Static Values: .get[index]() returns the integer value of the Dim at index, but only if it’s static. Like Dim.get(), use with caution or after checking. var dl = DimList(10, Dim(), 30) if dl.at[0]().has_value(): print(\"Value of first dim:\", dl.get[0]()) # Output: Value of first dim: 10 . | Checking for Static Values: . | .has_value[index]() -&gt; Bool: Checks if the Dim at a specific index (compile-time known) has a static value. var dl = DimList(Dim(), 20) print(\"dl.has_value[0]():\", dl.has_value[0]()) # Output: dl.has_value[0](): False print(\"dl.has_value[1]():\", dl.has_value[1]()) # Output: dl.has_value[1](): True . | .all_known[count]() -&gt; Bool: Checks if the first count dimensions are all static. count must be known at compile time. | .all_known[start, end]() -&gt; Bool: Checks if dimensions in the range [start, end) are all static. start and end must be compile-time known. var lst_all_known = DimList(1, 2, 3, 4) var lst_some_unknown = DimList(Dim(), 2, 3, 4) print(\"lst_all_known.all_known[4]():\", lst_all_known.all_known[4]()) # Output: True print(\"lst_some_unknown.all_known[4]():\", lst_some_unknown.all_known[4]()) # Output: False // Check only from index 1 up to (but not including) index 4 print(\"lst_some_unknown.all_known[1, 4]():\", lst_some_unknown.all_known[1, 4]()) # Output: True (dims at 1,2,3 are 2,3,4) . | . | Product of Dimensions: .product() calculates the total number of elements if all dimensions were multiplied together. | If all dimensions involved are static, it returns a static Dim with the product. | If any dimension involved is dynamic (?), it returns a dynamic Dim (?`). | You can also get the product of a sub-range: .product[count]() or .product[start, end](). ```mojo var dl_static = DimList(2, 3, 4) # Product = 234 = 24 print(“Product of dl_static:”, dl_static.product()) # Output: Product of dl_static: 24 print(“Product of dl_static:”, dl_static.product().get()) # Output: Product of dl_static: 24 | . var dl_dynamic = DimList(2, Dim(), 4) print(“Product of dl_dynamic:”, dl_dynamic.product()) # Output: Product of dl_dynamic: ? . ",
    "url": "/mojo-v1/04_dimlist_.html#the-list-of-dimensions-dimlist",
    
    "relUrl": "/mojo-v1/04_dimlist_.html#the-list-of-dimensions-dimlist"
  },"143": {
    "doc": "Chapter 4: DimList",
    "title": "Product of the first 2 elements of dl_static (2*3=6)",
    "content": "print(“Product of dl_static[0..1]:”, dl_static.product2) # Output: Product of dl_static[0..1]: 6 ``` . | Comparing DimLists: You can check if two DimLists are equal (==). They are equal if they have the same length, and for each position, their Dims are equal. (A static Dim is only equal to another static Dim with the same value. A dynamic Dim is equal to another dynamic Dim.) fn demo_dimlist_eq(): var d1 = DimList(Dim(), 42, Dim()) var d2 = DimList(Dim(), 42, Dim()) var d3 = DimList(1, 42, Dim()) print(\"d1 == d2:\", d1 == d2) # Output: d1 == d2: True print(\"d1 == d3:\", d1 == d3) # Output: d1 == d3: False . | . ",
    "url": "/mojo-v1/04_dimlist_.html#product-of-the-first-2-elements-of-dl_static-236",
    
    "relUrl": "/mojo-v1/04_dimlist_.html#product-of-the-first-2-elements-of-dl_static-236"
  },"144": {
    "doc": "Chapter 4: DimList",
    "title": "Why DimList? The Power of Static and Dynamic",
    "content": "The real strength of Dim and DimList comes from this ability to mix compile-time (static) and runtime (dynamic) information. | Static Dimensions = Optimization: If Mojo knows the exact shape of your data when it compiles your code (e.g., DimList(10, 20, 3)), it can perform many powerful optimizations. It can unroll loops, calculate memory offsets precisely, and generate highly efficient machine code. | Dynamic Dimensions = Flexibility: If your data’s shape isn’t known until your program runs (e.g., you ask the user for dimensions, or load a file), DimList(Dim(), Dim(), Dim()) can represent this. Your code can then adapt to the actual sizes encountered at runtime. | . NDBuffer, which we’ll see next, uses DimList for its shape and strides parameters. This allows you to create NDBuffers that are either fully optimized for a known shape or flexible enough to handle shapes determined on the fly. ",
    "url": "/mojo-v1/04_dimlist_.html#why-dimlist-the-power-of-static-and-dynamic",
    
    "relUrl": "/mojo-v1/04_dimlist_.html#why-dimlist-the-power-of-static-and-dynamic"
  },"145": {
    "doc": "Chapter 4: DimList",
    "title": "Examples from Tests",
    "content": "Let’s look at a combined example inspired by stdlib/test/buffer/test_dimlist.mojo: . from buffer.dimlist import Dim, DimList from testing import assert_equal // A helper for tests, not strictly needed for understanding fn main_dimlist_showcase(): print(\"== DimList Showcase ==\") var lst0 = DimList(1, 2, 3, 4) var lst1 = DimList(Dim(), 2, 3, 4) # First dimension is dynamic print(\"lst0:\", lst0) # Output: lst0: [1, 2, 3, 4] print(\"lst1:\", lst1) # Output: lst1: [?, 2, 3, 4] # Product # For lst0, all are known, so 1*2*3*4 = 24 print(\"lst0.product[4]().get():\", lst0.product[4]().get()) # Output: 24 # For lst1, the first Dim is unknown, so product involving it is unknown # But product of lst1[1,4) (dims at index 1,2,3 -&gt; 2,3,4) is 2*3*4 = 24 print(\"lst1.product[1,4]().get():\", lst1.product[1,4]().get()) # Output: 24 # Overall product of lst1 is dynamic print(\"lst1.product():\", lst1.product()) # Output: ? # all_known print(\"lst0.all_known[4]():\", lst0.all_known[4]()) # Output: True print(\"lst1.all_known[4]():\", lst1.all_known[4]()) # Output: False (due to first Dim) print(\"lst1.all_known[1, 4]():\", lst1.all_known[1, 4]()) # Output: True (dims at 1,2,3 are known) # has_value for specific elements print(\"lst1.has_value[0]():\", lst1.has_value[0]()) # Output: False (Dim() at index 0) print(\"lst1.has_value[1]():\", lst1.has_value[1]()) # Output: True (Dim(2) at index 1) # String representations print(\"String(Dim()):\", String(Dim())) # Output: ? print(\"String(Dim(33)):\", String(Dim(33))) # Output: 33 print(\"String(DimList(2, Dim(), 3)):\", String(DimList(2, Dim(), 3))) # Output: [2, ?, 3] # Creating fully unknown DimList var unknown_list = DimList.create_unknown[5]() print(\"DimList.create_unknown[5]():\", unknown_list) # Output: [?, ?, ?, ?, ?] # To run this example: # main_dimlist_showcase() . ",
    "url": "/mojo-v1/04_dimlist_.html#examples-from-tests",
    
    "relUrl": "/mojo-v1/04_dimlist_.html#examples-from-tests"
  },"146": {
    "doc": "Chapter 4: DimList",
    "title": "Summary",
    "content": ". | A Dim represents a single dimension’s size, which can be static (known at compile time, e.g., Dim(5)) or dynamic (unknown until runtime, e.g., Dim()). | A DimList is a list of Dims, used to define properties like shape and strides for multi-dimensional data structures. | DimList allows mixing static and dynamic dimensions, giving you both optimization potential and runtime flexibility. | Key Dim methods: has_value(), is_dynamic(), get(). Prints as a number or ?. | Key DimList methods: len(), .at[idx](), .get[idx](), .has_value[idx](), .all_known[...], .product[...]. Prints as [val1, ?, val3, ...]. | DimList.create_unknown[N]() creates a list of N dynamic dimensions. | . ",
    "url": "/mojo-v1/04_dimlist_.html#summary",
    
    "relUrl": "/mojo-v1/04_dimlist_.html#summary"
  },"147": {
    "doc": "Chapter 4: DimList",
    "title": "What’s Next?",
    "content": "With Dim and DimList under our belt, we’re now perfectly equipped to understand how Mojo’s primary N-dimensional data structure, NDBuffer, is defined and how it uses these tools to manage data of various (and variably known) shapes. Get ready for NDBuffer in Chapter 5! . Table of Contents . | Chapter 1: Understanding Memory Neighborhoods with AddressSpace | Chapter 2: Peeking into Memory with UnsafePointer | Chapter 3: Working with Multiple Dimensions: IndexList | Chapter 4: Describing Dimensions with Dim and DimList (You are here) | Coming Soon: Chapter 5: The N-Dimensional Buffer: NDBuffer | Coming Soon: Chapter 6: N-D to 1D Indexing Logic (Strided Memory Access) ``` | . Generated by AI Codebase Knowledge Builder . ",
    "url": "/mojo-v1/04_dimlist_.html#whats-next",
    
    "relUrl": "/mojo-v1/04_dimlist_.html#whats-next"
  },"148": {
    "doc": "Chapter 4: Model Worker",
    "title": "Chapter 4: Model Worker",
    "content": "Welcome back! In Chapter 3: LLM Pipeline Orchestrator (TokenGeneratorPipeline), we saw how the TokenGeneratorPipeline acts like an assembly line manager, preparing your requests (like “tell me a story”) and coordinating their journey. But where does the actual story generation, the heavy lifting of the AI model, happen? . That’s the job of the Model Worker! Think of it as the dedicated, high-tech workshop where our most skilled craftsman (the AI model) performs its magic. ",
    "url": "/modular_max/04_model_worker_.html",
    
    "relUrl": "/modular_max/04_model_worker_.html"
  },"149": {
    "doc": "Chapter 4: Model Worker",
    "title": "What Problem Does the Model Worker Solve?",
    "content": "Imagine our AI application is like a busy restaurant. The API layer (Chapter 2) is the front-of-house taking orders. The TokenGeneratorPipeline (Chapter 3) is the head chef who organizes the order tickets and makes sure ingredients are prepped. Now, what if cooking a complex dish (running the AI model) takes a long time? If the head chef also had to cook every single part of every dish, the whole restaurant would grind to a halt while they’re busy at one stove. New customers couldn’t even place orders! . The Model Worker solves this by being a specialized chef in a separate kitchen (a separate process). | It loads the actual AI model (the powerful, but resource-intensive cooking equipment). | It receives “cooking orders” (batches of prepared prompts) from the head chef via a dedicated delivery system (the EngineQueue). | It does the intensive work (running the model to generate text, embeddings, etc.). | It sends the finished “dish” (the model’s output) back. | . Because it runs separately, the main restaurant (the API server) stays responsive and can keep taking new orders even if the Model Worker is busy crafting a masterpiece. ",
    "url": "/modular_max/04_model_worker_.html#what-problem-does-the-model-worker-solve",
    
    "relUrl": "/modular_max/04_model_worker_.html#what-problem-does-the-model-worker-solve"
  },"150": {
    "doc": "Chapter 4: Model Worker",
    "title": "Meet the Model Worker: The AI’s Powerhouse",
    "content": "The Model Worker is where the core AI computation happens. It’s a separate process launched by the main modular application. Here are its key characteristics: . | Loads the AI Model: When the Model Worker starts, its first job is to load the actual AI model (e.g., a large language model) into memory. This model is its primary “tool.” | Runs in a Separate Process: This is crucial. By using Python’s multiprocessing, the Model Worker operates independently of the main API server process. If the model takes 5 seconds to generate a response, the API server isn’t frozen for those 5 seconds; it can continue to handle other incoming requests. | Receives Batched Requests: It doesn’t just get one prompt at a time. It receives requests that have been grouped into “batches” from the EngineQueue. This is often more efficient for the AI model. | Internal Scheduler: Inside the Model Worker, there’s a Scheduler (which we’ll explore in the next chapter). This internal scheduler takes the batches from the EngineQueue and intelligently manages how they are fed to the AI model for processing. | Processes Through the Model: This is the “heavy lifting.” The Model Worker takes the batched inputs and runs them through the loaded AI model to perform tasks like: . | Generating the next token in a sequence (for text generation). | Calculating embeddings for text. | . | Sends Results Back: Once the model produces an output, the Model Worker sends these results back, typically via the same EngineQueue, to the component that requested them (like the LLM Pipeline Orchestrator (TokenGeneratorPipeline)). | . Think of the Model Worker as a highly specialized craftsman in a workshop. The workshop (the Model Worker process) contains all the tools (the AI model). The craftsman receives orders (batches of prompts from the EngineQueue), uses their tools to create the product (model inference), and then sends the product out. ",
    "url": "/modular_max/04_model_worker_.html#meet-the-model-worker-the-ais-powerhouse",
    
    "relUrl": "/modular_max/04_model_worker_.html#meet-the-model-worker-the-ais-powerhouse"
  },"151": {
    "doc": "Chapter 4: Model Worker",
    "title": "The Journey of a Request Batch to and from the Model Worker",
    "content": "Let’s trace how a request batch interacts with the Model Worker, picking up from where the LLM Pipeline Orchestrator (TokenGeneratorPipeline) (from Chapter 3) sends a request: . | Request Sent to Queue: The TokenGeneratorPipeline (in the main process) puts a processed request (or a bundle of them) onto the EngineQueue. | Worker Receives from Queue: The Model Worker process is constantly monitoring the EngineQueue. It picks up the request. | Internal Scheduling: The Scheduler inside the Model Worker takes this request. It might hold onto it briefly to combine it with other requests into an optimal batch for the AI model. | Model Inference: The Scheduler passes the finalized batch to the loaded AI model. The model performs its computation (e.g., predicts the next word). | Result Captured: The AI model produces an output (e.g., the generated word/token). | Result Sent to Queue: The Model Worker (via its internal Scheduler) sends this result back to the EngineQueue. | Pipeline Receives Result: The TokenGeneratorPipeline (back in the main process) picks up the result from the EngineQueue and continues processing it (e.g., decoding it for the user). | . Here’s a diagram showing this flow: . sequenceDiagram participant TGP as LLM Pipeline Orchestrator (Main Process) participant EQ as EngineQueue (Shared Communication) participant MW as Model Worker Process participant Sched as Scheduler (Inside Model Worker) participant AIModel as AI Model (Loaded in Model Worker) TGP-&gt;&gt;EQ: Sends prepared request (e.g., tokenized prompt) Note over EQ: Request waits in queue MW-&gt;&gt;EQ: Retrieves request from queue MW-&gt;&gt;Sched: Passes request to internal Scheduler Sched-&gt;&gt;Sched: Optimizes/Manages batch for AI Model Sched-&gt;&gt;AIModel: Feeds batch to AI Model AIModel-&gt;&gt;AIModel: Performs inference (e.g., generates next token) AIModel--&gt;&gt;Sched: Returns inference result (e.g., raw token) Sched--&gt;&gt;MW: Forwards result MW-&gt;&gt;EQ: Sends result back to queue EQ--&gt;&gt;TGP: Delivers result to Orchestrator . This separation ensures that the demanding task of AI inference doesn’t slow down the main application’s ability to accept and manage new requests. ",
    "url": "/modular_max/04_model_worker_.html#the-journey-of-a-request-batch-to-and-from-the-model-worker",
    
    "relUrl": "/modular_max/04_model_worker_.html#the-journey-of-a-request-batch-to-and-from-the-model-worker"
  },"152": {
    "doc": "Chapter 4: Model Worker",
    "title": "Under the Hood: Starting and Running the Model Worker",
    "content": "The Model Worker is primarily managed by code in src/max/serve/pipelines/model_worker.py. Let’s look at simplified versions of key functions. Launching the Model Worker Process . The start_model_worker function is responsible for creating and starting the new process for the Model Worker. # Simplified from: src/max/serve/pipelines/model_worker.py import multiprocessing # @asynccontextmanager async def start_model_worker( model_factory: callable, # A function that creates the AI model instance # ... other args like batch_config, settings ... ): mp_context = multiprocessing.get_context(\"spawn\") # ... (ProcessControl and EngineQueue setup) ... worker_process = mp_context.Process( name=\"model-worker-process\", target=_model_worker_process_fn, # Function to run in the new process daemon=True, args=( # ... (arguments for _model_worker_process_fn) ... model_factory, # Pass the factory to the new process # ... ), ) worker_process.start() # Start the new process! # ... (health checks and monitoring logic) ... # yield engine_queue # The queue to communicate with the worker . | multiprocessing.get_context(\"spawn\"): This gets a multiprocessing context. “spawn” is a way to start a fresh process. | mp_context.Process(...): This creates a new process. | target=_model_worker_process_fn: This tells the new process to run the _model_worker_process_fn function. | args=(...): These are the arguments passed to _model_worker_process_fn. Importantly, model_factory is passed so the new process knows how to create the AI model. | . | worker_process.start(): This actually launches the new process. | The actual function in modular includes sophisticated health checks and process monitoring using ProcessControl and ProcessMonitor to ensure the worker is running correctly. It also sets up the EngineQueue for communication. | . The Worker’s Main Job . Once the new process starts, it runs _model_worker_process_fn, which in turn calls model_worker_run_v3 (in modular’s current structure). This is where the core logic of the worker resides. # Simplified from: src/max/serve/pipelines/model_worker.py # This function runs in the separate Model Worker process async def model_worker_run_v3( pc: ProcessControl, # For health and lifecycle management model_factory: callable, # Function to load/create the AI model pipeline_config: TokenGeneratorPipelineConfig, # Config for batching queues: Mapping[str, MaxQueue], # Queues for communication (EngineQueue parts) settings: Settings, # Application settings from Chapter 1 # ... (metric_client_factory for telemetry) ... ): # ... (logging and metrics setup) ... # 1. Load the AI Model # `model_factory()` calls the function to get an instance of the model pipeline # This could be a TokenGenerator or EmbeddingsGenerator ai_model_pipeline = model_factory() logger.info(\"AI Model loaded in Model Worker.\") # 2. Create the Scheduler # The Scheduler will manage requests from the queue and feed the model scheduler = _create_internal_scheduler( # Helper to pick correct scheduler ai_model_pipeline, pc, pipeline_config, queues ) logger.info(\"Internal Scheduler created.\") pc.set_started() # Signal that the worker is ready # 3. Run the Scheduler's main loop # This loop continuously processes requests scheduler.run() pc.set_completed() # Signal that the worker has finished (e.g., on shutdown) logger.info(\"Model Worker stopped.\") . Let’s break this down: . | Load the AI Model: ai_model_pipeline = model_factory() is where the actual AI model (like TokenGenerator or EmbeddingsGenerator) is instantiated. The model_factory was passed in when the process started. This can be a time-consuming step, which is another good reason it happens in a separate worker process. | Create the Scheduler: scheduler = _create_internal_scheduler(...) sets up the internal Scheduler. This scheduler (e.g., TokenGenerationScheduler or EmbeddingsScheduler) is responsible for taking items from the input part of the EngineQueue, forming batches according to pipeline_config, running them through ai_model_pipeline, and putting results onto the output part of the EngineQueue. We’ll learn all about this scheduler in Chapter 5: Scheduler (TokenGenerationScheduler, EmbeddingsScheduler). | Run the Scheduler’s Loop: scheduler.run() starts the main processing loop. The scheduler will now continuously: . | Check the EngineQueue for incoming requests. | Form batches. | Execute batches on the ai_model_pipeline. | Send results back via the EngineQueue. This loop continues until the worker is told to shut down. | . | . The Settings object (from Chapter 1: Settings (Settings class)) is also passed to the worker, influencing its behavior (e.g., logging levels, specific model configurations). ",
    "url": "/modular_max/04_model_worker_.html#under-the-hood-starting-and-running-the-model-worker",
    
    "relUrl": "/modular_max/04_model_worker_.html#under-the-hood-starting-and-running-the-model-worker"
  },"153": {
    "doc": "Chapter 4: Model Worker",
    "title": "Why is a Separate Process So Important?",
    "content": "Using a separate process for the Model Worker offers several advantages: . | Non-Blocking API Server: The most significant benefit. The API server (handling HTTP requests) can remain quick and responsive, even if the AI model takes several seconds or more to process a request. It doesn’t get “stuck” waiting for the model. | CPU-Intensive Work Isolation: AI model inference is often CPU-bound (or GPU-bound). Isolating this to a separate process can help manage system resources more effectively and prevents it from starving the API server process for CPU time. (Note: Python’s Global Interpreter Lock (GIL) means true parallelism for CPU-bound tasks in one process is limited, making separate processes even more beneficial). | Potential for Scalability: While modular’s current design focuses on a single Model Worker per main application instance, this architectural separation is a common pattern in systems that scale out by running multiple worker processes (perhaps even on different machines). | Resilience (Limited): If the Model Worker encounters a critical error and crashes, the main API server process might remain alive, allowing it to log the error or attempt a restart of the worker. However, without the worker, it can’t serve model requests. | . ",
    "url": "/modular_max/04_model_worker_.html#why-is-a-separate-process-so-important",
    
    "relUrl": "/modular_max/04_model_worker_.html#why-is-a-separate-process-so-important"
  },"154": {
    "doc": "Chapter 4: Model Worker",
    "title": "Conclusion",
    "content": "The Model Worker is the powerhouse of modular, the dedicated workshop where the AI model performs its demanding computations. By running in a separate process, it ensures that the main application remains responsive while the heavy lifting of model inference is handled efficiently. It loads the model, receives batched requests via the EngineQueue, and uses an internal Scheduler to manage the flow of data through the AI model, sending results back for the user. You’ve now seen how the request gets to the Model Worker. But how does the worker inside itself manage these requests, decide what to run next, and form efficient batches for the AI model? That’s the role of the Scheduler, which we’ll explore in detail in Chapter 5: Scheduler (TokenGenerationScheduler, EmbeddingsScheduler). Generated by AI Codebase Knowledge Builder . ",
    "url": "/modular_max/04_model_worker_.html#conclusion",
    
    "relUrl": "/modular_max/04_model_worker_.html#conclusion"
  },"155": {
    "doc": "Chapter 4: Strides and Offset Computation",
    "title": "Chapter 4: Strides and Offset Computation",
    "content": "Okay, here’s Chapter 4 of the Mojo tutorial, focusing on Strides and Offset Computation, designed to be very beginner-friendly. ",
    "url": "/mojo-v2/04_strides_and_offset_computation_.html",
    
    "relUrl": "/mojo-v2/04_strides_and_offset_computation_.html"
  },"156": {
    "doc": "Chapter 4: Strides and Offset Computation",
    "title": "Chapter 4: Finding Your Way - Strides and Offset Computation",
    "content": "In the previous chapters, we’ve learned a lot about NDBuffer: . | It uses an UnsafePointer (its data field) to know the memory address where its elements begin (Chapter 1). | It uses DimList to define its shape (the size of each dimension), which can be static or dynamic (Chapter 2). | The NDBuffer struct itself combines these pieces of information to act as a “lens” on N-dimensional data (Chapter 3), storing the actual runtime shape in its dynamic_shape field and, as we’ll see, runtime strides in dynamic_stride. | . Now, a crucial question arises: if an NDBuffer represents a 2D grid (like a matrix matrix[row, col]) or even a 3D cube, but computer memory is just one long, flat line of storage cells, how does NDBuffer find the exact memory location for a specific element, say, at matrix[1, 2]? . This is where strides and offset computation come into play. They are the magic map NDBuffer uses to navigate its N-dimensional view of 1D memory. As the project overview puts it: . This refers to the core mechanism NDBuffer uses to locate any specific element within its N-dimensional view of memory. Given an N-dimensional index (e.g., [row, col]), the NDBuffer calculates a 1D offset from its base data pointer. This calculation relies on strides: an array where each element specifies how many memory items to skip to advance one step along the corresponding dimension. ",
    "url": "/mojo-v2/04_strides_and_offset_computation_.html#chapter-4-finding-your-way---strides-and-offset-computation",
    
    "relUrl": "/mojo-v2/04_strides_and_offset_computation_.html#chapter-4-finding-your-way---strides-and-offset-computation"
  },"157": {
    "doc": "Chapter 4: Strides and Offset Computation",
    "title": "The Memory Maze: From N-Dimensions to One Dimension",
    "content": "Imagine your computer’s memory as a single, incredibly long bookshelf. Each spot on the shelf can hold one data element (like one number). This bookshelf is 1-dimensional. However, we often want to work with data in more dimensions: . | A list of numbers (1D) | A table or spreadsheet (2D) | A stack of images, forming a video (3D) | . NDBuffer allows us to think about our data in these convenient N-dimensional ways. But underneath, the data still lives on that 1D bookshelf. The challenge is to create a system that can translate an N-dimensional coordinate (like “row 2, column 5”) into a single, unique position on the 1D bookshelf. ",
    "url": "/mojo-v2/04_strides_and_offset_computation_.html#the-memory-maze-from-n-dimensions-to-one-dimension",
    
    "relUrl": "/mojo-v2/04_strides_and_offset_computation_.html#the-memory-maze-from-n-dimensions-to-one-dimension"
  },"158": {
    "doc": "Chapter 4: Strides and Offset Computation",
    "title": "Introducing Strides: Your Memory Navigation Map",
    "content": "Strides are the key to this translation. For an N-dimensional NDBuffer, the strides are a list of numbers, with one number for each dimension. Each number in the strides list tells you: . “To move one step forward in this particular dimension, while keeping your position in all other dimensions the same, how many actual items do you need to skip over in the 1D memory?” . Let’s use the multi-story car park analogy from the project description: Imagine your NDBuffer.data pointer is the entrance to a vast, continuous parking lot (our 1D memory). You want to organize this flat lot to represent a multi-story car park with levels, rows within levels, and spots within rows (your N-D view). Let’s say you want to find your car using its [level_index, row_index, spot_index]. | NDBuffer.data: The entrance to the entire car park. | N-D Index (e.g., [level, row, spot]): How you conceptually remember your car’s location. | Shape (e.g., [num_levels, num_rows_per_level, num_spots_per_row]): The capacity and structure of your conceptual car park. | Strides: This is your navigation guide. | stride_spot: How many actual 1D parking spaces do you move to get from spot_X to spot_X+1 in the same row and on the same level? (This is usually 1, as spots are typically side-by-side). | stride_row: How many actual 1D parking spaces do you move to get from spot_X in row_Y to spot_X in row_Y+1 on the same level? (This would be the total number of spots in one complete row, e.g., num_spots_per_row). | stride_level: How many actual 1D parking spaces do you move to get from spot_X in row_Y on level_Z to spot_X in row_Y on level_Z+1? (This would be the total number of spots on one entire level, e.g., num_spots_per_row * num_rows_per_level). | . | . The NDBuffer stores these runtime stride values in its dynamic_stride field (which is an IndexList, similar to dynamic_shape). ",
    "url": "/mojo-v2/04_strides_and_offset_computation_.html#introducing-strides-your-memory-navigation-map",
    
    "relUrl": "/mojo-v2/04_strides_and_offset_computation_.html#introducing-strides-your-memory-navigation-map"
  },"159": {
    "doc": "Chapter 4: Strides and Offset Computation",
    "title": "Contiguous Memory and Calculating Default Strides (_compute_ndbuffer_stride)",
    "content": "Often, N-dimensional data is stored “contiguously” in memory. This means the elements are packed tightly together without any gaps, in a predictable order. A very common contiguous layout is row-major order (also known as C-style order for arrays). Row-Major Order (Example: 2D Matrix) Imagine a 2x3 matrix (2 rows, 3 columns): . A = | a b c | d e f | . In row-major order, this matrix would be stored in memory as a flat sequence: a, b, c, d, e, f. The first row (a, b, c) is laid out completely, followed by the second row (d, e, f). For such a contiguous, row-major layout, NDBuffer can calculate default strides. The Mojo standard library provides the _compute_ndbuffer_stride function (found in stdlib/src/buffer/buffer.mojo) for this. Here’s how it generally works: . | The stride for the innermost (last) dimension is always 1. (To go to the next column in the same row, you just move to the very next element in memory). | For any other dimension d, its stride is calculated by taking the size (from the shape) of the next dimension (d+1) and multiplying it by the stride of that next dimension (d+1). You work your way from the inside out. | . Let’s calculate default strides for our 2x3 matrix A. | Shape: [2, 3] (rows=2, columns=3) | Rank (number of dimensions): 2 | . | Innermost dimension (columns, dimension index 1): stride_col = dynamic_stride[1] = 1 . | Next dimension moving outwards (rows, dimension index 0): stride_row = dynamic_stride[0] = shape[1] * dynamic_stride[1] stride_row = 3 * 1 = 3 . | . So, for a 2x3 row-major matrix, the default strides are [3, 1]. | To move to the next element in the same row (advance one column), you jump 1 memory spot. | To move to the same column position in the next row (advance one row), you jump 3 memory spots (which is the entire length of a row). | . The _compute_ndbuffer_stride function in buffer.mojo implements this logic: . @always_inline fn _compute_ndbuffer_stride[ rank: Int ](shape: IndexList[rank, **_]) -&gt; __type_of(shape): \"\"\"Computes the NDBuffer's default dynamic strides using the input shape. The default strides correspond to contiguous memory layout... \"\"\" constrained[rank &gt; 0]() @parameter if rank == 1: // Special case for 1D arrays return __type_of(shape)(1) // Stride is just 1 var stride = shape // Temporary, will be overwritten stride[rank - 1] = 1 // Stride for the innermost dimension is 1 // Iterate from the second-to-last dimension backwards to the first @parameter for i in reversed(range(rank - 1)): // Correctly iterates i from rank-2 down to 0 // Example: rank=2, i=0. stride[0]=shape[1]*stride[1] // Example: rank=3, i=1,0. stride[1]=shape[2]*stride[2], then stride[0]=shape[1]*stride[1] stride[i] = shape[i + 1] * stride[i + 1] return stride . When you create an NDBuffer and don’t explicitly provide strides, this function is typically used to calculate them based on the shape, assuming a contiguous, row-major layout. ",
    "url": "/mojo-v2/04_strides_and_offset_computation_.html#contiguous-memory-and-calculating-default-strides-_compute_ndbuffer_stride",
    
    "relUrl": "/mojo-v2/04_strides_and_offset_computation_.html#contiguous-memory-and-calculating-default-strides-_compute_ndbuffer_stride"
  },"160": {
    "doc": "Chapter 4: Strides and Offset Computation",
    "title": "Calculating the Offset: Finding the Exact Spot (_compute_ndbuffer_offset)",
    "content": "Once an NDBuffer has: . | Its data pointer (the starting address of the memory block). | The N-dimensional index of the element you want (e.g., [row_idx, col_idx]). | Its dynamic_stride list (e.g., [stride_for_row, stride_for_col]). | . It can calculate the 1D offset from the data pointer to your desired element. The formula is a sum of products: . offset = (index_dim0 * stride_dim0) + (index_dim1 * stride_dim1) + ... + (index_dimN-1 * stride_dimN-1) . Let’s use our 2x3 matrix A again: . | data points to where a is stored. | Shape [2, 3]. | Strides [3, 1]. (i.e., stride_row = 3, stride_col = 1) | . Suppose we want to find element A[1, 2] (this is element f in [[a,b,c], [d,e,f]]). The N-D index is [1, 2]. | row_idx = 1 | col_idx = 2 | . offset = (row_idx * stride_row) + (col_idx * stride_col) offset = (1 * 3) + (2 * 1) offset = 3 + 2 offset = 5 . This means element A[1, 2] is 5 memory spots away from the start (data). If data points to memory location P: . | P+0: a (A[0,0]) | P+1: b (A[0,1]) | P+2: c (A[0,2]) | P+3: d (A[1,0]) | P+4: e (A[1,1]) | P+5: f (A[1,2]) &lt;– We found it! | . The NDBuffer method _offset() uses the helper function _compute_ndbuffer_offset to do exactly this. Here’s a simplified look at _compute_ndbuffer_offset from buffer.mojo: . @always_inline fn _compute_ndbuffer_offset( buf: NDBuffer, index: VariadicList[Int], // Could also be StaticTuple or IndexList ) -&gt; Int: \"\"\"Computes the NDBuffer's offset using the index positions provided... \"\"\" // ... (rank 0 and 32-bit indexing checks omitted for simplicity) ... var result: Int = 0 @parameter for i in range(buf.rank): // buf.stride[i]() gets the stride for the i-th dimension // index[i] gets the index for the i-th dimension // fma(a, b, c) is \"fused multiply-add\", calculates (a*b) + c efficiently result = fma(buf.stride[i](), index[i], result) return result . This function iterates through each dimension, multiplies the index for that dimension by its corresponding stride, and adds it to the accumulating result. This is the heart of how NDBuffer navigates its data. ",
    "url": "/mojo-v2/04_strides_and_offset_computation_.html#calculating-the-offset-finding-the-exact-spot-_compute_ndbuffer_offset",
    
    "relUrl": "/mojo-v2/04_strides_and_offset_computation_.html#calculating-the-offset-finding-the-exact-spot-_compute_ndbuffer_offset"
  },"161": {
    "doc": "Chapter 4: Strides and Offset Computation",
    "title": "The Power of Strides: More Than Just Contiguous",
    "content": "The really cool thing about strides is that they aren’t limited to describing just standard row-major (or column-major) layouts. By manually defining or manipulating the strides, an NDBuffer can represent many different views of the same underlying memory without copying any data. | Column-Major Order (Fortran-style): For our 2x3 matrix A, if it were stored column-by-column (a, d, b, e, c, f), the strides would be [1, 2]. | stride_row = 1: To get to the next row in the same column, jump 1 spot. | stride_col = 2: To get to the next column, jump 2 spots (the length of a column). | . | Slices (Views of Data): Imagine you have a large 10x10 matrix, but you only want to work with a 3x3 sub-section of it (a slice). You can create a new NDBuffer for this 3x3 slice. This new NDBuffer would: . | Have its data pointer offset to the start of the 3x3 region within the original 10x10 data. | Have a shape of [3, 3]. | Use the same strides as the original 10x10 matrix! No data is copied; the slice is just a different “lens” on the existing memory. | . | Transposed Matrices: You can “transpose” a matrix (swap its rows and columns) simply by changing its shape and strides, without moving data. If matrix M has shape [R, C] and strides [stride_R, stride_C]. Its transpose M_T will have shape [C, R] and strides [stride_C, stride_R]. Again, it’s a new view on the same data. | Broadcasting: Sometimes in numerical computing, you want to operate between arrays of different shapes (e.g., adding a vector to each row of a matrix). Strides can help represent this. If a dimension has a stride of 0, accessing any index along that “broadcasted” dimension will always return data from the same memory location(s). This effectively makes that dimension appear to repeat its data. | . This flexibility to represent various data layouts and views through strides makes NDBuffer incredibly efficient and powerful for tasks in scientific computing, machine learning, and data analysis, where avoiding data copies is crucial for performance. ",
    "url": "/mojo-v2/04_strides_and_offset_computation_.html#the-power-of-strides-more-than-just-contiguous",
    
    "relUrl": "/mojo-v2/04_strides_and_offset_computation_.html#the-power-of-strides-more-than-just-contiguous"
  },"162": {
    "doc": "Chapter 4: Strides and Offset Computation",
    "title": "Key Takeaways for Chapter 4",
    "content": ". | Strides define how many memory items to skip to advance one step along each dimension of an NDBuffer. | NDBuffer uses the N-D index provided by you (e.g., my_buffer[r,c]) and its internal dynamic_stride values to calculate a 1D offset from its base data pointer. | The function _compute_ndbuffer_stride (in stdlib/src/buffer/buffer.mojo) can calculate default strides for a contiguous, row-major memory layout based on the NDBuffer’s shape. | The function _compute_ndbuffer_offset (in stdlib/src/buffer/buffer.mojo) performs the actual calculation: offset = sum(index_dim_i * stride_dim_i). | Strides are powerful because they allow NDBuffer to represent various memory layouts (row-major, column-major, slices, transposes, broadcasted arrays) efficiently, often without needing to copy the underlying data. | . ",
    "url": "/mojo-v2/04_strides_and_offset_computation_.html#key-takeaways-for-chapter-4",
    
    "relUrl": "/mojo-v2/04_strides_and_offset_computation_.html#key-takeaways-for-chapter-4"
  },"163": {
    "doc": "Chapter 4: Strides and Offset Computation",
    "title": "What’s Next?",
    "content": "We now understand how NDBuffer can pinpoint any individual element within its N-dimensional view. But in high-performance computing, we often want to read or write multiple elements at once to take advantage of modern CPU capabilities. This is where SIMD (Single Instruction, Multiple Data) operations come in. In the next chapter, Chapter 5: SIMD Data Access, we’ll explore how NDBuffer allows you to load and store data in chunks, further boosting performance. Navigation . | UnsafePointer (as used by NDBuffer) | DimList and Dim | NDBuffer | Strides and Offset Computation (You are here) | SIMD Data Access ``` | . Generated by AI Codebase Knowledge Builder . ",
    "url": "/mojo-v2/04_strides_and_offset_computation_.html#whats-next",
    
    "relUrl": "/mojo-v2/04_strides_and_offset_computation_.html#whats-next"
  },"164": {
    "doc": "Chapter 4: URL Filtering & Scoring",
    "title": "Chapter 4: URL Filtering &amp; Scoring",
    "content": "In Chapter 3: Content Extraction Pipeline, we learned how to process and extract useful information from web pages. But how do we decide which URLs to crawl in the first place, and in what order? That’s where URL filtering and scoring come in! . ",
    "url": "/Crawl4AI/04_url_filtering___scoring_.html#chapter-4-url-filtering--scoring",
    
    "relUrl": "/Crawl4AI/04_url_filtering___scoring_.html#chapter-4-url-filtering--scoring"
  },"165": {
    "doc": "Chapter 4: URL Filtering & Scoring",
    "title": "Understanding URL Filtering and Scoring",
    "content": "Imagine you’re hosting a party with a VIP section. You need two people: . | A bouncer who decides who gets in based on specific rules (the URL filter) | A host who decides the order people enter based on their importance (the URL scorer) | . When crawling the web, you don’t want to crawl everything - that would be inefficient and might get you blocked! Instead, you want to: . | Filter out URLs you don’t care about (like advertisements or irrelevant domains) | Prioritize the most valuable URLs to crawl first | . Let’s see how crawl4ai helps us do this! . ",
    "url": "/Crawl4AI/04_url_filtering___scoring_.html#understanding-url-filtering-and-scoring",
    
    "relUrl": "/Crawl4AI/04_url_filtering___scoring_.html#understanding-url-filtering-and-scoring"
  },"166": {
    "doc": "Chapter 4: URL Filtering & Scoring",
    "title": "URL Filters: The Bouncers",
    "content": "URL filters are simple yes/no decision makers. They look at a URL and decide: “Should we crawl this or not?” . Creating a Simple Domain Filter . Let’s say you only want to crawl pages from example.com: . from crawl4ai.deep_crawling.filters import DomainFilter # Create a filter that only allows example.com domain_filter = DomainFilter(allowed_domains=[\"example.com\"]) # Test it is_allowed = domain_filter.apply(\"https://example.com/page1\") # True is_blocked = domain_filter.apply(\"https://other-site.com/page1\") # False . This filter acts like a bouncer who only lets in people from a specific city! . Filtering by Content Type . Maybe you only want HTML pages and PDFs, not images or other files: . from crawl4ai.deep_crawling.filters import ContentTypeFilter # Create a filter for HTML and PDF files content_filter = ContentTypeFilter(allowed_types=[\"text/html\", \"application/pdf\"]) # Now only HTML and PDF URLs will pass through . This filter checks the content type (often inferred from the URL extension) and only allows specified types. Pattern-Based Filtering . You can also filter URLs based on patterns: . from crawl4ai.deep_crawling.filters import URLPatternFilter # Exclude admin pages and pages with \"login\" in the URL pattern_filter = URLPatternFilter( patterns=[\"/admin/*\", \"*/login*\"], reverse=True # Exclude matching patterns instead of including them ) . This filter looks for specific patterns in URLs and rejects or accepts them accordingly. ",
    "url": "/Crawl4AI/04_url_filtering___scoring_.html#url-filters-the-bouncers",
    
    "relUrl": "/Crawl4AI/04_url_filtering___scoring_.html#url-filters-the-bouncers"
  },"167": {
    "doc": "Chapter 4: URL Filtering & Scoring",
    "title": "URL Scorers: The Judges",
    "content": "While filters make yes/no decisions, scorers assign a numerical value to each URL. Higher scores mean higher priority! . Scoring by Keyword Relevance . Let’s score URLs higher if they contain keywords we’re interested in: . from crawl4ai.deep_crawling.scorers import KeywordRelevanceScorer # URLs with these keywords will score higher keyword_scorer = KeywordRelevanceScorer( keywords=[\"python\", \"tutorial\", \"programming\"], weight=1.0 ) . This scorer gives higher scores to URLs containing our keywords. The weight parameter lets us control how important this particular scoring factor is. Scoring by Freshness . For news or time-sensitive information, you might want to prioritize recent content: . from crawl4ai.deep_crawling.scorers import FreshnessScorer # Prioritize URLs with recent dates in them freshness_scorer = FreshnessScorer(weight=0.8, current_year=2024) . This scorer looks for dates in URLs (like /2024/04/ or 2023-news) and scores newer content higher. Combining Multiple Scorers . You’ll often want to combine multiple scoring criteria: . from crawl4ai.deep_crawling.scorers import CompositeScorer # Combine our scorers combined_scorer = CompositeScorer([ keyword_scorer, # From previous example freshness_scorer, # From previous example # Add more scorers as needed ]) . Each scorer contributes to the final score, letting you balance different factors when deciding crawling priority. ",
    "url": "/Crawl4AI/04_url_filtering___scoring_.html#url-scorers-the-judges",
    
    "relUrl": "/Crawl4AI/04_url_filtering___scoring_.html#url-scorers-the-judges"
  },"168": {
    "doc": "Chapter 4: URL Filtering & Scoring",
    "title": "Putting It All Together",
    "content": "Now let’s see how to use filters and scorers together in a complete crawling setup: . from crawl4ai import AsyncWebCrawler, CrawlerRunConfig from crawl4ai.deep_crawling import BestFirstCrawlingStrategy from crawl4ai.deep_crawling.filters import FilterChain # Create a chain of filters filter_chain = FilterChain([domain_filter, content_filter, pattern_filter]) # Create a crawler strategy with our filters and scorer strategy = BestFirstCrawlingStrategy( max_depth=3, # How deep to crawl filter_chain=filter_chain, # Our filters url_scorer=combined_scorer, # Our scorer max_pages=100 # Limit total pages ) # Set up the crawler with our strategy config = CrawlerRunConfig(deep_crawl_strategy=strategy) # Start crawling! async with AsyncWebCrawler() as crawler: results = await crawler.arun( url=\"https://example.com\", config=config ) . This sets up a complete crawling system that: . | Starts at example.com | Only crawls URLs that pass all our filters | Prioritizes URLs based on our scoring criteria | Stops after 100 pages or when it reaches depth 3 | . ",
    "url": "/Crawl4AI/04_url_filtering___scoring_.html#putting-it-all-together",
    
    "relUrl": "/Crawl4AI/04_url_filtering___scoring_.html#putting-it-all-together"
  },"169": {
    "doc": "Chapter 4: URL Filtering & Scoring",
    "title": "What Happens Under the Hood",
    "content": "Let’s look at what happens when you run a crawler with filters and scorers: . sequenceDiagram participant WC as WebCrawler participant ST as BestFirstStrategy participant FC as FilterChain participant SC as URLScorer participant Q as PriorityQueue WC-&gt;&gt;ST: arun(start_url) ST-&gt;&gt;Q: add start_url (score=0) loop For each URL in queue ST-&gt;&gt;Q: get highest-scored URL ST-&gt;&gt;FC: can_process_url(url)? FC-&gt;&gt;ST: yes/no Note over ST: Skip if filter says no ST-&gt;&gt;WC: crawl(url) WC-&gt;&gt;ST: crawl_result ST-&gt;&gt;ST: extract links from result loop For each new link ST-&gt;&gt;FC: can_process_url(link)? FC-&gt;&gt;ST: yes/no Note over ST: Skip if filter says no ST-&gt;&gt;SC: score(link) SC-&gt;&gt;ST: priority score ST-&gt;&gt;Q: add link with score end end . When you run a crawler with URL filtering and scoring: . | The crawler adds the start URL to a priority queue | It takes the highest-scored URL from the queue | It checks if the URL passes all filters | If it passes, the URL is crawled | New URLs found during crawling go through the same process: . | Check if they pass the filters | Score them to determine priority | Add them to the priority queue | . | The process repeats until the queue is empty or limits are reached | . ",
    "url": "/Crawl4AI/04_url_filtering___scoring_.html#what-happens-under-the-hood",
    
    "relUrl": "/Crawl4AI/04_url_filtering___scoring_.html#what-happens-under-the-hood"
  },"170": {
    "doc": "Chapter 4: URL Filtering & Scoring",
    "title": "Implementation Details",
    "content": "Let’s look at how the BestFirstCrawlingStrategy implements this process: . # From crawl4ai/deep_crawling/bff_strategy.py async def _arun_best_first(self, start_url, crawler, config): queue = asyncio.PriorityQueue() # Start with the initial URL (score 0, depth 0) await queue.put((0, 0, start_url, None)) visited = set() while not queue.empty(): # Get highest priority URL score, depth, url, parent_url = await queue.get() if url in visited: continue visited.add(url) # Crawl the URL result = await crawler.arun(url=url, config=config) yield result # Only process successful crawls if result.success: # Discover new links new_links = [] await self.link_discovery(result, url, depth, visited, new_links) # Score and add links to queue for new_url, new_parent in new_links: new_score = self.url_scorer.score(new_url) await queue.put((new_score, depth+1, new_url, new_parent)) . The key parts of this implementation are: . | Using a priority queue to always process the highest-scored URL next | Checking if URLs have been visited to avoid duplicates | Using filters during link discovery to filter out unwanted URLs | Scoring new URLs to determine their priority in the queue | . Let’s also look at how filters are applied in the filter chain: . # From crawl4ai/deep_crawling/filters.py async def apply(self, url: str) -&gt; bool: \"\"\"Apply all filters concurrently when possible\"\"\" self.stats._counters[0] += 1 # Count total URLs processed # Apply each filter for filter_ in self.filters: result = filter_.apply(url) # If filter returns False, URL is rejected if not result: self.stats._counters[2] += 1 # Count rejections return False # All filters passed self.stats._counters[1] += 1 # Count accepted URLs return True . This shows how the filter chain works - it applies each filter in sequence, and if any filter says “no,” the URL is immediately rejected. ",
    "url": "/Crawl4AI/04_url_filtering___scoring_.html#implementation-details",
    
    "relUrl": "/Crawl4AI/04_url_filtering___scoring_.html#implementation-details"
  },"171": {
    "doc": "Chapter 4: URL Filtering & Scoring",
    "title": "Real-World Examples",
    "content": "Technical Documentation Crawler . Let’s say you want to crawl Python documentation but only care about tutorials and library references: . # Filter for docs.python.org domain_filter = DomainFilter(allowed_domains=[\"docs.python.org\"]) # Only interested in tutorial and library sections pattern_filter = URLPatternFilter(patterns=[\"/tutorial/*\", \"/library/*\"]) # Prioritize based on keywords keyword_scorer = KeywordRelevanceScorer([ \"beginners\", \"tutorial\", \"examples\", \"howto\" ], weight=1.5) # Give higher weight to beginner-friendly content . News Crawler . For a news crawler that focuses on recent technology articles: . # Only allow news sites domain_filter = DomainFilter(allowed_domains=[ \"techcrunch.com\", \"theverge.com\", \"wired.com\" ]) # Exclude category pages and author pages pattern_filter = URLPatternFilter( patterns=[\"/author/*\", \"/category/*\", \"/tag/*\"], reverse=True ) # Prioritize recent articles and tech keywords freshness = FreshnessScorer(weight=1.2) # High weight on recency tech_terms = KeywordRelevanceScorer( [\"ai\", \"python\", \"machine learning\", \"data science\"], weight=1.0 ) . ",
    "url": "/Crawl4AI/04_url_filtering___scoring_.html#real-world-examples",
    
    "relUrl": "/Crawl4AI/04_url_filtering___scoring_.html#real-world-examples"
  },"172": {
    "doc": "Chapter 4: URL Filtering & Scoring",
    "title": "Conclusion",
    "content": "URL filtering and scoring are powerful tools that help you focus your web crawling on the most relevant and valuable content. Filters act as gatekeepers that eliminate unwanted URLs, while scorers help you prioritize the most important content first. By combining different filters and scorers, you can create sophisticated crawling strategies that efficiently collect exactly the data you need. This makes your crawlers more effective and less likely to waste resources on irrelevant content. In the next chapter, Deep Crawling System, we’ll explore how to use these filtering and scoring techniques to build complete web crawling systems that can navigate complex website structures. Generated by AI Codebase Knowledge Builder . ",
    "url": "/Crawl4AI/04_url_filtering___scoring_.html#conclusion",
    
    "relUrl": "/Crawl4AI/04_url_filtering___scoring_.html#conclusion"
  },"173": {
    "doc": "Chapter 4: URL Filtering & Scoring",
    "title": "Chapter 4: URL Filtering & Scoring",
    "content": " ",
    "url": "/Crawl4AI/04_url_filtering___scoring_.html",
    
    "relUrl": "/Crawl4AI/04_url_filtering___scoring_.html"
  },"174": {
    "doc": "Chapter 5: Deep Crawling System",
    "title": "Chapter 5: Deep Crawling System",
    "content": "In Chapter 4: URL Filtering &amp; Scoring, we learned how to filter and prioritize URLs. Now, let’s take the next step: exploring websites beyond a single page using the Deep Crawling System. ",
    "url": "/Crawl4AI/05_deep_crawling_system_.html",
    
    "relUrl": "/Crawl4AI/05_deep_crawling_system_.html"
  },"175": {
    "doc": "Chapter 5: Deep Crawling System",
    "title": "What is a Deep Crawling System?",
    "content": "Imagine you’re exploring a vast library. You could just look at one book (a single webpage), or you could navigate through the entire library systematically (deep crawling). The Deep Crawling System is like your exploration strategy for that library. It answers questions like: . | Which doors should I go through first? | How far should I venture from the entrance? | Which rooms are most likely to contain what I’m looking for? | . When crawling websites, these become decisions about: . | Which links to follow first | How many pages deep to go | When to stop exploring | . Let’s learn how to explore websites intelligently using crawl4ai! . ",
    "url": "/Crawl4AI/05_deep_crawling_system_.html#what-is-a-deep-crawling-system",
    
    "relUrl": "/Crawl4AI/05_deep_crawling_system_.html#what-is-a-deep-crawling-system"
  },"176": {
    "doc": "Chapter 5: Deep Crawling System",
    "title": "A Simple Example: Exploring a Documentation Site",
    "content": "Let’s say you want to crawl the Python documentation site to collect all tutorial pages: . from crawl4ai import AsyncWebCrawler, CrawlerRunConfig from crawl4ai.deep_crawling import BFSDeepCrawlStrategy from crawl4ai.deep_crawling.filters import DomainFilter async def crawl_python_docs(): # Create a strategy that explores up to 3 levels deep strategy = BFSDeepCrawlStrategy( max_depth=3, filter_chain=DomainFilter(allowed_domains=[\"docs.python.org\"]) ) # Configure the crawler to use our strategy config = CrawlerRunConfig(deep_crawl_strategy=strategy) # Start crawling from the tutorials page async with AsyncWebCrawler() as crawler: results = await crawler.arun( url=\"https://docs.python.org/3/tutorial/\", config=config ) # Process the results for result in results: print(f\"Page: {result.url}\") . This code sets up a crawler that: . | Starts at the Python tutorial page | Explores up to 3 links deep | Only stays within the docs.python.org domain | Returns all the pages it found | . ",
    "url": "/Crawl4AI/05_deep_crawling_system_.html#a-simple-example-exploring-a-documentation-site",
    
    "relUrl": "/Crawl4AI/05_deep_crawling_system_.html#a-simple-example-exploring-a-documentation-site"
  },"177": {
    "doc": "Chapter 5: Deep Crawling System",
    "title": "Understanding Crawling Strategies",
    "content": "The Deep Crawling System offers different ways to explore a website, just like you’d have different strategies for exploring a city: . 1. Breadth-First Search (BFS) . from crawl4ai.deep_crawling import BFSDeepCrawlStrategy bfs_strategy = BFSDeepCrawlStrategy(max_depth=2) . BFS is like exploring a city one block at a time, checking all streets on your current block before going to the next block. In web terms, it explores all links on the current page before going deeper. 2. Depth-First Search (DFS) . from crawl4ai.deep_crawling import DFSDeepCrawlStrategy dfs_strategy = DFSDeepCrawlStrategy(max_depth=2) . DFS is like following a single street as far as it goes before coming back to try the next street. In web terms, it follows each path to its maximum depth before trying other paths. 3. Best-First Search . from crawl4ai.deep_crawling import BestFirstCrawlingStrategy from crawl4ai.deep_crawling.scorers import KeywordRelevanceScorer scorer = KeywordRelevanceScorer(keywords=[\"tutorial\", \"guide\"]) best_first = BestFirstCrawlingStrategy(max_depth=2, url_scorer=scorer) . Best-First is like having a treasure map that guides you to the most promising areas first. It uses scorers to decide which links are most likely to have what you’re looking for. ",
    "url": "/Crawl4AI/05_deep_crawling_system_.html#understanding-crawling-strategies",
    
    "relUrl": "/Crawl4AI/05_deep_crawling_system_.html#understanding-crawling-strategies"
  },"178": {
    "doc": "Chapter 5: Deep Crawling System",
    "title": "Setting Crawling Boundaries",
    "content": "Just as you’d set limits on a real-world exploration, you need to set boundaries for your web crawler: . # Limit by depth (how many clicks from the start page) strategy = BFSDeepCrawlStrategy(max_depth=3) # Limit by total pages strategy = BFSDeepCrawlStrategy(max_depth=3, max_pages=100) # Limit by domain from crawl4ai.deep_crawling.filters import DomainFilter filter_chain = DomainFilter(allowed_domains=[\"example.com\"]) strategy = BFSDeepCrawlStrategy(max_depth=3, filter_chain=filter_chain) . These boundaries help keep your crawler focused and efficient. ",
    "url": "/Crawl4AI/05_deep_crawling_system_.html#setting-crawling-boundaries",
    
    "relUrl": "/Crawl4AI/05_deep_crawling_system_.html#setting-crawling-boundaries"
  },"179": {
    "doc": "Chapter 5: Deep Crawling System",
    "title": "Streaming vs. Batch Processing",
    "content": "You can process crawled pages as they come in (streaming) or wait for all results (batch): . # Streaming: Process pages as they're found config = CrawlerRunConfig(deep_crawl_strategy=strategy, stream=True) async with AsyncWebCrawler() as crawler: async for result in await crawler.arun(url=\"https://example.com\", config=config): print(f\"Just found: {result.url}\") . Streaming is great when you want to start processing right away, without waiting for the entire crawl to finish. ",
    "url": "/Crawl4AI/05_deep_crawling_system_.html#streaming-vs-batch-processing",
    
    "relUrl": "/Crawl4AI/05_deep_crawling_system_.html#streaming-vs-batch-processing"
  },"180": {
    "doc": "Chapter 5: Deep Crawling System",
    "title": "A Real-World Example: News Aggregator",
    "content": "Let’s build a simple news aggregator that collects recent articles from a news site: . from crawl4ai import AsyncWebCrawler, CrawlerRunConfig from crawl4ai.deep_crawling import BestFirstCrawlingStrategy from crawl4ai.deep_crawling.scorers import FreshnessScorer, KeywordRelevanceScorer from crawl4ai.deep_crawling.filters import URLPatternFilter async def crawl_news(): # Create a scorer that prioritizes recent articles about technology scorer = FreshnessScorer(weight=0.7) # Exclude category pages and author pages filter = URLPatternFilter(patterns=[\"/author/*\", \"/category/*\"], reverse=True) # Create a strategy that favors fresh content strategy = BestFirstCrawlingStrategy( max_depth=2, max_pages=10, # Limit to 10 articles url_scorer=scorer, filter_chain=filter ) # Start crawling config = CrawlerRunConfig(deep_crawl_strategy=strategy) async with AsyncWebCrawler() as crawler: results = await crawler.arun(url=\"https://news-site.com\", config=config) return results . This crawler will prioritize the most recent articles and ignore pages like author profiles and category listings. ",
    "url": "/Crawl4AI/05_deep_crawling_system_.html#a-real-world-example-news-aggregator",
    
    "relUrl": "/Crawl4AI/05_deep_crawling_system_.html#a-real-world-example-news-aggregator"
  },"181": {
    "doc": "Chapter 5: Deep Crawling System",
    "title": "What Happens Under the Hood",
    "content": "What actually happens when you start a deep crawl? Let’s visualize the process: . sequenceDiagram participant User as Your Code participant Crawler as WebCrawler participant Strategy as DeepCrawlStrategy participant Queue as URL Queue participant FC as Filter Chain User-&gt;&gt;Crawler: arun(start_url, config) Crawler-&gt;&gt;Strategy: arun(start_url) Strategy-&gt;&gt;Queue: add start_url loop Until queue is empty Strategy-&gt;&gt;Queue: get next URL Strategy-&gt;&gt;FC: can_process_url(url)? FC-&gt;&gt;Strategy: yes/no Strategy-&gt;&gt;Crawler: crawler.arun(url) Crawler-&gt;&gt;Strategy: return result Strategy-&gt;&gt;Strategy: extract new URLs loop For each new URL Strategy-&gt;&gt;FC: can_process_url(new_url)? FC-&gt;&gt;Strategy: yes/no Strategy-&gt;&gt;Queue: add URL if approved end Strategy-&gt;&gt;User: yield result (if streaming) end Strategy-&gt;&gt;User: return all results (if batch mode) . This process repeats until either: . | The queue is empty (no more URLs to crawl) | We’ve reached the maximum depth | We’ve crawled the maximum number of pages | . ",
    "url": "/Crawl4AI/05_deep_crawling_system_.html#what-happens-under-the-hood",
    
    "relUrl": "/Crawl4AI/05_deep_crawling_system_.html#what-happens-under-the-hood"
  },"182": {
    "doc": "Chapter 5: Deep Crawling System",
    "title": "Implementation Details",
    "content": "Let’s look at how the BFS crawling strategy is implemented: . # Simplified from crawl4ai/deep_crawling/bfs_strategy.py async def _arun_stream(self, start_url, crawler, config): visited = set() current_level = [(start_url, None)] # (url, parent_url) depths = {start_url: 0} while current_level and not self._cancel_event.is_set(): next_level = [] urls = [url for url, _ in current_level] # Crawl all URLs at the current level stream_gen = await crawler.arun_many(urls=urls, config=config) async for result in stream_gen: # Process each result url = result.url depth = depths.get(url, 0) result.metadata[\"depth\"] = depth yield result # Discover new links for the next level if result.success: await self.link_discovery( result, url, depth, visited, next_level, depths ) # Move to the next level current_level = next_level . This implementation shows how the BFS strategy works: . | It starts with a single URL at depth 0 | It processes all URLs at the current depth | It extracts new links and adds them to the next level | It moves to the next level and repeats | . The link_discovery method is where URLs are filtered: . # Simplified from link_discovery method async def link_discovery(self, result, source_url, current_depth, visited, next_level, depths): next_depth = current_depth + 1 if next_depth &gt; self.max_depth: return links = result.links.get(\"internal\", []) for link in links: url = link.get(\"href\") if url in visited: continue if not await self.can_process_url(url, next_depth): continue visited.add(url) next_level.append((url, source_url)) depths[url] = next_depth . This method: . | Checks if we’ve reached the maximum depth | Gets all the links from the current page | Filters out already visited URLs | Applies URL filters to decide which URLs to follow | Adds approved URLs to the next level | . ",
    "url": "/Crawl4AI/05_deep_crawling_system_.html#implementation-details",
    
    "relUrl": "/Crawl4AI/05_deep_crawling_system_.html#implementation-details"
  },"183": {
    "doc": "Chapter 5: Deep Crawling System",
    "title": "Choosing the Right Strategy",
    "content": "Which strategy should you choose? Here’s a quick guide: . | BFS: When you want a broad overview of a website, exploring many sections . | DFS: When you’re looking for something specific and want to go deep into specific paths . | Best-First: When you have a good idea of what you’re looking for and want to find the most relevant content first . | . ",
    "url": "/Crawl4AI/05_deep_crawling_system_.html#choosing-the-right-strategy",
    
    "relUrl": "/Crawl4AI/05_deep_crawling_system_.html#choosing-the-right-strategy"
  },"184": {
    "doc": "Chapter 5: Deep Crawling System",
    "title": "Advanced Usage: Combining Strategies",
    "content": "You can combine different aspects of the Deep Crawling System for more sophisticated crawling: . from crawl4ai.deep_crawling import BestFirstCrawlingStrategy from crawl4ai.deep_crawling.filters import FilterChain, DomainFilter, ContentTypeFilter from crawl4ai.deep_crawling.scorers import CompositeScorer, KeywordScorer, FreshnessScorer # Create multiple filters domain_filter = DomainFilter(allowed_domains=[\"example.com\"]) content_filter = ContentTypeFilter(allowed_types=[\"text/html\"]) filter_chain = FilterChain([domain_filter, content_filter]) # Create multiple scorers keyword_scorer = KeywordScorer(keywords=[\"python\", \"tutorial\"]) freshness_scorer = FreshnessScorer() composite_scorer = CompositeScorer([keyword_scorer, freshness_scorer]) # Combine them in a strategy strategy = BestFirstCrawlingStrategy( max_depth=3, filter_chain=filter_chain, url_scorer=composite_scorer, max_pages=50 ) . This creates a powerful crawler that stays within example.com, only crawls HTML pages, and prioritizes fresh content about Python tutorials. ",
    "url": "/Crawl4AI/05_deep_crawling_system_.html#advanced-usage-combining-strategies",
    
    "relUrl": "/Crawl4AI/05_deep_crawling_system_.html#advanced-usage-combining-strategies"
  },"185": {
    "doc": "Chapter 5: Deep Crawling System",
    "title": "Working with Crawl Results",
    "content": "Once you’ve collected pages, you can do many things with the results: . async def process_results(results): for result in results: # Access metadata about the crawl depth = result.metadata.get(\"depth\", 0) parent = result.metadata.get(\"parent_url\") # Process the content print(f\"Page {result.url} at depth {depth}\") print(f\"Title: {result.title}\") print(f\"Content: {result.markdown[:100]}...\") # First 100 chars # Check for specific patterns if \"download\" in result.markdown.lower(): print(\"Found a download page!\") . Each result includes the page content, metadata about the crawl, and all the links that were found. ",
    "url": "/Crawl4AI/05_deep_crawling_system_.html#working-with-crawl-results",
    
    "relUrl": "/Crawl4AI/05_deep_crawling_system_.html#working-with-crawl-results"
  },"186": {
    "doc": "Chapter 5: Deep Crawling System",
    "title": "Conclusion",
    "content": "The Deep Crawling System is a powerful tool that lets you explore websites systematically. By choosing the right strategy, setting appropriate boundaries, and configuring filters and scorers, you can efficiently find and extract exactly the content you need. In this chapter, we’ve learned: . | How to set up different crawling strategies (BFS, DFS, Best-First) | How to set boundaries on your crawling | How to process results in real-time or in batch | How the Deep Crawling System works under the hood | . In the next chapter, Caching System, we’ll learn how to store and reuse crawled content to make our crawlers even more efficient. Generated by AI Codebase Knowledge Builder . ",
    "url": "/Crawl4AI/05_deep_crawling_system_.html#conclusion",
    
    "relUrl": "/Crawl4AI/05_deep_crawling_system_.html#conclusion"
  },"187": {
    "doc": "Chapter 5: NDBuffer",
    "title": "Chapter 5: NDBuffer",
    "content": "Okay, here is Chapter 5 on NDBuffer, designed to be very beginner-friendly and in Markdown format. # Chapter 5: The N-Dimensional Buffer: `NDBuffer` Welcome to Chapter 5! So far, we've explored some fundamental Mojo concepts: * [Chapter 1: `AddressSpace`](01_addressspace_.md) (memory \"neighborhoods\") * [Chapter 2: `UnsafePointer`](02_unsafepointer_.md) (direct memory addresses) * [Chapter 3: `IndexList`](03_indexlist_.md) (for multi-dimensional coordinates) * [Chapter 4: `Dim` and `DimList`](04_dimlist_.md) (for describing dimension sizes, statically or dynamically) Now, we're going to put these pieces together to understand one of Mojo's most powerful data structures for numerical computing: `NDBuffer`. ## What is `NDBuffer`? The official description states: &gt; `NDBuffer` represents a multi-dimensional array or tensor, commonly used in numerical computing and machine learning. Think of it as a smart \"view\" or a \"lens\" that lets you interpret a flat, one-dimensional block of memory as a structured grid (like a 2D image, 3D volume, or higher-dimensional data). It doesn't own the memory itself but holds essential metadata: the data type of elements, the shape (dimensions of the grid), and strides (how many memory elements to skip to move to the next item along each dimension). This design allows for flexible data layouts (e.g., row-major, column-major) and efficient access without needing to copy the underlying data. Let's break that down: * **Multi-dimensional array/tensor**: Imagine a Python list, but it can have multiple dimensions. * A 1D `NDBuffer` is like a simple list or vector: `[1, 2, 3, 4]` * A 2D `NDBuffer` is like a grid or matrix (think spreadsheet or image): ``` [[1, 2, 3], [4, 5, 6]] ``` * A 3D `NDBuffer` is like a cube or volume of data: ``` [[[1,2], [3,4]], [[5,6], [7,8]]] ``` And it can go to even higher dimensions! * **\"View\" or \"Lens\"**: This is super important! Most of the time, an `NDBuffer` doesn't hold the actual data values itself. Instead, it *points* to some memory (managed by an `UnsafePointer` internally) and tells Mojo how to interpret that raw memory as a structured N-dimensional grid. * *Analogy*: Imagine you have a long, single line of LEGO bricks (this is your flat memory). An `NDBuffer` is like a special pair of glasses that lets you see those bricks as if they were arranged in a 2x2 square or a 3x4 rectangle, without actually moving the bricks. * **Metadata**: To act as this \"lens,\" `NDBuffer` stores crucial information: * **Data Type (`DType`)**: What kind of data is in each cell (e.g., `Float32`, `Int64`). * **Shape (`DimList`)**: The size of each dimension (e.g., a 2x3 matrix has shape `(2, 3)`). * **Strides (`DimList`)**: How many actual memory slots to jump over to get to the next element along each dimension. (We'll dive deeper into strides in the next chapter, but for now, know it helps `NDBuffer` find elements quickly in that flat memory). This \"view\" approach is very powerful because it allows for: * **Flexibility**: The same underlying data can be viewed in different ways (e.g., as a 4x4 matrix or as a 2x8 matrix, if memory layout allows). * **Efficiency**: You can perform operations like transposing a matrix or taking a slice (a sub-section) often without copying any data, just by creating a new `NDBuffer` with different shape/stride metadata. ## Anatomy of an `NDBuffer` Let's look at the parameters that define an `NDBuffer`. You'll see how concepts from previous chapters fit in: ```mojo struct NDBuffer[ mut: Bool, // Is the data mutable (changeable) through this NDBuffer? type: DType, // The data type of each element (e.g., DType.float32) rank: Int, // The number of dimensions (e.g., 2 for a matrix) origin: Origin[mut], // Advanced: Tracks memory validity and source shape: DimList = DimList.create_unknown[rank](), // From Ch4: Sizes of dimensions strides: DimList = DimList.create_unknown[rank](), // From Ch4: Memory step sizes *, // Parameters below are keyword-only alignment: Int = 1, // Memory alignment preference address_space: AddressSpace = AddressSpace.GENERIC, // From Ch1: Where memory lives exclusive: Bool = True, // Advanced: If this is the only pointer to the memory ] . Inside an NDBuffer, there are important fields: . | data: UnsafePointer[...]: (From Ch2) An UnsafePointer to the actual start of the data in memory. | dynamic_shape: IndexList[...]: (From Ch3) If the shape DimList had dynamic dimensions (?), this IndexList holds their actual runtime values. | dynamic_stride: IndexList[...]: (From Ch3) Similar to dynamic_shape, but for strides. | . For beginners, the most immediately important parameters are type, rank, and shape. mut determines if you can change data. address_space is often AddressSpace.GENERIC for CPU memory. ",
    "url": "/mojo-v1/05_ndbuffer_.html",
    
    "relUrl": "/mojo-v1/05_ndbuffer_.html"
  },"188": {
    "doc": "Chapter 5: NDBuffer",
    "title": "Creating an NDBuffer",
    "content": "There are a few ways to create an NDBuffer. 1. Using Existing Memory (e.g., an InlineArray) . Often, you’ll have some memory already (perhaps in an InlineArray which stores data directly, or from another source), and you want NDBuffer to provide a view over it. Let’s look at the test_ndbuffer example from stdlib/test/buffer/test_ndbuffer.mojo: . from buffer import NDBuffer, DimList // Import NDBuffer and DimList from memory import InlineArray // For stack-allocated flat array from builtin import DType, Scalar // For data types fn create_ndbuffer_from_inline_array(): // First, reserve some flat memory. // InlineArray creates memory on the \"stack\" (fast, temporary memory). // We need space for 4*4 = 16 elements of type DType.index. // DType.index is Mojo's default integer type, often 64-bit. var matrix_data_storage = InlineArray[Scalar[DType.index], 16](uninitialized=True) // Now, create an NDBuffer to view this memory as a 4x4 matrix. var matrix = NDBuffer[ DType.index, // Element data type 2, // Rank: 2 dimensions (it's a matrix) _, // Origin: We use `_` to let Mojo infer a suitable default. // This is an advanced parameter for tracking memory origin. DimList(4, 4) // Shape: A 4x4 matrix (Static dimensions from Chapter 4) ](matrix_data_storage) // The NDBuffer will \"view\" the data in matrix_data_storage // At this point, 'matrix' is a 4x4 NDBuffer, but its data is uninitialized. // We can now fill it. For example: matrix[0, 0] = 0 matrix[0, 1] = 1 // ... and so on. print(\"Created matrix. Element at (0,0) after setting:\", matrix[0,0]) . In this case: . | matrix_data_storage holds the raw, flat memory. | matrix (the NDBuffer) doesn’t copy the data; it just knows how to interpret the memory in matrix_data_storage as a 4x4 grid. | DimList(4, 4) tells Mojo the shape is fixed at compile time. If the shape were dynamic, we might use DimList(Dim(), Dim()) and provide an IndexList with actual dimensions later. | . Memory Ownership Note: When you create an NDBuffer this way, you are responsible for making sure matrix_data_storage (or whatever memory matrix.data points to) stays valid for as long as matrix is used. NDBuffer itself doesn’t manage this external memory’s lifetime. 2. Using NDBuffer.stack_allocation() . Sometimes, you want an NDBuffer that also manages its own small piece of memory, allocated on the stack. This is useful for small, temporary N-dimensional arrays where you know the size at compile time. From test_aligned_load_store in stdlib/test/buffer/test_ndbuffer.mojo: . from buffer import NDBuffer, DimList from memory import MutableAnyOrigin // Needed for stack_allocation's origin from builtin import DType fn create_ndbuffer_on_stack(): // Create a 4x4 NDBuffer. The memory will be allocated on the stack. var matrix_on_stack = NDBuffer[ DType.index, // Element data type 2, // Rank: 2 dimensions MutableAnyOrigin, // Origin: Special type indicating new, mutable stack memory DimList(4, 4) // Shape: A 4x4 matrix (must be static for stack_allocation) ].stack_allocation[alignment=128]() // Allocate memory on stack, specify alignment // Now matrix_on_stack is ready to use. matrix_on_stack[0, 0] = 123 print(\"Stack allocated matrix. Element at (0,0):\", matrix_on_stack[0,0]) // Memory for matrix_on_stack is automatically reclaimed when it goes out of scope. Key differences: . | The shape (DimList(4, 4)) must be fully static (no Dim()). | We use MutableAnyOrigin to tell Mojo this is fresh memory. | The .stack_allocation() call does the memory allocation. | Memory Ownership: In this case, the NDBuffer does manage the lifetime of its stack-allocated memory. It’s automatically freed when matrix_on_stack is no longer in use. | . ",
    "url": "/mojo-v1/05_ndbuffer_.html#creating-an-ndbuffer",
    
    "relUrl": "/mojo-v1/05_ndbuffer_.html#creating-an-ndbuffer"
  },"189": {
    "doc": "Chapter 5: NDBuffer",
    "title": "Working with NDBuffer Elements",
    "content": "Setting Values . You can set values in an NDBuffer using square brackets []. You can provide the indices as a sequence of integers or as an IndexList (from Chapter 3). from utils import IndexList // For using IndexList explicitly // Assuming 'matrix' is our 4x4 NDBuffer from earlier: // Method 1: Multiple integer arguments matrix[0, 0] = 0 matrix[0, 1] = 1 matrix[0, 2] = 2 matrix[0, 3] = 3 matrix[1, 0] = 4 // ... and so on for all 16 elements. // Method 2: Using an IndexList matrix[IndexList[2](3, 3)] = 15 // Set the element at row 3, column 3 . The test_ndbuffer function in stdlib/test/buffer/test_ndbuffer.mojo shows populating a 4x4 matrix like this: . // (matrix is a 4x4 NDBuffer[DType.index, 2, _, DimList(4,4)]) matrix[IndexList[2](0, 0)] = 0 matrix[IndexList[2](0, 1)] = 1 // ... matrix[IndexList[2](3, 3)] = 15 . Or, more simply with multiple arguments, as shown in later parts of the same test file for reading: matrix[0,0]. Getting Values . Similarly, you use [] to get values: . // Assuming 'matrix' is populated var val_0_0 = matrix[0, 0] // val_0_0 will be 0 var val_1_2 = matrix[1, 2] // val_1_2 will be 6 var val_3_3 = matrix[IndexList[2](3,3)] // val_3_3 will be 15 print(\"Value at (0,0) is:\", val_0_0) print(\"Value at (1,2) is:\", val_1_2) . The test_ndbuffer code prints all elements this way: . // CHECK: 0 print(matrix[0, 0]) // CHECK: 1 print(matrix[0, 1]) // ... // CHECK: 15 print(matrix[3, 3]) . ",
    "url": "/mojo-v1/05_ndbuffer_.html#working-with-ndbuffer-elements",
    
    "relUrl": "/mojo-v1/05_ndbuffer_.html#working-with-ndbuffer-elements"
  },"190": {
    "doc": "Chapter 5: NDBuffer",
    "title": "Common NDBuffer Operations and Methods",
    "content": "NDBuffer comes with many useful methods. Let’s look at some from stdlib/test/buffer/test_ndbuffer.mojo. Getting Information . | get_rank() -&gt; Int: Returns the number of dimensions. // For our 4x4 matrix: print(matrix.get_rank()) // CHECK: 2 . | size() -&gt; Int (or num_elements() -&gt; Int): Returns the total number of elements. // For our 4x4 matrix (4 * 4 = 16 elements): print(matrix.size()) // CHECK: 16 . | dim[compile_time_index]() -&gt; Int or dim(runtime_index: Int) -&gt; Int: Gets the size of a specific dimension. var shape_of_matrix = matrix.get_shape() // Returns an IndexList, e.g., (4, 4) print(\"Dim 0:\", matrix.dim[0]()) // Compile-time index: matrix.dim[0]() -&gt; 4 print(\"Dim 1:\", matrix.dim(1)) // Runtime index: matrix.dim(1) -&gt; 4 . | get_shape() -&gt; IndexList[rank]: Returns the shape as an IndexList. | get_strides() -&gt; IndexList[rank]: Returns the strides (more on this in the next chapter). | . Modifying Content . | fill(value: Scalar[type]): Sets all elements in the NDBuffer to a single value. The buffer must be contiguous (its elements laid out sequentially in memory, which is typical for default NDBuffers). // From test_fill: var filled_stack = InlineArray[Scalar[DType.index], 3 * 3](uninitialized=True) var filled_buffer = NDBuffer[DType.index, 2, _, DimList(3, 3)](filled_stack) filled_buffer.fill(1) // All 9 elements of filled_buffer become 1 // (The test then compares this to a manually filled buffer) . | . Converting Indices . | get_nd_index(flat_index: Int) -&gt; IndexList[rank]: Converts a 1D “flat” index (as if all elements were in a single line) into N-Dimensional coordinates. // From test_get_nd_index: // matrix0 is a 2x3 NDBuffer (total 6 elements, indexed 0 to 5) // [[(0,0), (0,1), (0,2)], // [(1,0), (1,1), (1,2)]] // Flat indices: 0, 1, 2, 3, 4, 5 // matrix0.get_nd_index(0) -&gt; (0,0) // matrix0.get_nd_index(1) -&gt; (0,1) // matrix0.get_nd_index(3) -&gt; (1,0) // matrix0.get_nd_index(5) -&gt; (1,2) // In the test: // var matrix0 = NDBuffer[DType.index, 2, _, DimList(2, 3)](...) // print(matrix0.get_nd_index(3)) // CHECK: (1, 0) // print(matrix0.get_nd_index(5)) // CHECK: (1, 2) . | . Slicing and Viewing (Creating Tiles) . | tile[*tile_sizes: Dim](tile_coords: IndexList[rank]) -&gt; NDBuffer: Creates a new NDBuffer that is a “view” (not a copy!) of a smaller N-D sub-section of the original buffer. This is powerful for working with parts of larger arrays. *tile_sizes are the dimensions of the tile you want to extract. tile_coords are the coordinates in terms of tiles. // From test_ndbuffer_tile (simplified conceptual view) // buff is an 8x8 NDBuffer // Let's get a 4x4 tile starting at the first tile block (0,0) // var tile_4x4 = buff.tile[4, 4](Index(0,0)) // 'tile_4x4' is now a 4x4 NDBuffer viewing the top-left 4x4 part of 'buff'. // Changes to 'tile_4x4' will affect 'buff' and vice-versa. // The test code iterates to show tiling: // alias M = 8, N = 8, m0_tile_size = 4, n0_tile_size = 4 // var buff = NDBuffer[DType.float32, 2, _, DimList(M, N)](...) // ... fill buff ... // for tile_i in range(M // m0_tile_size): // 0 to 1 // for tile_j in range(N // n0_tile_size): // 0 to 1 // print(\"tile-0[\", tile_i, tile_j, \"]\") // var tile_4x4 = buff.tile[m0_tile_size, n0_tile_size]( // tile_coords=Index(tile_i, tile_j) // ) // print_buffer(tile_4x4) // Prints the 4x4 tile . This is a very efficient way to work with sub-arrays. | . Printing . | Printing an NDBuffer or converting it to a String gives a nice, human-readable representation. // From test_print: // buffer is a 2x2x3 NDBuffer // String(buffer) produces: // \"NDBuffer([[[0, 1, 2], // [3, 4, 5]], // [[6, 7, 8], // [9, 10, 11]]], dtype=index, shape=2x2x3)\" . | . File I/O . | tofile(path: Path): Writes the raw byte contents of the NDBuffer to a binary file. The buffer should be contiguous. // From test_ndbuffer_tofile: // var buf = NDBuffer[DType.float32, 2, _, DimList(2, 2)](...) // buf.fill(2.0) // with NamedTemporaryFile(name=String(\"test_ndbuffer\")) as TEMP_FILE: // buf.tofile(TEMP_FILE.name) // // The test then reads it back to verify . | . Performance Features (Brief Mention for Awareness) . NDBuffer also has features for advanced performance tuning, which you might encounter later: . | prefetch[PrefetchOptions()](*idx: Int): Hints to the CPU/GPU to start loading data from a specific index into its cache before it’s explicitly needed. (See test_ndbuffer_prefetch). | load[width=N](*idx: Int) / store[width=N](*idx: Int, value: SIMD[...]): Perform vectorized (SIMD) loads and stores, reading/writing multiple elements at once if the hardware supports it and memory is aligned. (See test_aligned_load_store). These require contiguous memory. | . ",
    "url": "/mojo-v1/05_ndbuffer_.html#common-ndbuffer-operations-and-methods",
    
    "relUrl": "/mojo-v1/05_ndbuffer_.html#common-ndbuffer-operations-and-methods"
  },"191": {
    "doc": "Chapter 5: NDBuffer",
    "title": "Key Takeaways for NDBuffer",
    "content": ". | NDBuffer is Mojo’s primary tool for working with multi-dimensional data (vectors, matrices, tensors). | It’s usually a “view” or “lens” over existing memory, defined by metadata (dtype, shape, strides). | It doesn’t typically own the memory unless created with stack_allocation(). You must manage the lifetime of the underlying memory if NDBuffer is just a view. | Creation involves specifying element DType, rank (number of dimensions), and shape (using DimList). | Elements are accessed and modified using [index0, index1, ...]. | Provides useful methods like get_rank(), size(), fill(), get_nd_index(), tile(), and tofile(). | The ability to mix static and dynamic dimensions (via DimList) allows for both compile-time optimization and runtime flexibility. | . ",
    "url": "/mojo-v1/05_ndbuffer_.html#key-takeaways-for-ndbuffer",
    
    "relUrl": "/mojo-v1/05_ndbuffer_.html#key-takeaways-for-ndbuffer"
  },"192": {
    "doc": "Chapter 5: NDBuffer",
    "title": "Full Example: A Simple 2x3 Matrix",
    "content": "Let’s create a small 2x3 matrix, fill it, and print some info. from buffer import NDBuffer, DimList from memory import InlineArray, MutableAnyOrigin from builtin import DType, Scalar from utils import IndexList fn main_ndbuffer_example(): print(\"--- NDBuffer Simple 2x3 Matrix Example ---\") // Method 1: View over InlineArray print(\"\\n1. NDBuffer as a view over InlineArray:\") var flat_data = InlineArray[Scalar[DType.int32], 2 * 3](uninitialized=True) var matrix_view = NDBuffer[ DType.int32, 2, // rank _, // origin (inferred) DimList(2, 3) // shape: 2 rows, 3 columns ](flat_data) // Populate matrix_view for r in range(2): for c in range(3): matrix_view[r, c] = r * 10 + c print(\"Matrix View:\") print(matrix_view) print(\"Rank:\", matrix_view.get_rank()) print(\"Size:\", matrix_view.size()) print(\"Shape:\", matrix_view.get_shape()) print(\"Element (1,1):\", matrix_view[1,1]) // Method 2: Stack-allocated NDBuffer print(\"\\n2. Stack-allocated NDBuffer:\") var matrix_stack = NDBuffer[ DType.int32, 2, // rank MutableAnyOrigin, DimList(2,3) // shape ].stack_allocation() // Populate matrix_stack for r in range(2): for c in range(3): matrix_stack[r, c] = (r+1) * 100 + (c+1) print(\"Matrix Stack:\") print(matrix_stack) print(\"Element (0,2):\", matrix_stack[0,2]) print(\"\\n--- End of Example ---\") # To run this, you'd call main_ndbuffer_example() # main_ndbuffer_example() . ",
    "url": "/mojo-v1/05_ndbuffer_.html#full-example-a-simple-2x3-matrix",
    
    "relUrl": "/mojo-v1/05_ndbuffer_.html#full-example-a-simple-2x3-matrix"
  },"193": {
    "doc": "Chapter 5: NDBuffer",
    "title": "Next Steps",
    "content": "NDBuffer is incredibly versatile. A key to its power is how it uses strides to navigate its underlying flat memory. In the next chapter, we’ll delve into “N-D to 1D Indexing Logic (Strided Memory Access)” to understand how NDBuffer calculates the exact memory location for an element like matrix[row, col, depth] and how strides enable different memory layouts (like row-major vs. column-major). This will complete your foundational understanding of NDBuffer! . Table of Contents . | Chapter 1: Understanding Memory Neighborhoods with AddressSpace | Chapter 2: Peeking into Memory with UnsafePointer | Chapter 3: Working with Multiple Dimensions: IndexList | Chapter 4: Describing Dimensions with Dim and DimList | Chapter 5: The N-Dimensional Buffer: NDBuffer (You are here) | Coming Soon: Chapter 6: N-D to 1D Indexing Logic (Strided Memory Access) ``` | . Generated by AI Codebase Knowledge Builder . ",
    "url": "/mojo-v1/05_ndbuffer_.html#next-steps",
    
    "relUrl": "/mojo-v1/05_ndbuffer_.html#next-steps"
  },"194": {
    "doc": "RelevantContentFilter",
    "title": "Chapter 5: Focusing on What Matters - RelevantContentFilter",
    "content": "In Chapter 4: Cleaning Up the Mess - ContentScrapingStrategy, we learned how Crawl4AI takes the raw, messy HTML from a webpage and cleans it up using a ContentScrapingStrategy. This gives us a tidier version of the HTML (cleaned_html) and extracts basic elements like links and images. But even after this initial cleanup, the page might still contain a lot of “noise” relative to what we actually care about. Imagine a news article page: the ContentScrapingStrategy might remove scripts and styles, but it could still leave the main article text, plus related article links, user comments, sidebars with ads, and maybe a lengthy footer. If our goal is just to get the main article content (e.g., to summarize it or feed it to an AI), all that extra stuff is just noise. How can we filter the cleaned content even further to keep only the truly relevant parts? . ",
    "url": "/Crawl4AI/05_relevantcontentfilter.html#chapter-5-focusing-on-what-matters---relevantcontentfilter",
    
    "relUrl": "/Crawl4AI/05_relevantcontentfilter.html#chapter-5-focusing-on-what-matters---relevantcontentfilter"
  },"195": {
    "doc": "RelevantContentFilter",
    "title": "What Problem Does RelevantContentFilter Solve?",
    "content": "Think of the cleaned_html from the previous step like flour that’s been roughly sifted – the biggest lumps are gone, but there might still be smaller clumps or bran mixed in. If you want super fine flour for a delicate cake, you need a finer sieve. RelevantContentFilter acts as this finer sieve or a Relevance Sieve. It’s a strategy applied after the initial cleaning by ContentScrapingStrategy but before the final processing (like generating the final Markdown output or using an AI for extraction). Its job is to go through the cleaned content and decide which parts are truly relevant to our goal, removing the rest. This helps us: . | Reduce Noise: Eliminate irrelevant sections like comments, footers, navigation bars, or tangential “related content” blocks. | Focus AI: If we’re sending the content to a Large Language Model (LLM), feeding it only the most relevant parts saves processing time (and potentially money) and can lead to better results. | Improve Accuracy: By removing distracting noise, subsequent steps like data extraction are less likely to grab the wrong information. | . ",
    "url": "/Crawl4AI/05_relevantcontentfilter.html#what-problem-does-relevantcontentfilter-solve",
    
    "relUrl": "/Crawl4AI/05_relevantcontentfilter.html#what-problem-does-relevantcontentfilter-solve"
  },"196": {
    "doc": "RelevantContentFilter",
    "title": "What is RelevantContentFilter?",
    "content": "RelevantContentFilter is an abstract concept (a blueprint) in Crawl4AI representing a method for identifying and retaining only the relevant portions of cleaned HTML content. It defines that we need a way to filter for relevance, but the specific technique used can vary. This allows us to choose different filtering approaches depending on the task and the type of content. ",
    "url": "/Crawl4AI/05_relevantcontentfilter.html#what-is-relevantcontentfilter",
    
    "relUrl": "/Crawl4AI/05_relevantcontentfilter.html#what-is-relevantcontentfilter"
  },"197": {
    "doc": "RelevantContentFilter",
    "title": "The Different Filters: Tools for Sieving",
    "content": "Crawl4AI provides several concrete implementations (the actual sieves) of RelevantContentFilter: . | BM25ContentFilter (The Keyword Sieve): . | Analogy: Like a mini search engine operating within the webpage. | How it Works: You give it (or it figures out) some keywords related to what you’re looking for (e.g., from a user query like “product specifications” or derived from the page title). It then uses a search algorithm called BM25 to score different chunks of the cleaned HTML based on how relevant they are to those keywords. Only the chunks scoring above a certain threshold are kept. | Good For: Finding specific sections about a known topic within a larger page (e.g., finding only the paragraphs discussing “climate change impact” on a long environmental report page). | . | PruningContentFilter (The Structural Sieve): . | Analogy: Like a gardener pruning a bush, removing weak or unnecessary branches based on their structure. | How it Works: This filter doesn’t care about keywords. Instead, it looks at the structure and characteristics of the HTML elements. It removes elements that often represent noise, such as those with very little text compared to the number of links (low text density), elements with common “noise” words in their CSS classes or IDs (like sidebar, comments, footer), or elements deemed structurally insignificant. | Good For: Removing common boilerplate sections (like headers, footers, simple sidebars, navigation) based purely on layout and density clues, even if you don’t have a specific topic query. | . | LLMContentFilter (The AI Sieve): . | Analogy: Asking a smart assistant to read the cleaned content and pick out only the parts relevant to your request. | How it Works: This filter sends the cleaned HTML (often broken into manageable chunks) to a Large Language Model (like GPT). You provide an instruction (e.g., “Extract only the main article content, removing all comments and related links” or “Keep only the sections discussing financial results”). The AI uses its understanding of language and context to identify and return only the relevant parts, often already formatted nicely (like in Markdown). | Good For: Handling complex relevance decisions that require understanding meaning and context, following nuanced natural language instructions. (Note: Requires configuring LLM access, like API keys, and can be slower and potentially costlier than other methods). | . | . ",
    "url": "/Crawl4AI/05_relevantcontentfilter.html#the-different-filters-tools-for-sieving",
    
    "relUrl": "/Crawl4AI/05_relevantcontentfilter.html#the-different-filters-tools-for-sieving"
  },"198": {
    "doc": "RelevantContentFilter",
    "title": "How RelevantContentFilter is Used (Via Markdown Generation)",
    "content": "In Crawl4AI, the RelevantContentFilter is typically integrated into the Markdown generation step. The standard markdown generator (DefaultMarkdownGenerator) can accept a RelevantContentFilter instance. When configured this way: . | The AsyncWebCrawler fetches the page and uses the ContentScrapingStrategy to get cleaned_html. | It then calls the DefaultMarkdownGenerator to produce the Markdown output. | The generator first creates the standard, “raw” Markdown from the entire cleaned_html. | If a RelevantContentFilter was provided to the generator, it then uses this filter on the cleaned_html to select only the relevant HTML fragments. | It converts these filtered fragments into Markdown. This becomes the fit_markdown. | . So, the CrawlResult will contain both: . | result.markdown.raw_markdown: Markdown based on the full cleaned_html. | result.markdown.fit_markdown: Markdown based only on the parts deemed relevant by the filter. | . Let’s see how to configure this. Example 1: Using BM25ContentFilter to find specific content . Imagine we crawled a page about renewable energy, but we only want the parts specifically discussing solar power. # chapter5_example_1.py import asyncio from crawl4ai import ( AsyncWebCrawler, CrawlerRunConfig, DefaultMarkdownGenerator, # The standard markdown generator BM25ContentFilter # The keyword-based filter ) async def main(): # 1. Create the BM25 filter with our query solar_filter = BM25ContentFilter(user_query=\"solar power technology\") print(f\"Filter created for query: '{solar_filter.user_query}'\") # 2. Create a Markdown generator that USES this filter markdown_generator_with_filter = DefaultMarkdownGenerator( content_filter=solar_filter ) print(\"Markdown generator configured with BM25 filter.\") # 3. Create CrawlerRunConfig using this specific markdown generator run_config = CrawlerRunConfig( markdown_generator=markdown_generator_with_filter ) # 4. Run the crawl async with AsyncWebCrawler() as crawler: # Example URL (replace with a real page having relevant content) url_to_crawl = \"https://en.wikipedia.org/wiki/Renewable_energy\" print(f\"\\nCrawling {url_to_crawl}...\") result = await crawler.arun(url=url_to_crawl, config=run_config) if result.success: print(\"\\nCrawl successful!\") print(f\"Raw Markdown length: {len(result.markdown.raw_markdown)}\") print(f\"Fit Markdown length: {len(result.markdown.fit_markdown)}\") # The fit_markdown should be shorter and focused on solar power print(\"\\n--- Start of Fit Markdown (Solar Power Focus) ---\") # Print first 500 chars of the filtered markdown print(result.markdown.fit_markdown[:500] + \"...\") print(\"--- End of Fit Markdown Snippet ---\") else: print(f\"\\nCrawl failed: {result.error_message}\") if __name__ == \"__main__\": asyncio.run(main()) . Explanation: . | Create Filter: We make an instance of BM25ContentFilter, telling it we’re interested in “solar power technology”. | Create Generator: We make an instance of DefaultMarkdownGenerator and pass our solar_filter to its content_filter parameter. | Configure Run: We create CrawlerRunConfig and tell it to use our special markdown_generator_with_filter for this run. | Crawl &amp; Check: We run the crawl as usual. In the result, result.markdown.raw_markdown will have the markdown for the whole page, while result.markdown.fit_markdown will only contain markdown derived from the HTML parts that the BM25ContentFilter scored highly for relevance to “solar power technology”. You’ll likely see the fit_markdown is significantly shorter. | . Example 2: Using PruningContentFilter to remove boilerplate . Now, let’s try removing common noise like sidebars or footers based on structure, without needing a specific query. # chapter5_example_2.py import asyncio from crawl4ai import ( AsyncWebCrawler, CrawlerRunConfig, DefaultMarkdownGenerator, PruningContentFilter # The structural filter ) async def main(): # 1. Create the Pruning filter (no query needed) pruning_filter = PruningContentFilter() print(\"Filter created: PruningContentFilter (structural)\") # 2. Create a Markdown generator that uses this filter markdown_generator_with_filter = DefaultMarkdownGenerator( content_filter=pruning_filter ) print(\"Markdown generator configured with Pruning filter.\") # 3. Create CrawlerRunConfig using this generator run_config = CrawlerRunConfig( markdown_generator=markdown_generator_with_filter ) # 4. Run the crawl async with AsyncWebCrawler() as crawler: # Example URL (replace with a real page that has boilerplate) url_to_crawl = \"https://www.python.org/\" # Python homepage likely has headers/footers print(f\"\\nCrawling {url_to_crawl}...\") result = await crawler.arun(url=url_to_crawl, config=run_config) if result.success: print(\"\\nCrawl successful!\") print(f\"Raw Markdown length: {len(result.markdown.raw_markdown)}\") print(f\"Fit Markdown length: {len(result.markdown.fit_markdown)}\") # fit_markdown should have less header/footer/sidebar content print(\"\\n--- Start of Fit Markdown (Pruned) ---\") print(result.markdown.fit_markdown[:500] + \"...\") print(\"--- End of Fit Markdown Snippet ---\") else: print(f\"\\nCrawl failed: {result.error_message}\") if __name__ == \"__main__\": asyncio.run(main()) . Explanation: . The structure is the same as the BM25 example, but: . | We instantiate PruningContentFilter(), which doesn’t require a user_query. | We pass this filter to the DefaultMarkdownGenerator. | The resulting result.markdown.fit_markdown should contain Markdown primarily from the main content areas of the page, with structurally identified boilerplate removed. | . Example 3: Using LLMContentFilter (Conceptual) . Using LLMContentFilter follows the same pattern, but requires setting up LLM provider details. # chapter5_example_3_conceptual.py import asyncio from crawl4ai import ( AsyncWebCrawler, CrawlerRunConfig, DefaultMarkdownGenerator, LLMContentFilter, # Assume LlmConfig is set up correctly (see LLM-specific docs) # from crawl4ai.async_configs import LlmConfig ) # Assume llm_config is properly configured with API keys, provider, etc. # Example: llm_config = LlmConfig(provider=\"openai\", api_token=\"env:OPENAI_API_KEY\") # For this example, we'll pretend it's ready. class MockLlmConfig: # Mock for demonstration provider = \"mock_provider\" api_token = \"mock_token\" base_url = None llm_config = MockLlmConfig() async def main(): # 1. Create the LLM filter with an instruction instruction = \"Extract only the main news article content. Remove headers, footers, ads, comments, and related links.\" llm_filter = LLMContentFilter( instruction=instruction, llmConfig=llm_config # Pass the LLM configuration ) print(f\"Filter created: LLMContentFilter\") print(f\"Instruction: '{llm_filter.instruction}'\") # 2. Create a Markdown generator using this filter markdown_generator_with_filter = DefaultMarkdownGenerator( content_filter=llm_filter ) print(\"Markdown generator configured with LLM filter.\") # 3. Create CrawlerRunConfig run_config = CrawlerRunConfig( markdown_generator=markdown_generator_with_filter ) # 4. Run the crawl async with AsyncWebCrawler() as crawler: # Example URL (replace with a real news article) url_to_crawl = \"https://httpbin.org/html\" # Using simple page for demo print(f\"\\nCrawling {url_to_crawl}...\") # In a real scenario, this would call the LLM API result = await crawler.arun(url=url_to_crawl, config=run_config) if result.success: print(\"\\nCrawl successful!\") # The fit_markdown would contain the AI-filtered content print(\"\\n--- Start of Fit Markdown (AI Filtered - Conceptual) ---\") # Because we used a mock LLM/simple page, fit_markdown might be empty or simple. # On a real page with a real LLM, it would ideally contain just the main article. print(result.markdown.fit_markdown[:500] + \"...\") print(\"--- End of Fit Markdown Snippet ---\") else: print(f\"\\nCrawl failed: {result.error_message}\") if __name__ == \"__main__\": asyncio.run(main()) . Explanation: . | We create LLMContentFilter, providing our natural language instruction and the necessary llmConfig (which holds provider details and API keys - mocked here for simplicity). | We integrate it into DefaultMarkdownGenerator and CrawlerRunConfig as before. | When arun is called, the LLMContentFilter would (in a real scenario) interact with the configured LLM API, sending chunks of the cleaned_html and the instruction, then assembling the AI’s response into the fit_markdown. | . ",
    "url": "/Crawl4AI/05_relevantcontentfilter.html#how-relevantcontentfilter-is-used-via-markdown-generation",
    
    "relUrl": "/Crawl4AI/05_relevantcontentfilter.html#how-relevantcontentfilter-is-used-via-markdown-generation"
  },"199": {
    "doc": "RelevantContentFilter",
    "title": "Under the Hood: How Filtering Fits In",
    "content": "The RelevantContentFilter doesn’t run on its own; it’s invoked by another component, typically the DefaultMarkdownGenerator. Here’s the sequence: . sequenceDiagram participant User participant AWC as AsyncWebCrawler participant Config as CrawlerRunConfig participant Scraper as ContentScrapingStrategy participant MDGen as DefaultMarkdownGenerator participant Filter as RelevantContentFilter participant Result as CrawlResult User-&gt;&gt;AWC: arun(url, config=my_config) Note over AWC: Config includes Markdown Generator with a Filter AWC-&gt;&gt;Scraper: scrap(raw_html) Scraper--&gt;&gt;AWC: cleaned_html, links, etc. AWC-&gt;&gt;MDGen: generate_markdown(cleaned_html, config=my_config) Note over MDGen: Uses html2text for raw markdown MDGen--&gt;&gt;MDGen: raw_markdown = html2text(cleaned_html) Note over MDGen: Now, check for content_filter alt Filter Provided in MDGen MDGen-&gt;&gt;Filter: filter_content(cleaned_html) Filter--&gt;&gt;MDGen: filtered_html_fragments Note over MDGen: Uses html2text on filtered fragments MDGen--&gt;&gt;MDGen: fit_markdown = html2text(filtered_html_fragments) else No Filter Provided MDGen--&gt;&gt;MDGen: fit_markdown = \"\" (or None) end Note over MDGen: Generate citations if needed MDGen--&gt;&gt;AWC: MarkdownGenerationResult (raw, fit, references) AWC-&gt;&gt;Result: Package everything AWC--&gt;&gt;User: Return CrawlResult . Code Glimpse: . Inside crawl4ai/markdown_generation_strategy.py, the DefaultMarkdownGenerator’s generate_markdown method has logic like this (simplified): . # Simplified from markdown_generation_strategy.py from .models import MarkdownGenerationResult from .html2text import CustomHTML2Text from .content_filter_strategy import RelevantContentFilter # Import filter base class class DefaultMarkdownGenerator(MarkdownGenerationStrategy): # ... __init__ stores self.content_filter ... def generate_markdown( self, cleaned_html: str, # ... other params like base_url, options ... content_filter: Optional[RelevantContentFilter] = None, **kwargs, ) -&gt; MarkdownGenerationResult: h = CustomHTML2Text(...) # Setup html2text converter # ... apply options ... # 1. Generate raw markdown from the full cleaned_html raw_markdown = h.handle(cleaned_html) # ... post-process raw_markdown ... # 2. Convert links to citations (if enabled) markdown_with_citations, references_markdown = self.convert_links_to_citations(...) # 3. Generate fit markdown IF a filter is available fit_markdown = \"\" filtered_html = \"\" # Use the filter passed directly, or the one stored during initialization active_filter = content_filter or self.content_filter if active_filter: try: # Call the filter's main method filtered_html_fragments = active_filter.filter_content(cleaned_html) # Join fragments (assuming filter returns list of HTML strings) filtered_html = \"\\n\".join(filtered_html_fragments) # Convert ONLY the filtered HTML to markdown fit_markdown = h.handle(filtered_html) except Exception as e: fit_markdown = f\"Error during filtering: {e}\" # Log error... return MarkdownGenerationResult( raw_markdown=raw_markdown, markdown_with_citations=markdown_with_citations, references_markdown=references_markdown, fit_markdown=fit_markdown, # Contains the filtered result fit_html=filtered_html, # The HTML fragments kept by the filter ) . And inside crawl4ai/content_filter_strategy.py, you find the blueprint and implementations: . # Simplified from content_filter_strategy.py from abc import ABC, abstractmethod from typing import List # ... other imports like BeautifulSoup, BM25Okapi ... class RelevantContentFilter(ABC): \"\"\"Abstract base class for content filtering strategies\"\"\" def __init__(self, user_query: str = None, ...): self.user_query = user_query # ... common setup ... @abstractmethod def filter_content(self, html: str) -&gt; List[str]: \"\"\" Takes cleaned HTML, returns a list of HTML fragments deemed relevant by the specific strategy. \"\"\" pass # ... common helper methods like extract_page_query, is_excluded ... class BM25ContentFilter(RelevantContentFilter): def __init__(self, user_query: str = None, bm25_threshold: float = 1.0, ...): super().__init__(user_query) self.bm25_threshold = bm25_threshold # ... BM25 specific setup ... def filter_content(self, html: str) -&gt; List[str]: # 1. Parse HTML (e.g., with BeautifulSoup) # 2. Extract text chunks (candidates) # 3. Determine query (user_query or extracted) # 4. Tokenize query and chunks # 5. Calculate BM25 scores for chunks vs query # 6. Filter chunks based on score and threshold # 7. Return the HTML string of the selected chunks # ... implementation details ... relevant_html_fragments = [\"&lt;p&gt;Relevant paragraph 1...&lt;/p&gt;\", \"&lt;h2&gt;Relevant Section&lt;/h2&gt;...\"] # Placeholder return relevant_html_fragments # ... Implementations for PruningContentFilter and LLMContentFilter ... The key is that each filter implements the filter_content method, returning the list of HTML fragments it considers relevant. The DefaultMarkdownGenerator then uses these fragments to create the fit_markdown. ",
    "url": "/Crawl4AI/05_relevantcontentfilter.html#under-the-hood-how-filtering-fits-in",
    
    "relUrl": "/Crawl4AI/05_relevantcontentfilter.html#under-the-hood-how-filtering-fits-in"
  },"200": {
    "doc": "RelevantContentFilter",
    "title": "Conclusion",
    "content": "You’ve learned about RelevantContentFilter, Crawl4AI’s “Relevance Sieve”! . | It addresses the problem that even cleaned HTML can contain noise relative to a specific goal. | It acts as a strategy to filter cleaned HTML, keeping only the relevant parts. | Different filter types exist: BM25ContentFilter (keywords), PruningContentFilter (structure), and LLMContentFilter (AI/semantic). | It’s typically used within the DefaultMarkdownGenerator to produce a focused fit_markdown output in the CrawlResult, alongside the standard raw_markdown. | You configure it by passing the chosen filter instance to the DefaultMarkdownGenerator and then passing that generator to the CrawlerRunConfig. | . By using RelevantContentFilter, you can significantly improve the signal-to-noise ratio of the content you get from webpages, making downstream tasks like summarization or analysis more effective. But what if just getting relevant text isn’t enough? What if you need specific, structured data like product names, prices, and ratings from an e-commerce page, or names and affiliations from a list of conference speakers? . Next: Let’s explore how to extract structured data with Chapter 6: Getting Specific Data - ExtractionStrategy. Generated by AI Codebase Knowledge Builder . ",
    "url": "/Crawl4AI/05_relevantcontentfilter.html#conclusion",
    
    "relUrl": "/Crawl4AI/05_relevantcontentfilter.html#conclusion"
  },"201": {
    "doc": "RelevantContentFilter",
    "title": "RelevantContentFilter",
    "content": " ",
    "url": "/Crawl4AI/05_relevantcontentfilter.html",
    
    "relUrl": "/Crawl4AI/05_relevantcontentfilter.html"
  },"202": {
    "doc": "Chapter 5: Scheduler",
    "title": "Chapter 5: Scheduler (TokenGenerationScheduler, EmbeddingsScheduler)",
    "content": "In Chapter 4: Model Worker, we learned about the Model Worker, the dedicated workshop where the AI model does its heavy lifting. We saw that it receives requests from the EngineQueue. But how does the Model Worker decide when and how to feed these requests to the AI model? Does it process them one by one? Or is there a smarter way? . That’s where the Scheduler comes in! It’s like an intelligent traffic controller or a theme park ride operator working inside the Model Worker. Its main job is to make sure the AI model (our star attraction) is used as efficiently as possible. ",
    "url": "/modular_max/05_scheduler___tokengenerationscheduler____embeddingsscheduler___.html#chapter-5-scheduler-tokengenerationscheduler-embeddingsscheduler",
    
    "relUrl": "/modular_max/05_scheduler___tokengenerationscheduler____embeddingsscheduler___.html#chapter-5-scheduler-tokengenerationscheduler-embeddingsscheduler"
  },"203": {
    "doc": "Chapter 5: Scheduler",
    "title": "What Problem Does the Scheduler Solve?",
    "content": "Imagine our AI model is like a super-fast, super-powerful rollercoaster. | If we send only one person on the rollercoaster at a time, a lot of the seats will be empty. It’s not very efficient! | If we wait too long to fill the rollercoaster, people in line get impatient. | . The Scheduler’s job is to gather individual requests (people wanting to ride the rollercoaster) and group them into optimal batches (filling up the rollercoaster carts efficiently) before sending them to the AI model for processing. This “batching” is crucial for: . | Maximizing GPU Utilization: Modern AI models, especially those running on GPUs, perform much better when they process multiple pieces of data at once. Running one request at a time can leave the GPU underutilized. | Improving Throughput: By processing requests in batches, the overall number of requests handled per second (throughput) increases significantly. | . Think of the Scheduler as a theme park ride operator. They don’t start the ride with just one person in a 20-seat cart. They try to fill the cart (or a good portion of it) to make each run worthwhile. Similarly, the Scheduler groups incoming requests based on things like how many requests are waiting, how long they’ve been waiting, or how “big” each request is (e.g., how many tokens need processing). ",
    "url": "/modular_max/05_scheduler___tokengenerationscheduler____embeddingsscheduler___.html#what-problem-does-the-scheduler-solve",
    
    "relUrl": "/modular_max/05_scheduler___tokengenerationscheduler____embeddingsscheduler___.html#what-problem-does-the-scheduler-solve"
  },"204": {
    "doc": "Chapter 5: Scheduler",
    "title": "Meet the Schedulers: TokenGenerationScheduler and EmbeddingsScheduler",
    "content": "modular has different types of AI tasks, and so it has specialized Schedulers: . | TokenGenerationScheduler: This scheduler is used when the AI model’s job is to generate text (like writing a story or answering a question). It has specific strategies for: . | Context Encoding (CE): Processing the initial prompt (e.g., “Tell me a story about a dragon”). This often involves processing many tokens at once. | Token Generation (TG): Generating the subsequent words or tokens of the story, often one or a few at a time, repeatedly. The TokenGenerationScheduler is smart about forming batches for both these phases. | . | EmbeddingsScheduler: This scheduler is used when the AI model’s job is to calculate embeddings (numerical representations of text, useful for tasks like semantic search). The requests are typically more straightforward: take some text, produce a list of numbers. | . Both schedulers operate on the same core principle: batch requests efficiently. How Schedulers are Created . Remember in Chapter 4: Model Worker, we saw the model_worker_run_v3 function inside the Model Worker process? That’s where the appropriate scheduler is created. # Simplified from: src/max/serve/pipelines/model_worker.py # (Inside model_worker_run_v3) # ... # pipeline = model_factory() # This loads the AI model (TokenGenerator or EmbeddingsGenerator) scheduler: Scheduler # This will hold our chosen scheduler if isinstance(pipeline, TokenGenerator): # If it's a text-generating model, use TokenGenerationScheduler scheduler = _create_token_generation_scheduler( pipeline, pc, pipeline_config, queues ) elif isinstance(pipeline, EmbeddingsGenerator): # If it's an embeddings model, use EmbeddingsScheduler scheduler = _create_embeddings_scheduler( pipeline, pc, pipeline_config, queues ) else: raise ValueError(f\"Invalid pipeline type: {type(pipeline)}\") # ... # pc.set_started() # Signal that the worker is ready # scheduler.run() # Start the scheduler's main loop! # ... This code snippet shows that based on the type of AI model (pipeline) loaded, the Model Worker creates either a TokenGenerationScheduler or an EmbeddingsScheduler. The pipeline_config (which comes from Settings (Settings class)) and queues (parts of the EngineQueue for communication) are passed to configure the scheduler. Once created, scheduler.run() is called. This starts the scheduler’s main job: continuously looking for requests, batching them, and sending them to the AI model. ",
    "url": "/modular_max/05_scheduler___tokengenerationscheduler____embeddingsscheduler___.html#meet-the-schedulers-tokengenerationscheduler-and-embeddingsscheduler",
    
    "relUrl": "/modular_max/05_scheduler___tokengenerationscheduler____embeddingsscheduler___.html#meet-the-schedulers-tokengenerationscheduler-and-embeddingsscheduler"
  },"205": {
    "doc": "Chapter 5: Scheduler",
    "title": "The Scheduler’s Main Loop: A Day in the Life",
    "content": "Let’s imagine we’re the TokenGenerationScheduler. Here’s a simplified version of what our run() loop might do: . | Check for New Riders (Requests): We look at our request_q (the part of the EngineQueue where new requests arrive from the LLM Pipeline Orchestrator (TokenGeneratorPipeline)). Are there any people waiting in line? . | Decide on the Next Cart (Batch Formation): . | If there are requests for Context Encoding (processing new prompts), we might try to group several of them. We might wait a short time (a batch_timeout) to see if more arrive, or until we have enough to fill a “batch” up to max_batch_size_ce. | If there are requests that are already part-way through Token Generation (generating the next word of an ongoing story), we gather these. | The scheduler uses its configuration (like TokenGenerationSchedulerConfig) to make these decisions. For example, target_tokens_per_batch_ce helps decide how many total prompt tokens to aim for in a context encoding batch. | . | Load the Cart (Prepare the Batch): We select a group of requests to form the current batch. This is done by methods like _create_batch_to_execute(). | Start the Ride (Send to Model): We send this batch of requests to the actual AI model (self.pipeline.next_token(...) for text generation or self.pipeline.encode(...) for embeddings). | Collect Souvenirs (Get Results): The AI model processes the batch and produces results (e.g., the next set of generated tokens for each request in the batch). | Guide Riders to Exit (Send Results Back): We take these results and put them onto our response_q (another part of the EngineQueue), so they can be sent back to the LLM Pipeline Orchestrator (TokenGeneratorPipeline) and eventually to the user. | Repeat!: We go back to step 1 and do this all over again, continuously. | . This loop ensures a steady flow of efficiently batched requests to the AI model. Visualizing the Scheduler’s Role . Here’s a diagram showing the Scheduler at work inside the Model Worker: . sequenceDiagram participant InQueue as Input Queue (from EngineQueue) participant Sched as Scheduler participant AIModel as AI Model Pipeline participant OutQueue as Output Queue (to EngineQueue) Note over Sched: Scheduler's main run() loop active loop Processing Cycle Sched-&gt;&gt;InQueue: Check for/Get incoming requests InQueue--&gt;&gt;Sched: Provides requests (e.g., prompt A, prompt B) Sched-&gt;&gt;Sched: Forms a batch (e.g., [A, B]) based on strategy Sched-&gt;&gt;AIModel: Sends batch for processing AIModel-&gt;&gt;AIModel: Processes batch (e.g., generates text for A &amp; B) AIModel--&gt;&gt;Sched: Returns results for the batch Sched-&gt;&gt;OutQueue: Puts results onto output queue end . ",
    "url": "/modular_max/05_scheduler___tokengenerationscheduler____embeddingsscheduler___.html#the-schedulers-main-loop-a-day-in-the-life",
    
    "relUrl": "/modular_max/05_scheduler___tokengenerationscheduler____embeddingsscheduler___.html#the-schedulers-main-loop-a-day-in-the-life"
  },"206": {
    "doc": "Chapter 5: Scheduler",
    "title": "Under the Hood: A Peek into TokenGenerationScheduler",
    "content": "Let’s look at a simplified version of how the TokenGenerationScheduler might work, focusing on its run method and how it creates and schedules batches. The actual code is in src/max/serve/pipelines/scheduler.py. # Simplified from: src/max/serve/pipelines/scheduler.py class TokenGenerationScheduler(Scheduler): def __init__( self, # ... process_control, scheduler_config, pipeline, queues ... paged_manager: Optional[PagedKVCacheManager] = None, # For KV Cache ): # self.scheduler_config = scheduler_config (stores max_batch_size_tg, etc.) # self.pipeline = pipeline (the AI model like TokenGenerator) # self.request_q = queues[\"REQUEST\"] (where new requests arrive) # self.response_q = queues[\"RESPONSE\"] (where results are sent) # self.active_batch = {} (requests currently being processed for TG) # self.paged_manager = paged_manager (manages KV Cache, see Chapter 6) # ... pass def run(self): # Main loop: continuously process requests while True: # Simplified; actual loop checks for cancellation # self.pc.beat() # Heartbeat to show it's alive # 1. Decide what kind of batch to make next (CE or TG) # and gather requests for it. batch_to_execute: SchedulerOutput = self._create_batch_to_execute() # SchedulerOutput contains the batch_inputs, batch_type (CE/TG), etc. if batch_to_execute.batch_size == 0: # No requests to process right now, maybe sleep briefly time.sleep(0.001) # Small delay continue # 2. Schedule the batch on the AI model self._schedule(batch_to_execute) # This calls the model # ... (handle cancelled requests, logging, etc.) ... | The __init__ method sets up the scheduler with its configuration, the AI model pipeline it will use, the communication queues, and a reference to the paged_manager for KV Cache Management (more on this in the next chapter!). | The run() method is the heart. | _create_batch_to_execute(): This internal method is responsible for looking at waiting requests and current active_batch (for ongoing token generation) and deciding what to run next. It considers scheduler_config parameters like max_batch_size_ce, max_batch_size_tg, batch_timeout, and target_tokens_per_batch_ce. It also interacts with the paged_manager to ensure there’s enough KV Cache space. | _schedule(): This method takes the SchedulerOutput (which contains the batch) and actually sends it to the AI model. | . | . Let’s look at a simplified _schedule method: . # Simplified from: src/max/serve/pipelines/scheduler.py # (Inside TokenGenerationScheduler class) def _schedule(self, sch_output: SchedulerOutput): # sch_output contains: # - sch_output.batch_inputs: the actual dictionary of requests for the model # - sch_output.batch_type: tells if it's ContextEncoding or TokenGeneration # - sch_output.num_steps: how many tokens to generate for TG batch_responses = {} if sch_output.batch_type == BatchType.ContextEncoding: # Processing initial prompts batch_responses = self.pipeline.next_token( sch_output.batch_inputs, # The batch of new prompts num_steps=sch_output.num_steps, # Usually 1 for CE ) # ... logic to add these requests to self.active_batch if not done ... elif sch_output.batch_type == BatchType.TokenGeneration: # Generating subsequent tokens for ongoing requests batch_responses = self.pipeline.next_token( sch_output.batch_inputs, # The batch of active requests num_steps=sch_output.num_steps, # How many tokens to generate ) # ... logic to handle terminated responses (remove from active_batch) ... # Send all responses back to the main process via the response_q self._stream_responses_to_frontend(batch_responses) . This shows that _schedule calls self.pipeline.next_token() with the batch. The pipeline is the TokenGenerator instance loaded in the Model Worker. The results (batch_responses) are then sent back using _stream_responses_to_frontend(), which puts them on the response_q. The EmbeddingsScheduler has a simpler structure because it typically just encodes a batch of texts without the ongoing generation state: . # Simplified from: src/max/serve/pipelines/scheduler.py class EmbeddingsScheduler(Scheduler): # ... __init__ similar to TokenGenerationScheduler but with EmbeddingsSchedulerConfig ... def run(self): while True: # Simplified # ... batch_to_execute = self._create_batch_to_execute() # Gets requests from queue if len(batch_to_execute) == 0: time.sleep(0.001) continue # For embeddings, it's usually one call to encode the batch batch_responses = self.pipeline.encode(batch_to_execute) # Send responses back self.response_q.put_nowait([batch_responses]) # Note the list format # ... Here, _create_batch_to_execute would pull items from request_q up to max_batch_size from EmbeddingsSchedulerConfig. Then, self.pipeline.encode() processes the whole batch. Batching Configuration . The behavior of these schedulers is heavily influenced by their configuration objects: . | TokenGenerationSchedulerConfig: Defines things like max_batch_size_tg (max requests in a token generation batch), max_batch_size_ce (max requests in a context encoding batch), batch_timeout (how long to wait for more CE requests), and target_tokens_per_batch_ce (aim for this many total prompt tokens in a CE batch). | EmbeddingsSchedulerConfig: Simpler, mainly max_batch_size. | . These configurations are derived from the overall application Settings (Settings class) and TokenGeneratorPipelineConfig. ",
    "url": "/modular_max/05_scheduler___tokengenerationscheduler____embeddingsscheduler___.html#under-the-hood-a-peek-into-tokengenerationscheduler",
    
    "relUrl": "/modular_max/05_scheduler___tokengenerationscheduler____embeddingsscheduler___.html#under-the-hood-a-peek-into-tokengenerationscheduler"
  },"207": {
    "doc": "Chapter 5: Scheduler",
    "title": "Why is This Batching So Important for Performance?",
    "content": "AI models, especially large ones running on GPUs, are like massive parallel processors. | High Latency, High Throughput: Sending a single request to a GPU might have some unavoidable startup cost (latency). But once it’s going, the GPU can perform calculations on many pieces of data simultaneously very quickly. | Filling the Pipes: Batching helps “fill the pipes” of the GPU, ensuring that its many processing units are kept busy. If you only send one piece of data, most of the GPU is idle. | . The Scheduler’s intelligent batching strategies aim to find the sweet spot: creating batches large enough to keep the GPU busy, but not waiting so long to form a batch that users experience high latency. For TokenGenerationScheduler, it’s even more nuanced. Context encoding (processing the initial prompt) can be very different from generating subsequent tokens. The scheduler tries to optimize both. For example, it might prioritize getting ongoing token generation (TG) requests through quickly if the KV cache (memory for past tokens) is getting full, as TG requests are generally smaller and faster per step. ",
    "url": "/modular_max/05_scheduler___tokengenerationscheduler____embeddingsscheduler___.html#why-is-this-batching-so-important-for-performance",
    
    "relUrl": "/modular_max/05_scheduler___tokengenerationscheduler____embeddingsscheduler___.html#why-is-this-batching-so-important-for-performance"
  },"208": {
    "doc": "Chapter 5: Scheduler",
    "title": "Conclusion",
    "content": "The Scheduler (TokenGenerationScheduler and EmbeddingsScheduler) is the unsung hero of performance within the Model Worker. It acts as an intelligent traffic controller: . | It gathers individual requests from the EngineQueue. | It groups them into optimal batches based on strategies and configuration. | It dispatches these batches to the AI model pipeline. | This batching is key to maximizing GPU utilization and overall application throughput. | . By efficiently managing how requests are fed to the AI model, the Scheduler ensures that modular can serve many users quickly and make the most of its powerful hardware. One crucial aspect that the TokenGenerationScheduler often interacts with is how the model remembers previous parts of a conversation or text. This involves something called a KV Cache. In the next chapter, we’ll explore Chapter 6: KV Cache Management and see how it plays a vital role in efficient text generation. Generated by AI Codebase Knowledge Builder . ",
    "url": "/modular_max/05_scheduler___tokengenerationscheduler____embeddingsscheduler___.html#conclusion",
    
    "relUrl": "/modular_max/05_scheduler___tokengenerationscheduler____embeddingsscheduler___.html#conclusion"
  },"209": {
    "doc": "Chapter 5: Scheduler",
    "title": "Chapter 5: Scheduler",
    "content": " ",
    "url": "/modular_max/05_scheduler___tokengenerationscheduler____embeddingsscheduler___.html",
    
    "relUrl": "/modular_max/05_scheduler___tokengenerationscheduler____embeddingsscheduler___.html"
  },"210": {
    "doc": "Chapter 5: SIMD Data Access",
    "title": "Chapter 5: SIMD Data Access",
    "content": "Welcome to Chapter 5! We’ve covered a lot about NDBuffer: . | How it uses UnsafePointer to find its data’s starting point (Chapter 1). | How Dim and DimList define its shape (Chapter 2). | The NDBuffer structure itself (Chapter 3). | How strides and offset computation help locate any single element (Chapter 4). | . Now, we’re going to explore how NDBuffer helps you work with data much faster by processing multiple elements at once. This is achieved through SIMD operations. NDBuffer facilitates high-performance data access through SIMD (Single Instruction, Multiple Data) operations. Methods like load[width=W] and store[width=W] allow reading or writing W data elements (a “vector”) at once, rather than one by one. This can significantly speed up computations. The module also provides partial_simd_load and partial_simd_store utility functions to handle situations where the memory access for a full SIMD vector would go out of bounds, allowing for safe vectorized processing of edge cases with padding or masking. Think of SIMD access as using a wide tray to carry multiple teacups (data elements) at once between a shelf (memory) and a table (CPU registers), instead of carrying them individually. This is much faster. The partial_ functions are like carefully using the tray when you’re at the end of the shelf and only have a few cups left, or the shelf doesn’t perfectly fit a full tray width, ensuring you don’t drop any or pick up empty air. ",
    "url": "/mojo-v2/05_simd_data_access_.html",
    
    "relUrl": "/mojo-v2/05_simd_data_access_.html"
  },"211": {
    "doc": "Chapter 5: SIMD Data Access",
    "title": "What is SIMD? (Single Instruction, Multiple Data)",
    "content": "Imagine you’re a chef, and you have a dozen potatoes to peel. | Scalar operation (one by one): You pick up one potato, peel it, put it down. Pick up the next, peel it, put it down. And so on for all twelve. | SIMD operation (multiple at once): You have a special peeler that can handle, say, four potatoes simultaneously! You load four potatoes into your super-peeler, activate it once, and all four are peeled. You’d do this three times to peel all twelve. | . SIMD works on a similar principle inside your computer’s processor. Modern CPUs have special hardware units that can perform the same operation (like addition, multiplication, etc.) on multiple pieces of data at the same time with a single instruction. This “chunk” of data is often called a SIMD vector. Using SIMD can lead to massive speedups in programs that process large amounts of data, like in scientific computing, graphics, machine learning, and image processing. ",
    "url": "/mojo-v2/05_simd_data_access_.html#what-is-simd-single-instruction-multiple-data",
    
    "relUrl": "/mojo-v2/05_simd_data_access_.html#what-is-simd-single-instruction-multiple-data"
  },"212": {
    "doc": "Chapter 5: SIMD Data Access",
    "title": "Full Chunks: NDBuffer.load[width=W] and NDBuffer.store[width=W]",
    "content": "NDBuffer provides convenient methods to perform SIMD operations: . | load[width=W](index) -&gt; SIMD[type, W]: Reads W elements starting from the memory location corresponding to index in the NDBuffer. It returns these W elements as a SIMD[type, W] object. | store[width=W](index, value: SIMD[type, W]): Writes the W elements from the SIMD vector value to the NDBuffer, starting at the memory location corresponding to index. | . Here, W is a parameter you specify, representing the number of data elements in your SIMD vector (e.g., 2, 4, 8, depending on the data type and CPU capabilities). type is the DType of the elements in your NDBuffer. How it Works (Simplified): . | You call my_ndbuffer.load[width=W](some_index). | NDBuffer uses its magic (strides and the _offset() method we saw in Chapter 4) to calculate the precise 1D memory address for some_index. Let’s call this target_address_pointer. | It then effectively tells this target_address_pointer (which is an UnsafePointer): “Load W elements starting from here.” | The UnsafePointer performs the SIMD load from memory. Storing works similarly but in reverse. | . An Important Condition: Contiguity! SIMD operations usually assume that the W elements you want to load or store are sitting right next to each other in memory (i.e., they are contiguous). | If you’re working with a 1D NDBuffer (a simple array), its elements are typically contiguous. | For a 2D NDBuffer (matrix), if its elements are laid out row-by-row contiguously (row-major, stride of 1 for the last dimension), you can use SIMD to load/store parts of a row. | . The load and store methods in NDBuffer actually check for this: . // Inside NDBuffer's load method (simplified) fn load[*, width: Int = 1, ...](self, idx: ...) -&gt; SIMD[type, width]: debug_assert( self.is_contiguous() or width == 1, // This check is important! \"Function requires contiguous buffer for width &gt; 1.\", ) return self._offset(idx).load[width=width, alignment=alignment]() . If the NDBuffer isn’t contiguous (meaning its innermost dimension’s stride isn’t 1) and you try a width &gt; 1 SIMD operation, the debug_assert will trigger (in debug builds) because the W elements at the calculated offset might not correspond to W logically adjacent elements in your N-D view. Scalar operations (width=1) are always fine. Example (Conceptual 1D NDBuffer): Let’s say my_buffer is an NDBuffer of Float32s: [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, ...] . from memory import DType from simd import SIMD // Assume my_buffer is an NDBuffer[True, DType.float32, 1, ...] // and it's contiguous. // Load 4 Float32s starting at index 0 let simd_vec: SIMD[DType.float32, 4] = my_buffer.load[width=4](0) // simd_vec now holds [1.0, 2.0, 3.0, 4.0] // Let's say we process it (e.g., add 10.0 to each element) let processed_vec = simd_vec + SIMD[DType.float32, 4](10.0) // processed_vec is now [11.0, 12.0, 13.0, 14.0] // Store it back my_buffer.store[width=4](0, processed_vec) // my_buffer now starts with: [11.0, 12.0, 13.0, 14.0, 5.0, 6.0, ...] . ",
    "url": "/mojo-v2/05_simd_data_access_.html#full-chunks-ndbufferloadwidthw-and-ndbufferstorewidthw",
    
    "relUrl": "/mojo-v2/05_simd_data_access_.html#full-chunks-ndbufferloadwidthw-and-ndbufferstorewidthw"
  },"213": {
    "doc": "Chapter 5: SIMD Data Access",
    "title": "The “Edge” Problem: Handling Leftovers",
    "content": "The load[width=W] and store[width=W] methods are great when your data neatly aligns with the SIMD width. But what if your NDBuffer has, say, 10 elements, and your SIMD width W is 4? . | You can process elements 0-3 with one SIMD operation. | You can process elements 4-7 with another SIMD operation. | But then you have elements 8-9 left over (only 2 elements). | . If you try to do a my_buffer.load[width=4](8), it will attempt to read 4 elements starting from index 8 (my_buffer[8], my_buffer[9], my_buffer[10], my_buffer[11]). But my_buffer[10] and my_buffer[11] are out of bounds! This can lead to crashes or incorrect data. This is the “edge” problem. ",
    "url": "/mojo-v2/05_simd_data_access_.html#the-edge-problem-handling-leftovers",
    
    "relUrl": "/mojo-v2/05_simd_data_access_.html#the-edge-problem-handling-leftovers"
  },"214": {
    "doc": "Chapter 5: SIMD Data Access",
    "title": "Safe Edges: partial_simd_load and partial_simd_store",
    "content": "Mojo’s buffer module provides utility functions to handle these edge cases safely: . | partial_simd_load[type, width](storage_ptr, lbound, rbound, pad_value) -&gt; SIMD[type, width] | partial_simd_store[type, width](storage_ptr, lbound, rbound, data_vec) | . These are not methods of NDBuffer itself. They are standalone functions that operate on an UnsafePointer (which you can get from an NDBuffer’s data field, appropriately offset). | type: The DType of the elements. | width: The SIMD vector width (e.g., 4). | storage_ptr: UnsafePointer[Scalar[type], ... ]: A pointer to the memory location where the data segment begins. For the “leftover” elements, this would be the pointer to the first leftover element. | lbound: Int: The starting index within the SIMD vector where valid data should come from memory. For loading N leftover elements, this is usually 0. | rbound: Int: The ending index (exclusive) within the SIMD vector for valid data. For loading N leftover elements, this is N. | pad_value: Scalar[type] (for partial_simd_load): The value to use for elements in the SIMD vector that are outside the [lbound, rbound) range. | data_vec: SIMD[type, width] (for partial_simd_store): The SIMD vector containing data to be stored. Only elements within the [lbound, rbound) range will actually be written to memory. | . How they work: Internally, these functions use special CPU instructions called masked loads and stores. They create a “mask” (a sequence of true/false values, one for each lane of the SIMD vector). | partial_simd_load: For each lane i in the SIMD vector (from 0 to width-1): . | If lbound &lt;= i &lt; rbound, it loads storage_ptr[i] into lane i of the result. | Otherwise, it fills lane i with pad_value. | . | partial_simd_store: For each lane i: . | If lbound &lt;= i &lt; rbound, it stores data_vec[i] into storage_ptr[i]. | Otherwise, it does nothing for that memory location (storage_ptr[i] is not written to). | . | . This ensures you only read from or write to valid memory locations, even if your data segment is smaller than the SIMD width. Example (Handling leftovers from our previous example): Suppose my_buffer has 10 Float32 elements, and width=4. We’ve processed elements 0-7. Leftovers are my_buffer[8] and my_buffer[9]. There are 2 leftover elements. from memory import UnsafePointer, DType from simd import SIMD from buffer import partial_simd_load, partial_simd_store // Assume my_buffer: NDBuffer[..., DType.float32, ...] of length 10 let W: Int = 4 // SIMD width let num_leftover: Int = 2 let pad_val: Float32 = 0.0 // Get a pointer to the first leftover element (my_buffer[8]) // In a real NDBuffer, you'd calculate offset for index 8. // let ptr_to_leftovers = my_buffer.data.offset(my_buffer._offset_of_index(8)) // For simplicity, let's assume we have this UnsafePointer: var ptr_to_leftovers: UnsafePointer[Float32] = ... ; // Points to my_buffer[8] // Partial load let leftover_vec_loaded = partial_simd_load[DType.float32, W]( ptr_to_leftovers, 0, // lbound: valid data starts at SIMD lane 0 num_leftover, // rbound: valid data ends before SIMD lane 2 (i.e., lanes 0, 1 are valid) pad_val ) // If my_buffer[8]=8.8, my_buffer[9]=9.9, then // leftover_vec_loaded is [8.8, 9.9, 0.0, 0.0] // Process it (e.g., add 10.0) let processed_leftovers = leftover_vec_loaded + SIMD[DType.float32, W](10.0) // processed_leftovers is now [18.8, 19.9, 10.0, 10.0] // Note: We also \"processed\" the padded parts. This is often fine. // Partial store partial_simd_store[DType.float32, W]( ptr_to_leftovers, 0, num_leftover, processed_leftovers ) // This will write: // - processed_leftovers[0] (18.8) to ptr_to_leftovers[0] (my_buffer[8]) // - processed_leftovers[1] (19.9) to ptr_to_leftovers[1] (my_buffer[9]) // - It will NOT write processed_leftovers[2] or [3] to memory. // So, my_buffer is now: [..., 18.8, 19.9] . ",
    "url": "/mojo-v2/05_simd_data_access_.html#safe-edges-partial_simd_load-and-partial_simd_store",
    
    "relUrl": "/mojo-v2/05_simd_data_access_.html#safe-edges-partial_simd_load-and-partial_simd_store"
  },"215": {
    "doc": "Chapter 5: SIMD Data Access",
    "title": "A Peek at the Standard Library Code",
    "content": "Let’s see how these are defined in stdlib/src/buffer/buffer.mojo. NDBuffer.load (and store is similar): . @always_inline(\"nodebug\") fn load[ *, width: Int = 1, alignment: Int = Self._default_alignment[width]() ](self, idx: StaticTuple[Int, rank]) -&gt; SIMD[type, width]: \"\"\"Loads a simd value from the buffer at the specified index. Constraints: The buffer must be contiguous or width must be 1... \"\"\" debug_assert( self.is_contiguous() or width == 1, \"Function requires contiguous buffer.\", ) // 1. Calculate the flat 1D offset for the N-D index `idx` // 2. Get the UnsafePointer at that offset // 3. Call the UnsafePointer's own .load() method return self._offset(idx).load[width=width, alignment=alignment]() . The alignment parameter is a hint about memory alignment, which can sometimes improve performance. _default_alignment provides a sensible default. partial_simd_load function: . @always_inline fn partial_simd_load[ type: DType, //, width: Int ]( storage: UnsafePointer[Scalar[type], **_], // The pointer to memory lbound: Int, // Start of valid data in SIMD vector rbound: Int, // End of valid data in SIMD vector pad_value: Scalar[type], // Value for padding ) -&gt; SIMD[type, width]: \"\"\"Loads a vector with dynamic bound. Out of bound data will be filled with pad value... \"\"\" # Create a mask based on input bounds. var effective_lbound = max(0, lbound) var effective_rbound = min(width, rbound) var incr = iota[DType.int32, width]() // Generates [0, 1, 2, ..., width-1] var mask = (incr &gt;= effective_lbound) &amp; (incr &lt; effective_rbound) // Use the masked_load intrinsic: // Loads from `storage` where mask is true, uses `pad_value` where mask is false. return masked_load[width](storage, mask, pad_value) . partial_simd_store is structured similarly, using the masked_store intrinsic. ",
    "url": "/mojo-v2/05_simd_data_access_.html#a-peek-at-the-standard-library-code",
    
    "relUrl": "/mojo-v2/05_simd_data_access_.html#a-peek-at-the-standard-library-code"
  },"216": {
    "doc": "Chapter 5: SIMD Data Access",
    "title": "Putting It All Together: Vectorizing a Loop (Conceptual)",
    "content": "Imagine you want to add a constant C to all elements of a large 1D NDBuffer my_data`. let N = len(my_data) let W = simdwidthof[MyDataType]() // Get native SIMD width for the data type // 1. Main SIMD loop for full vectors var i: Int = 0 while i + W &lt;= N: let data_vec = my_data.load[width=W](i) let result_vec = data_vec + SIMD[MyDataType, W](C) my_data.store[width=W](i, result_vec) i += W // 2. Handle remaining elements using partial SIMD (if any) if i &lt; N: let num_remaining = N - i let ptr_to_remaining = my_data.data.offset(my_data._offset_of_index(i)) // Get pointer let partial_data_vec = partial_simd_load[MyDataType, W]( ptr_to_remaining, 0, num_remaining, Scalar[MyDataType](0) // Pad with 0 ) let partial_result_vec = partial_data_vec + SIMD[MyDataType, W](C) partial_simd_store[MyDataType, W]( ptr_to_remaining, 0, num_remaining, partial_result_vec ) . This pattern (a main SIMD loop and a smaller loop or partial operation for the remainder) is very common in high-performance code. ",
    "url": "/mojo-v2/05_simd_data_access_.html#putting-it-all-together-vectorizing-a-loop-conceptual",
    
    "relUrl": "/mojo-v2/05_simd_data_access_.html#putting-it-all-together-vectorizing-a-loop-conceptual"
  },"217": {
    "doc": "Chapter 5: SIMD Data Access",
    "title": "Key Takeaways for Chapter 5",
    "content": ". | SIMD (Single Instruction, Multiple Data) allows processing multiple data elements simultaneously, significantly boosting performance. | NDBuffer supports full SIMD vector operations via load[width=W]() and store[width=W]() methods. | These require the NDBuffer to be contiguous (innermost stride is 1) if width &gt; 1. | . | The “edge problem” occurs when the number of elements isn’t a perfect multiple of SIMD width W. | The buffer module provides partial_simd_load and partial_simd_store utility functions to safely handle these edge cases using masking. | These functions operate on UnsafePointers and take lbound and rbound parameters to define the valid portion of the SIMD vector. | . | Combining full SIMD operations for the bulk of the data and partial SIMD operations for the remainder is a common strategy for vectorization. | . By understanding and using these SIMD capabilities, you can write much faster Mojo code for data-intensive tasks! . Navigation . | UnsafePointer (as used by NDBuffer) | DimList and Dim | NDBuffer | Strides and Offset Computation | SIMD Data Access (You are here) ``` | . Generated by AI Codebase Knowledge Builder . ",
    "url": "/mojo-v2/05_simd_data_access_.html#key-takeaways-for-chapter-5",
    
    "relUrl": "/mojo-v2/05_simd_data_access_.html#key-takeaways-for-chapter-5"
  },"218": {
    "doc": "Chapter 6: Caching System",
    "title": "Chapter 6: Caching System",
    "content": "In Chapter 5: Deep Crawling System, we learned how to explore websites beyond a single page. As you crawl multiple pages, you might notice something: crawling the same page multiple times wastes time and resources. This is where the Caching System comes in! . ",
    "url": "/Crawl4AI/06_caching_system_.html",
    
    "relUrl": "/Crawl4AI/06_caching_system_.html"
  },"219": {
    "doc": "Chapter 6: Caching System",
    "title": "What is the Caching System?",
    "content": "Imagine you’re doing research for a school project. When you find a useful article online, you might save a copy or print it out so you don’t have to search for it again later. The Caching System in crawl4ai works the same way: . | It stores content you’ve already crawled (like HTML, extracted text, images) | It lets you retrieve this content later without having to download it again | It saves time, reduces network traffic, and is more polite to the websites you visit | . Let’s see how this works with a simple example: . from crawl4ai import AsyncWebCrawler, CrawlerRunConfig from crawl4ai.cache_context import CacheMode async def crawl_with_cache(): # First run: will download and cache the page async with AsyncWebCrawler() as crawler: result1 = await crawler.arun( url=\"https://example.com\", config=CrawlerRunConfig(cache_mode=CacheMode.ENABLED) ) # Second run: will use the cached version (much faster!) async with AsyncWebCrawler() as crawler: result2 = await crawler.arun( url=\"https://example.com\", config=CrawlerRunConfig(cache_mode=CacheMode.ENABLED) ) . In this example, the first crawl downloads the page from the internet and saves it to the cache. The second crawl doesn’t need to download the page again - it just retrieves the saved version from the cache! . ",
    "url": "/Crawl4AI/06_caching_system_.html#what-is-the-caching-system",
    
    "relUrl": "/Crawl4AI/06_caching_system_.html#what-is-the-caching-system"
  },"220": {
    "doc": "Chapter 6: Caching System",
    "title": "How Caching Works: Core Components",
    "content": "The Caching System has three main components: . 1. CacheMode Enum: Controlling Caching Behavior . The CacheMode enum lets you decide how caching should work for each crawl operation: . from crawl4ai.cache_context import CacheMode # Different caching options enabled = CacheMode.ENABLED # Normal caching (read and write) disabled = CacheMode.DISABLED # No caching at all read_only = CacheMode.READ_ONLY # Only read from cache, don't write write_only = CacheMode.WRITE_ONLY # Only write to cache, don't read bypass = CacheMode.BYPASS # Skip cache for this operation . These modes give you precise control over when to use and update the cache. 2. CacheContext: Making Caching Decisions . The CacheContext class helps decide whether to read from or write to the cache for a specific URL: . from crawl4ai.cache_context import CacheContext # Create a cache context for a URL context = CacheContext( url=\"https://example.com\", cache_mode=CacheMode.ENABLED ) # Check if we should read from cache if context.should_read(): # Try to get content from cache # Check if we should write to cache if context.should_write(): # Save content to cache . The CacheContext considers both the cache mode and the URL type (for example, you can’t cache raw HTML). 3. AsyncDatabaseManager: Storing and Retrieving Data . The AsyncDatabaseManager is the storage engine that actually saves and retrieves your cached content: . from crawl4ai.async_database import async_db_manager # Check if a URL is in the cache cached_result = await async_db_manager.aget_cached_url(url) if cached_result: # Use the cached content # Store a new result in the cache await async_db_manager.acache_url(result) . You don’t usually need to use this directly - the AsyncWebCrawler handles it for you. ",
    "url": "/Crawl4AI/06_caching_system_.html#how-caching-works-core-components",
    
    "relUrl": "/Crawl4AI/06_caching_system_.html#how-caching-works-core-components"
  },"221": {
    "doc": "Chapter 6: Caching System",
    "title": "Using the Cache: Practical Examples",
    "content": "Let’s look at some common use cases for the Caching System: . Example 1: Basic Caching (Default Behavior) . By default, crawl4ai has caching enabled: . from crawl4ai import AsyncWebCrawler async def basic_crawl(): async with AsyncWebCrawler() as crawler: # Caching is enabled by default result = await crawler.arun(url=\"https://example.com\") . This automatically saves the result and will use the cached version next time. Example 2: Refreshing the Cache (Bypassing) . Sometimes you want fresh content instead of cached content: . from crawl4ai import AsyncWebCrawler, CrawlerRunConfig from crawl4ai.cache_context import CacheMode async def refresh_content(): async with AsyncWebCrawler() as crawler: # Skip cache and fetch fresh content result = await crawler.arun( url=\"https://example.com\", config=CrawlerRunConfig(cache_mode=CacheMode.BYPASS) ) . This fetches a fresh copy from the website and updates the cache. Example 3: Reading from Cache Only (Offline Mode) . If you want to work offline or reduce server load: . from crawl4ai import AsyncWebCrawler, CrawlerRunConfig from crawl4ai.cache_context import CacheMode async def offline_mode(): async with AsyncWebCrawler() as crawler: # Only use cache, don't fetch from the internet result = await crawler.arun( url=\"https://example.com\", config=CrawlerRunConfig(cache_mode=CacheMode.READ_ONLY) ) . This will only return content if it’s already in the cache, and won’t attempt to download it. ",
    "url": "/Crawl4AI/06_caching_system_.html#using-the-cache-practical-examples",
    
    "relUrl": "/Crawl4AI/06_caching_system_.html#using-the-cache-practical-examples"
  },"222": {
    "doc": "Chapter 6: Caching System",
    "title": "How It Works Under the Hood",
    "content": "When you crawl a URL with caching enabled, here’s what happens behind the scenes: . sequenceDiagram participant User as Your Code participant WC as WebCrawler participant CC as CacheContext participant DB as DatabaseManager participant Web as Website User-&gt;&gt;WC: arun(\"https://example.com\") WC-&gt;&gt;CC: should_read()? CC-&gt;&gt;WC: Yes WC-&gt;&gt;DB: aget_cached_url() alt URL is in cache DB-&gt;&gt;WC: Return cached content WC-&gt;&gt;User: Return CrawlResult else URL not in cache DB-&gt;&gt;WC: Return None WC-&gt;&gt;Web: Download page Web-&gt;&gt;WC: Return HTML WC-&gt;&gt;CC: should_write()? CC-&gt;&gt;WC: Yes WC-&gt;&gt;DB: acache_url(result) WC-&gt;&gt;User: Return CrawlResult end . | Your code calls crawler.arun() with a URL | The crawler creates a CacheContext to determine caching behavior | If reading from cache is allowed, it checks the database for the URL | If the URL is in the cache, it returns the cached content | If not, it downloads the page from the website | If writing to cache is allowed, it saves the result to the database | Finally, it returns the result to your code | . ",
    "url": "/Crawl4AI/06_caching_system_.html#how-it-works-under-the-hood",
    
    "relUrl": "/Crawl4AI/06_caching_system_.html#how-it-works-under-the-hood"
  },"223": {
    "doc": "Chapter 6: Caching System",
    "title": "Implementation Details",
    "content": "Let’s look at how this is implemented in the code. Here’s a simplified version of how caching works in the arun method: . # From the AsyncWebCrawler.arun method async def arun(self, url: str, config: CrawlerRunConfig = None): # Create cache context cache_context = CacheContext(url, config.cache_mode, False) # Try to get cached result if appropriate if cache_context.should_read(): cached_result = await async_db_manager.aget_cached_url(url) if cached_result: # Use cached content return CrawlResultContainer(cached_result) else: # Fetch and process new content async_response = await self.crawler_strategy.crawl(url, config=config) crawl_result = await self.aprocess_html(url=url, html=async_response.html) # Update cache if appropriate if cache_context.should_write(): await async_db_manager.acache_url(crawl_result) return CrawlResultContainer(crawl_result) . This simplified code shows the core caching logic: . | Create a CacheContext with the URL and cache mode | If we should read from cache, try to get the cached result | If we have a cached result, return it | Otherwise, fetch and process the content | If we should write to cache, save the result | Return the result | . The actual database operations happen in the AsyncDatabaseManager class. Here’s a simplified example of how it saves content to the database: . # From the AsyncDatabaseManager.acache_url method async def acache_url(self, result: CrawlResult): # Store content files and get hashes content_hashes = await self._store_content_files(result) # Store metadata in SQLite database await self.execute_with_retry( \"INSERT INTO crawled_data VALUES (?, ?, ?, ?)\", (result.url, content_hashes[\"html\"], content_hashes[\"markdown\"]) ) . The AsyncDatabaseManager does more than just use a database - it also: . | Stores large content (HTML, screenshots) as separate files | Uses hash values to avoid duplicating content | Manages database connections efficiently | Handles errors and retries failed operations | . ",
    "url": "/Crawl4AI/06_caching_system_.html#implementation-details",
    
    "relUrl": "/Crawl4AI/06_caching_system_.html#implementation-details"
  },"224": {
    "doc": "Chapter 6: Caching System",
    "title": "Advanced Usage: Optimizing Caching",
    "content": "Here are some tips for getting the most out of the Caching System: . Memory-Efficient Crawling . If you’re crawling many pages but don’t need to update the cache: . from crawl4ai import AsyncWebCrawler, CrawlerRunConfig from crawl4ai.cache_context import CacheMode async def memory_efficient_crawl(): config = CrawlerRunConfig( cache_mode=CacheMode.READ_ONLY, # Don't write to cache screenshot=False # Screenshots use a lot of storage ) async with AsyncWebCrawler() as crawler: result = await crawler.arun(url=\"https://example.com\", config=config) . Refreshing Specific Content Types . Sometimes you want to update screenshots but use cached text: . async def refresh_screenshots(): # First check if we have a cached version config_check = CrawlerRunConfig(cache_mode=CacheMode.READ_ONLY) async with AsyncWebCrawler() as crawler: cached = await crawler.arun(url=\"https://example.com\", config=config_check) # If we have text but no screenshot, get a fresh version with screenshot if cached.success and not cached.screenshot: config_refresh = CrawlerRunConfig( cache_mode=CacheMode.ENABLED, screenshot=True ) await crawler.arun(url=\"https://example.com\", config=config_refresh) . Clearing the Cache . If you need to clear the cache: . from crawl4ai.async_database import async_db_manager async def clear_cache(): # Clear all cached data await async_db_manager.aclear_db() # Or completely rebuild the database await async_db_manager.aflush_db() await async_db_manager.ainit_db() . Be careful with these operations, as they will delete all your cached data! . ",
    "url": "/Crawl4AI/06_caching_system_.html#advanced-usage-optimizing-caching",
    
    "relUrl": "/Crawl4AI/06_caching_system_.html#advanced-usage-optimizing-caching"
  },"225": {
    "doc": "Chapter 6: Caching System",
    "title": "Conclusion",
    "content": "The Caching System in crawl4ai is like your personal web library - it stores copies of web content you’ve already downloaded, saving time and resources when you need to access the same content again. In this chapter, we learned: . | How the Caching System stores and retrieves web content | How to control caching behavior with the CacheMode enum | How the CacheContext and AsyncDatabaseManager work together | How to use the cache effectively for different scenarios | . Using the Caching System makes your web crawling more efficient and respectful to the websites you’re crawling. Instead of repeatedly downloading the same content, you can use your local copies, reducing network traffic and making your crawling faster. In the next chapter, Dispatcher Framework, we’ll learn how to manage multiple crawling tasks efficiently using the dispatcher system. Generated by AI Codebase Knowledge Builder . ",
    "url": "/Crawl4AI/06_caching_system_.html#conclusion",
    
    "relUrl": "/Crawl4AI/06_caching_system_.html#conclusion"
  },"226": {
    "doc": "ExtractionStrategy",
    "title": "Chapter 6: Getting Specific Data - ExtractionStrategy",
    "content": "In the previous chapter, Chapter 5: Focusing on What Matters - RelevantContentFilter, we learned how to sift through the cleaned webpage content to keep only the parts relevant to our query or goal, producing a focused fit_markdown. This is great for tasks like summarization or getting the main gist of an article. But sometimes, we need more than just relevant text. Imagine you’re analyzing an e-commerce website listing products. You don’t just want the description; you need the exact product name, the specific price, the customer rating, and maybe the SKU number, all neatly organized. How do we tell Crawl4AI to find these specific pieces of information and return them in a structured format, like a JSON object? . ",
    "url": "/Crawl4AI/06_extractionstrategy.html#chapter-6-getting-specific-data---extractionstrategy",
    
    "relUrl": "/Crawl4AI/06_extractionstrategy.html#chapter-6-getting-specific-data---extractionstrategy"
  },"227": {
    "doc": "ExtractionStrategy",
    "title": "What Problem Does ExtractionStrategy Solve?",
    "content": "Think of the content we’ve processed so far (like the cleaned HTML or the generated Markdown) as a detailed report delivered by a researcher. RelevantContentFilter helped trim the report down to the most relevant pages. Now, we need to give specific instructions to an Analyst to go through that focused report and pull out precise data points. We don’t just want the report; we want a filled-in spreadsheet with columns for “Product Name,” “Price,” and “Rating.” . ExtractionStrategy is the set of instructions we give to this Analyst. It defines how to locate and extract specific, structured information (like fields in a database or keys in a JSON object) from the content. ",
    "url": "/Crawl4AI/06_extractionstrategy.html#what-problem-does-extractionstrategy-solve",
    
    "relUrl": "/Crawl4AI/06_extractionstrategy.html#what-problem-does-extractionstrategy-solve"
  },"228": {
    "doc": "ExtractionStrategy",
    "title": "What is ExtractionStrategy?",
    "content": "ExtractionStrategy is a core concept (a blueprint) in Crawl4AI that represents the method used to extract structured data from the processed content (which could be HTML or Markdown). It specifies that we need a way to find specific fields, but the actual technique used to find them can vary. This allows us to choose the best “Analyst” for the job, depending on the complexity of the website and the data we need. ",
    "url": "/Crawl4AI/06_extractionstrategy.html#what-is-extractionstrategy",
    
    "relUrl": "/Crawl4AI/06_extractionstrategy.html#what-is-extractionstrategy"
  },"229": {
    "doc": "ExtractionStrategy",
    "title": "The Different Analysts: Ways to Extract Data",
    "content": "Crawl4AI offers several concrete implementations (the different Analysts) for extracting structured data: . | The Precise Locator (JsonCssExtractionStrategy &amp; JsonXPathExtractionStrategy) . | Analogy: An analyst who uses very precise map coordinates (CSS Selectors or XPath expressions) to find information on a page. They need to be told exactly where to look. “The price is always in the HTML element with the ID #product-price.” | How it works: You define a schema (a Python dictionary) that maps the names of the fields you want (e.g., “product_name”, “price”) to the specific CSS selector (JsonCssExtractionStrategy) or XPath expression (JsonXPathExtractionStrategy) that locates that information within the HTML structure. | Pros: Very fast and reliable if the website structure is consistent and predictable. Doesn’t require external AI services. | Cons: Can break easily if the website changes its layout (selectors become invalid). Requires you to inspect the HTML and figure out the correct selectors. | Input: Typically works directly on the raw or cleaned HTML. | . | The Smart Interpreter (LLMExtractionStrategy) . | Analogy: A highly intelligent analyst who can read and understand the content. You give them a list of fields you need (a schema) or even just natural language instructions (“Find the product name, its price, and a short description”). They read the content (usually Markdown) and use their understanding of language and context to figure out the values, even if the layout isn’t perfectly consistent. | How it works: You provide a desired output schema (e.g., a Pydantic model or a dictionary structure) or a natural language instruction. The strategy sends the content (often the generated Markdown, possibly split into chunks) along with your schema/instruction to a configured Large Language Model (LLM) like GPT or Llama. The LLM reads the text and generates the structured data (usually JSON) according to your request. | Pros: Much more resilient to website layout changes. Can understand context and handle variations. Can extract data based on meaning, not just location. | Cons: Requires setting up access to an LLM (API keys, potentially costs). Can be significantly slower than selector-based methods. The quality of extraction depends on the LLM’s capabilities and the clarity of your instructions/schema. | Input: Often works best on the cleaned Markdown representation of the content, but can sometimes use HTML. | . | . ",
    "url": "/Crawl4AI/06_extractionstrategy.html#the-different-analysts-ways-to-extract-data",
    
    "relUrl": "/Crawl4AI/06_extractionstrategy.html#the-different-analysts-ways-to-extract-data"
  },"230": {
    "doc": "ExtractionStrategy",
    "title": "How to Use an ExtractionStrategy",
    "content": "You tell the AsyncWebCrawler which extraction strategy to use (if any) by setting the extraction_strategy parameter within the CrawlerRunConfig object you pass to arun or arun_many. Example 1: Extracting Data with JsonCssExtractionStrategy . Let’s imagine we want to extract the title (from the &lt;h1&gt; tag) and the main heading (from the &lt;h1&gt; tag) of the simple httpbin.org/html page. # chapter6_example_1.py import asyncio import json from crawl4ai import ( AsyncWebCrawler, CrawlerRunConfig, JsonCssExtractionStrategy # Import the CSS strategy ) async def main(): # 1. Define the extraction schema (Field Name -&gt; CSS Selector) extraction_schema = { \"baseSelector\": \"body\", # Operate within the body tag \"fields\": [ {\"name\": \"page_title\", \"selector\": \"title\", \"type\": \"text\"}, {\"name\": \"main_heading\", \"selector\": \"h1\", \"type\": \"text\"} ] } print(\"Extraction Schema defined using CSS selectors.\") # 2. Create an instance of the strategy with the schema css_extractor = JsonCssExtractionStrategy(schema=extraction_schema) print(f\"Using strategy: {css_extractor.__class__.__name__}\") # 3. Create CrawlerRunConfig and set the extraction_strategy run_config = CrawlerRunConfig( extraction_strategy=css_extractor ) # 4. Run the crawl async with AsyncWebCrawler() as crawler: url_to_crawl = \"https://httpbin.org/html\" print(f\"\\nCrawling {url_to_crawl} to extract structured data...\") result = await crawler.arun(url=url_to_crawl, config=run_config) if result.success and result.extracted_content: print(\"\\nExtraction successful!\") # The extracted data is stored as a JSON string in result.extracted_content # Parse the JSON string to work with the data as a Python object extracted_data = json.loads(result.extracted_content) print(\"Extracted Data:\") # Print the extracted data nicely formatted print(json.dumps(extracted_data, indent=2)) elif result.success: print(\"\\nCrawl successful, but no structured data extracted.\") else: print(f\"\\nCrawl failed: {result.error_message}\") if __name__ == \"__main__\": asyncio.run(main()) . Explanation: . | Schema Definition: We create a Python dictionary extraction_schema. | baseSelector: \"body\" tells the strategy to look for items within the &lt;body&gt; tag of the HTML. | fields is a list of dictionaries, each defining a field to extract: . | name: The key for this field in the output JSON (e.g., “page_title”). | selector: The CSS selector to find the element containing the data (e.g., “title” finds the &lt;title&gt; tag, “h1” finds the &lt;h1&gt; tag). | type: How to get the data from the selected element (\"text\" means get the text content). | . | . | Instantiate Strategy: We create an instance of JsonCssExtractionStrategy, passing our extraction_schema. This strategy knows its input format should be HTML. | Configure Run: We create a CrawlerRunConfig and assign our css_extractor instance to the extraction_strategy parameter. | Crawl: We run crawler.arun. After fetching and basic scraping, the AsyncWebCrawler will see the extraction_strategy in the config and call our css_extractor. | Result: The CrawlResult object now contains a field called extracted_content. This field holds the structured data found by the strategy, formatted as a JSON string. We use json.loads() to convert this string back into a Python list/dictionary. | . Expected Output (Conceptual): . Extraction Schema defined using CSS selectors. Using strategy: JsonCssExtractionStrategy Crawling https://httpbin.org/html to extract structured data... Extraction successful! Extracted Data: [ { \"page_title\": \"Herman Melville - Moby-Dick\", \"main_heading\": \"Moby Dick\" } ] . (Note: The actual output is a list containing one dictionary because baseSelector: \"body\" matches one element, and we extract fields relative to that.) . Example 2: Extracting Data with LLMExtractionStrategy (Conceptual) . Now, let’s imagine we want the same information (title, heading) but using an AI. We’ll provide a schema describing what we want. (Note: This requires setting up LLM access separately, e.g., API keys). # chapter6_example_2.py import asyncio import json from crawl4ai import ( AsyncWebCrawler, CrawlerRunConfig, LLMExtractionStrategy, # Import the LLM strategy LlmConfig # Import LLM configuration helper ) # Assume llm_config is properly configured with provider, API key, etc. # This is just a placeholder - replace with your actual LLM setup # E.g., llm_config = LlmConfig(provider=\"openai\", api_token=\"env:OPENAI_API_KEY\") class MockLlmConfig: provider=\"mock\"; api_token=\"mock\"; base_url=None llm_config = MockLlmConfig() async def main(): # 1. Define the desired output schema (what fields we want) # This helps guide the LLM. output_schema = { \"page_title\": \"string\", \"main_heading\": \"string\" } print(\"Extraction Schema defined for LLM.\") # 2. Create an instance of the LLM strategy # We pass the schema and the LLM configuration. # We also specify input_format='markdown' (common for LLMs). llm_extractor = LLMExtractionStrategy( schema=output_schema, llmConfig=llm_config, # Pass the LLM provider details input_format=\"markdown\" # Tell it to read the Markdown content ) print(f\"Using strategy: {llm_extractor.__class__.__name__}\") print(f\"LLM Provider (mocked): {llm_config.provider}\") # 3. Create CrawlerRunConfig with the strategy run_config = CrawlerRunConfig( extraction_strategy=llm_extractor ) # 4. Run the crawl async with AsyncWebCrawler() as crawler: url_to_crawl = \"https://httpbin.org/html\" print(f\"\\nCrawling {url_to_crawl} using LLM to extract...\") # This would make calls to the configured LLM API result = await crawler.arun(url=url_to_crawl, config=run_config) if result.success and result.extracted_content: print(\"\\nExtraction successful (using LLM)!\") # Extracted data is a JSON string try: extracted_data = json.loads(result.extracted_content) print(\"Extracted Data:\") print(json.dumps(extracted_data, indent=2)) except json.JSONDecodeError: print(\"Could not parse LLM output as JSON:\") print(result.extracted_content) elif result.success: print(\"\\nCrawl successful, but no structured data extracted by LLM.\") # This might happen if the mock LLM doesn't return valid JSON # or if the content was too small/irrelevant for extraction. else: print(f\"\\nCrawl failed: {result.error_message}\") if __name__ == \"__main__\": asyncio.run(main()) . Explanation: . | Schema Definition: We define a simple dictionary output_schema telling the LLM we want fields named “page_title” and “main_heading”, both expected to be strings. | Instantiate Strategy: We create LLMExtractionStrategy, passing: . | schema=output_schema: Our desired output structure. | llmConfig=llm_config: The configuration telling the strategy which LLM to use and how to authenticate (here, it’s mocked). | input_format=\"markdown\": Instructs the strategy to feed the generated Markdown content (from result.markdown.raw_markdown) to the LLM, which is often easier for LLMs to parse than raw HTML. | . | Configure Run &amp; Crawl: Same as before, we set the extraction_strategy in CrawlerRunConfig and run the crawl. | Result: The AsyncWebCrawler calls the llm_extractor. The strategy sends the Markdown content and the schema instructions to the configured LLM. The LLM analyzes the text and (hopefully) returns a JSON object matching the schema. This JSON is stored as a string in result.extracted_content. | . Expected Output (Conceptual, with a real LLM): . Extraction Schema defined for LLM. Using strategy: LLMExtractionStrategy LLM Provider (mocked): mock Crawling https://httpbin.org/html using LLM to extract... Extraction successful (using LLM)! Extracted Data: [ { \"page_title\": \"Herman Melville - Moby-Dick\", \"main_heading\": \"Moby Dick\" } ] . (Note: LLM output format might vary slightly, but it aims to match the requested schema based on the content it reads.) . ",
    "url": "/Crawl4AI/06_extractionstrategy.html#how-to-use-an-extractionstrategy",
    
    "relUrl": "/Crawl4AI/06_extractionstrategy.html#how-to-use-an-extractionstrategy"
  },"231": {
    "doc": "ExtractionStrategy",
    "title": "How It Works Inside (Under the Hood)",
    "content": "When you provide an extraction_strategy in the CrawlerRunConfig, how does AsyncWebCrawler use it? . | Fetch &amp; Scrape: The crawler fetches the raw HTML (AsyncCrawlerStrategy) and performs initial cleaning/scraping (ContentScrapingStrategy) to get cleaned_html, links, etc. | Markdown Generation: It usually generates Markdown representation (DefaultMarkdownGenerator). | Check for Strategy: The AsyncWebCrawler (specifically in its internal aprocess_html method) checks if config.extraction_strategy is set. | Execute Strategy: If a strategy exists: . | It determines the required input format (e.g., “html” for JsonCssExtractionStrategy, “markdown” for LLMExtractionStrategy based on its input_format attribute). | It retrieves the corresponding content (e.g., result.cleaned_html or result.markdown.raw_markdown). | If the content is long and the strategy supports chunking (like LLMExtractionStrategy), it might first split the content into smaller chunks. | It calls the strategy’s run method, passing the content chunk(s). | The strategy performs its logic (applying selectors, calling LLM API). | The strategy returns the extracted data (typically as a list of dictionaries). | . | Store Result: The AsyncWebCrawler converts the returned structured data into a JSON string and stores it in CrawlResult.extracted_content. | . Here’s a simplified view: . sequenceDiagram participant User participant AWC as AsyncWebCrawler participant Config as CrawlerRunConfig participant Processor as HTML Processing participant Extractor as ExtractionStrategy participant Result as CrawlResult User-&gt;&gt;AWC: arun(url, config=my_config) Note over AWC: Config includes an Extraction Strategy AWC-&gt;&gt;Processor: Process HTML (scrape, generate markdown) Processor--&gt;&gt;AWC: Processed Content (HTML, Markdown) AWC-&gt;&gt;Extractor: Run extraction on content (using Strategy's input format) Note over Extractor: Applying logic (CSS, XPath, LLM...) Extractor--&gt;&gt;AWC: Structured Data (List[Dict]) AWC-&gt;&gt;AWC: Convert data to JSON String AWC-&gt;&gt;Result: Store JSON String in extracted_content AWC--&gt;&gt;User: Return CrawlResult . Code Glimpse (extraction_strategy.py) . Inside the crawl4ai library, the file extraction_strategy.py defines the blueprint and the implementations. The Blueprint (Abstract Base Class): . # Simplified from crawl4ai/extraction_strategy.py from abc import ABC, abstractmethod from typing import List, Dict, Any class ExtractionStrategy(ABC): \"\"\"Abstract base class for all extraction strategies.\"\"\" def __init__(self, input_format: str = \"markdown\", **kwargs): self.input_format = input_format # e.g., 'html', 'markdown' # ... other common init ... @abstractmethod def extract(self, url: str, content_chunk: str, *q, **kwargs) -&gt; List[Dict[str, Any]]: \"\"\"Extract structured data from a single chunk of content.\"\"\" pass def run(self, url: str, sections: List[str], *q, **kwargs) -&gt; List[Dict[str, Any]]: \"\"\"Process content sections (potentially chunked) and call extract.\"\"\" # Default implementation might process sections in parallel or sequentially all_extracted_data = [] for section in sections: all_extracted_data.extend(self.extract(url, section, **kwargs)) return all_extracted_data . Example Implementation (JsonCssExtractionStrategy): . # Simplified from crawl4ai/extraction_strategy.py from bs4 import BeautifulSoup # Uses BeautifulSoup for CSS selectors class JsonCssExtractionStrategy(ExtractionStrategy): def __init__(self, schema: Dict[str, Any], **kwargs): # Force input format to HTML for CSS selectors super().__init__(input_format=\"html\", **kwargs) self.schema = schema # Store the user-defined schema def extract(self, url: str, html_content: str, *q, **kwargs) -&gt; List[Dict[str, Any]]: # Parse the HTML content chunk soup = BeautifulSoup(html_content, \"html.parser\") extracted_items = [] # Find base elements defined in the schema base_elements = soup.select(self.schema.get(\"baseSelector\", \"body\")) for element in base_elements: item = {} # Extract fields based on schema selectors and types fields_to_extract = self.schema.get(\"fields\", []) for field_def in fields_to_extract: try: # Find the specific sub-element using CSS selector target_element = element.select_one(field_def[\"selector\"]) if target_element: if field_def[\"type\"] == \"text\": item[field_def[\"name\"]] = target_element.get_text(strip=True) elif field_def[\"type\"] == \"attribute\": item[field_def[\"name\"]] = target_element.get(field_def[\"attribute\"]) # ... other types like 'html', 'list', 'nested' ... except Exception as e: # Handle errors, maybe log them if verbose pass if item: extracted_items.append(item) return extracted_items # run() method likely uses the default implementation from base class . Example Implementation (LLMExtractionStrategy): . # Simplified from crawl4ai/extraction_strategy.py # Needs imports for LLM interaction (e.g., perform_completion_with_backoff) from .utils import perform_completion_with_backoff, chunk_documents, escape_json_string from .prompts import PROMPT_EXTRACT_SCHEMA_WITH_INSTRUCTION # Example prompt class LLMExtractionStrategy(ExtractionStrategy): def __init__(self, schema: Dict = None, instruction: str = None, llmConfig=None, input_format=\"markdown\", **kwargs): super().__init__(input_format=input_format, **kwargs) self.schema = schema self.instruction = instruction self.llmConfig = llmConfig # Contains provider, API key, etc. # ... other LLM specific setup ... def extract(self, url: str, content_chunk: str, *q, **kwargs) -&gt; List[Dict[str, Any]]: # Prepare the prompt for the LLM prompt = self._build_llm_prompt(url, content_chunk) # Call the LLM API response = perform_completion_with_backoff( provider=self.llmConfig.provider, prompt_with_variables=prompt, api_token=self.llmConfig.api_token, base_url=self.llmConfig.base_url, json_response=True # Often expect JSON from LLM for extraction # ... pass other necessary args ... ) # Parse the LLM's response (which should ideally be JSON) try: extracted_data = json.loads(response.choices[0].message.content) # Ensure it's a list if isinstance(extracted_data, dict): extracted_data = [extracted_data] return extracted_data except Exception as e: # Handle LLM response parsing errors print(f\"Error parsing LLM response: {e}\") return [{\"error\": \"Failed to parse LLM output\", \"raw_output\": response.choices[0].message.content}] def _build_llm_prompt(self, url: str, content_chunk: str) -&gt; str: # Logic to construct the prompt using self.schema or self.instruction # and the content_chunk. Example: prompt_template = PROMPT_EXTRACT_SCHEMA_WITH_INSTRUCTION # Choose appropriate prompt variable_values = { \"URL\": url, \"CONTENT\": escape_json_string(content_chunk), # Send Markdown or HTML chunk \"SCHEMA\": json.dumps(self.schema) if self.schema else \"{}\", \"REQUEST\": self.instruction if self.instruction else \"Extract relevant data based on the schema.\" } prompt = prompt_template for var, val in variable_values.items(): prompt = prompt.replace(\"{\" + var + \"}\", str(val)) return prompt # run() method might override the base to handle chunking specifically for LLMs def run(self, url: str, sections: List[str], *q, **kwargs) -&gt; List[Dict[str, Any]]: # Potentially chunk sections based on token limits before calling extract # chunked_content = chunk_documents(sections, ...) # extracted_data = [] # for chunk in chunked_content: # extracted_data.extend(self.extract(url, chunk, **kwargs)) # return extracted_data # Simplified for now: return super().run(url, sections, *q, **kwargs) . ",
    "url": "/Crawl4AI/06_extractionstrategy.html#how-it-works-inside-under-the-hood",
    
    "relUrl": "/Crawl4AI/06_extractionstrategy.html#how-it-works-inside-under-the-hood"
  },"232": {
    "doc": "ExtractionStrategy",
    "title": "Conclusion",
    "content": "You’ve learned about ExtractionStrategy, Crawl4AI’s way of giving instructions to an “Analyst” to pull out specific, structured data from web content. | It solves the problem of needing precise data points (like product names, prices) in an organized format, not just blocks of text. | You can choose your “Analyst”: . | Precise Locators (JsonCssExtractionStrategy, JsonXPathExtractionStrategy): Use exact CSS/XPath selectors defined in a schema. Fast but brittle. | Smart Interpreter (LLMExtractionStrategy): Uses an AI (LLM) guided by a schema or instructions. More flexible but slower and needs setup. | . | You configure the desired strategy within the CrawlerRunConfig. | The extracted structured data is returned as a JSON string in the CrawlResult.extracted_content field. | . Now that we understand how to fetch, clean, filter, and extract data, let’s put it all together and look at the final package that Crawl4AI delivers after a crawl. Next: Let’s dive into the details of the output with Chapter 7: Understanding the Results - CrawlResult. Generated by AI Codebase Knowledge Builder . ",
    "url": "/Crawl4AI/06_extractionstrategy.html#conclusion",
    
    "relUrl": "/Crawl4AI/06_extractionstrategy.html#conclusion"
  },"233": {
    "doc": "ExtractionStrategy",
    "title": "ExtractionStrategy",
    "content": " ",
    "url": "/Crawl4AI/06_extractionstrategy.html",
    
    "relUrl": "/Crawl4AI/06_extractionstrategy.html"
  },"234": {
    "doc": "Chapter 6: KV Cache Management",
    "title": "Chapter 6: KV Cache Management",
    "content": "In Chapter 5: Scheduler (TokenGenerationScheduler, EmbeddingsScheduler), we saw how the Scheduler intelligently batches requests to keep our AI model busy. But when an LLM generates text, especially long sequences, how does it “remember” what it said at the beginning of a sentence or paragraph to make sense of what comes next? If it had to re-read and re-calculate everything from scratch for every new word, it would be incredibly slow! . This is where KV Cache Management comes to the rescue. It’s like giving our LLM a super-fast, very smart scratchpad. ",
    "url": "/modular_max/06_kv_cache_management_.html",
    
    "relUrl": "/modular_max/06_kv_cache_management_.html"
  },"235": {
    "doc": "Chapter 6: KV Cache Management",
    "title": "What Problem Does KV Cache Management Solve?",
    "content": "Imagine you’re asking an LLM to write a story: “Once upon a time, in a land far, far away, lived a brave…” . When the LLM generates “brave,” it needs to remember “Once upon a time, in a land far, far away, lived a” to make sure “brave” fits the context. This “memory” involves complex calculations called “attention” (specifically, Key and Value tensors, or KV for short). If, for the next word (e.g., “knight”), the LLM had to re-calculate the attention for “Once upon a time… lived a brave” all over again, it would be a huge waste of time and energy. KV Cache Management solves this by: . | Storing these intermediate attention calculations (the Keys and Values) from previously processed tokens in a special memory area called the KV Cache. | Reusing these stored calculations when generating subsequent tokens. | . This dramatically speeds up text generation because the model only needs to compute attention for the newest token, not the entire sequence again and again. It’s like a high-speed “thought buffer” for the LLM. The KVCacheManager is the component in modular responsible for handling this critical memory. ",
    "url": "/modular_max/06_kv_cache_management_.html#what-problem-does-kv-cache-management-solve",
    
    "relUrl": "/modular_max/06_kv_cache_management_.html#what-problem-does-kv-cache-management-solve"
  },"236": {
    "doc": "Chapter 6: KV Cache Management",
    "title": "Key Concepts: The LLM’s Smart Scratchpad",
    "content": "Let’s break down how this “smart scratchpad” works, especially with advanced techniques like Paged Attention: . 1. The KV Cache: What’s Stored? . When the LLM processes a sequence of tokens (like your prompt “Tell me a story”), for each token, it computes Key (K) and Value (V) tensors in its attention layers. These K and V tensors represent the “importance” and “content” of previous tokens. | Keys (K): Help the model decide which past tokens are relevant to the current token. | Values (V): Provide the information from those relevant past tokens. | . The KV Cache stores these K and V tensors for all processed tokens in a sequence. So, when generating the 100th word, the K and V tensors for the first 99 words are already in the cache, ready to be used. 2. Paged Attention: Virtual Memory for the GPU . Storing the KV Cache for many long sequences can take up a lot of GPU memory. If managed poorly, this can lead to “out of memory” errors or limit how many requests we can process at once. Paged Attention is a clever technique, similar to how virtual memory works in your computer’s operating system, but for the GPU’s KV Cache. | Analogy: Imagine your LLM’s scratchpad isn’t one long scroll of paper, but a binder full of fixed-size pages. | Blocks/Pages: The KV Cache is divided into fixed-size chunks called “blocks” or “pages.” Each block can store the KV tensors for a certain number of tokens (e.g., 128 tokens). | Flexibility: Instead of needing one large contiguous memory block for each request’s entire KV history, the system can allocate these smaller, non-contiguous pages as needed. | . This provides several benefits: . | Reduced Fragmentation: It’s easier to find and allocate memory for new requests because we’re looking for small pages, not one huge chunk. | Higher Concurrency: More requests can share the GPU memory efficiently. | Prefix Caching: If multiple requests start with the same prompt (e.g., a system message), their initial KV Cache pages can be shared, saving memory and computation. This is like having pre-written common introductions in our page binder. | . 3. The KVCacheManager and BlockManager . | KVCacheManager: This is the main abstraction (an abstract base class defined in src/max/nn/kv_cache/manager.py). It defines the interface for managing the KV Cache. A key implementation is the PagedKVCacheManager (in src/max/nn/kv_cache/paged_cache/paged_cache.py), which uses Paged Attention. | BlockManager: Used by PagedKVCacheManager, this component (in src/max/nn/kv_cache/paged_cache/block_manager.py) is the low-level accountant for the pages. It keeps track of: . | Which pages are free. | Which pages are assigned to which requests. | Handles allocation, deallocation, and potentially even copying pages (e.g., for prefix caching or swapping to CPU memory if GPU memory is full). | . | . 4. Communicating with the Model: KVCacheInputs . When the model needs to perform an attention calculation, the KVCacheManager provides it with KVCacheInputs. These are special data structures (like RaggedKVCacheInputs or PaddedKVCacheInputs) that tell the model: . | Where the actual KV cache data is stored (e.g., a pointer to the GPU memory holding all blocks). | A “lookup table” or similar mechanism that maps logical token positions in a request to the specific physical blocks in the cache. | The current lengths of the cached sequences. | . # Simplified dataclass definitions from src/max/nn/kv_cache/manager.py @dataclass class KVCacheInputs: # Base class pass @dataclass class RaggedKVCacheInputs(KVCacheInputs): # For Paged Attention, these tell the model how to find its \"pages\" blocks: Tensor # The big tensor on GPU holding all cache blocks cache_lengths: Tensor # How many tokens are already cached for each request lookup_table: Tensor # Maps request tokens to physical block locations max_lengths: Tensor # Helps manage generation limits . This RaggedKVCacheInputs essentially gives the model a map to navigate its “paged scratchpad.” . ",
    "url": "/modular_max/06_kv_cache_management_.html#key-concepts-the-llms-smart-scratchpad",
    
    "relUrl": "/modular_max/06_kv_cache_management_.html#key-concepts-the-llms-smart-scratchpad"
  },"237": {
    "doc": "Chapter 6: KV Cache Management",
    "title": "How the Scheduler Uses the KV Cache Manager",
    "content": "The Scheduler (TokenGenerationScheduler, EmbeddingsScheduler) (Chapter 5), which lives inside the Model Worker (Chapter 4), heavily relies on the KVCacheManager (specifically, an instance like PagedKVCacheManager) to manage memory for the requests it’s processing. Here’s a simplified flow: . | New Request Arrives: . | The Scheduler gets a new request. Before it can be processed, it needs space in the KV Cache. | Scheduler: “Hey KVCacheManager, I have a new request req_A. Can you reserve a spot for it?” | KVCacheManager.claim(1): The manager reserves a logical “slot” for this request. Let’s say it gets slot ID 0. | Scheduler (to itself): “Okay, req_A is now seq_id=0 in the cache.” | . | Preparing the First Batch (Context Encoding): . | The Scheduler wants to process the initial prompt for req_A. | Scheduler: “Hey KVCacheManager (paged_manager in TokenGenerationScheduler), prepare the cache for req_A (now ctx_A which knows its seq_id=0) and tell me if it’s possible.” | PagedKVCacheManager.prefetch(ctx_A): . | The internal BlockManager looks for free pages on the GPU. | It might reuse pages if req_A’s prompt has a common prefix already cached (prefix caching). | It allocates the necessary pages for req_A’s prompt. | If successful, it returns True. If not (e.g., no GPU memory), it returns False, and the Scheduler might have to wait or evict something. | . | This prefetch step ensures that when fetch is called later, the memory is ready. | . | Running the Model (Getting Cache Inputs): . | Scheduler: “Okay, KVCacheManager, give me the actual cache input tensors for the current batch (which includes req_A).” | PagedKVCacheManager.fetch([ctx_A, ...]): . | This constructs RaggedKVCacheInputs (or similar). It populates the lookup_table that tells the model which physical pages correspond to req_A’s tokens. | These inputs are passed to the AI model. The model uses them to read from and write to the correct pages in the KV Cache during attention. | . | . | After Model Step (Committing to Cache): . | The model generates a new token for req_A. | Scheduler: “Hey KVCacheManager, req_A has a new token. Update its state in the cache.” | PagedKVCacheManager.step([ctx_A, ...]): . | The internal BlockManager “commits” the newly generated token’s KV data. This might involve marking a page as fully used and potentially making it available for future prefix caching. | . | . | Request Finishes: . | req_A has finished generating all its tokens. | Scheduler: “KVCacheManager, req_A (identified by seq_id=0) is done. You can free its memory.” | PagedKVCacheManager.release(seq_id=0): The BlockManager returns all pages used by req_A to the free pool. | . | . This interaction ensures that GPU memory for the KV Cache is used efficiently, allowing many requests to be processed. ",
    "url": "/modular_max/06_kv_cache_management_.html#how-the-scheduler-uses-the-kv-cache-manager",
    
    "relUrl": "/modular_max/06_kv_cache_management_.html#how-the-scheduler-uses-the-kv-cache-manager"
  },"238": {
    "doc": "Chapter 6: KV Cache Management",
    "title": "Under the Hood: A Closer Look at PagedKVCacheManager",
    "content": "Let’s peek at the internal workings. High-Level Flow with Paged Attention: . sequenceDiagram participant Sched as Scheduler participant PKVCM as PagedKVCacheManager participant BM as BlockManager (inside PKVCM) participant BP as BlockPool (inside BM) participant Model as AI Model Note over Sched: New request `req_A` arrives Sched-&gt;&gt;PKVCM: claim(1) -&gt; returns slot_id (e.g., 0) Note over Sched: Scheduler associates `req_A` with slot_id 0 Note over Sched: Prepare for prompt processing Sched-&gt;&gt;PKVCM: prefetch(req_A_context) PKVCM-&gt;&gt;BM: prefetch(req_A_context) BM-&gt;&gt;BM: Check prefix cache (reuse blocks?) BM-&gt;&gt;BP: Allocate new blocks for req_A's prompt BP--&gt;&gt;BM: Allocated block IDs BM--&gt;&gt;PKVCM: Success/Failure PKVCM--&gt;&gt;Sched: Prefetch successful Note over Sched: Time to run model Sched-&gt;&gt;PKVCM: fetch(batch_with_req_A) PKVCM-&gt;&gt;PKVCM: Create lookup_table for batch PKVCM--&gt;&gt;Sched: KVCacheInputs (with lookup_table) Sched-&gt;&gt;Model: Execute with KVCacheInputs Model-&gt;&gt;Model: Reads/Writes KV Cache using lookup_table Model--&gt;&gt;Sched: Generated tokens Note over Sched: Commit new tokens Sched-&gt;&gt;PKVCM: step(batch_with_req_A) PKVCM-&gt;&gt;BM: step(req_A_context) BM-&gt;&gt;BM: Commit blocks, update prefix cache info Note over Sched: Request `req_A` completes Sched-&gt;&gt;PKVCM: release(slot_id=0) PKVCM-&gt;&gt;BM: release(slot_id=0) BM-&gt;&gt;BP: Free blocks used by req_A . Code Snippets: . 1. KVCacheManager Abstract Base Class (src/max/nn/kv_cache/manager.py) This class defines the contract that all KV cache managers must follow. # Simplified from: src/max/nn/kv_cache/manager.py class KVCacheManager(ABC, Generic[T]): # T is KVCacheAwareContext # ... (constructor) ... @abstractmethod def fetch(self, batch: list[T], num_steps: int = 1) -&gt; list[KVCacheInputs]: # Returns KVCacheInputs for the model to use ... def claim(self, n: int) -&gt; list[int]: # Claims 'n' logical slots for new sequences, returns their IDs seq_ids = [] for _ in range(n): seq_id = self.available.pop() # 'available' is a set of free slot IDs self.active.add(seq_id) seq_ids.append(seq_id) return seq_ids def release(self, seq_id: int) -&gt; None: # Releases a slot ID, making it available again self.active.remove(seq_id) self.available.add(seq_id) @abstractmethod def step(self, batch: list[T]) -&gt; None: # Commits new tokens into the cache after a model step ... This shows the key methods fetch, claim, and release that the Scheduler interacts with. 2. PagedKVCacheManager Initialization (src/max/nn/kv_cache/paged_cache/paged_cache.py) This is where the paged attention system is set up. # Simplified from: src/max/nn/kv_cache/paged_cache/paged_cache.py class PagedKVCacheManager(KVCacheManager): def __init__(self, params: KVCacheParams, ..., page_size: int = 128, ...): super().__init__(...) self.page_size = page_size # Calculate total_num_pages based on available GPU memory and page_size # ... # Allocate the main GPU tensor to hold all cache blocks (pages) self.device_tensors: list[Tensor] = [] # One per GPU device for device in self.devices: self.device_tensors.append( Tensor(shape=self.block_shape(), ...) # shape like [num_pages, 2, layers, page_size, heads, dim] ) # Initialize the BlockManager to manage these pages self.block_manager = BlockManager( total_num_blocks=self.total_num_pages, block_size=self.page_size, # ... other args like block_copy_engine for prefix caching ... ) self.prefetched_seq_ids: set[int] = set() . The PagedKVCacheManager allocates a large chunk of memory on the GPU (self.device_tensors) to hold all the blocks/pages. It then creates a BlockManager to manage these pages. 3. PagedKVCacheManager.prefetch and fetch The prefetch method ensures blocks are available before fetch is called. # Simplified from: src/max/nn/kv_cache/paged_cache/paged_cache.py @traced # For performance profiling def prefetch(self, data: KVCacheAwareContext, num_steps: int = 1) -&gt; bool: seq_id = data.cache_seq_id # Ask BlockManager to reuse/allocate blocks for this request scheduled = self.block_manager.prefetch(data, num_steps) if scheduled: self.prefetched_seq_ids.add(seq_id) return scheduled @traced def fetch(self, batch: list[KVCacheAwareContext], num_steps: int = 1) -&gt; list[RaggedKVCacheInputs]: # ... (Ensure prefetch was called for items in batch or call it now) ... # For each request in the batch: # Get its allocated block IDs from self.block_manager.get_req_blocks(seq_id) # Construct a lookup_table_np (numpy array) mapping [batch_idx, logical_page_idx] to physical_block_id # Construct cache_lengths_np (how many tokens already cached) # ... # Convert numpy arrays to Tensors # lut_table_host = Tensor.from_numpy(lookup_table_np) # cache_lengths_host = Tensor.from_numpy(cache_lengths_np) # Create RaggedKVCacheInputs for each device ret_list = [] for i, device in enumerate(self.devices): ret_list.append( RaggedKVCacheInputs( blocks=self.device_tensors[i], # The big GPU tensor of blocks cache_lengths=cache_lengths_host.to(device=device), lookup_table=lut_table_host.to(device=device), max_lengths=max_lengths_host, # For generation limits ) ) return ret_list . prefetch relies on the BlockManager to do the heavy lifting of block allocation. fetch then assembles the RaggedKVCacheInputs that the model will use, including the crucial lookup_table. 4. BlockManager in Action (src/max/nn/kv_cache/paged_cache/block_manager.py) The BlockManager handles the actual page allocations and prefix caching. # Simplified from: src/max/nn/kv_cache/paged_cache/block_manager.py class BlockManager(Generic[T]): # T is KVCacheAwareContext def __init__(self, ..., total_num_blocks: int, block_size: int, ...): self.block_size = block_size self.device_block_pool = BlockPool(total_num_blocks, ...) # self.host_block_pool (if swapping to CPU is enabled) self.req_to_blocks: dict[int, list[KVCacheBlock]] = defaultdict(list) # Tracks blocks per request def prefetch(self, ctx: T, num_steps: int = 1) -&gt; bool: self.reuse_blocks_from_prefix_cache(ctx) # Try to use existing cached blocks try: self.allocate_new_blocks(ctx, num_steps) # Allocate remaining needed blocks except RuntimeError: # Not enough free blocks return False return True def allocate_new_blocks(self, ctx: T, num_steps: int = 1): # Calculate num_new_blocks needed for ctx.current_length + num_steps # ... if num_new_blocks &gt; len(self.device_block_pool.free_block_queue): raise RuntimeError(\"Not enough free blocks\") for _ in range(num_new_blocks): # Get a KVCacheBlock object from the pool new_block = self.device_block_pool.alloc_block() self.req_to_blocks[ctx.cache_seq_id].append(new_block) def step(self, ctx: T) -&gt; None: # Called after model generates tokens if self.enable_prefix_caching: self.commit_to_prefix_cache(ctx) # Marks blocks as \"committed\" def release(self, seq_id: int) -&gt; None: for block in self.req_to_blocks[seq_id]: self.device_block_pool.free_block(block) # Return block to the pool self.req_to_blocks[seq_id] = [] . The BlockManager uses a BlockPool (which manages lists of free and used blocks) to perform its duties. reuse_blocks_from_prefix_cache and commit_to_prefix_cache are key for the prefix caching optimization. What About Swapping and the KVCacheAgent? . | Swapping to Host Memory: If GPU memory for the KV Cache is extremely full, the BlockManager can be configured (via total_num_host_blocks and a host_block_pool) to copy some less-recently-used blocks from GPU to regular CPU RAM. If those blocks are needed again, they’re copied back. This is like your computer using its hard drive as extra RAM, but slower. (See maybe_offload_gpu_block_to_host in BlockManager). | KVCacheAgent: modular has an experimental feature called the KVCacheAgent. If enabled (via the experimental_enable_kvcache_agent setting in src/max/entrypoints/cli/serve.py), it starts a separate gRPC server (src/max/serve/kvcache_agent/kvcache_agent.py started by src/max/serve/pipelines/kvcache_worker.py). | Purpose: This agent can expose the state of the KV Cache (e.g., which blocks are on GPU, which are on CPU, how full the cache is) to external monitoring systems or potentially even to other processes that might want to influence cache management. | How it works: The BlockManager (or other parts of the cache system) would send messages (like KVCacheChangeMessage) to a queue. The KVCacheAgentServer reads from this queue and updates its internal state, which subscribers can then query via gRPC. This is an advanced feature for observability and control. | . | . ",
    "url": "/modular_max/06_kv_cache_management_.html#under-the-hood-a-closer-look-at-pagedkvcachemanager",
    
    "relUrl": "/modular_max/06_kv_cache_management_.html#under-the-hood-a-closer-look-at-pagedkvcachemanager"
  },"239": {
    "doc": "Chapter 6: KV Cache Management",
    "title": "Conclusion",
    "content": "KV Cache Management is a cornerstone of efficient LLM inference. By storing and reusing intermediate attention computations (Keys and Values), it drastically reduces redundant calculations. Techniques like Paged Attention, managed by components like PagedKVCacheManager and BlockManager, further optimize this by: . | Treating GPU memory like a set of manageable pages. | Reducing memory fragmentation. | Enabling higher concurrency and features like prefix caching. | Potentially swapping less-used cache data to host memory. | . This “smart scratchpad” allows the LLM to “remember” its previous thought process quickly and efficiently, leading to faster generation of coherent text. The Scheduler (TokenGenerationScheduler, EmbeddingsScheduler) works hand-in-hand with the KV Cache Manager to orchestrate this memory usage. But how do all these components—the Pipeline Orchestrator, the Model Worker, the Scheduler, and the KV Cache Manager—talk to each other, especially when they might be in different processes? That’s where our next topic, the EngineQueue, comes in! . Generated by AI Codebase Knowledge Builder . ",
    "url": "/modular_max/06_kv_cache_management_.html#conclusion",
    
    "relUrl": "/modular_max/06_kv_cache_management_.html#conclusion"
  },"240": {
    "doc": "Chapter 6: N-D to 1D Indexing Logic",
    "title": "Chapter 6: N-D to 1D Indexing Logic (Strided Memory Access)",
    "content": "Welcome to Chapter 6! We’ve come a long way in understanding how Mojo handles memory and N-dimensional data. Let’s recap our journey: . | Chapter 1: AddressSpace taught us about memory “neighborhoods.” | Chapter 2: UnsafePointer showed us how Mojo directly references memory locations. | Chapter 3: IndexList introduced a way to handle multi-dimensional coordinates. | Chapter 4: Dim and DimList explained how to describe dimension sizes, statically or dynamically. | Chapter 5: NDBuffer unveiled Mojo’s powerful N-dimensional array, which acts as a “view” over a flat block of memory. | . In the last chapter, we saw that an NDBuffer uses an UnsafePointer (its data field) to point to the beginning of its raw, 1D memory. But if the memory is flat, how does NDBuffer find the element at, say, matrix[row, column, depth]? This is where the magic of strided memory access comes in. This chapter dives into the core mechanism that NDBuffer uses to translate multi-dimensional indices (like [row, col, depth]) into a single, flat offset from its base UnsafePointer. This calculation crucially involves the strides associated with each dimension. ",
    "url": "/mojo-v1/06_n_d_to_1d_indexing_logic__strided_memory_access__.html#chapter-6-n-d-to-1d-indexing-logic-strided-memory-access",
    
    "relUrl": "/mojo-v1/06_n_d_to_1d_indexing_logic__strided_memory_access__.html#chapter-6-n-d-to-1d-indexing-logic-strided-memory-access"
  },"241": {
    "doc": "Chapter 6: N-D to 1D Indexing Logic",
    "title": "The Car Park Analogy Revisited",
    "content": "Remember the official description’s analogy? It’s perfect for understanding strides: . Imagine a multi-story car park where cars are arranged in rows on different levels. To find a specific car (indexed by [level, row, spot]), you’d calculate its position in a conceptual single line of all spots: (level * spots_per_level) + (row * spots_per_row) + spot_number. NDBuffer uses a similar formula: offset = sum(index[i] * stride[i]). Let’s break this down: . | [level, row, spot]: This is your multi-dimensional index. | spots_per_level: This is the “stride” for the level dimension. It’s how many total spots you skip to move from one level to the next, assuming you keep the row and spot number the same. | spots_per_row: This is the “stride” for the row dimension (within a level). It’s how many spots you skip to move from one row to the next, keeping the spot number the same. | spot_number: The index for the spot dimension. Its implicit stride is 1 (to get to the next spot in the same row, you just move 1 position). | . The total calculation gives you a single number – the position of your car if all spots from all levels and all rows were laid out in one giant, continuous line. This single number is the offset from the very first car spot in the park. ",
    "url": "/mojo-v1/06_n_d_to_1d_indexing_logic__strided_memory_access__.html#the-car-park-analogy-revisited",
    
    "relUrl": "/mojo-v1/06_n_d_to_1d_indexing_logic__strided_memory_access__.html#the-car-park-analogy-revisited"
  },"242": {
    "doc": "Chapter 6: N-D to 1D Indexing Logic",
    "title": "What are Strides, Exactly?",
    "content": "In the context of NDBuffer and computer memory: . Strides are a list of numbers, one for each dimension of the NDBuffer. The stride for a particular dimension tells you how many elements you need to “jump” over in the flat, 1D underlying memory to get to the next element in that dimension, while keeping all other dimension indices the same. | NDBuffer stores these strides in its dynamic_stride field (an IndexList we learned about in Chapter 3), or they can be partly known statically via the strides: DimList parameter. | . Consider a 2D NDBuffer (a matrix) with shape = (num_rows, num_cols): . | It will have two strides: stride_row and stride_col. | stride_col: Tells you how many elements to jump in the 1D memory to move from matrix[r, c] to matrix[r, c+1] (next column, same row). | stride_row: Tells you how many elements to jump in the 1D memory to move from matrix[r, c] to matrix[r+1, c] (next row, same column). | . ",
    "url": "/mojo-v1/06_n_d_to_1d_indexing_logic__strided_memory_access__.html#what-are-strides-exactly",
    
    "relUrl": "/mojo-v1/06_n_d_to_1d_indexing_logic__strided_memory_access__.html#what-are-strides-exactly"
  },"243": {
    "doc": "Chapter 6: N-D to 1D Indexing Logic",
    "title": "The Magic Formula: Calculating the 1D Offset",
    "content": "As stated in the introduction, NDBuffer translates an N-D index idx = [idx_0, idx_1, ..., idx_N-1] into a 1D offset from its data pointer using this formula: . offset = (idx_0 * stride_0) + (idx_1 * stride_1) + ... + (idx_N-1 * stride_N-1) . Or, more compactly: offset = sum(index[i] * stride[i]) for i from 0 to rank-1. Once this offset is calculated, the NDBuffer can access the element by moving offset elements from its base data pointer: element_address = ndbuffer.data + offset. ",
    "url": "/mojo-v1/06_n_d_to_1d_indexing_logic__strided_memory_access__.html#the-magic-formula-calculating-the-1d-offset",
    
    "relUrl": "/mojo-v1/06_n_d_to_1d_indexing_logic__strided_memory_access__.html#the-magic-formula-calculating-the-1d-offset"
  },"244": {
    "doc": "Chapter 6: N-D to 1D Indexing Logic",
    "title": "How NDBuffer Gets Its Strides",
    "content": "When you create an NDBuffer, you can provide explicit strides. However, if you only provide a shape (like in many examples from Chapter 5), NDBuffer calculates default strides for you. These default strides correspond to a contiguous memory layout, typically row-major (which we’ll discuss soon). This calculation happens in the _compute_ndbuffer_stride function found in stdlib/src/buffer/buffer.mojo. Let’s look at its logic conceptually for an NDBuffer with rank &gt; 0: . // Conceptual logic from _compute_ndbuffer_stride fn _compute_ndbuffer_stride[rank](shape: IndexList[rank]) -&gt; IndexList[rank]: var stride = IndexList[rank]() // Initialize strides if rank == 1: stride[0] = 1 // For a 1D array, stride is just 1 return stride // For rank &gt; 1 (e.g., 2D, 3D): // The stride for the very last dimension is 1 (elements are next to each other) stride[rank - 1] = 1 // Work backwards from the second-to-last dimension for i in reversed(range(rank - 1)): // e.g., for rank 3, i goes 1, 0 // Stride of current dimension = shape of NEXT dimension * stride of NEXT dimension stride[i] = shape[i + 1] * stride[i + 1] return stride . Example: 3D NDBuffer with shape = (depth, rows, cols) (e.g., (2, 3, 4)) . | rank = 3. | stride[2] (for cols dimension) = 1. | i = 1 (for rows dimension): stride[1] = shape[2] * stride[2] = cols * 1 = cols. So, stride_row = 4. | i = 0 (for depth dimension): stride[0] = shape[1] * stride[1] = rows * cols. So, stride_depth = 3 * 4 = 12. | . The resulting strides would be (rows * cols, cols, 1). For shape (2, 3, 4), strides are (12, 4, 1). ",
    "url": "/mojo-v1/06_n_d_to_1d_indexing_logic__strided_memory_access__.html#how-ndbuffer-gets-its-strides",
    
    "relUrl": "/mojo-v1/06_n_d_to_1d_indexing_logic__strided_memory_access__.html#how-ndbuffer-gets-its-strides"
  },"245": {
    "doc": "Chapter 6: N-D to 1D Indexing Logic",
    "title": "How NDBuffer Calculates the Offset: Inside _compute_ndbuffer_offset",
    "content": "The actual calculation of the 1D offset using the indices and strides is performed by the _compute_ndbuffer_offset function in stdlib/src/buffer/buffer.mojo. There are a few versions of this function to handle different ways you might provide indices (e.g., as a VariadicList[Int], a StaticTuple, or an IndexList). Let’s look at the core logic from one of these: . // Simplified from _compute_ndbuffer_offset in stdlib/src/buffer/buffer.mojo @always_inline fn _compute_ndbuffer_offset( buf: NDBuffer, index: IndexList[buf.rank], // The N-D indices [idx_0, idx_1, ...] ) -&gt; Int: alias rank = buf.rank @parameter // Means this can be optimized heavily at compile time if buf.rank == 0: // Scalar NDBuffer return 0 // This `_use_32bit_indexing` check is interesting! // It checks if we're on a GPU and if the memory is in certain // fast GPU address spaces (like SHARED or CONSTANT, from Chapter 1). // If so, it might use 32-bit math for speed if the offset fits. @parameter if _use_32bit_indexing[buf.address_space](): var result: Int32 = 0 @parameter for i in range(rank): // fma is \"fused multiply-add\": (a*b)+c // result = (buf.stride[i]() * Int32(index[i])) + result result = fma(Int32(buf.stride[i]()), Int32(index[i]), result) return Int(result) // Convert back to standard Int else: // For CPU or other GPU memory, use standard Int (usually 64-bit) var result: Int = 0 @parameter for i in range(rank): // result = (buf.stride[i]() * index[i]) + result result = fma(buf.stride[i](), index[i], result) return result . The key part is the loop: result = fma(buf.stride[i](), index[i], result) . This iteratively calculates sum(index[i] * stride[i]). The fma (fused multiply-add) instruction computes (a*b)+c often more efficiently and accurately than separate multiply and add operations. So, if you call my_ndbuffer[idx0, idx1, idx2], NDBuffer internally calls a function like _compute_ndbuffer_offset with your indices. This function uses the NDBuffer’s stored dynamic_stride values and your idx values to calculate the final 1D offset. Then, it accesses my_ndbuffer.data.offset(calculated_offset).load() or .store(). ",
    "url": "/mojo-v1/06_n_d_to_1d_indexing_logic__strided_memory_access__.html#how-ndbuffer-calculates-the-offset-inside-_compute_ndbuffer_offset",
    
    "relUrl": "/mojo-v1/06_n_d_to_1d_indexing_logic__strided_memory_access__.html#how-ndbuffer-calculates-the-offset-inside-_compute_ndbuffer_offset"
  },"246": {
    "doc": "Chapter 6: N-D to 1D Indexing Logic",
    "title": "Strides in Action: Memory Layouts",
    "content": "The power of strides is that they define how the N-dimensional data is actually laid out in the 1D memory block. The two most common layouts are row-major and column-major. 1. Row-Major Order (C-style, Default in Mojo/NumPy) . In row-major order, elements of a row are contiguous in memory. You go through all columns of row 0, then all columns of row 1, and so on. Consider a 2x3 matrix: A = [[a, b, c], [d, e, f]] . | Shape: (2, 3) (2 rows, 3 columns) | Memory Layout: a, b, c, d, e, f | Strides (calculated by _compute_ndbuffer_stride): . | stride[1] (for columns) = 1 (to go from a to b, jump 1 element) | stride[0] (for rows) = shape[1] * stride[1] = 3 * 1 = 3 (to go from a to d, jump 3 elements: b, c, d) | So, strides are (3, 1). | . | . To find element A[r, c]: offset = (r * stride_row) + (c * stride_col) = (r * 3) + (c * 1) . | A[0,0] (a): (0*3) + (0*1) = 0. Offset 0 from base. | A[0,1] (b): (0*3) + (1*1) = 1. Offset 1 from base. | A[1,0] (d): (1*3) + (0*1) = 3. Offset 3 from base. | A[1,2] (f): (1*3) + (2*1) = 5. Offset 5 from base. | . 2. Column-Major Order (Fortran-style) . In column-major order, elements of a column are contiguous. You go through all rows of column 0, then all rows of column 1, etc. Consider the same 2x3 matrix: A = [[a, b, c], [d, e, f]] . | Shape: (2, 3) (2 rows, 3 columns) | Memory Layout: a, d, b, e, c, f | Strides: . | Here, to make columns contiguous, the stride for the first dimension (rows) must be 1. | stride[0] (for rows) = 1 (to go from a to d, jump 1 element) | stride[1] (for columns) = shape[0] * stride[0] = 2 * 1 = 2 (to go from a to b, jump 2 elements: d,b) | So, strides are (1, 2). | . | . To find element A[r, c] (using the same formula, just different stride values): offset = (r * stride_row) + (c * stride_col) = (r * 1) + (c * 2) . | A[0,0] (a): (0*1) + (0*2) = 0. Offset 0. | A[0,1] (b): (0*1) + (1*2) = 2. Offset 2. (Memory: a,d,b &lt;- b is at offset 2) | A[1,0] (d): (1*1) + (0*2) = 1. Offset 1. (Memory: a,d &lt;- d is at offset 1) | A[1,2] (f): (1*1) + (2*2) = 5. Offset 5. (Memory: a,d,b,e,c,f &lt;- f is at offset 5) | . NDBuffer can represent data in either layout (or even more complex ones!) simply by having the correct strides parameter. The access logic (matrix[r,c]) and the formula sum(index[i] * stride[i]) remain the same. This is incredibly flexible! . ",
    "url": "/mojo-v1/06_n_d_to_1d_indexing_logic__strided_memory_access__.html#strides-in-action-memory-layouts",
    
    "relUrl": "/mojo-v1/06_n_d_to_1d_indexing_logic__strided_memory_access__.html#strides-in-action-memory-layouts"
  },"247": {
    "doc": "Chapter 6: N-D to 1D Indexing Logic",
    "title": "Why Strided Access Matters",
    "content": ". | Efficiency: Calculating an offset is a fast arithmetic operation. It allows direct access to any element without needing to iterate or search. | Flexibility: . | Supports different memory layouts (row-major, column-major) for compatibility with various libraries or hardware preferences. | Enables advanced array manipulations. | . | Zero-Copy Operations: This is a big one! Many operations that might seem to require data copying can be done by just creating a new NDBuffer with different shape/strides that points to the same underlying data. | Slicing: Taking a sub-matrix B = A[rows_slice, cols_slice] can often be done by creating B with an offset base pointer from A and new strides/shape, without copying A’s data. The tile() method we saw in Chapter 5 does this. | Transposing: Transposing a matrix A to get A_T involves swapping its shape dimensions and its stride dimensions. The new NDBuffer A_T can point to the exact same memory as A. | Broadcasting: In operations like adding a vector to each row of a matrix, strides can be set to 0 for the broadcasted dimension, effectively reusing the vector’s data without copying. | . | . These zero-copy views save memory and significantly speed up computations by avoiding redundant data movement. ",
    "url": "/mojo-v1/06_n_d_to_1d_indexing_logic__strided_memory_access__.html#why-strided-access-matters",
    
    "relUrl": "/mojo-v1/06_n_d_to_1d_indexing_logic__strided_memory_access__.html#why-strided-access-matters"
  },"248": {
    "doc": "Chapter 6: N-D to 1D Indexing Logic",
    "title": "Example Walkthrough: A 2x2 NDBuffer",
    "content": "Let’s create a simple 2x2 NDBuffer and trace the indexing. from buffer import NDBuffer, DimList from memory import InlineArray, MutableAnyOrigin from builtin import DType, Scalar from utils import IndexList fn main_stride_example(): print(\"--- Strides Example: 2x2 Matrix ---\") // 1. Create storage and an NDBuffer (default row-major) var data_storage = InlineArray[Scalar[DType.int32], 2 * 2](uninitialized=True) var matrix = NDBuffer[ DType.int32, 2, // rank _, // origin DimList(2, 2) // shape: 2 rows, 2 columns ](data_storage) // Let's see its shape and calculated strides var shape = matrix.get_shape() // Should be (2, 2) var strides = matrix.get_strides() // Should be (2, 1) for row-major print(\"Shape:\", shape) print(\"Strides:\", strides) // Fill the matrix: // matrix[0,0] = 10, matrix[0,1] = 11 // matrix[1,0] = 20, matrix[1,1] = 21 var counter: Int32 = 10 for r in range(matrix.dim[0]()): // Iterate rows for c in range(matrix.dim[1]()): // Iterate columns matrix[r,c] = counter if c == 0 and r == 1: counter = 20 # Just to make values 10,11,20,21 else: counter += 1 print(\"Matrix content (via NDBuffer access):\") print(matrix[0,0], matrix[0,1]) print(matrix[1,0], matrix[1,1]) // Let's manually calculate offset for matrix[1,0] (should be 20) // Indices: r=1, c=0 // Strides: stride_r=2, stride_c=1 var r_idx = 1 var c_idx = 0 var stride_r = strides[0] // Is 2 var stride_c = strides[1] // Is 1 var offset = (r_idx * stride_r) + (c_idx * stride_c) // offset = (1 * 2) + (0 * 1) = 2 + 0 = 2 print(\"Calculated offset for matrix[1,0]:\", offset) // Verify by looking at the raw data (if we could easily inspected InlineArray) // The flat data_storage would look like: [10, 11, 20, 21] // matrix.data.offset(2).load() would give us 20. // matrix[1,0] internally does this! print(\"Value at matrix[1,0] using NDBuffer:\", matrix[1,0]) . If you could run this (e.g., by calling main_stride_example()), you’d see: . --- Strides Example: 2x2 Matrix --- Shape: (2, 2) Strides: (2, 1) Matrix content (via NDBuffer access): 10 11 20 21 Calculated offset for matrix[1,0]: 2 Value at matrix[1,0] using NDBuffer: 20 . This confirms that NDBuffer used the strides (2,1) to correctly find the element matrix[1,0] at offset 2 in the flat memory. ",
    "url": "/mojo-v1/06_n_d_to_1d_indexing_logic__strided_memory_access__.html#example-walkthrough-a-2x2-ndbuffer",
    
    "relUrl": "/mojo-v1/06_n_d_to_1d_indexing_logic__strided_memory_access__.html#example-walkthrough-a-2x2-ndbuffer"
  },"249": {
    "doc": "Chapter 6: N-D to 1D Indexing Logic",
    "title": "Summary",
    "content": ". | NDBuffer uses strides to map N-dimensional indices to a 1D memory offset. | The formula is simple but powerful: offset = sum(index[i] * stride[i]). | _compute_ndbuffer_stride calculates default (contiguous, row-major) strides if not provided. | _compute_ndbuffer_offset implements the offset calculation, often using efficient fma operations. | Strides define the actual memory layout (row-major, column-major, etc.) and enable NDBuffer to handle them transparently. | This mechanism is key to NDBuffer’s efficiency, flexibility, and its ability to perform zero-copy views for operations like slicing and transposing. | . Understanding strided memory access unlocks a deeper appreciation for how N-dimensional arrays are handled efficiently in systems like Mojo, and why NDBuffer is such a versatile tool. ",
    "url": "/mojo-v1/06_n_d_to_1d_indexing_logic__strided_memory_access__.html#summary",
    
    "relUrl": "/mojo-v1/06_n_d_to_1d_indexing_logic__strided_memory_access__.html#summary"
  },"250": {
    "doc": "Chapter 6: N-D to 1D Indexing Logic",
    "title": "What’s Next?",
    "content": "This chapter concludes our deep dive into the fundamental building blocks and internal mechanics of NDBuffer. You now have a solid understanding of: . | Memory organization (AddressSpace). | Raw memory access (UnsafePointer). | Representing N-D coordinates and shapes (IndexList, DimList). | The NDBuffer structure itself. | And how NDBuffer navigates its data using strides. | . With this foundation, you’re well-equipped to use NDBuffer effectively in Mojo for various numerical computing tasks. Future explorations might involve using NDBuffer in more complex algorithms, interfacing with other Mojo libraries, or diving into performance optimization techniques that leverage these concepts. Congratulations on completing this foundational series on NDBuffer! . Table of Contents . | Chapter 1: Understanding Memory Neighborhoods with AddressSpace | Chapter 2: Peeking into Memory with UnsafePointer | Chapter 3: Working with Multiple Dimensions: IndexList | Chapter 4: Describing Dimensions with Dim and DimList | Chapter 5: The N-Dimensional Buffer: NDBuffer | Chapter 6: N-D to 1D Indexing Logic (Strided Memory Access) (You are here) ``` | . Generated by AI Codebase Knowledge Builder . ",
    "url": "/mojo-v1/06_n_d_to_1d_indexing_logic__strided_memory_access__.html#whats-next",
    
    "relUrl": "/mojo-v1/06_n_d_to_1d_indexing_logic__strided_memory_access__.html#whats-next"
  },"251": {
    "doc": "Chapter 6: N-D to 1D Indexing Logic",
    "title": "Chapter 6: N-D to 1D Indexing Logic",
    "content": " ",
    "url": "/mojo-v1/06_n_d_to_1d_indexing_logic__strided_memory_access__.html",
    
    "relUrl": "/mojo-v1/06_n_d_to_1d_indexing_logic__strided_memory_access__.html"
  },"252": {
    "doc": "CrawlResult",
    "title": "Chapter 7: Understanding the Results - CrawlResult",
    "content": "In the previous chapter, Chapter 6: Getting Specific Data - ExtractionStrategy, we learned how to teach Crawl4AI to act like an analyst, extracting specific, structured data points from a webpage using an ExtractionStrategy. We’ve seen how Crawl4AI can fetch pages, clean them, filter them, and even extract precise information. But after all that work, where does all the gathered information go? When you ask the AsyncWebCrawler to crawl a URL using arun(), what do you actually get back? . ",
    "url": "/Crawl4AI/07_crawlresult.html#chapter-7-understanding-the-results---crawlresult",
    
    "relUrl": "/Crawl4AI/07_crawlresult.html#chapter-7-understanding-the-results---crawlresult"
  },"253": {
    "doc": "CrawlResult",
    "title": "What Problem Does CrawlResult Solve?",
    "content": "Imagine you sent a research assistant to the library (a website) with a set of instructions: “Find this book (URL), make a clean copy of the relevant chapter (clean HTML/Markdown), list all the cited references (links), take photos of the illustrations (media), find the author and publication date (metadata), and maybe extract specific quotes (structured data).” . When the assistant returns, they wouldn’t just hand you a single piece of paper. They’d likely give you a folder containing everything you asked for: the clean copy, the list of references, the photos, the metadata notes, and the extracted quotes, all neatly organized. They might also include a note if they encountered any problems (errors). CrawlResult is exactly this final report folder or delivery package. It’s a single object that neatly contains all the information Crawl4AI gathered and processed for a specific URL during a crawl operation. Instead of getting lots of separate pieces of data back, you get one convenient container. ",
    "url": "/Crawl4AI/07_crawlresult.html#what-problem-does-crawlresult-solve",
    
    "relUrl": "/Crawl4AI/07_crawlresult.html#what-problem-does-crawlresult-solve"
  },"254": {
    "doc": "CrawlResult",
    "title": "What is CrawlResult?",
    "content": "CrawlResult is a Python object (specifically, a Pydantic model, which is like a super-powered dictionary) that acts as a data container. It holds the results of a single crawl task performed by AsyncWebCrawler.arun() or one of the results from arun_many(). Think of it as a toolbox filled with different tools and information related to the crawled page. Key Information Stored in CrawlResult: . | url (string): The original URL that was requested. | success (boolean): Did the crawl complete without critical errors? True if successful, False otherwise. Always check this first! | html (string): The raw, original HTML source code fetched from the page. | cleaned_html (string): The HTML after initial cleaning by the ContentScrapingStrategy (e.g., scripts, styles removed). | markdown (object): An object containing different Markdown representations of the content. | markdown.raw_markdown: Basic Markdown generated from cleaned_html. | markdown.fit_markdown: Markdown generated only from content deemed relevant by a RelevantContentFilter (if one was used). Might be empty if no filter was applied. | (Other fields like markdown_with_citations might exist) | . | extracted_content (string): If you used an ExtractionStrategy, this holds the extracted structured data, usually formatted as a JSON string. None if no extraction was performed or nothing was found. | metadata (dictionary): Information extracted from the page’s metadata tags, like the page title (metadata['title']), description, keywords, etc. | links (object): Contains lists of links found on the page. | links.internal: List of links pointing to the same website. | links.external: List of links pointing to other websites. | . | media (object): Contains lists of media items found. | media.images: List of images (&lt;img&gt; tags). | media.videos: List of videos (&lt;video&gt; tags). | (Other media types might be included) | . | screenshot (string): If you requested a screenshot (screenshot=True in CrawlerRunConfig), this holds the file path to the saved image. None otherwise. | pdf (bytes): If you requested a PDF (pdf=True in CrawlerRunConfig), this holds the PDF data as bytes. None otherwise. (Note: Previously might have been a path, now often bytes). | error_message (string): If success is False, this field usually contains details about what went wrong. | status_code (integer): The HTTP status code received from the server (e.g., 200 for OK, 404 for Not Found). | response_headers (dictionary): The HTTP response headers sent by the server. | redirected_url (string): If the original URL redirected, this shows the final URL the crawler landed on. | . ",
    "url": "/Crawl4AI/07_crawlresult.html#what-is-crawlresult",
    
    "relUrl": "/Crawl4AI/07_crawlresult.html#what-is-crawlresult"
  },"255": {
    "doc": "CrawlResult",
    "title": "Accessing the CrawlResult",
    "content": "You get a CrawlResult object back every time you await a call to crawler.arun(): . # chapter7_example_1.py import asyncio from crawl4ai import AsyncWebCrawler async def main(): async with AsyncWebCrawler() as crawler: url = \"https://httpbin.org/html\" print(f\"Crawling {url}...\") # The 'arun' method returns a CrawlResult object result: CrawlResult = await crawler.arun(url=url) # Type hint optional print(\"Crawl finished!\") # Now 'result' holds all the information print(f\"Result object type: {type(result)}\") if __name__ == \"__main__\": asyncio.run(main()) . Explanation: . | We call crawler.arun(url=url). | The await keyword pauses execution until the crawl is complete. | The value returned by arun is assigned to the result variable. | This result variable is our CrawlResult object. | . If you use crawler.arun_many(), it returns a list where each item is a CrawlResult object for one of the requested URLs (or an async generator if stream=True). ",
    "url": "/Crawl4AI/07_crawlresult.html#accessing-the-crawlresult",
    
    "relUrl": "/Crawl4AI/07_crawlresult.html#accessing-the-crawlresult"
  },"256": {
    "doc": "CrawlResult",
    "title": "Exploring the Attributes: Using the Toolbox",
    "content": "Once you have the result object, you can access its attributes using dot notation (e.g., result.success, result.markdown). 1. Checking for Success (Most Important!) . Before you try to use any data, always check if the crawl was successful: . # chapter7_example_2.py import asyncio from crawl4ai import AsyncWebCrawler, CrawlResult # Import CrawlResult for type hint async def main(): async with AsyncWebCrawler() as crawler: url = \"https://httpbin.org/html\" # A working URL # url = \"https://httpbin.org/status/404\" # Try this URL to see failure result: CrawlResult = await crawler.arun(url=url) # --- ALWAYS CHECK 'success' FIRST! --- if result.success: print(f\"✅ Successfully crawled: {result.url}\") # Now it's safe to access other attributes print(f\" Page Title: {result.metadata.get('title', 'N/A')}\") else: print(f\"❌ Failed to crawl: {result.url}\") print(f\" Error: {result.error_message}\") print(f\" Status Code: {result.status_code}\") if __name__ == \"__main__\": asyncio.run(main()) . Explanation: . | We use an if result.success: block. | If True, we proceed to access other data like result.metadata. | If False, we print the result.error_message and result.status_code to understand why it failed. | . 2. Accessing Content (HTML, Markdown) . # chapter7_example_3.py import asyncio from crawl4ai import AsyncWebCrawler, CrawlResult async def main(): async with AsyncWebCrawler() as crawler: url = \"https://httpbin.org/html\" result: CrawlResult = await crawler.arun(url=url) if result.success: print(\"--- Content ---\") # Print the first 150 chars of raw HTML print(f\"Raw HTML snippet: {result.html[:150]}...\") # Access the raw markdown if result.markdown: # Check if markdown object exists print(f\"Markdown snippet: {result.markdown.raw_markdown[:150]}...\") else: print(\"Markdown not generated.\") else: print(f\"Crawl failed: {result.error_message}\") if __name__ == \"__main__\": asyncio.run(main()) . Explanation: . | We access result.html for the original HTML. | We access result.markdown.raw_markdown for the main Markdown content. Note the two dots: result.markdown gives the MarkdownGenerationResult object, and .raw_markdown accesses the specific string within it. We also check if result.markdown: first, just in case markdown generation failed for some reason. | . 3. Getting Metadata, Links, and Media . # chapter7_example_4.py import asyncio from crawl4ai import AsyncWebCrawler, CrawlResult async def main(): async with AsyncWebCrawler() as crawler: url = \"https://httpbin.org/links/10/0\" # A page with links result: CrawlResult = await crawler.arun(url=url) if result.success: print(\"--- Metadata &amp; Links ---\") print(f\"Title: {result.metadata.get('title', 'N/A')}\") print(f\"Found {len(result.links.internal)} internal links.\") print(f\"Found {len(result.links.external)} external links.\") if result.links.internal: print(f\" First internal link text: '{result.links.internal[0].text}'\") # Similarly access result.media.images etc. else: print(f\"Crawl failed: {result.error_message}\") if __name__ == \"__main__\": asyncio.run(main()) . Explanation: . | result.metadata is a dictionary; use .get() for safe access. | result.links and result.media are objects containing lists (internal, external, images, etc.). We can check their lengths (len()) and access individual items by index (e.g., [0]). | . 4. Checking for Extracted Data, Screenshots, PDFs . # chapter7_example_5.py import asyncio import json from crawl4ai import ( AsyncWebCrawler, CrawlResult, CrawlerRunConfig, JsonCssExtractionStrategy # Example extractor ) async def main(): # Define a simple extraction strategy (from Chapter 6) schema = {\"baseSelector\": \"body\", \"fields\": [{\"name\": \"heading\", \"selector\": \"h1\", \"type\": \"text\"}]} extractor = JsonCssExtractionStrategy(schema=schema) # Configure the run to extract and take a screenshot config = CrawlerRunConfig( extraction_strategy=extractor, screenshot=True ) async with AsyncWebCrawler() as crawler: url = \"https://httpbin.org/html\" result: CrawlResult = await crawler.arun(url=url, config=config) if result.success: print(\"--- Extracted Data &amp; Media ---\") # Check if structured data was extracted if result.extracted_content: print(\"Extracted Data found:\") data = json.loads(result.extracted_content) # Parse the JSON string print(json.dumps(data, indent=2)) else: print(\"No structured data extracted.\") # Check if a screenshot was taken if result.screenshot: print(f\"Screenshot saved to: {result.screenshot}\") else: print(\"Screenshot not taken.\") # Check for PDF (would be bytes if requested and successful) if result.pdf: print(f\"PDF data captured ({len(result.pdf)} bytes).\") else: print(\"PDF not generated.\") else: print(f\"Crawl failed: {result.error_message}\") if __name__ == \"__main__\": asyncio.run(main()) . Explanation: . | We check if result.extracted_content is not None or empty before trying to parse it as JSON. | We check if result.screenshot is not None to see if the file path exists. | We check if result.pdf is not None to see if the PDF data (bytes) was captured. | . ",
    "url": "/Crawl4AI/07_crawlresult.html#exploring-the-attributes-using-the-toolbox",
    
    "relUrl": "/Crawl4AI/07_crawlresult.html#exploring-the-attributes-using-the-toolbox"
  },"257": {
    "doc": "CrawlResult",
    "title": "How is CrawlResult Created? (Under the Hood)",
    "content": "You don’t interact with the CrawlResult constructor directly. The AsyncWebCrawler creates it for you at the very end of the arun process, typically inside its internal aprocess_html method (or just before returning if fetching from cache). Here’s a simplified sequence: . | Fetch: AsyncWebCrawler calls the AsyncCrawlerStrategy to get the raw html, status_code, response_headers, etc. | Scrape: It passes the html to the ContentScrapingStrategy to get cleaned_html, links, media, metadata. | Markdown: It generates Markdown using the configured generator, possibly involving a RelevantContentFilter, resulting in a MarkdownGenerationResult object. | Extract (Optional): If an ExtractionStrategy is configured, it runs it on the appropriate content (HTML or Markdown) to get extracted_content. | Screenshot/PDF (Optional): If requested, the fetching strategy captures the screenshot path or pdf data. | Package: AsyncWebCrawler gathers all these pieces (url, html, cleaned_html, the markdown object, links, media, metadata, extracted_content, screenshot, pdf, success status, error_message, etc.). | Instantiate: It creates the CrawlResult object, passing all the gathered data into its constructor. | Return: It returns this fully populated CrawlResult object to your code. | . ",
    "url": "/Crawl4AI/07_crawlresult.html#how-is-crawlresult-created-under-the-hood",
    
    "relUrl": "/Crawl4AI/07_crawlresult.html#how-is-crawlresult-created-under-the-hood"
  },"258": {
    "doc": "CrawlResult",
    "title": "Code Glimpse (models.py)",
    "content": "The CrawlResult is defined in the crawl4ai/models.py file. It uses Pydantic, a library that helps define data structures with type hints and validation. Here’s a simplified view: . # Simplified from crawl4ai/models.py from pydantic import BaseModel, HttpUrl from typing import List, Dict, Optional, Any # Other related models (simplified) class MarkdownGenerationResult(BaseModel): raw_markdown: str fit_markdown: Optional[str] = None # ... other markdown fields ... class Links(BaseModel): internal: List[Dict] = [] external: List[Dict] = [] class Media(BaseModel): images: List[Dict] = [] videos: List[Dict] = [] # The main CrawlResult model class CrawlResult(BaseModel): url: str html: str success: bool cleaned_html: Optional[str] = None media: Media = Media() # Use the Media model links: Links = Links() # Use the Links model screenshot: Optional[str] = None pdf: Optional[bytes] = None # Uses a private attribute and property for markdown for compatibility _markdown: Optional[MarkdownGenerationResult] = None # Actual storage extracted_content: Optional[str] = None # JSON string metadata: Optional[Dict[str, Any]] = None error_message: Optional[str] = None status_code: Optional[int] = None response_headers: Optional[Dict[str, str]] = None redirected_url: Optional[str] = None # ... other fields like session_id, ssl_certificate ... # Custom property to access markdown data @property def markdown(self) -&gt; Optional[MarkdownGenerationResult]: return self._markdown # Configuration for Pydantic class Config: arbitrary_types_allowed = True # Custom init and model_dump might exist for backward compatibility handling # ... (omitted for simplicity) ... Explanation: . | It’s defined as a class CrawlResult(BaseModel):. | Each attribute (like url, html, success) is defined with a type hint (like str, bool, Optional[str]). Optional[str] means the field can be a string or None. | Some attributes are themselves complex objects defined by other Pydantic models (like media: Media, links: Links). | The markdown field uses a common pattern (property wrapping a private attribute) to provide the MarkdownGenerationResult object while maintaining some backward compatibility. You access it simply as result.markdown. | . ",
    "url": "/Crawl4AI/07_crawlresult.html#code-glimpse-modelspy",
    
    "relUrl": "/Crawl4AI/07_crawlresult.html#code-glimpse-modelspy"
  },"259": {
    "doc": "CrawlResult",
    "title": "Conclusion",
    "content": "You’ve now met the CrawlResult object – the final, comprehensive report delivered by Crawl4AI after processing a URL. | It acts as a container holding all gathered information (HTML, Markdown, metadata, links, media, extracted data, errors, etc.). | It’s the return value of AsyncWebCrawler.arun() and arun_many(). | The most crucial attribute is success (boolean), which you should always check first. | You can easily access all the different pieces of information using dot notation (e.g., result.metadata['title'], result.markdown.raw_markdown, result.links.external). | . Understanding the CrawlResult is key to effectively using the information Crawl4AI provides. So far, we’ve focused on crawling single pages or lists of specific URLs. But what if you want to start at one page and automatically discover and crawl linked pages, exploring a website more deeply? . Next: Let’s explore how to perform multi-page crawls with Chapter 8: Exploring Websites - DeepCrawlStrategy. Generated by AI Codebase Knowledge Builder . ",
    "url": "/Crawl4AI/07_crawlresult.html#conclusion",
    
    "relUrl": "/Crawl4AI/07_crawlresult.html#conclusion"
  },"260": {
    "doc": "CrawlResult",
    "title": "CrawlResult",
    "content": " ",
    "url": "/Crawl4AI/07_crawlresult.html",
    
    "relUrl": "/Crawl4AI/07_crawlresult.html"
  },"261": {
    "doc": "Chapter 7: Dispatcher Framework",
    "title": "Chapter 7: Dispatcher Framework",
    "content": "In Chapter 6: Caching System, we learned how to store and reuse web content to make our crawlers more efficient. Now, let’s explore how to manage multiple crawling operations happening at the same time with the Dispatcher Framework. ",
    "url": "/Crawl4AI/07_dispatcher_framework_.html",
    
    "relUrl": "/Crawl4AI/07_dispatcher_framework_.html"
  },"262": {
    "doc": "Chapter 7: Dispatcher Framework",
    "title": "What is the Dispatcher Framework?",
    "content": "Imagine you’re running a busy restaurant kitchen. You need a head chef (the dispatcher) who coordinates all the cooks (crawler instances), making sure they don’t overcrowd the kitchen or use too many ingredients at once. The Dispatcher Framework in crawl4ai works the same way - it coordinates multiple crawler instances: . | It controls how many web pages are crawled at the same time | It makes sure your computer’s memory doesn’t get overwhelmed | It ensures websites aren’t bombarded with too many requests at once | . Let’s see a simple example of how this works: . from crawl4ai import AsyncWebCrawler, SemaphoreDispatcher, CrawlerRunConfig async def crawl_multiple_sites(): # Create a dispatcher that allows 3 crawls at a time dispatcher = SemaphoreDispatcher(semaphore_count=3) # List of URLs to crawl urls = [\"https://example.com\", \"https://example.org\", \"https://example.net\"] # Create a crawler and run multiple URLs using our dispatcher async with AsyncWebCrawler() as crawler: results = await crawler.arun_many( urls=urls, dispatcher=dispatcher, config=CrawlerRunConfig() ) return results . In this example, we’re using a SemaphoreDispatcher to crawl three URLs at the same time, not one after another. This is much faster! . ",
    "url": "/Crawl4AI/07_dispatcher_framework_.html#what-is-the-dispatcher-framework",
    
    "relUrl": "/Crawl4AI/07_dispatcher_framework_.html#what-is-the-dispatcher-framework"
  },"263": {
    "doc": "Chapter 7: Dispatcher Framework",
    "title": "Two Types of Dispatchers",
    "content": "The Dispatcher Framework offers two main types of dispatchers, each with different strengths: . 1. SemaphoreDispatcher: Simple and Predictable . The SemaphoreDispatcher is like a restaurant with a fixed number of tables. It allows a specific number of crawls to happen at the same time - no more, no less. from crawl4ai import SemaphoreDispatcher # Create a dispatcher that allows 5 simultaneous crawls dispatcher = SemaphoreDispatcher(semaphore_count=5) . This dispatcher is great when: . | You want predictable behavior | You know exactly how many concurrent crawls your system can handle | You want to set a strict limit on resource usage | . 2. MemoryAdaptiveDispatcher: Smart and Flexible . The MemoryAdaptiveDispatcher is like a restaurant that adjusts how many tables are available based on how busy the kitchen is. It monitors your computer’s memory and automatically adjusts how many crawls happen at once. from crawl4ai import MemoryAdaptiveDispatcher # Create a dispatcher that adapts to memory usage dispatcher = MemoryAdaptiveDispatcher( memory_threshold_percent=90.0, # Slow down when memory reaches 90% max_session_permit=20 # Never exceed 20 simultaneous crawls ) . This dispatcher is perfect when: . | You want to maximize efficiency without crashing your program | Your system’s available memory changes over time | You’re crawling pages with unpredictable memory requirements | . ",
    "url": "/Crawl4AI/07_dispatcher_framework_.html#two-types-of-dispatchers",
    
    "relUrl": "/Crawl4AI/07_dispatcher_framework_.html#two-types-of-dispatchers"
  },"264": {
    "doc": "Chapter 7: Dispatcher Framework",
    "title": "Managing Rate Limits with RateLimiter",
    "content": "When crawling websites, it’s important to be polite and not send too many requests too quickly. The RateLimiter helps with this: . from crawl4ai import RateLimiter, MemoryAdaptiveDispatcher # Create a rate limiter that waits 1-3 seconds between requests rate_limiter = RateLimiter( base_delay=(1.0, 3.0), # Wait 1-3 seconds between requests max_delay=60.0, # Never wait more than 60 seconds max_retries=3 # Try a maximum of 3 times if rate limited ) # Add the rate limiter to our dispatcher dispatcher = MemoryAdaptiveDispatcher(rate_limiter=rate_limiter) . The RateLimiter tracks each domain separately and adjusts wait times if it detects rate limiting (like HTTP 429 responses). ",
    "url": "/Crawl4AI/07_dispatcher_framework_.html#managing-rate-limits-with-ratelimiter",
    
    "relUrl": "/Crawl4AI/07_dispatcher_framework_.html#managing-rate-limits-with-ratelimiter"
  },"265": {
    "doc": "Chapter 7: Dispatcher Framework",
    "title": "Processing Results in Real-Time with Streaming",
    "content": "Sometimes you don’t want to wait for all URLs to finish crawling before processing results. Streaming allows you to process results as they come in: . from crawl4ai import AsyncWebCrawler, CrawlerRunConfig async def stream_results(): urls = [\"https://example.com\", \"https://example.org\", \"https://example.net\"] # Enable streaming in the configuration config = CrawlerRunConfig(stream=True) async with AsyncWebCrawler() as crawler: # This returns an async generator results_generator = await crawler.arun_many(urls=urls, config=config) # Process results as they arrive async for result in results_generator: print(f\"Just finished: {result.url}\") # Process each result immediately . With streaming, you can start working with results from fast-loading pages while slower pages are still being crawled. ",
    "url": "/Crawl4AI/07_dispatcher_framework_.html#processing-results-in-real-time-with-streaming",
    
    "relUrl": "/Crawl4AI/07_dispatcher_framework_.html#processing-results-in-real-time-with-streaming"
  },"266": {
    "doc": "Chapter 7: Dispatcher Framework",
    "title": "A Complete Example: News Crawler with Memory Adaptation",
    "content": "Let’s put everything together in a more complete example. Imagine we’re building a news crawler that needs to handle many URLs efficiently: . from crawl4ai import ( AsyncWebCrawler, MemoryAdaptiveDispatcher, RateLimiter, CrawlerRunConfig ) async def crawl_news_sites(): # Create a memory-adaptive dispatcher with rate limiting dispatcher = MemoryAdaptiveDispatcher( memory_threshold_percent=85.0, # Be conservative with memory rate_limiter=RateLimiter(base_delay=(2.0, 5.0)) # Be extra polite ) # List of news sites to crawl news_sites = [ \"https://example-news.com/tech\", \"https://example-news.com/business\", # ... more URLs ... ] # Configure how we want to crawl config = CrawlerRunConfig( screenshot=True, # Take screenshots stream=True # Process results as they arrive ) # Run the crawler with our dispatcher async with AsyncWebCrawler() as crawler: results_stream = await crawler.arun_many( urls=news_sites, config=config, dispatcher=dispatcher ) # Process each result as it completes async for result in results_stream: if result.success: print(f\"Successfully crawled: {result.url}\") # Process the content... else: print(f\"Failed to crawl: {result.url}\") . This example shows how to: . | Create a memory-adaptive dispatcher with rate limiting | Configure it for cautious memory usage and polite crawling | Stream and process results as they become available | . ",
    "url": "/Crawl4AI/07_dispatcher_framework_.html#a-complete-example-news-crawler-with-memory-adaptation",
    
    "relUrl": "/Crawl4AI/07_dispatcher_framework_.html#a-complete-example-news-crawler-with-memory-adaptation"
  },"267": {
    "doc": "Chapter 7: Dispatcher Framework",
    "title": "What Happens Under the Hood",
    "content": "When you use the Dispatcher Framework, a lot happens behind the scenes. Let’s look at a simplified view: . sequenceDiagram participant App as Your Code participant WC as WebCrawler participant DP as Dispatcher participant RL as RateLimiter participant MM as Memory Monitor App-&gt;&gt;WC: arun_many(urls, dispatcher) WC-&gt;&gt;DP: run_urls(urls, crawler, config) activate DP DP-&gt;&gt;MM: Start monitoring memory loop For each URL (controlled by dispatcher) DP-&gt;&gt;RL: wait_if_needed(url) RL--&gt;&gt;DP: OK to proceed alt Memory usage is normal DP-&gt;&gt;WC: arun(url, config) WC--&gt;&gt;DP: Return result else Memory pressure is high DP-&gt;&gt;DP: Re-queue URL with higher wait time end DP-&gt;&gt;App: Yield result (if streaming) end DP-&gt;&gt;MM: Stop monitoring DP--&gt;&gt;WC: Return all results WC--&gt;&gt;App: Return results deactivate DP . Here’s what happens step by step: . | Your code calls crawler.arun_many() with a list of URLs and a dispatcher | The WebCrawler passes the URLs to the dispatcher’s run_urls method | The dispatcher starts a memory monitor if it’s a MemoryAdaptiveDispatcher | For each URL: . | The dispatcher checks with the rate limiter if it’s OK to crawl | If memory usage is normal, it crawls the URL | If memory is under pressure, it might delay some URLs | It yields or collects the result | . | Finally, all results are returned to your code | . ",
    "url": "/Crawl4AI/07_dispatcher_framework_.html#what-happens-under-the-hood",
    
    "relUrl": "/Crawl4AI/07_dispatcher_framework_.html#what-happens-under-the-hood"
  },"268": {
    "doc": "Chapter 7: Dispatcher Framework",
    "title": "Implementation Details: MemoryAdaptiveDispatcher",
    "content": "Let’s look at how the MemoryAdaptiveDispatcher is implemented: . # From crawl4ai/async_dispatcher.py (simplified) class MemoryAdaptiveDispatcher(BaseDispatcher): def __init__( self, memory_threshold_percent=90.0, critical_threshold_percent=95.0, recovery_threshold_percent=85.0, max_session_permit=20, rate_limiter=None, ): self.memory_threshold_percent = memory_threshold_percent self.critical_threshold_percent = critical_threshold_percent self.max_session_permit = max_session_permit self.rate_limiter = rate_limiter self.memory_pressure_mode = False . The constructor sets up thresholds for memory management: . | memory_threshold_percent: When to start slowing down (90% by default) | critical_threshold_percent: When to pause crawling (95% by default) | max_session_permit: Maximum concurrent crawls (20 by default) | . The dispatcher monitors memory with a background task: . # From the _memory_monitor_task method (simplified) async def _memory_monitor_task(self): while True: # Check current memory usage self.current_memory_percent = psutil.virtual_memory().percent # Enter pressure mode if memory is high if not self.memory_pressure_mode and self.current_memory_percent &gt;= self.memory_threshold_percent: self.memory_pressure_mode = True # Exit pressure mode if memory is lower elif self.memory_pressure_mode and self.current_memory_percent &lt;= self.recovery_threshold_percent: self.memory_pressure_mode = False await asyncio.sleep(self.check_interval) . This continuously checks memory usage and adjusts the memory_pressure_mode flag, which affects how URLs are processed. ",
    "url": "/Crawl4AI/07_dispatcher_framework_.html#implementation-details-memoryadaptivedispatcher",
    
    "relUrl": "/Crawl4AI/07_dispatcher_framework_.html#implementation-details-memoryadaptivedispatcher"
  },"269": {
    "doc": "Chapter 7: Dispatcher Framework",
    "title": "Implementation Details: SemaphoreDispatcher",
    "content": "The SemaphoreDispatcher is simpler but still powerful: . # From crawl4ai/async_dispatcher.py (simplified) class SemaphoreDispatcher(BaseDispatcher): def __init__(self, semaphore_count=5, rate_limiter=None): super().__init__(rate_limiter) self.semaphore_count = semaphore_count async def crawl_url(self, url, config, task_id, semaphore): async with semaphore: # Crawl the URL with rate limiting if self.rate_limiter: await self.rate_limiter.wait_if_needed(url) result = await self.crawler.arun(url, config=config) return result . The SemaphoreDispatcher uses an asyncio Semaphore to limit concurrent tasks. When a task is done, it releases its slot, allowing another task to start. ",
    "url": "/Crawl4AI/07_dispatcher_framework_.html#implementation-details-semaphoredispatcher",
    
    "relUrl": "/Crawl4AI/07_dispatcher_framework_.html#implementation-details-semaphoredispatcher"
  },"270": {
    "doc": "Chapter 7: Dispatcher Framework",
    "title": "Advanced Usage: Monitoring Crawl Progress",
    "content": "You can monitor the progress of your crawls using the CrawlerMonitor: . from crawl4ai import AsyncWebCrawler, MemoryAdaptiveDispatcher, CrawlerMonitor async def monitored_crawl(): # Create a monitor monitor = CrawlerMonitor() # Create a dispatcher with the monitor dispatcher = MemoryAdaptiveDispatcher(monitor=monitor) # Run your crawl async with AsyncWebCrawler() as crawler: results = await crawler.arun_many( urls=[\"https://example.com\", \"https://example.org\"], dispatcher=dispatcher ) # Get statistics about the crawl stats = monitor.get_statistics() print(f\"Completed: {stats['completed']} URLs\") print(f\"Failed: {stats['failed']} URLs\") print(f\"Average memory usage: {stats['avg_memory_usage']} MB\") . The monitor collects statistics about each URL, including success/failure, memory usage, and timing. ",
    "url": "/Crawl4AI/07_dispatcher_framework_.html#advanced-usage-monitoring-crawl-progress",
    
    "relUrl": "/Crawl4AI/07_dispatcher_framework_.html#advanced-usage-monitoring-crawl-progress"
  },"271": {
    "doc": "Chapter 7: Dispatcher Framework",
    "title": "Conclusion",
    "content": "The Dispatcher Framework is like having an expert traffic controller for your web crawling operations. It helps you crawl multiple websites simultaneously while being careful about system resources and polite to the websites you’re visiting. In this chapter, we’ve learned: . | How to use the SemaphoreDispatcher for simple concurrent crawling | How to use the MemoryAdaptiveDispatcher for smart resource management | How to enforce rate limits with the RateLimiter | How to process results in real-time with streaming | How the dispatch process works under the hood | . With the Dispatcher Framework, you can build efficient, robust, and polite web crawlers that make the most of your system’s resources without overwhelming them. In the next chapter, Async Logging Infrastructure, we’ll learn how to track what’s happening in our crawlers with detailed logs, which is especially important when dealing with complex crawling operations. Generated by AI Codebase Knowledge Builder . ",
    "url": "/Crawl4AI/07_dispatcher_framework_.html#conclusion",
    
    "relUrl": "/Crawl4AI/07_dispatcher_framework_.html#conclusion"
  },"272": {
    "doc": "Chapter 7: EngineQueue",
    "title": "Chapter 7: EngineQueue",
    "content": "Welcome back! In Chapter 6: KV Cache Management, we learned how modular intelligently manages memory for ongoing text generation using a “smart scratchpad” called the KV Cache. We’ve seen the LLM Pipeline Orchestrator (TokenGeneratorPipeline) prepare requests and the Model Worker (with its Scheduler (TokenGenerationScheduler, EmbeddingsScheduler)) do the heavy AI lifting. But how do these two major components, which actually run in different processes (like separate offices in a building), talk to each other? If the TokenGeneratorPipeline wants to send a new story prompt to the Model Worker, how does it do that? And how does the Model Worker send the generated story back? . This is where the EngineQueue steps in. It’s the primary communication backbone of modular. ",
    "url": "/modular_max/07_enginequeue_.html",
    
    "relUrl": "/modular_max/07_enginequeue_.html"
  },"273": {
    "doc": "Chapter 7: EngineQueue",
    "title": "What Problem Does EngineQueue Solve?",
    "content": "Imagine a large organization with a main office (our API server) and a specialized research lab (our Model Worker). The main office receives customer requests (like “analyze this sample”), and the lab has the special equipment and experts to do the analysis. | The main office can’t just shout across town to the lab. | The lab can’t just appear in the main office with the results. | They need a reliable way to send documents (data) back and forth. | . This is exactly the situation between our main API server process and the Model Worker process. They operate independently. The EngineQueue solves this by providing a robust and efficient inter-process communication (IPC) mechanism. Think of it as a sophisticated, multi-lane pneumatic tube system connecting the main office and the research lab. It allows: . | The API server (via the TokenGeneratorPipeline) to send new tasks (like “generate text for this prompt”) to the Model Worker. | The Model Worker to send results (the generated text) back to the API server. | A way to send cancellation signals if a task needs to be stopped early. | . Without EngineQueue, these two core parts of modular couldn’t cooperate! . ",
    "url": "/modular_max/07_enginequeue_.html#what-problem-does-enginequeue-solve",
    
    "relUrl": "/modular_max/07_enginequeue_.html#what-problem-does-enginequeue-solve"
  },"274": {
    "doc": "Chapter 7: EngineQueue",
    "title": "Meet the EngineQueue: The Pneumatic Tube System",
    "content": "The EngineQueue isn’t just one tube; it’s a set of specialized inter-process queues designed for high-performance message passing. Each queue is like a dedicated pneumatic tube for a specific type of “document”: . | Request Queue (request_q): Carries new tasks (e.g., tokenized prompts) from the TokenGeneratorPipeline (in the main API server process) to the Scheduler (in the Model Worker process). | Analogy: A tube labeled “NEW WORK ORDERS” going from the main office to the lab. | . | Response Queue (response_q): Carries completed results (e.g., generated tokens or embeddings) from the Scheduler back to the TokenGeneratorPipeline. | Analogy: A tube labeled “COMPLETED RESULTS” going from the lab back to the main office. | . | Cancellation Queue (cancel_q): Carries signals to tell the Scheduler to stop working on a specific task if it’s no longer needed. | Analogy: An express tube labeled “URGENT: STOP TASK XYZ” going to the lab. | . | . modular can use different technologies for these queues, primarily: . | Standard Python multiprocessing.Queue: A built-in Python way to share data between processes. Good and reliable. | ZMQ (ZeroMQ): A high-performance messaging library. Often faster, especially for more complex scenarios or larger data, but adds an external dependency. | . The EngineQueue abstraction allows modular to potentially switch between these underlying technologies based on configuration or needs. ",
    "url": "/modular_max/07_enginequeue_.html#meet-the-enginequeue-the-pneumatic-tube-system",
    
    "relUrl": "/modular_max/07_enginequeue_.html#meet-the-enginequeue-the-pneumatic-tube-system"
  },"275": {
    "doc": "Chapter 7: EngineQueue",
    "title": "How It Works: A Simplified Flow",
    "content": "Let’s trace how a request for text generation flows through the EngineQueue: . | API Server Gets a Request: A user asks modular to write a poem. The Serving API Layer (FastAPI App &amp; Routers) passes this to the TokenGeneratorPipeline. | Pipeline Prepares and Sends to EngineQueue: . | The TokenGeneratorPipeline tokenizes the poem prompt (“Write a poem about a cat”). | It now needs to send this to the Model Worker. | It calls a method on the EngineQueue (like stream()), which internally puts the tokenized prompt and a unique request ID onto the request queue (request_q). | . | Model Worker’s Scheduler Picks Up the Task: . | The Scheduler inside the Model Worker process is constantly watching the request queue (request_q). | It sees the new poem prompt, takes it off the queue, and prepares to feed it to the AI model. | . | Model Worker Generates Text: The AI model in the Model Worker starts generating the poem, token by token. | Scheduler Sends Results via EngineQueue: . | As each token (e.g., “The”, “fluffy”, “cat”) is generated, the Scheduler takes this result. | It puts the generated token (associated with the original request ID) onto the response queue (response_q). | . | Pipeline Receives Results: . | Back in the main API server process, the EngineQueue (specifically, a helper task it runs called response_worker) is watching the response queue (response_q). | It sees the incoming tokens for the poem, takes them off the queue, and passes them to the correct part of the TokenGeneratorPipeline that is waiting for them. | The TokenGeneratorPipeline then decodes these tokens back into text and sends the poem to the user. | . | What if the User Cancels?: . | If the user cancels the request while the poem is being generated, the TokenGeneratorPipeline can send a message with the request ID to the cancellation queue (cancel_q). | The Scheduler in the Model Worker checks this queue and, if it sees the cancellation, stops generating the poem. | . | . This system ensures that the main server and the model worker can work independently but stay perfectly synchronized. ",
    "url": "/modular_max/07_enginequeue_.html#how-it-works-a-simplified-flow",
    
    "relUrl": "/modular_max/07_enginequeue_.html#how-it-works-a-simplified-flow"
  },"276": {
    "doc": "Chapter 7: EngineQueue",
    "title": "Visualizing the Flow",
    "content": "Here’s how data moves for a typical request: . sequenceDiagram participant TGP as TokenGeneratorPipeline (Main Process) participant EngQ_Req as EngineQueue (request_q) participant Sched as Scheduler (Model Worker Process) participant AIModel as AI Model participant EngQ_Resp as EngineQueue (response_q) TGP-&gt;&gt;EngQ_Req: Put (Request ID, Tokenized Prompt) EngQ_Req--&gt;&gt;Sched: Get (Request ID, Tokenized Prompt) Sched-&gt;&gt;AIModel: Process Prompt AIModel--&gt;&gt;Sched: Generated Token(s) Sched-&gt;&gt;EngQ_Resp: Put (Request ID, Generated Token(s)) EngQ_Resp--&gt;&gt;TGP: Get (Request ID, Generated Token(s)) TGP-&gt;&gt;TGP: Decode tokens, send to user . ",
    "url": "/modular_max/07_enginequeue_.html#visualizing-the-flow",
    
    "relUrl": "/modular_max/07_enginequeue_.html#visualizing-the-flow"
  },"277": {
    "doc": "Chapter 7: EngineQueue",
    "title": "Under the Hood: Key Code Aspects",
    "content": "The main logic for EngineQueue is in src/max/serve/scheduler/queues.py. The EngineQueue Class . This class brings together the request, response, and cancellation queues. # Simplified from: src/max/serve/scheduler/queues.py class EngineQueue(Generic[ReqId, ReqInput, ReqOutput]): def __init__( self, context: multiprocessing.context.BaseContext, # For creating queues worker_pc: ProcessControl, # To monitor Model Worker health queue_type: QueueType = QueueType.ZMQ, # ZMQ or MP ): self.request_q = create_queue(queue_type, context) self.response_q = create_queue(queue_type, context) self.cancel_q = create_queue(queue_type, context) # For tracking responses to specific requests self.pending_out_queues: dict[ReqId, asyncio.Queue] = {} self.worker_pc = worker_pc # ProcessControl for Model Worker # ... | context: A multiprocessing context, used by create_queue to make either ZMQ or standard multiprocessing.Queue instances. | worker_pc: A ProcessControl object. EngineQueue uses this to check if the Model Worker process is still healthy. | pending_out_queues: This is a clever part! When TokenGeneratorPipeline sends a request, EngineQueue creates a temporary, in-memory asyncio.Queue just for that request’s responses. This dictionary maps the request ID to its dedicated response queue. | . Sending a Request: The stream Method . When the TokenGeneratorPipeline wants to send a request and get a stream of responses, it uses engine_queue.stream(). # Simplified from: src/max/serve/scheduler/queues.py @contextlib.contextmanager def open_channel( self, req_id: ReqId, data: ReqInput ) -&gt; Generator[asyncio.Queue, None, None]: try: # Create a temporary asyncio.Queue for this request's responses out_queue: asyncio.Queue = asyncio.Queue() self.pending_out_queues[req_id] = out_queue # Put the actual request onto the inter-process request_q self.request_q.put_nowait((req_id, data)) yield out_queue # The pipeline will listen to this out_queue finally: # Clean up when done del self.pending_out_queues[req_id] async def stream( self, req_id: ReqId, data: ReqInput ) -&gt; AsyncGenerator[ReqOutput, None]: # open_channel sets up the temporary queue and sends to request_q with self.open_channel(req_id, data) as queue_for_this_req: # Wait for items on the temporary queue while (item := await queue_for_this_req.get()) is not STOP_STREAM: yield item # Yield each response item . | open_channel is a context manager. It: . | Creates a standard asyncio.Queue (just for this one request, req_id). | Stores this queue in self.pending_out_queues using req_id as the key. | Puts the (req_id, data) tuple onto the actual self.request_q (which is an inter-process queue like ZMQ or mp.Queue). | yield out_queue: The stream method gets this temporary out_queue. | . | The stream method then asynchronously waits (await queue_for_this_req.get()) for responses to appear on this temporary out_queue and yields them one by one. STOP_STREAM is a special signal that means the response is finished. | . Receiving Responses: The response_worker Method . So, who puts items onto that temporary out_queue? A helper method called response_worker does this. It runs as an asynchronous task within the main API server process (started by the TokenGeneratorPipeline when it initializes). # Simplified from: src/max/serve/scheduler/queues.py async def response_worker(self): try: while True: # Loop forever # Check if the Model Worker process is still healthy if not self.is_worker_healthy(): # Handle error, maybe stop logger.error(\"Model worker process is not healthy\") raise Exception(\"Worker failed!\") try: # Get a batch of responses from the inter-process response_q # responses_batch is like: [{req_id1: token_A, req_id2: token_B}, ...] responses_batch = self.response_q.get_nowait() for responses_in_item in responses_batch: for req_id, response_data in responses_in_item.items(): # Find the temporary asyncio.Queue for this req_id if req_id in self.pending_out_queues: # Put the response data onto that specific queue await self.pending_out_queues[req_id].put(response_data) else: # Request might have been cancelled or finished # self.cancel_q.put_nowait([req_id]) # Example pass except queue.Empty: # queue is the standard Python queue module await asyncio.sleep(0) # brief pause if response_q is empty finally: logger.debug(\"Response worker stopping.\") . The response_worker: . | Continuously checks the main inter-process self.response_q for incoming messages from the Model Worker. | When it gets a response, it looks up the req_id in self.pending_out_queues to find the specific asyncio.Queue created for that request. | It then puts the response_data onto that asyncio.Queue. | This wakes up the stream() method (which was awaiting on that asyncio.Queue), allowing it to yield the response to the TokenGeneratorPipeline. It also includes important health checks for the Model Worker using self.is_worker_healthy(). | . The Actual Queue Implementations: MaxQueue, MpQueue, ZmqQueue . EngineQueue uses an abstraction called MaxQueue (src/max/serve/scheduler/max_queue.py) which defines a common interface (put_nowait, get_nowait, qsize, empty). Then, modular provides concrete implementations: . | MpQueue (src/max/serve/scheduler/mp_queue.py): This is a wrapper around Python’s standard multiprocessing.Queue. # Simplified from: src/max/serve/scheduler/mp_queue.py from multiprocessing import context, queues import queue # Standard Python queue module for exceptions class MpQueue(MaxQueue): def __init__(self, ctx: context.BaseContext, ...): self.queue: queues.Queue = ctx.Queue(...) # Creates a multiprocessing.Queue # self.counter = AtomicInt(ctx, 0) # For qsize on macOS ... def get_nowait(self) -&gt; Any: try: # x = self.queue.get(block=False) # Attempt to get immediately # self.counter.dec() # return x return self.queue.get(block=True, timeout=0.005) # Small timeout except queue.Empty: # Catches standard queue.Empty raise queue.Empty() def put_nowait(self, item: Any) -&gt; None: # self.queue.put(item, block=False) # Attempt to put immediately # self.counter.inc() self.queue.put(item, block=False) . It uses the standard ctx.Queue() to create the underlying inter-process queue. | ZmqQueue (src/max/serve/scheduler/zmq_queue.py): This uses ZeroMQ sockets for potentially higher performance. # Simplified from: src/max/serve/scheduler/zmq_queue.py import zmq # ZeroMQ library import queue # Standard Python queue module for exceptions class ZmqQueue(MaxQueue): def __init__(self, ...): self.zmq_ipc_path = _generate_zmq_ipc_path() # e.g., \"ipc:///tmp/some-uuid\" self.zmq_pull_socket: Optional[zmq.Socket] = None # For receiving self.zmq_push_socket: Optional[zmq.Socket] = None # For sending # ... (counter for qsize) def _get_or_init_pull_socket(self) -&gt; zmq.Socket: # Creates/returns a ZMQ PULL socket connected to zmq_ipc_path if self.zmq_pull_socket is None: # zmq_ctx = self._get_or_init_zmq_ctx() # Gets a ZMQ context # self.zmq_pull_socket = _open_zmq_socket(zmq_ctx, self.zmq_ipc_path, zmq.PULL) pass # Simplified return self.zmq_pull_socket def _get_or_init_push_socket(self) -&gt; zmq.Socket: # Creates/returns a ZMQ PUSH socket bound to zmq_ipc_path if self.zmq_push_socket is None: pass # Simplified return self.zmq_push_socket def get_nowait(self) -&gt; Any: # pull_socket = self._get_or_init_pull_socket() try: # return pull_socket.recv_pyobj(flags=zmq.NOBLOCK) # Receive Python object pass # Simplified - actual call to ZMQ except zmq.ZMQError as e: if e.errno == zmq.EAGAIN: # Means no message waiting raise queue.Empty() raise # Other ZMQ error def put_nowait(self, item: Any) -&gt; None: # push_socket = self._get_or_init_push_socket() try: # push_socket.send_pyobj(item, flags=zmq.NOBLOCK) # Send Python object pass # Simplified - actual call to ZMQ except zmq.ZMQError as e: if e.errno == zmq.EAGAIN: # Means send buffer is full (or receiver not ready) # ZmqQueue might retry or raise queue.Full() here pass raise # Other ZMQ error . ZMQ uses a “socket” paradigm. One end creates a PUSH socket and sends messages. The other end creates a PULL socket and receives them. _generate_zmq_ipc_path() creates a unique file system path that ZMQ uses for local inter-process communication. | . The choice between QueueType.ZMQ and QueueType.MP is typically determined by settings loaded from the Settings (Settings class) when modular starts. ",
    "url": "/modular_max/07_enginequeue_.html#under-the-hood-key-code-aspects",
    
    "relUrl": "/modular_max/07_enginequeue_.html#under-the-hood-key-code-aspects"
  },"278": {
    "doc": "Chapter 7: EngineQueue",
    "title": "Why Different Queue Types?",
    "content": ". | multiprocessing.Queue (MP): . | Pros: Built into Python, easy to use, no external dependencies. Generally reliable for many use cases. | Cons: Can sometimes have performance limitations with very high throughput or very large messages due to its underlying implementation (often uses pipes and pickling). | . | ZeroMQ (ZMQ): . | Pros: Designed for high-performance messaging. Can handle large volumes of data and high message rates more efficiently. Offers more advanced messaging patterns (though modular uses a simple PUSH/PULL here). | Cons: Adds an external dependency (the pyzmq library and its underlying C library). Can be slightly more complex to set up and debug. | . | . modular provides both options to allow users to choose based on their specific performance needs and deployment environment. For many typical scenarios, multiprocessing.Queue is sufficient. For demanding, high-throughput applications, ZMQ might offer an edge. ",
    "url": "/modular_max/07_enginequeue_.html#why-different-queue-types",
    
    "relUrl": "/modular_max/07_enginequeue_.html#why-different-queue-types"
  },"279": {
    "doc": "Chapter 7: EngineQueue",
    "title": "Conclusion",
    "content": "The EngineQueue is the vital communication backbone that enables the main API server process and the separate Model Worker process to work together seamlessly. It acts like a sophisticated pneumatic tube system, with dedicated queues for requests, responses, and cancellation signals. By abstracting the underlying queue technology (be it standard multiprocessing queues or high-performance ZMQ), EngineQueue ensures reliable and efficient message passing. This separation allows the API server to remain responsive while the Model Worker crunches through demanding AI tasks. Now that we’ve seen how different parts of modular communicate, how do we keep an eye on what’s happening inside? How do we measure performance and gather diagnostic information? That’s where our next chapter comes in: Telemetry and Metrics (METRICS, MetricClient). Generated by AI Codebase Knowledge Builder . ",
    "url": "/modular_max/07_enginequeue_.html#conclusion",
    
    "relUrl": "/modular_max/07_enginequeue_.html#conclusion"
  },"280": {
    "doc": "Chapter 8: Async Logging Infrastructure",
    "title": "Chapter 8: Async Logging Infrastructure",
    "content": "In Chapter 7: Dispatcher Framework, we learned how to manage multiple crawling operations efficiently. Now, let’s explore how to keep track of what’s happening during these operations with the Async Logging Infrastructure. ",
    "url": "/Crawl4AI/08_async_logging_infrastructure_.html",
    
    "relUrl": "/Crawl4AI/08_async_logging_infrastructure_.html"
  },"281": {
    "doc": "Chapter 8: Async Logging Infrastructure",
    "title": "What is the Async Logging Infrastructure?",
    "content": "Imagine you’re exploring a vast, dark cave system. Would you venture in without a flashlight and a way to leave breadcrumbs? Probably not! Similarly, when your crawler is exploring the web, you need a way to see what it’s doing and track its progress. The Async Logging Infrastructure is like your flashlight and breadcrumb system for web crawling: . | It shines a light on what’s happening in real-time | It leaves a trail of where you’ve been and what happened | It alerts you when something goes wrong | It helps you understand why your crawler is behaving a certain way | . Let’s see a simple example of how this works: . from crawl4ai import AsyncWebCrawler, AsyncLogger # Create a custom logger with colored output logger = AsyncLogger(verbose=True) # Use it with our crawler async with AsyncWebCrawler(logger=logger) as crawler: result = await crawler.arun(url=\"https://example.com\") # The logger will automatically show: # - When the crawler starts # - When a page is fetched # - When content is processed # - Any errors that occur # - When the operation completes . When you run this code, you’ll see colorful, informative messages in your console showing exactly what’s happening during the crawl! . ",
    "url": "/Crawl4AI/08_async_logging_infrastructure_.html#what-is-the-async-logging-infrastructure",
    
    "relUrl": "/Crawl4AI/08_async_logging_infrastructure_.html#what-is-the-async-logging-infrastructure"
  },"282": {
    "doc": "Chapter 8: Async Logging Infrastructure",
    "title": "Understanding Log Levels",
    "content": "Just like how a news reporter might classify stories as “breaking news,” “feature,” or “update,” the Async Logging Infrastructure uses different log levels to categorize messages: . from crawl4ai.async_logger import LogLevel # Different log levels from least to most severe debug_level = LogLevel.DEBUG # Detailed information for debugging info_level = LogLevel.INFO # General information about progress success_level = LogLevel.SUCCESS # Successful operations warning_level = LogLevel.WARNING # Something might be wrong error_level = LogLevel.ERROR # Something definitely went wrong . Each level is displayed in a different color to help you quickly identify what’s happening: . | DEBUG is light black (for detailed but non-essential information) | INFO is cyan (for regular progress updates) | SUCCESS is green (for successful operations) | WARNING is yellow (for potential issues) | ERROR is red (for definite problems) | . ",
    "url": "/Crawl4AI/08_async_logging_infrastructure_.html#understanding-log-levels",
    
    "relUrl": "/Crawl4AI/08_async_logging_infrastructure_.html#understanding-log-levels"
  },"283": {
    "doc": "Chapter 8: Async Logging Infrastructure",
    "title": "Basic Logging: Your First Log Messages",
    "content": "Let’s start with the basics - how to use the logger to display information: . from crawl4ai import AsyncLogger # Create a logger logger = AsyncLogger() # Log messages at different levels logger.debug(\"Detailed information for troubleshooting\") logger.info(\"The crawler is starting\") logger.success(\"Successfully crawled the page\") logger.warning(\"The page took a long time to load\") logger.error(\"Failed to access the website\") . Each of these methods displays a message with appropriate formatting and coloring. You’ll see something like: . [DEBUG].... ⋯ Detailed information for troubleshooting [INFO]..... ℹ The crawler is starting [SUCCESS].. ✔ Successfully crawled the page [WARNING].. ⚠ The page took a long time to load [ERROR].... × Failed to access the website . ",
    "url": "/Crawl4AI/08_async_logging_infrastructure_.html#basic-logging-your-first-log-messages",
    
    "relUrl": "/Crawl4AI/08_async_logging_infrastructure_.html#basic-logging-your-first-log-messages"
  },"284": {
    "doc": "Chapter 8: Async Logging Infrastructure",
    "title": "Tracking URL Status",
    "content": "When crawling, you often want to know which URLs were successfully processed. The logger has a special method for this: . # Log the status of a URL fetch logger.url_status( url=\"https://example.com\", success=True, timing=1.25, # seconds tag=\"FETCH\" ) . This produces a nicely formatted output like: . [FETCH].... ↓ https://example.com | ✓ | ⏱: 1.25s . If a URL fails, you can log the error: . # Log an error for a URL logger.error_status( url=\"https://example.com\", error=\"Connection refused\", tag=\"ERROR\" ) . This shows: . [ERROR].... × https://example.com | Error: Connection refused . ",
    "url": "/Crawl4AI/08_async_logging_infrastructure_.html#tracking-url-status",
    
    "relUrl": "/Crawl4AI/08_async_logging_infrastructure_.html#tracking-url-status"
  },"285": {
    "doc": "Chapter 8: Async Logging Infrastructure",
    "title": "Customizing Your Logger",
    "content": "Like customizing your news feed, you can customize how the logger displays information: . # Create a customized logger logger = AsyncLogger( log_file=\"crawl_logs.txt\", # Save logs to a file log_level=LogLevel.INFO, # Only show INFO and above tag_width=12, # Width for the tag column verbose=True # Show output in console ) . This creates a logger that: . | Saves all logs to a file called “crawl_logs.txt” | Only displays messages at INFO level or higher (INFO, SUCCESS, WARNING, ERROR) | Uses a 12-character width for the tag column | Shows all output in the console | . ",
    "url": "/Crawl4AI/08_async_logging_infrastructure_.html#customizing-your-logger",
    
    "relUrl": "/Crawl4AI/08_async_logging_infrastructure_.html#customizing-your-logger"
  },"286": {
    "doc": "Chapter 8: Async Logging Infrastructure",
    "title": "Tracking a Complete Crawl Operation",
    "content": "Now let’s see a complete example of logging during a crawl operation: . from crawl4ai import AsyncWebCrawler, AsyncLogger async def log_complete_crawl(): # Create a logger that saves to a file logger = AsyncLogger(log_file=\"crawl_log.txt\") # Create a crawler with our logger async with AsyncWebCrawler(logger=logger) as crawler: # Log before we start logger.info(\"Starting crawl of example.com\", tag=\"CRAWL\") # Run the crawler result = await crawler.arun(url=\"https://example.com\") # Log completion if result.success: logger.success( \"Crawl complete with {words} words\", tag=\"COMPLETE\", params={\"words\": len(result.markdown.split())} ) else: logger.error( \"Crawl failed: {error}\", tag=\"COMPLETE\", params={\"error\": result.error_message} ) . The logger is used at each step of the process, giving you a complete picture of what happened. ",
    "url": "/Crawl4AI/08_async_logging_infrastructure_.html#tracking-a-complete-crawl-operation",
    
    "relUrl": "/Crawl4AI/08_async_logging_infrastructure_.html#tracking-a-complete-crawl-operation"
  },"287": {
    "doc": "Chapter 8: Async Logging Infrastructure",
    "title": "Structured Logging with Parameters",
    "content": "Sometimes you want to include dynamic data in your log messages. The AsyncLogger makes this easy with parameter substitution: . # Log with parameters logger.info( message=\"Found {link_count} links on {url}\", tag=\"LINKS\", params={ \"link_count\": len(result.links), \"url\": result.url }, colors={ \"link_count\": LogColor.GREEN, # Highlight the count in green \"url\": LogColor.CYAN # Show the URL in cyan } ) . This creates a message with colorized parts, making important information stand out! . ",
    "url": "/Crawl4AI/08_async_logging_infrastructure_.html#structured-logging-with-parameters",
    
    "relUrl": "/Crawl4AI/08_async_logging_infrastructure_.html#structured-logging-with-parameters"
  },"288": {
    "doc": "Chapter 8: Async Logging Infrastructure",
    "title": "How the Logger Works Under the Hood",
    "content": "When you use the AsyncLogger, here’s what happens behind the scenes: . sequenceDiagram participant YC as Your Code participant L as AsyncLogger participant C as Console participant F as Log File YC-&gt;&gt;L: logger.info(\"Starting crawl\") L-&gt;&gt;L: Format message with tag and icon L-&gt;&gt;L: Apply colors to message parts L-&gt;&gt;C: Print colored message to console L-&gt;&gt;F: Write plain text message to file YC-&gt;&gt;L: logger.url_status(url, success, timing) L-&gt;&gt;L: Format URL status with colors L-&gt;&gt;C: Print URL status to console L-&gt;&gt;F: Write URL status to file . The logger takes your message, formats it with tags, icons, and colors, then outputs it to the console and/or a file. It’s like a news editor taking your story, adding headlines and formatting, then publishing it to different channels. ",
    "url": "/Crawl4AI/08_async_logging_infrastructure_.html#how-the-logger-works-under-the-hood",
    
    "relUrl": "/Crawl4AI/08_async_logging_infrastructure_.html#how-the-logger-works-under-the-hood"
  },"289": {
    "doc": "Chapter 8: Async Logging Infrastructure",
    "title": "Understanding the Implementation",
    "content": "Let’s look at how the AsyncLogger is implemented: . # From crawl4ai/async_logger.py (simplified) class AsyncLogger(AsyncLoggerBase): def __init__( self, log_file=None, log_level=LogLevel.DEBUG, tag_width=10, verbose=True, ): self.log_file = log_file self.log_level = log_level self.tag_width = tag_width self.verbose = verbose self.console = Console() # Rich console for colored output . The constructor sets up basic properties like where to log, what level to log at, and whether to show output in the console. The main logging method handles formatting and output: . # Core logging method (simplified) def _log(self, level, message, tag, params=None, colors=None): # Skip if level is below our threshold if level.value &lt; self.log_level.value: return # Format the message with parameters if provided if params: formatted_message = message.format(**params) # Apply colors to parameters if specified # ... else: formatted_message = message # Format the complete log line with tag and icon color = self.colors[level] log_line = f\"[{color}]{self._format_tag(tag)} {self._get_icon(tag)} {formatted_message}[/{color}]\" # Output to console if verbose if self.verbose: self.console.print(log_line) # Write to file if configured if self.log_file: self._write_to_file(log_line) . This method: . | Checks if the log level is high enough to display | Formats the message with any parameters | Applies colors to specific parts if requested | Adds the tag and icon to the message | Outputs to the console and/or file | . The specific logging methods like info() and error() are just convenient wrappers around this core method: . def info(self, message, tag=\"INFO\", **kwargs): \"\"\"Log an info message.\"\"\" self._log(LogLevel.INFO, message, tag, **kwargs) def error(self, message, tag=\"ERROR\", **kwargs): \"\"\"Log an error message.\"\"\" self._log(LogLevel.ERROR, message, tag, **kwargs) . ",
    "url": "/Crawl4AI/08_async_logging_infrastructure_.html#understanding-the-implementation",
    
    "relUrl": "/Crawl4AI/08_async_logging_infrastructure_.html#understanding-the-implementation"
  },"290": {
    "doc": "Chapter 8: Async Logging Infrastructure",
    "title": "Using File Logging for Long-Running Crawls",
    "content": "For long-running crawls, you might want to save logs to a file for later analysis: . from crawl4ai import AsyncLogger # Create a logger that saves to a file logger = AsyncLogger( log_file=\"crawls/my_project/crawler_log.txt\", verbose=True # Show in console AND save to file ) # Use the logger with your crawler # ... # Later, you can examine the log file to see what happened . The log file contains all the messages in plain text format with timestamps: . [2023-04-15 14:32:45.123] [INFO] [INIT] Crawl4AI 1.0.0 [2023-04-15 14:32:46.456] [INFO] [FETCH] https://example.com | ✓ | ⏱: 0.85s [2023-04-15 14:32:47.789] [SUCCESS] [COMPLETE] https://example.com | ✓ | ⏱: 2.25s . ",
    "url": "/Crawl4AI/08_async_logging_infrastructure_.html#using-file-logging-for-long-running-crawls",
    
    "relUrl": "/Crawl4AI/08_async_logging_infrastructure_.html#using-file-logging-for-long-running-crawls"
  },"291": {
    "doc": "Chapter 8: Async Logging Infrastructure",
    "title": "Customizing Log Format with Colors and Icons",
    "content": "You can customize the icons and colors used by the logger: . from crawl4ai import AsyncLogger from crawl4ai.async_logger import LogLevel, LogColor # Custom icons for different tags custom_icons = { \"CRAWL\": \"🕸️\", \"DATA\": \"📊\", \"FETCH\": \"📥\" } # Custom colors for different log levels custom_colors = { LogLevel.INFO: LogColor.CYAN, LogLevel.SUCCESS: LogColor.GREEN, LogLevel.ERROR: LogColor.MAGENTA # Use magenta for errors } # Create logger with custom formatting logger = AsyncLogger( icons=custom_icons, colors=custom_colors ) . This gives you complete control over how your logs look, making them more informative and easier to understand at a glance. ",
    "url": "/Crawl4AI/08_async_logging_infrastructure_.html#customizing-log-format-with-colors-and-icons",
    
    "relUrl": "/Crawl4AI/08_async_logging_infrastructure_.html#customizing-log-format-with-colors-and-icons"
  },"292": {
    "doc": "Chapter 8: Async Logging Infrastructure",
    "title": "Integrating with Your Crawler",
    "content": "When creating your crawler, you can pass your custom logger: . from crawl4ai import AsyncWebCrawler, AsyncLogger # Create a custom logger logger = AsyncLogger(log_file=\"crawl_logs.txt\") # Pass it to the crawler async with AsyncWebCrawler(logger=logger) as crawler: # The crawler will now use your logger for all messages result = await crawler.arun(url=\"https://example.com\") . This ensures all crawler operations use your logger configuration, giving you consistent logging throughout your application. ",
    "url": "/Crawl4AI/08_async_logging_infrastructure_.html#integrating-with-your-crawler",
    
    "relUrl": "/Crawl4AI/08_async_logging_infrastructure_.html#integrating-with-your-crawler"
  },"293": {
    "doc": "Chapter 8: Async Logging Infrastructure",
    "title": "Real-World Example: Monitoring a Deep Crawl",
    "content": "Let’s put everything together in a real-world example - monitoring a deep crawl of a website: . from crawl4ai import AsyncWebCrawler, AsyncLogger, BFSDeepCrawlStrategy from crawl4ai.async_logger import LogLevel async def monitor_deep_crawl(): # Create a detailed logger logger = AsyncLogger( log_file=\"deep_crawl.log\", log_level=LogLevel.DEBUG, # Capture everything verbose=True ) # Set up a crawl strategy strategy = BFSDeepCrawlStrategy(max_depth=2, max_pages=10) # Create a crawler with our logger async with AsyncWebCrawler(logger=logger) as crawler: logger.info(\"Starting deep crawl of example.com\", tag=\"DEEP\") # Run the deep crawl results = await crawler.arun( url=\"https://example.com\", config=strategy ) # Log completion logger.success( \"Deep crawl complete. Visited {page_count} pages\", tag=\"DEEP\", params={\"page_count\": len(results)} ) . This gives you a complete log of the deep crawl operation, showing each page visited, any errors encountered, and the final results. ",
    "url": "/Crawl4AI/08_async_logging_infrastructure_.html#real-world-example-monitoring-a-deep-crawl",
    
    "relUrl": "/Crawl4AI/08_async_logging_infrastructure_.html#real-world-example-monitoring-a-deep-crawl"
  },"294": {
    "doc": "Chapter 8: Async Logging Infrastructure",
    "title": "Conclusion",
    "content": "The Async Logging Infrastructure is your window into what your crawler is doing. Like a journalist documenting an expedition, it provides real-time updates, helps you track progress, and alerts you to any issues. In this chapter, we’ve learned: . | How to create and customize a logger | How to log messages at different levels | How to track URL status and errors | How to use structured logging with parameters | How to save logs to files for later analysis | How the logger works internally | . With good logging, you can understand what your crawler is doing, diagnose problems quickly, and ensure your crawling operations are running smoothly. In the next chapter, API &amp; Docker Integration, we’ll learn how to expose your crawler as an API and run it in containers, making it easier to deploy and integrate with other systems. Generated by AI Codebase Knowledge Builder . ",
    "url": "/Crawl4AI/08_async_logging_infrastructure_.html#conclusion",
    
    "relUrl": "/Crawl4AI/08_async_logging_infrastructure_.html#conclusion"
  },"295": {
    "doc": "DeepCrawlStrategy",
    "title": "Chapter 8: Exploring Websites - DeepCrawlStrategy",
    "content": "In Chapter 7: Understanding the Results - CrawlResult, we saw the final report (CrawlResult) that Crawl4AI gives us after processing a single URL. This report contains cleaned content, links, metadata, and maybe even extracted data. But what if you want to explore a website beyond just the first page? Imagine you land on a blog’s homepage. You don’t just want the homepage content; you want to automatically discover and crawl all the individual blog posts linked from it. How can you tell Crawl4AI to act like an explorer, following links and venturing deeper into the website? . ",
    "url": "/Crawl4AI/08_deepcrawlstrategy.html#chapter-8-exploring-websites---deepcrawlstrategy",
    
    "relUrl": "/Crawl4AI/08_deepcrawlstrategy.html#chapter-8-exploring-websites---deepcrawlstrategy"
  },"296": {
    "doc": "DeepCrawlStrategy",
    "title": "What Problem Does DeepCrawlStrategy Solve?",
    "content": "Think of the AsyncWebCrawler.arun() method we’ve used so far like visiting just the entrance hall of a vast library. You get information about that specific hall, but you don’t automatically explore the adjoining rooms or different floors. What if you want to systematically explore the library? You need a plan: . | Do you explore room by room on the current floor before going upstairs? (Level by level) | Do you pick one wing and explore all its rooms down to the very end before exploring another wing? (Go deep first) | Do you have a map highlighting potentially interesting sections and prioritize visiting those first? (Prioritize promising paths) | . DeepCrawlStrategy provides this exploration plan. It defines the logic for how Crawl4AI should discover and crawl new URLs starting from the initial one(s) by following the links it finds on each page. It turns the crawler from a single-page visitor into a website explorer. ",
    "url": "/Crawl4AI/08_deepcrawlstrategy.html#what-problem-does-deepcrawlstrategy-solve",
    
    "relUrl": "/Crawl4AI/08_deepcrawlstrategy.html#what-problem-does-deepcrawlstrategy-solve"
  },"297": {
    "doc": "DeepCrawlStrategy",
    "title": "What is DeepCrawlStrategy?",
    "content": "DeepCrawlStrategy is a concept (a blueprint) in Crawl4AI that represents the method or logic used to navigate and crawl multiple pages by following links. It tells the crawler which links to follow and in what order to visit them. It essentially takes over the process when you call arun() if a deep crawl is requested, managing a queue or list of URLs to visit and coordinating the crawling of those URLs, potentially up to a certain depth or number of pages. ",
    "url": "/Crawl4AI/08_deepcrawlstrategy.html#what-is-deepcrawlstrategy",
    
    "relUrl": "/Crawl4AI/08_deepcrawlstrategy.html#what-is-deepcrawlstrategy"
  },"298": {
    "doc": "DeepCrawlStrategy",
    "title": "Different Exploration Plans: The Strategies",
    "content": "Crawl4AI provides several concrete exploration plans (implementations) for DeepCrawlStrategy: . | BFSDeepCrawlStrategy (Level-by-Level Explorer): . | Analogy: Like ripples spreading in a pond. | How it works: It first crawls the starting URL (Level 0). Then, it crawls all the valid links found on that page (Level 1). Then, it crawls all the valid links found on those pages (Level 2), and so on. It explores the website layer by layer. | Good for: Finding the shortest path to all reachable pages, getting a broad overview quickly near the start page. | . | DFSDeepCrawlStrategy (Deep Path Explorer): . | Analogy: Like exploring one specific corridor in a maze all the way to the end before backtracking and trying another corridor. | How it works: It starts at the initial URL, follows one link, then follows a link from that page, and continues going deeper down one path as far as possible (or until a specified depth limit). Only when it hits a dead end or the limit does it backtrack and try another path. | Good for: Exploring specific branches of a website thoroughly, potentially reaching deeper pages faster than BFS (if the target is down a specific path). | . | BestFirstCrawlingStrategy (Priority Explorer): . | Analogy: Like using a treasure map where some paths are marked as more promising than others. | How it works: This strategy uses a scoring system. It looks at all the discovered (but not yet visited) links and assigns a score to each one based on how “promising” it seems (e.g., does the URL contain relevant keywords? Is it from a trusted domain?). It then crawls the link with the best score first, regardless of its depth. | Good for: Focusing the crawl on the most relevant or important pages first, especially useful when you can’t crawl the entire site and need to prioritize. | . | . Guiding the Explorer: Filters and Scorers . Deep crawl strategies often work together with: . | Filters: Rules that decide if a discovered link should even be considered for crawling. Examples: . | DomainFilter: Only follow links within the starting website’s domain. | URLPatternFilter: Only follow links matching a specific pattern (e.g., /blog/posts/...). | ContentTypeFilter: Avoid following links to non-HTML content like PDFs or images. | . | Scorers: (Used mainly by BestFirstCrawlingStrategy) Rules that assign a score to a potential link to help prioritize it. Examples: . | KeywordRelevanceScorer: Scores links higher if the URL contains certain keywords. | PathDepthScorer: Might score links differently based on how deep they are. | . | . These act like instructions for the explorer: “Only explore rooms on this floor (filter),” “Ignore corridors marked ‘Staff Only’ (filter),” or “Check rooms marked with a star first (scorer).” . ",
    "url": "/Crawl4AI/08_deepcrawlstrategy.html#different-exploration-plans-the-strategies",
    
    "relUrl": "/Crawl4AI/08_deepcrawlstrategy.html#different-exploration-plans-the-strategies"
  },"299": {
    "doc": "DeepCrawlStrategy",
    "title": "How to Use a DeepCrawlStrategy",
    "content": "You enable deep crawling by adding a DeepCrawlStrategy instance to your CrawlerRunConfig. Let’s try exploring a website layer by layer using BFSDeepCrawlStrategy, going only one level deep from the start page. # chapter8_example_1.py import asyncio from crawl4ai import ( AsyncWebCrawler, CrawlerRunConfig, BFSDeepCrawlStrategy, # 1. Import the desired strategy DomainFilter # Import a filter to stay on the same site ) async def main(): # 2. Create an instance of the strategy # - max_depth=1: Crawl start URL (depth 0) + links found (depth 1) # - filter_chain: Use DomainFilter to only follow links on the same website bfs_explorer = BFSDeepCrawlStrategy( max_depth=1, filter_chain=[DomainFilter()] # Stay within the initial domain ) print(f\"Strategy: BFS, Max Depth: {bfs_explorer.max_depth}\") # 3. Create CrawlerRunConfig and set the deep_crawl_strategy # Also set stream=True to get results as they come in. run_config = CrawlerRunConfig( deep_crawl_strategy=bfs_explorer, stream=True # Get results one by one using async for ) # 4. Run the crawl - arun now handles the deep crawl! async with AsyncWebCrawler() as crawler: start_url = \"https://httpbin.org/links/10/0\" # A page with 10 internal links print(f\"\\nStarting deep crawl from: {start_url}...\") crawl_results_generator = await crawler.arun(url=start_url, config=run_config) crawled_count = 0 # Iterate over the results as they are yielded async for result in crawl_results_generator: crawled_count += 1 status = \"✅\" if result.success else \"❌\" depth = result.metadata.get(\"depth\", \"N/A\") parent = result.metadata.get(\"parent_url\", \"Start\") url_short = result.url.split('/')[-1] # Show last part of URL print(f\" {status} Crawled: {url_short:&lt;6} (Depth: {depth})\") print(f\"\\nFinished deep crawl. Total pages processed: {crawled_count}\") # Expecting 1 (start URL) + 10 (links) = 11 results if __name__ == \"__main__\": asyncio.run(main()) . Explanation: . | Import: We import AsyncWebCrawler, CrawlerRunConfig, BFSDeepCrawlStrategy, and DomainFilter. | Instantiate Strategy: We create BFSDeepCrawlStrategy. | max_depth=1: We tell it to crawl the starting URL (depth 0) and any valid links it finds on that page (depth 1), but not to go any further. | filter_chain=[DomainFilter()]: We provide a list containing DomainFilter. This tells the strategy to only consider following links that point to the same domain as the start_url. Links to external sites will be ignored. | . | Configure Run: We create a CrawlerRunConfig and pass our bfs_explorer instance to the deep_crawl_strategy parameter. We also set stream=True so we can process results as soon as they are ready, rather than waiting for the entire crawl to finish. | Crawl: We call await crawler.arun(url=start_url, config=run_config). Because the config contains a deep_crawl_strategy, arun doesn’t just crawl the single start_url. Instead, it activates the deep crawl logic defined by BFSDeepCrawlStrategy. | Process Results: Since we used stream=True, the return value is an asynchronous generator. We use async for result in crawl_results_generator: to loop through the CrawlResult objects as they are produced by the deep crawl. For each result, we print its status and depth. | . You’ll see the output showing the crawl starting, then processing the initial page (links/10/0 at depth 0), followed by the 10 linked pages (e.g., 9, 8, … 0 at depth 1). ",
    "url": "/Crawl4AI/08_deepcrawlstrategy.html#how-to-use-a-deepcrawlstrategy",
    
    "relUrl": "/Crawl4AI/08_deepcrawlstrategy.html#how-to-use-a-deepcrawlstrategy"
  },"300": {
    "doc": "DeepCrawlStrategy",
    "title": "How It Works (Under the Hood)",
    "content": "How does simply putting a strategy in the config change arun’s behavior? It involves a bit of Python magic called a decorator. | Decorator: When you create an AsyncWebCrawler, its arun method is automatically wrapped by a DeepCrawlDecorator. | Check Config: When you call await crawler.arun(url=..., config=...), this decorator checks if config.deep_crawl_strategy is set. | Delegate or Run Original: . | If a strategy is set, the decorator doesn’t run the original single-page crawl logic. Instead, it calls the arun method of your chosen DeepCrawlStrategy instance (e.g., bfs_explorer.arun(...)), passing it the crawler itself, the start_url, and the config. | If no strategy is set, the decorator simply calls the original arun logic to crawl the single page. | . | Strategy Takes Over: The DeepCrawlStrategy’s arun method now manages the crawl. | It maintains a list or queue of URLs to visit (e.g., current_level in BFS, a stack in DFS, a priority queue in BestFirst). | It repeatedly takes batches of URLs from its list/queue. | For each batch, it calls crawler.arun_many(urls=batch_urls, config=batch_config) (with deep crawling disabled in batch_config to avoid infinite loops!). | As results come back from arun_many, the strategy processes them: . | It yields the CrawlResult if running in stream mode. | It extracts links using its link_discovery method. | link_discovery uses can_process_url (which applies filters) to validate links. | Valid new links are added to the list/queue for future crawling. | . | This continues until the list/queue is empty, the max depth/pages limit is reached, or it’s cancelled. | . | . sequenceDiagram participant User participant Decorator as DeepCrawlDecorator participant Strategy as DeepCrawlStrategy (e.g., BFS) participant AWC as AsyncWebCrawler User-&gt;&gt;Decorator: arun(start_url, config_with_strategy) Decorator-&gt;&gt;Strategy: arun(start_url, crawler=AWC, config) Note over Strategy: Initialize queue/level with start_url loop Until Queue Empty or Limits Reached Strategy-&gt;&gt;Strategy: Get next batch of URLs from queue Note over Strategy: Create batch_config (deep_crawl=None) Strategy-&gt;&gt;AWC: arun_many(batch_urls, config=batch_config) AWC--&gt;&gt;Strategy: batch_results (List/Stream of CrawlResult) loop For each result in batch_results Strategy-&gt;&gt;Strategy: Process result (yield if streaming) Strategy-&gt;&gt;Strategy: Discover links (apply filters) Strategy-&gt;&gt;Strategy: Add valid new links to queue end end Strategy--&gt;&gt;Decorator: Final result (List or Generator) Decorator--&gt;&gt;User: Final result . ",
    "url": "/Crawl4AI/08_deepcrawlstrategy.html#how-it-works-under-the-hood",
    
    "relUrl": "/Crawl4AI/08_deepcrawlstrategy.html#how-it-works-under-the-hood"
  },"301": {
    "doc": "DeepCrawlStrategy",
    "title": "Code Glimpse",
    "content": "Let’s peek at the simplified structure: . 1. The Decorator (deep_crawling/base_strategy.py) . # Simplified from deep_crawling/base_strategy.py from contextvars import ContextVar from functools import wraps # ... other imports class DeepCrawlDecorator: deep_crawl_active = ContextVar(\"deep_crawl_active\", default=False) def __init__(self, crawler: AsyncWebCrawler): self.crawler = crawler def __call__(self, original_arun): @wraps(original_arun) async def wrapped_arun(url: str, config: CrawlerRunConfig = None, **kwargs): # Is a strategy present AND not already inside a deep crawl? if config and config.deep_crawl_strategy and not self.deep_crawl_active.get(): # Mark that we are starting a deep crawl token = self.deep_crawl_active.set(True) try: # Call the STRATEGY's arun method instead of the original strategy_result = await config.deep_crawl_strategy.arun( crawler=self.crawler, start_url=url, config=config ) # Handle streaming if needed if config.stream: # Return an async generator that resets the context var on exit async def result_wrapper(): try: async for result in strategy_result: yield result finally: self.deep_crawl_active.reset(token) return result_wrapper() else: return strategy_result # Return the list of results directly finally: # Reset the context var if not streaming (or handled in wrapper) if not config.stream: self.deep_crawl_active.reset(token) else: # No strategy or already deep crawling, call the original single-page arun return await original_arun(url, config=config, **kwargs) return wrapped_arun . 2. The Strategy Blueprint (deep_crawling/base_strategy.py) . # Simplified from deep_crawling/base_strategy.py from abc import ABC, abstractmethod # ... other imports class DeepCrawlStrategy(ABC): @abstractmethod async def _arun_batch(self, start_url, crawler, config) -&gt; List[CrawlResult]: # Implementation for non-streaming mode pass @abstractmethod async def _arun_stream(self, start_url, crawler, config) -&gt; AsyncGenerator[CrawlResult, None]: # Implementation for streaming mode pass async def arun(self, start_url, crawler, config) -&gt; RunManyReturn: # Decides whether to call _arun_batch or _arun_stream if config.stream: return self._arun_stream(start_url, crawler, config) else: return await self._arun_batch(start_url, crawler, config) @abstractmethod async def can_process_url(self, url: str, depth: int) -&gt; bool: # Applies filters to decide if a URL is valid to crawl pass @abstractmethod async def link_discovery(self, result, source_url, current_depth, visited, next_level, depths): # Extracts, validates, and prepares links for the next step pass @abstractmethod async def shutdown(self): # Cleanup logic pass . 3. Example: BFS Implementation (deep_crawling/bfs_strategy.py) . # Simplified from deep_crawling/bfs_strategy.py # ... imports ... from .base_strategy import DeepCrawlStrategy # Import the base class class BFSDeepCrawlStrategy(DeepCrawlStrategy): def __init__(self, max_depth, filter_chain=None, url_scorer=None, ...): self.max_depth = max_depth self.filter_chain = filter_chain or FilterChain() # Use default if none self.url_scorer = url_scorer # ... other init ... self._pages_crawled = 0 async def can_process_url(self, url: str, depth: int) -&gt; bool: # ... (validation logic using self.filter_chain) ... is_valid = True # Placeholder if depth != 0 and not await self.filter_chain.apply(url): is_valid = False return is_valid async def link_discovery(self, result, source_url, current_depth, visited, next_level, depths): # ... (logic to get links from result.links) ... links = result.links.get(\"internal\", []) # Example: only internal for link_data in links: url = link_data.get(\"href\") if url and url not in visited: if await self.can_process_url(url, current_depth + 1): # Check scoring, max_pages limit etc. depths[url] = current_depth + 1 next_level.append((url, source_url)) # Add (url, parent) tuple async def _arun_batch(self, start_url, crawler, config) -&gt; List[CrawlResult]: visited = set() current_level = [(start_url, None)] # List of (url, parent_url) depths = {start_url: 0} all_results = [] while current_level: # While there are pages in the current level next_level = [] urls_in_level = [url for url, parent in current_level] visited.update(urls_in_level) # Create config for this batch (no deep crawl recursion) batch_config = config.clone(deep_crawl_strategy=None, stream=False) # Crawl all URLs in the current level batch_results = await crawler.arun_many(urls=urls_in_level, config=batch_config) for result in batch_results: # Add metadata (depth, parent) depth = depths.get(result.url, 0) result.metadata = result.metadata or {} result.metadata[\"depth\"] = depth # ... find parent ... all_results.append(result) # Discover links for the *next* level if result.success: await self.link_discovery(result, result.url, depth, visited, next_level, depths) current_level = next_level # Move to the next level return all_results async def _arun_stream(self, start_url, crawler, config) -&gt; AsyncGenerator[CrawlResult, None]: # Similar logic to _arun_batch, but uses 'yield result' # and processes results as they come from arun_many stream visited = set() current_level = [(start_url, None)] # List of (url, parent_url) depths = {start_url: 0} while current_level: next_level = [] urls_in_level = [url for url, parent in current_level] visited.update(urls_in_level) # Use stream=True for arun_many batch_config = config.clone(deep_crawl_strategy=None, stream=True) batch_results_gen = await crawler.arun_many(urls=urls_in_level, config=batch_config) async for result in batch_results_gen: # Add metadata depth = depths.get(result.url, 0) result.metadata = result.metadata or {} result.metadata[\"depth\"] = depth # ... find parent ... yield result # Yield result immediately # Discover links for the next level if result.success: await self.link_discovery(result, result.url, depth, visited, next_level, depths) current_level = next_level # ... shutdown method ... ",
    "url": "/Crawl4AI/08_deepcrawlstrategy.html#code-glimpse",
    
    "relUrl": "/Crawl4AI/08_deepcrawlstrategy.html#code-glimpse"
  },"302": {
    "doc": "DeepCrawlStrategy",
    "title": "Conclusion",
    "content": "You’ve learned about DeepCrawlStrategy, the component that turns Crawl4AI into a website explorer! . | It solves the problem of crawling beyond a single starting page by following links. | It defines the exploration plan: . | BFSDeepCrawlStrategy: Level by level. | DFSDeepCrawlStrategy: Deep paths first. | BestFirstCrawlingStrategy: Prioritized by score. | . | Filters and Scorers help guide the exploration. | You enable it by setting deep_crawl_strategy in the CrawlerRunConfig. | A decorator mechanism intercepts arun calls to activate the strategy. | The strategy manages the queue of URLs and uses crawler.arun_many to crawl them in batches. | . Deep crawling allows you to gather information from multiple related pages automatically. But how does Crawl4AI avoid re-fetching the same page over and over again, especially during these deeper crawls? The answer lies in caching. Next: Let’s explore how Crawl4AI smartly caches results with Chapter 9: Smart Fetching with Caching - CacheContext / CacheMode. Generated by AI Codebase Knowledge Builder . ",
    "url": "/Crawl4AI/08_deepcrawlstrategy.html#conclusion",
    
    "relUrl": "/Crawl4AI/08_deepcrawlstrategy.html#conclusion"
  },"303": {
    "doc": "DeepCrawlStrategy",
    "title": "DeepCrawlStrategy",
    "content": " ",
    "url": "/Crawl4AI/08_deepcrawlstrategy.html",
    
    "relUrl": "/Crawl4AI/08_deepcrawlstrategy.html"
  },"304": {
    "doc": "Chapter 8: Telemetry and Metrics",
    "title": "Chapter 8: Telemetry and Metrics (METRICS, MetricClient)",
    "content": "Welcome to the final chapter of our core concepts tour! In Chapter 7: EngineQueue, we explored how different parts of modular, even across separate processes, communicate with each other. Now, with all these components working together, how do we know what’s going on inside? Is the server running smoothly? Is it fast? Are there any bottlenecks? . This is where Telemetry and Metrics come into play. They are like the flight data recorder and cockpit instruments of an airplane, constantly logging vital statistics and displaying real-time performance indicators to the crew (the server operators). ",
    "url": "/modular_max/08_telemetry_and_metrics___metrics____metricclient___.html#chapter-8-telemetry-and-metrics-metrics-metricclient",
    
    "relUrl": "/modular_max/08_telemetry_and_metrics___metrics____metricclient___.html#chapter-8-telemetry-and-metrics-metrics-metricclient"
  },"305": {
    "doc": "Chapter 8: Telemetry and Metrics",
    "title": "What Problem Do Telemetry and Metrics Solve?",
    "content": "Imagine you’re running modular to serve a popular AI model. Users start reporting that it’s sometimes slow. How do you figure out why? . | Is the server getting too many requests? | Is the AI model itself taking a long time for certain prompts? | Is the KV Cache Management struggling? | Are prompts waiting too long in the EngineQueue? | . Without data, you’re just guessing. Telemetry and metrics provide this crucial data, giving you observability into the server’s operation. modular’s telemetry system collects and reports various metrics like: . | Request counts and error rates. | How long requests take to process (latency). | How fast tokens are being generated. | How well the KV cache is being utilized. | The number of requests waiting in queues. | . This information is vital for understanding performance, debugging issues, and making informed decisions about scaling or optimizing the service. ",
    "url": "/modular_max/08_telemetry_and_metrics___metrics____metricclient___.html#what-problem-do-telemetry-and-metrics-solve",
    
    "relUrl": "/modular_max/08_telemetry_and_metrics___metrics____metricclient___.html#what-problem-do-telemetry-and-metrics-solve"
  },"306": {
    "doc": "Chapter 8: Telemetry and Metrics",
    "title": "Key Concepts: Understanding the Instruments",
    "content": "Let’s look at the main components of modular’s telemetry system: . 1. Metrics: The Vital Signs . Metrics are specific, quantifiable measurements of the server’s behavior. For example: . | maxserve.request_count: A counter for how many HTTP requests have been received. | maxserve.request_time: A histogram (a way to track distribution of values) for how long requests take. | maxserve.itl: A histogram for “inter-token latency,” or how long it takes to generate each subsequent token. | maxserve.cache.num_used_blocks: A counter for how many KV cache blocks are currently in use. | . modular defines a set of these metrics (in src/max/serve/telemetry/metrics.py inside the SERVE_METRICS dictionary). 2. OpenTelemetry (OTEL): A Common Language for Metrics . Instead of inventing its own way to define and collect metrics, modular uses OpenTelemetry (OTEL). OTEL is an open standard for collecting telemetry data (metrics, logs, and traces). Using OTEL means modular’s metrics can be easily understood and processed by many different monitoring tools. 3. Exporters: Sending Metrics Out . Once metrics are collected, they need to be sent somewhere to be viewed and analyzed. modular can export these OTEL metrics to various systems. A common one is Prometheus. | Prometheus: A popular open-source monitoring system. modular can expose an HTTP endpoint (usually on a different port, e.g., 8001, configured by metrics_port in Settings (Settings class)) that Prometheus can “scrape” (read from) periodically to collect the latest metric values. | . 4. METRICS: The Easy Button for Recording . Inside the modular code, when something interesting happens that should be recorded (like a request finishing), developers don’t usually interact with OTEL directly. Instead, they use a global object called METRICS. # Simplified from: src/max/serve/telemetry/metrics.py # (This object is created and made globally available) # METRICS = _AsyncMetrics() # Example usage elsewhere in code: # METRICS.request_count(responseCode=200, urlPath=\"/v1/chat/completions\") # METRICS.request_time(value=55.3, urlPath=\"/v1/chat/completions\") # value in ms . This METRICS object (an instance of _AsyncMetrics) provides simple methods like request_count() or request_time() to record observations. 5. MetricClient: The Delivery Service . The METRICS object itself doesn’t do the raw recording. It delegates this to a MetricClient. The MetricClient is responsible for taking the metric observation and actually processing it according to the configured method. 6. Recording Methods: How Metrics are Processed . Sending metrics can take a tiny bit of time. To minimize any performance impact on the main server operations, modular supports different ways to record metrics, configured by the metric_recording setting (from Settings (Settings class)): . | MetricRecordingMethod.NOOP: “No Operation.” Metrics are not recorded at all. Useful for maximum performance if metrics aren’t needed. | MetricRecordingMethod.SYNC: “Synchronous.” Metrics are recorded immediately in the same thread. Simplest, but can have a small performance impact. | MetricRecordingMethod.ASYNCIO: Metrics are put into an asyncio queue and processed by a separate asyncio task in the same process. This offloads the recording work from the main request path. | MetricRecordingMethod.PROCESS: Metrics are sent to a completely separate dedicated process for recording. This offers the most isolation but is more complex. | . The default is usually ASYNCIO, offering a good balance. 7. Metric Levels: Controlling Detail . Some metrics are cheap to collect (e.g., incrementing a counter), while others might be more expensive or generate a lot of data. The metric_level setting (from Settings (Settings class)) controls how much detail is captured: . | MetricLevel.NONE: No metrics (similar to NOOP recording). | MetricLevel.BASIC: Only essential, low-impact metrics are recorded. | MetricLevel.DETAILED: More granular metrics are recorded, which might have a slight performance cost. | . This allows operators to choose the trade-off between observability detail and performance. ",
    "url": "/modular_max/08_telemetry_and_metrics___metrics____metricclient___.html#key-concepts-understanding-the-instruments",
    
    "relUrl": "/modular_max/08_telemetry_and_metrics___metrics____metricclient___.html#key-concepts-understanding-the-instruments"
  },"307": {
    "doc": "Chapter 8: Telemetry and Metrics",
    "title": "How modular Uses Telemetry",
    "content": "For the most part, as an end-user or even a developer working on a specific part of modular, you benefit from metrics automatically. Key components are already “instrumented” to record relevant data. For example: . | When the Serving API Layer (FastAPI App &amp; Routers) handles a request, it automatically records things like request count and processing time. | The LLM Pipeline Orchestrator (TokenGeneratorPipeline) records time-to-first-token and inter-token latency. | The Scheduler (TokenGenerationScheduler, EmbeddingsScheduler) records batch sizes and KV Cache Management statistics like cache hit rates. | . Operators would typically: . | Ensure disable_telemetry is False in their Settings (Settings class). | Set metric_recording and metric_level as desired. | Configure a Prometheus server (or similar tool) to scrape the /metrics endpoint on the metrics_port (e.g., http://&lt;modular_host&gt;:8001/metrics). | Use Prometheus’s query language (PromQL) and visualization tools (like Grafana) to create dashboards and alerts based on these metrics. | . For example, an operator might create a graph of maxserve.request_time to monitor API latency, or maxserve.num_requests_queued to see if the server is falling behind. ",
    "url": "/modular_max/08_telemetry_and_metrics___metrics____metricclient___.html#how-modular-uses-telemetry",
    
    "relUrl": "/modular_max/08_telemetry_and_metrics___metrics____metricclient___.html#how-modular-uses-telemetry"
  },"308": {
    "doc": "Chapter 8: Telemetry and Metrics",
    "title": "Under the Hood: The Journey of a Metric",
    "content": "Let’s trace what happens when a metric is recorded, for instance, when a request finishes: . sequenceDiagram participant AppCode as Application Code (e.g., API Router) participant METRICS as Global METRICS Object participant Client as Active MetricClient (Asyncio, Process, etc.) participant Recorder as Metric Recorder (e.g., Asyncio Task or Separate Process) participant OTELSDK as OpenTelemetry SDK participant Exporter as Prometheus Exporter AppCode-&gt;&gt;METRICS: METRICS.request_time(50.0, \"/v1/chat\") METRICS-&gt;&gt;Client: send_measurement(MaxMeasurement_object, MetricLevel.BASIC) alt MetricLevel.BASIC &lt;= configured_level Client-&gt;&gt;Recorder: Pass MaxMeasurement_object (e.g., put on a queue) Note over Recorder: Waits for processing slot Recorder-&gt;&gt;OTELSDK: MaxMeasurement_object.commit() OTELSDK-&gt;&gt;OTELSDK: Update internal metric representation OTELSDK--&gt;&gt;Exporter: Value is updated for Prometheus else MetricLevel.BASIC &gt; configured_level Client--&gt;&gt;METRICS: Metric dropped (level too high) end Note over Exporter: Later... MonitoringSystem-&gt;&gt;Exporter: HTTP GET /metrics (scrape) Exporter--&gt;&gt;MonitoringSystem: Current metric values . | Observation: Code in modular (e.g., an API endpoint handler) observes an event. It calls a method on the global METRICS object, like METRICS.request_time(value_ms, path). | METRICS Object (_AsyncMetrics): . | The METRICS object (an instance of _AsyncMetrics from src/max/serve/telemetry/metrics.py) receives this call. | It creates a MaxMeasurement dataclass instance. This object holds the metric name (e.g., “maxserve.request_time”), the observed value, any attributes (like the URL path), and the current timestamp. | It then calls self.client.send_measurement(measurement_object, metric_level_of_this_metric). The self.client is the currently active MetricClient. | . | MetricClient (send_measurement): . | The active MetricClient (e.g., AsyncioMetricClient, ProcessMetricClient, SyncClient, or NoopClient) gets the MaxMeasurement. | It first checks if the metric_level_of_this_metric (e.g., MetricLevel.BASIC) is less than or equal to the globally configured metric_level (from Settings (Settings class)). If not, the metric is dropped. | If the level is appropriate, the client handles the measurement based on its type: . | NoopClient: Does nothing. | SyncClient: Directly calls measurement_object.commit(). | AsyncioMetricClient: Puts the measurement_object onto an asyncio.Queue. A background asyncio task will later pick it up and call commit(). (See AsyncioMetricClient in src/max/serve/telemetry/asyncio_controller.py) | ProcessMetricClient: Sends the measurement_object (possibly batched) via a multiprocessing.Queue to a separate telemetry worker process. That process will then call commit(). (See ProcessMetricClient in src/max/serve/telemetry/process_controller.py) | . | . | MaxMeasurement.commit(): . | This method (defined in src/max/serve/telemetry/metrics.py) is where the metric observation finally interacts with the OpenTelemetry SDK. | It looks up the OTEL “instrument” (like a Counter or Histogram) corresponding to self.instrument_name from the SERVE_METRICS dictionary. | It creates an OTEL Measurement object with the value, timestamp, and attributes. | It then calls the OTEL instrument’s internal consumer to record this Measurement. | . | OpenTelemetry SDK &amp; Exporter: . | The OTEL SDK updates its internal representation of the metric. | If a Prometheus exporter is configured (via configure_metrics in src/max/serve/telemetry/common.py), this exporter makes the latest metric values available on an HTTP endpoint (e.g., /metrics on port 8001). | External systems like Prometheus can then scrape this endpoint. | . | . Key Code Snippets: . 1. Settings for Metrics (from src/max/serve/config.py) These settings from Chapter 1: Settings (Settings class) control telemetry behavior. # From: src/max/serve/config.py class MetricLevel(IntEnum): NONE = 0 BASIC = 10 DETAILED = 20 class MetricRecordingMethod(Enum): NOOP = \"NOOP\" SYNC = \"SYNC\" ASYNCIO = \"ASYNCIO\" PROCESS = \"PROCESS\" class Settings(BaseSettings): # ... other settings ... metrics_port: int = Field(default=8001, ...) disable_telemetry: bool = Field(default=False, ...) metric_recording: MetricRecordingMethod = Field(default=MetricRecordingMethod.ASYNCIO, ...) metric_level: MetricLevel = Field(default=MetricLevel.BASIC, ...) # ... This shows how metric levels and recording methods are defined as configurable options. 2. The Global METRICS Object (from src/max/serve/telemetry/metrics.py) This is the primary interface for code to record metrics. # Simplified from: src/max/serve/telemetry/metrics.py class _AsyncMetrics: def __init__(self): self.client: MetricClient = NoopClient() # Default, configured later def configure(self, client: MetricClient) -&gt; None: self.client = client def request_count(self, responseCode: int, urlPath: str) -&gt; None: self.client.send_measurement( MaxMeasurement( \"maxserve.request_count\", 1, {\"code\": f\"{responseCode:d}\", \"path\": urlPath} ), MetricLevel.BASIC, # This metric is considered 'BASIC' ) # ... other methods like request_time, ttft, etc... METRICS = _AsyncMetrics() # The global instance . Application code calls methods like METRICS.request_count(). This method creates a MaxMeasurement and passes it to the configured MetricClient. 3. MaxMeasurement Dataclass (from src/max/serve/telemetry/metrics.py) This holds the data for a single metric observation. # Simplified from: src/max/serve/telemetry/metrics.py @dataclass class MaxMeasurement: instrument_name: str value: Union[float, int] attributes: Optional[dict[str, str]] = None time_unix_nano: int = field(default_factory=time.time_ns) def commit(self): instrument = SERVE_METRICS[self.instrument_name] # Get OTEL instrument # ... (unwrap proxy instrument if necessary) ... # ... (create OTEL Measurement object `m`) ... # consumer = instrument._measurement_consumer # consumer.consume_measurement(m) # Record with OTEL logger.debug(f\"Committed measurement for {self.instrument_name}\") . The commit() method is the bridge to the OpenTelemetry SDK. 4. MetricClient and its Implementations (from src/max/serve/telemetry/metrics.py) This defines how measurements are handled. # Simplified from: src/max/serve/telemetry/metrics.py class MetricClient(abc.ABC): @abc.abstractmethod def send_measurement(self, metric: MaxMeasurement, level: MetricLevel) -&gt; None: pass # ... cross_process_factory omitted for brevity ... class NoopClient(MetricClient): def send_measurement(self, m: MaxMeasurement, level: MetricLevel) -&gt; None: pass # Does nothing class SyncClient(MetricClient): def __init__(self, settings: Settings): self.level = settings.metric_level # Configured detail level def send_measurement(self, m: MaxMeasurement, level: MetricLevel) -&gt; None: if level &gt; self.level: # Check if this metric's level is too detailed return m.commit() # Commit synchronously . The AsyncioMetricClient and ProcessMetricClient (found in asyncio_controller.py and process_controller.py respectively) would put m onto a queue for later processing of m.commit(). 5. Starting the Telemetry Consumer (from src/max/serve/pipelines/telemetry_worker.py) This function, usually called during server startup, selects and starts the correct MetricClient based on settings. # Simplified from: src/max/serve/pipelines/telemetry_worker.py @asynccontextmanager async def start_telemetry_consumer( settings: Settings, ) -&gt; AsyncGenerator[MetricClient, None]: method = settings.metric_recording if method == MetricRecordingMethod.NOOP: yield NoopClient() elif method == MetricRecordingMethod.SYNC: yield SyncClient(settings) elif method == MetricRecordingMethod.ASYNCIO: # async with start_asyncio_consumer(settings) as controller: # yield controller.Client(settings) pass # Simplified elif method == MetricRecordingMethod.PROCESS: # async with start_process_consumer(settings) as controller: # yield controller.Client(settings) pass # Simplified # ... The chosen client is then passed to METRICS.configure(chosen_client). 6. Configuring OTEL Exporters (from src/max/serve/telemetry/common.py) This function sets up OpenTelemetry to use the Prometheus exporter. # Simplified from: src/max/serve/telemetry/common.py def configure_metrics(settings: Settings): egress_enabled = not settings.disable_telemetry meterProviders: list[MetricReader] = [PrometheusMetricReader(True)] # For /metrics if egress_enabled: # Also add OTLPMetricExporter for remote telemetry system # meterProviders.append(PeriodicExportingMetricReader(OTLPMetricExporter(...))) pass set_meter_provider(MeterProvider(meterProviders, metrics_resource)) # ... (logging setup) ... This sets up the system so OTEL metrics can be scraped by Prometheus. If settings.disable_telemetry is false and an OTLP endpoint is configured, metrics might also be pushed to a remote Modular telemetry service. ",
    "url": "/modular_max/08_telemetry_and_metrics___metrics____metricclient___.html#under-the-hood-the-journey-of-a-metric",
    
    "relUrl": "/modular_max/08_telemetry_and_metrics___metrics____metricclient___.html#under-the-hood-the-journey-of-a-metric"
  },"309": {
    "doc": "Chapter 8: Telemetry and Metrics",
    "title": "Conclusion",
    "content": "The Telemetry and Metrics system (METRICS, MetricClient) is crucial for providing observability into modular’s operations. It uses the OpenTelemetry standard to collect vital statistics about request handling, model performance, and resource utilization. By exposing these metrics (often via Prometheus), operators can monitor the server’s health, diagnose issues, and optimize performance. With configurable recording methods and detail levels, modular balances the need for detailed insights with the desire for minimal performance overhead. This concludes our journey through the core abstractions of the modular serving application! You’ve seen how Settings (Settings class) configure the server, how the Serving API Layer (FastAPI App &amp; Routers) handles requests, how the LLM Pipeline Orchestrator (TokenGeneratorPipeline) manages model interactions, the role of the Model Worker and its Scheduler (TokenGenerationScheduler, EmbeddingsScheduler), the importance of KV Cache Management, the communication facilitated by the EngineQueue, and finally, how Telemetry keeps us informed. We hope this tour has given you a solid foundation for understanding and working with modular! . Generated by AI Codebase Knowledge Builder . ",
    "url": "/modular_max/08_telemetry_and_metrics___metrics____metricclient___.html#conclusion",
    
    "relUrl": "/modular_max/08_telemetry_and_metrics___metrics____metricclient___.html#conclusion"
  },"310": {
    "doc": "Chapter 8: Telemetry and Metrics",
    "title": "Chapter 8: Telemetry and Metrics",
    "content": " ",
    "url": "/modular_max/08_telemetry_and_metrics___metrics____metricclient___.html",
    
    "relUrl": "/modular_max/08_telemetry_and_metrics___metrics____metricclient___.html"
  },"311": {
    "doc": "Chapter 9: API & Docker Integration",
    "title": "Chapter 9: API &amp; Docker Integration",
    "content": "In Chapter 8: Async Logging Infrastructure, we learned how to track what our crawler is doing. Now, let’s explore how to share our crawler with others by turning it into a service that can be accessed over a network! . ",
    "url": "/Crawl4AI/09_api___docker_integration_.html#chapter-9-api--docker-integration",
    
    "relUrl": "/Crawl4AI/09_api___docker_integration_.html#chapter-9-api--docker-integration"
  },"312": {
    "doc": "Chapter 9: API & Docker Integration",
    "title": "What is API &amp; Docker Integration?",
    "content": "Imagine you’ve built a helpful robot (your crawler) that can gather information from websites. Initially, this robot lives only in your house and only you can use it. But what if your friends and family also want to use your robot? You could: . | Make copies of the robot for everyone (inefficient) | Set up a “robot service center” where people can call in their requests (better!) | . The API &amp; Docker Integration in crawl4ai is like setting up that service center: . | The API (Application Programming Interface) is like a receptionist who takes requests from callers and passes them to your robot | Docker is like a standardized shipping container that lets you easily move and set up your robot service center anywhere | . This means instead of everyone needing to install and run crawl4ai themselves, they can simply send requests to your service! . flowchart LR User1[User 1] --&gt;|Request| API User2[User 2] --&gt;|Request| API User3[User 3] --&gt;|Request| API API --&gt;|Processes| Docker[Docker Container] Docker --&gt;|Contains| Crawler[Web Crawler] Crawler --&gt;|Returns| API API --&gt;|Response| User1 API --&gt;|Response| User2 API --&gt;|Response| User3 . ",
    "url": "/Crawl4AI/09_api___docker_integration_.html#what-is-api--docker-integration",
    
    "relUrl": "/Crawl4AI/09_api___docker_integration_.html#what-is-api--docker-integration"
  },"313": {
    "doc": "Chapter 9: API & Docker Integration",
    "title": "Getting Started with the Docker Client",
    "content": "Let’s start by seeing how to connect to a crawler service that’s already running. The Crawl4aiDockerClient makes this easy: . from crawl4ai import Crawl4aiDockerClient async def simple_crawl(): # Connect to the crawler service async with Crawl4aiDockerClient() as client: # Authenticate (required for security) await client.authenticate(\"your@email.com\") # Crawl a website result = await client.crawl([\"https://example.com\"]) # Use the result print(result.markdown) . This code: . | Creates a client that connects to a crawl4ai service | Authenticates with the service using your email | Sends a request to crawl example.com | Prints the markdown content from the result | . It’s like calling the robot service center, identifying yourself, asking the robot to visit a website, and getting back what it found! . ",
    "url": "/Crawl4AI/09_api___docker_integration_.html#getting-started-with-the-docker-client",
    
    "relUrl": "/Crawl4AI/09_api___docker_integration_.html#getting-started-with-the-docker-client"
  },"314": {
    "doc": "Chapter 9: API & Docker Integration",
    "title": "Using Configuration with the Docker Client",
    "content": "Just like the local crawler we used in earlier chapters, you can customize how the remote crawler works: . from crawl4ai import Crawl4aiDockerClient, BrowserConfig, CrawlerRunConfig async def configured_crawl(): # Create configurations browser_config = BrowserConfig(headless=True) crawler_config = CrawlerRunConfig(screenshot=True) async with Crawl4aiDockerClient() as client: await client.authenticate(\"your@email.com\") # Use configurations with the crawl result = await client.crawl( urls=[\"https://example.com\"], browser_config=browser_config, crawler_config=crawler_config ) . This allows you to tell the remote crawler exactly how to behave, just like you would with a local crawler! . ",
    "url": "/Crawl4AI/09_api___docker_integration_.html#using-configuration-with-the-docker-client",
    
    "relUrl": "/Crawl4AI/09_api___docker_integration_.html#using-configuration-with-the-docker-client"
  },"315": {
    "doc": "Chapter 9: API & Docker Integration",
    "title": "Streaming Results for Faster Processing",
    "content": "If you’re crawling multiple URLs, you can process the results as they come in, rather than waiting for all of them to finish: . async def stream_results(): async with Crawl4aiDockerClient() as client: await client.authenticate(\"your@email.com\") # Enable streaming in the configuration config = CrawlerRunConfig(stream=True) # Get a streaming generator results = await client.crawl( urls=[\"https://example.com\", \"https://example.org\"], crawler_config=config ) # Process results as they arrive async for result in results: print(f\"Just received: {result.url}\") . This is like getting updates from the robot as it explores each website, rather than waiting for it to explore all websites before reporting back. ",
    "url": "/Crawl4AI/09_api___docker_integration_.html#streaming-results-for-faster-processing",
    
    "relUrl": "/Crawl4AI/09_api___docker_integration_.html#streaming-results-for-faster-processing"
  },"316": {
    "doc": "Chapter 9: API & Docker Integration",
    "title": "Setting Up Your Own Crawler Service",
    "content": "Now, let’s look at how to set up your own crawler service using Docker and FastAPI. This is a bit more advanced, but we’ll break it down: . | First, you need to have Docker installed on your computer or server | Then, you can create a Docker container with crawl4ai and its API server | Finally, you start the container to make the service available | . Here’s a simple example using Docker Compose (a tool for defining and running Docker applications): . # docker-compose.yml version: '3' services: crawl4ai: image: crawl4ai/service:latest ports: - \"8000:8000\" environment: - ALLOWED_EMAIL_DOMAINS=example.com,gmail.com . This configuration: . | Uses the pre-built crawl4ai/service Docker image | Maps port 8000 on your computer to port 8000 in the container | Sets an environment variable to control which email domains can authenticate | . To start the service, you would run: . docker-compose up -d . Now your crawler service is running and can be accessed at http://localhost:8000! . ",
    "url": "/Crawl4AI/09_api___docker_integration_.html#setting-up-your-own-crawler-service",
    
    "relUrl": "/Crawl4AI/09_api___docker_integration_.html#setting-up-your-own-crawler-service"
  },"317": {
    "doc": "Chapter 9: API & Docker Integration",
    "title": "Understanding the API Endpoints",
    "content": "The FastAPI server provides several endpoints for different functions: . # Available endpoints /token # Authenticate and get an access token /crawl # Crawl URLs and get results /crawl/stream # Stream crawl results as they complete /md # Generate markdown from a URL /llm # Use LLM to process content from a URL /schema # Get configuration schemas . Each endpoint serves a specific purpose: . | /token is for authentication | /crawl is for basic crawling | /crawl/stream is for streaming crawl results | /md is for directly getting markdown from a URL | /llm is for using AI models to process web content | /schema returns information about configuration options | . ",
    "url": "/Crawl4AI/09_api___docker_integration_.html#understanding-the-api-endpoints",
    
    "relUrl": "/Crawl4AI/09_api___docker_integration_.html#understanding-the-api-endpoints"
  },"318": {
    "doc": "Chapter 9: API & Docker Integration",
    "title": "What Happens Under the Hood",
    "content": "Let’s look at what happens when you send a request to crawl a website: . sequenceDiagram participant Client as Docker Client participant API as FastAPI Server participant Pool as Crawler Pool participant Crawler as Web Crawler participant Website as Website Client-&gt;&gt;API: POST /crawl with URLs API-&gt;&gt;API: Authenticate request API-&gt;&gt;Pool: Get crawler from pool Pool-&gt;&gt;Pool: Check available crawlers Pool--&gt;&gt;API: Return crawler instance API-&gt;&gt;Crawler: Crawl URLs Crawler-&gt;&gt;Website: Request pages Website--&gt;&gt;Crawler: Return HTML Crawler--&gt;&gt;API: Return crawl results API--&gt;&gt;Client: Return JSON response . | The client sends a request to the /crawl endpoint with URLs to crawl | The server authenticates the request using the provided token | The server gets a crawler from the crawler pool (which manages browser instances) | The crawler fetches the requested URLs | The server formats the results and returns them to the client | . ",
    "url": "/Crawl4AI/09_api___docker_integration_.html#what-happens-under-the-hood",
    
    "relUrl": "/Crawl4AI/09_api___docker_integration_.html#what-happens-under-the-hood"
  },"319": {
    "doc": "Chapter 9: API & Docker Integration",
    "title": "Crawler Pool: Efficient Resource Management",
    "content": "One of the most important parts of the API server is the crawler pool, which efficiently manages browser instances: . # Simplified crawler pool code async def get_crawler(cfg: BrowserConfig) -&gt; AsyncWebCrawler: sig = _sig(cfg) # Create a unique signature for this config async with LOCK: # Return existing crawler if available if sig in POOL: LAST_USED[sig] = time.time() return POOL[sig] # Create new crawler if memory allows if psutil.virtual_memory().percent &lt; MEM_LIMIT: crawler = AsyncWebCrawler(config=cfg) await crawler.start() POOL[sig] = crawler LAST_USED[sig] = time.time() return crawler . The crawler pool: . | Reuses existing crawler instances when possible | Creates new instances when needed (if memory allows) | Tracks when each crawler was last used | Closes idle crawlers to free up resources | . This is like having a team of robots that are put to sleep when not in use, and woken up when needed! . ",
    "url": "/Crawl4AI/09_api___docker_integration_.html#crawler-pool-efficient-resource-management",
    
    "relUrl": "/Crawl4AI/09_api___docker_integration_.html#crawler-pool-efficient-resource-management"
  },"320": {
    "doc": "Chapter 9: API & Docker Integration",
    "title": "Authentication and Security",
    "content": "Security is important for any web service. The API server includes authentication to ensure only authorized users can access it: . @app.post(\"/token\") async def get_token(req: TokenRequest): # Check if email domain is allowed if not verify_email_domain(req.email): raise HTTPException(400, \"Invalid email domain\") # Create a JWT token for the user token = create_access_token({\"sub\": req.email}) return { \"email\": req.email, \"access_token\": token, \"token_type\": \"bearer\" } . This endpoint: . | Verifies that the user’s email domain is in the allowed list | Creates a token (like a temporary ID card) for the user | Returns the token, which must be included in subsequent requests | . ",
    "url": "/Crawl4AI/09_api___docker_integration_.html#authentication-and-security",
    
    "relUrl": "/Crawl4AI/09_api___docker_integration_.html#authentication-and-security"
  },"321": {
    "doc": "Chapter 9: API & Docker Integration",
    "title": "Rate Limiting: Preventing Overuse",
    "content": "To prevent any single user from overwhelming the service, the API includes rate limiting: . @app.post(\"/crawl\") @limiter.limit(config[\"rate_limiting\"][\"default_limit\"]) async def crawl(request: Request, crawl_request: CrawlRequest): # Rate limiting is applied by the decorator above # ...process the crawl request... This decorator ensures that each user can only make a certain number of requests per time period, like limiting how many calls a person can make to your robot service center in an hour. ",
    "url": "/Crawl4AI/09_api___docker_integration_.html#rate-limiting-preventing-overuse",
    
    "relUrl": "/Crawl4AI/09_api___docker_integration_.html#rate-limiting-preventing-overuse"
  },"322": {
    "doc": "Chapter 9: API & Docker Integration",
    "title": "Creating a Complete Crawl Service",
    "content": "Putting it all together, here’s how you would create a complete crawl service: . | Set up the FastAPI server with all the endpoints | Configure authentication, rate limiting, and security | Create the crawler pool for efficient resource management | Start the server in a Docker container | . The server.py file in the code snippets shows a complete implementation of this service. ",
    "url": "/Crawl4AI/09_api___docker_integration_.html#creating-a-complete-crawl-service",
    
    "relUrl": "/Crawl4AI/09_api___docker_integration_.html#creating-a-complete-crawl-service"
  },"323": {
    "doc": "Chapter 9: API & Docker Integration",
    "title": "Real-World Example: Content Processing Service",
    "content": "Let’s see a real-world example of setting up a content processing service: . from crawl4ai import Crawl4aiDockerClient, CrawlerRunConfig from fastapi import FastAPI, HTTPException app = FastAPI() @app.get(\"/process/{url:path}\") async def process_url(url: str, query: str = None): try: async with Crawl4aiDockerClient() as client: await client.authenticate(\"service@example.com\") config = CrawlerRunConfig() if query: # Use LLM processing if query is provided config = CrawlerRunConfig( extraction_strategy=\"llm\", instruction=query ) result = await client.crawl([url], crawler_config=config) return {\"content\": result.markdown, \"url\": url} except Exception as e: raise HTTPException(500, str(e)) . This example creates a simple API that uses the crawl4ai service to process URLs and optionally extract specific information based on a query. ",
    "url": "/Crawl4AI/09_api___docker_integration_.html#real-world-example-content-processing-service",
    
    "relUrl": "/Crawl4AI/09_api___docker_integration_.html#real-world-example-content-processing-service"
  },"324": {
    "doc": "Chapter 9: API & Docker Integration",
    "title": "Conclusion",
    "content": "The API &amp; Docker Integration in crawl4ai transforms your crawler from a local tool into a powerful service that can be accessed from anywhere. By wrapping the crawler in a Docker container and exposing it through a FastAPI server, you make it available to multiple users and applications. In this chapter, we’ve learned: . | How to connect to a crawler service using the Docker client | How to configure and customize remote crawling operations | How to stream results for faster processing | How to set up your own crawler service using Docker | How the server manages resources with the crawler pool | How authentication and rate limiting protect the service | . This chapter completes our exploration of the crawl4ai library. You now have all the knowledge you need to build powerful web crawling applications, from simple scripts to full-featured services! . Remember that with great power comes great responsibility - always respect robots.txt rules, rate limits, and privacy considerations when crawling websites. Thank you for joining this crawl4ai tutorial journey! . Generated by AI Codebase Knowledge Builder . ",
    "url": "/Crawl4AI/09_api___docker_integration_.html#conclusion",
    
    "relUrl": "/Crawl4AI/09_api___docker_integration_.html#conclusion"
  },"325": {
    "doc": "Chapter 9: API & Docker Integration",
    "title": "Chapter 9: API & Docker Integration",
    "content": " ",
    "url": "/Crawl4AI/09_api___docker_integration_.html",
    
    "relUrl": "/Crawl4AI/09_api___docker_integration_.html"
  },"326": {
    "doc": "CacheContext & CacheMode",
    "title": "Chapter 9: Smart Fetching with Caching - CacheContext / CacheMode",
    "content": "In the previous chapter, Chapter 8: Exploring Websites - DeepCrawlStrategy, we saw how Crawl4AI can explore websites by following links, potentially visiting many pages. During such explorations, or even when you run the same crawl multiple times, the crawler might try to fetch the exact same webpage again and again. This can be slow and might unnecessarily put a load on the website you’re crawling. Wouldn’t it be smarter to remember the result from the first time and just reuse it? . ",
    "url": "/Crawl4AI/09_cachecontext___cachemode.html#chapter-9-smart-fetching-with-caching---cachecontext--cachemode",
    
    "relUrl": "/Crawl4AI/09_cachecontext___cachemode.html#chapter-9-smart-fetching-with-caching---cachecontext--cachemode"
  },"327": {
    "doc": "CacheContext & CacheMode",
    "title": "What Problem Does Caching Solve?",
    "content": "Imagine you need to download a large instruction manual (a webpage) from the internet. | Without Caching: Every single time you need the manual, you download the entire file again. This takes time and uses bandwidth every time. | With Caching: The first time you download it, you save a copy on your computer (the “cache”). The next time you need it, you first check your local copy. If it’s there, you use it instantly! You only download it again if you specifically want the absolute latest version or if your local copy is missing. | . Caching in Crawl4AI works the same way. It’s a mechanism to store the results of crawling a webpage locally (in a database file). When asked to crawl a URL again, Crawl4AI can check its cache first. If a valid result is already stored, it can return that saved result almost instantly, saving time and resources. ",
    "url": "/Crawl4AI/09_cachecontext___cachemode.html#what-problem-does-caching-solve",
    
    "relUrl": "/Crawl4AI/09_cachecontext___cachemode.html#what-problem-does-caching-solve"
  },"328": {
    "doc": "CacheContext & CacheMode",
    "title": "Introducing CacheMode and CacheContext",
    "content": "Crawl4AI uses two key concepts to manage this caching behavior: . | CacheMode (The Cache Policy): . | Think of this like setting the rules for how you interact with your saved instruction manuals. | It’s an instruction you give the crawler for a specific run, telling it how to use the cache. | Analogy: Should you always use your saved copy if you have one? (ENABLED) Should you ignore your saved copies and always download a fresh one? (BYPASS) Should you never save any copies? (DISABLED) Should you save new copies but never reuse old ones? (WRITE_ONLY) | CacheMode lets you choose the caching behavior that best fits your needs for a particular task. | . | CacheContext (The Decision Maker): . | This is an internal helper that Crawl4AI uses during a crawl. You don’t usually interact with it directly. | It looks at the CacheMode you provided (the policy) and the type of URL being processed. | Analogy: Imagine a librarian who checks the library’s borrowing rules (CacheMode) and the type of item you’re requesting (e.g., a reference book that can’t be checked out, like raw: HTML which isn’t cached). Based on these, the librarian (CacheContext) decides if you can borrow an existing copy (read from cache) or if a new copy should be added to the library (write to cache). | It helps the main AsyncWebCrawler make the right decision about reading from or writing to the cache for each specific URL based on the active policy. | . | . ",
    "url": "/Crawl4AI/09_cachecontext___cachemode.html#introducing-cachemode-and-cachecontext",
    
    "relUrl": "/Crawl4AI/09_cachecontext___cachemode.html#introducing-cachemode-and-cachecontext"
  },"329": {
    "doc": "CacheContext & CacheMode",
    "title": "Setting the Cache Policy: Using CacheMode",
    "content": "You control the caching behavior by setting the cache_mode parameter within the CrawlerRunConfig object that you pass to crawler.arun() or crawler.arun_many(). Let’s explore the most common CacheMode options: . 1. CacheMode.ENABLED (The Default Behavior - If not specified) . | Policy: “Use the cache if a valid result exists. If not, fetch the page, save the result to the cache, and then return it.” | This is the standard, balanced approach. It saves time on repeated crawls but ensures you get the content eventually. | Note: In recent versions, the default if cache_mode is left completely unspecified might be CacheMode.BYPASS. Always check the documentation or explicitly set the mode for clarity. For this tutorial, let’s assume we explicitly set it. | . # chapter9_example_1.py import asyncio from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode async def main(): url = \"https://httpbin.org/html\" async with AsyncWebCrawler() as crawler: # Explicitly set the mode to ENABLED config_enabled = CrawlerRunConfig(cache_mode=CacheMode.ENABLED) print(f\"Running with CacheMode: {config_enabled.cache_mode.name}\") # First run: Fetches, caches, and returns result print(\"First run (ENABLED)...\") result1 = await crawler.arun(url=url, config=config_enabled) print(f\"Got result 1? {'Yes' if result1.success else 'No'}\") # Second run: Finds result in cache and returns it instantly print(\"Second run (ENABLED)...\") result2 = await crawler.arun(url=url, config=config_enabled) print(f\"Got result 2? {'Yes' if result2.success else 'No'}\") # This second run should be much faster! if __name__ == \"__main__\": asyncio.run(main()) . Explanation: . | We create a CrawlerRunConfig with cache_mode=CacheMode.ENABLED. | The first arun call fetches the page from the web and saves the result in the cache. | The second arun call (for the same URL and config affecting cache key) finds the saved result in the cache and returns it immediately, skipping the web fetch. | . 2. CacheMode.BYPASS . | Policy: “Ignore any existing saved copy. Always fetch a fresh copy from the web. After fetching, save this new result to the cache (overwriting any old one).” | Useful when you always need the absolute latest version of the page, but you still want to update the cache for potential future use with CacheMode.ENABLED. | . # chapter9_example_2.py import asyncio from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode import time async def main(): url = \"https://httpbin.org/html\" async with AsyncWebCrawler() as crawler: # Set the mode to BYPASS config_bypass = CrawlerRunConfig(cache_mode=CacheMode.BYPASS) print(f\"Running with CacheMode: {config_bypass.cache_mode.name}\") # First run: Fetches, caches, and returns result print(\"First run (BYPASS)...\") start_time = time.perf_counter() result1 = await crawler.arun(url=url, config=config_bypass) duration1 = time.perf_counter() - start_time print(f\"Got result 1? {'Yes' if result1.success else 'No'} (took {duration1:.2f}s)\") # Second run: Ignores cache, fetches again, updates cache, returns result print(\"Second run (BYPASS)...\") start_time = time.perf_counter() result2 = await crawler.arun(url=url, config=config_bypass) duration2 = time.perf_counter() - start_time print(f\"Got result 2? {'Yes' if result2.success else 'No'} (took {duration2:.2f}s)\") # Both runs should take a similar amount of time (fetching time) if __name__ == \"__main__\": asyncio.run(main()) . Explanation: . | We set cache_mode=CacheMode.BYPASS. | Both the first and second arun calls will fetch the page directly from the web, ignoring any previously cached result. They will still write the newly fetched result to the cache. Notice both runs take roughly the same amount of time (network fetch time). | . 3. CacheMode.DISABLED . | Policy: “Completely ignore the cache. Never read from it, never write to it.” | Useful when you don’t want Crawl4AI to interact with the cache files at all, perhaps for debugging or if you have storage constraints. | . # chapter9_example_3.py import asyncio from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode import time async def main(): url = \"https://httpbin.org/html\" async with AsyncWebCrawler() as crawler: # Set the mode to DISABLED config_disabled = CrawlerRunConfig(cache_mode=CacheMode.DISABLED) print(f\"Running with CacheMode: {config_disabled.cache_mode.name}\") # First run: Fetches, returns result (does NOT cache) print(\"First run (DISABLED)...\") start_time = time.perf_counter() result1 = await crawler.arun(url=url, config=config_disabled) duration1 = time.perf_counter() - start_time print(f\"Got result 1? {'Yes' if result1.success else 'No'} (took {duration1:.2f}s)\") # Second run: Fetches again, returns result (does NOT cache) print(\"Second run (DISABLED)...\") start_time = time.perf_counter() result2 = await crawler.arun(url=url, config=config_disabled) duration2 = time.perf_counter() - start_time print(f\"Got result 2? {'Yes' if result2.success else 'No'} (took {duration2:.2f}s)\") # Both runs fetch fresh, and nothing is ever saved to the cache. if __name__ == \"__main__\": asyncio.run(main()) . Explanation: . | We set cache_mode=CacheMode.DISABLED. | Both arun calls fetch fresh content from the web. Crucially, neither run reads from nor writes to the cache database. | . Other Modes (READ_ONLY, WRITE_ONLY): . | CacheMode.READ_ONLY: Only uses existing cached results. If a result isn’t in the cache, it will fail or return an empty result rather than fetching it. Never saves anything new. | CacheMode.WRITE_ONLY: Never reads from the cache (always fetches fresh). It only writes the newly fetched result to the cache. | . ",
    "url": "/Crawl4AI/09_cachecontext___cachemode.html#setting-the-cache-policy-using-cachemode",
    
    "relUrl": "/Crawl4AI/09_cachecontext___cachemode.html#setting-the-cache-policy-using-cachemode"
  },"330": {
    "doc": "CacheContext & CacheMode",
    "title": "How Caching Works Internally",
    "content": "When you call crawler.arun(url=\"...\", config=...): . | Create Context: The AsyncWebCrawler creates a CacheContext instance using the url and the config.cache_mode. | Check Read: It asks the CacheContext, “Should I read from the cache?” (cache_context.should_read()). | Try Reading: If should_read() is True, it asks the database manager (AsyncDatabaseManager) to look for a cached result for the url. | Cache Hit? . | If a valid cached result is found: The AsyncWebCrawler returns this cached CrawlResult immediately. Done! | If no cached result is found (or if should_read() was False): Proceed to fetching. | . | Fetch: The AsyncWebCrawler calls the appropriate AsyncCrawlerStrategy to fetch the content from the web. | Process: It processes the fetched HTML (scraping, filtering, extracting) to create a new CrawlResult. | Check Write: It asks the CacheContext, “Should I write this result to the cache?” (cache_context.should_write()). | Write Cache: If should_write() is True, it tells the database manager to save the new CrawlResult into the cache database. | Return: The AsyncWebCrawler returns the newly created CrawlResult. | . sequenceDiagram participant User participant AWC as AsyncWebCrawler participant Ctx as CacheContext participant DB as DatabaseManager participant Fetcher as AsyncCrawlerStrategy User-&gt;&gt;AWC: arun(url, config) AWC-&gt;&gt;Ctx: Create CacheContext(url, config.cache_mode) AWC-&gt;&gt;Ctx: should_read()? alt Cache Read Allowed Ctx--&gt;&gt;AWC: Yes AWC-&gt;&gt;DB: aget_cached_url(url) DB--&gt;&gt;AWC: Cached Result (or None) alt Cache Hit &amp; Valid AWC--&gt;&gt;User: Return Cached CrawlResult else Cache Miss or Invalid AWC-&gt;&gt;AWC: Proceed to Fetch end else Cache Read Not Allowed Ctx--&gt;&gt;AWC: No AWC-&gt;&gt;AWC: Proceed to Fetch end Note over AWC: Fetching Required AWC-&gt;&gt;Fetcher: crawl(url, config) Fetcher--&gt;&gt;AWC: Raw Response AWC-&gt;&gt;AWC: Process HTML -&gt; New CrawlResult AWC-&gt;&gt;Ctx: should_write()? alt Cache Write Allowed Ctx--&gt;&gt;AWC: Yes AWC-&gt;&gt;DB: acache_url(New CrawlResult) DB--&gt;&gt;AWC: OK else Cache Write Not Allowed Ctx--&gt;&gt;AWC: No end AWC--&gt;&gt;User: Return New CrawlResult . ",
    "url": "/Crawl4AI/09_cachecontext___cachemode.html#how-caching-works-internally",
    
    "relUrl": "/Crawl4AI/09_cachecontext___cachemode.html#how-caching-works-internally"
  },"331": {
    "doc": "CacheContext & CacheMode",
    "title": "Code Glimpse",
    "content": "Let’s look at simplified code snippets. Inside async_webcrawler.py (where arun uses caching): . # Simplified from crawl4ai/async_webcrawler.py from .cache_context import CacheContext, CacheMode from .async_database import async_db_manager from .models import CrawlResult # ... other imports class AsyncWebCrawler: # ... (init, other methods) ... async def arun(self, url: str, config: CrawlerRunConfig = None) -&gt; CrawlResult: # ... (ensure config exists, set defaults) ... if config.cache_mode is None: config.cache_mode = CacheMode.ENABLED # Example default # 1. Create CacheContext cache_context = CacheContext(url, config.cache_mode) cached_result = None # 2. Check if cache read is allowed if cache_context.should_read(): # 3. Try reading from database cached_result = await async_db_manager.aget_cached_url(url) # 4. If cache hit and valid, return it if cached_result and self._is_cache_valid(cached_result, config): self.logger.info(\"Cache hit for: %s\", url) # Example log return cached_result # Return early # 5. Fetch fresh content (if no cache hit or read disabled) async_response = await self.crawler_strategy.crawl(url, config=config) html = async_response.html # ... and other data ... # 6. Process the HTML to get a new CrawlResult crawl_result = await self.aprocess_html( url=url, html=html, config=config, # ... other params ... ) # 7. Check if cache write is allowed if cache_context.should_write(): # 8. Write the new result to the database await async_db_manager.acache_url(crawl_result) # 9. Return the new result return crawl_result def _is_cache_valid(self, cached_result: CrawlResult, config: CrawlerRunConfig) -&gt; bool: # Internal logic to check if cached result meets current needs # (e.g., was screenshot requested now but not cached?) if config.screenshot and not cached_result.screenshot: return False if config.pdf and not cached_result.pdf: return False # ... other checks ... return True . Inside cache_context.py (defining the concepts): . # Simplified from crawl4ai/cache_context.py from enum import Enum class CacheMode(Enum): \"\"\"Defines the caching behavior for web crawling operations.\"\"\" ENABLED = \"enabled\" # Read and Write DISABLED = \"disabled\" # No Read, No Write READ_ONLY = \"read_only\" # Read Only, No Write WRITE_ONLY = \"write_only\" # Write Only, No Read BYPASS = \"bypass\" # No Read, Write Only (similar to WRITE_ONLY but explicit intention) class CacheContext: \"\"\"Encapsulates cache-related decisions and URL handling.\"\"\" def __init__(self, url: str, cache_mode: CacheMode, always_bypass: bool = False): self.url = url self.cache_mode = cache_mode self.always_bypass = always_bypass # Usually False # Determine if URL type is cacheable (e.g., not 'raw:') self.is_cacheable = url.startswith((\"http://\", \"https://\", \"file://\")) # ... other URL type checks ... def should_read(self) -&gt; bool: \"\"\"Determines if cache should be read based on context.\"\"\" if self.always_bypass or not self.is_cacheable: return False # Allow read if mode is ENABLED or READ_ONLY return self.cache_mode in [CacheMode.ENABLED, CacheMode.READ_ONLY] def should_write(self) -&gt; bool: \"\"\"Determines if cache should be written based on context.\"\"\" if self.always_bypass or not self.is_cacheable: return False # Allow write if mode is ENABLED, WRITE_ONLY, or BYPASS return self.cache_mode in [CacheMode.ENABLED, CacheMode.WRITE_ONLY, CacheMode.BYPASS] @property def display_url(self) -&gt; str: \"\"\"Returns the URL in display format.\"\"\" return self.url if not self.url.startswith(\"raw:\") else \"Raw HTML\" # Helper for backward compatibility (may be removed later) def _legacy_to_cache_mode(...) -&gt; CacheMode: # ... logic to convert old boolean flags ... pass . ",
    "url": "/Crawl4AI/09_cachecontext___cachemode.html#code-glimpse",
    
    "relUrl": "/Crawl4AI/09_cachecontext___cachemode.html#code-glimpse"
  },"332": {
    "doc": "CacheContext & CacheMode",
    "title": "Conclusion",
    "content": "You’ve learned how Crawl4AI uses caching to avoid redundant work and speed up repeated crawls! . | Caching stores results locally to reuse them later. | CacheMode is the policy you set in CrawlerRunConfig to control how the cache is used (ENABLED, BYPASS, DISABLED, etc.). | CacheContext is an internal helper that makes decisions based on the CacheMode and URL type. | Using the cache effectively (especially CacheMode.ENABLED) can significantly speed up your crawling tasks, particularly during development or when dealing with many URLs, including deep crawls. | . We’ve seen how Crawl4AI can crawl single pages, lists of pages (arun_many), and even explore websites (DeepCrawlStrategy). But how does arun_many or a deep crawl manage running potentially hundreds or thousands of individual crawl tasks efficiently without overwhelming your system or the target website? . Next: Let’s explore the component responsible for managing concurrent tasks: Chapter 10: Orchestrating the Crawl - BaseDispatcher. Generated by AI Codebase Knowledge Builder . ",
    "url": "/Crawl4AI/09_cachecontext___cachemode.html#conclusion",
    
    "relUrl": "/Crawl4AI/09_cachecontext___cachemode.html#conclusion"
  },"333": {
    "doc": "CacheContext & CacheMode",
    "title": "CacheContext & CacheMode",
    "content": " ",
    "url": "/Crawl4AI/09_cachecontext___cachemode.html",
    
    "relUrl": "/Crawl4AI/09_cachecontext___cachemode.html"
  },"334": {
    "doc": "BaseDispatcher",
    "title": "Chapter 10: Orchestrating the Crawl - BaseDispatcher",
    "content": "In Chapter 9: Smart Fetching with Caching - CacheContext / CacheMode, we learned how Crawl4AI uses caching to cleverly avoid re-fetching the same webpage multiple times, which is especially helpful when crawling many URLs. We’ve also seen how methods like arun_many() (Chapter 2: Meet the General Manager - AsyncWebCrawler) or strategies like DeepCrawlStrategy can lead to potentially hundreds or thousands of individual URLs needing to be crawled. This raises a question: if we have 1000 URLs to crawl, does Crawl4AI try to crawl all 1000 simultaneously? That would likely overwhelm your computer’s resources (like memory and CPU) and could also flood the target website with too many requests, potentially getting you blocked! How does Crawl4AI manage running many crawls efficiently and responsibly? . ",
    "url": "/Crawl4AI/10_basedispatcher.html#chapter-10-orchestrating-the-crawl---basedispatcher",
    
    "relUrl": "/Crawl4AI/10_basedispatcher.html#chapter-10-orchestrating-the-crawl---basedispatcher"
  },"335": {
    "doc": "BaseDispatcher",
    "title": "What Problem Does BaseDispatcher Solve?",
    "content": "Imagine you’re managing a fleet of delivery drones (AsyncWebCrawler tasks) that need to pick up packages from many different addresses (URLs). If you launch all 1000 drones at the exact same moment: . | Your control station (your computer) might crash due to the processing load. | The central warehouse (the target website) might get overwhelmed by simultaneous arrivals. | Some drones might collide or interfere with each other. | . You need a Traffic Controller or a Dispatch Center to manage the fleet. This controller decides: . | How many drones can be active in the air at any one time. | When to launch the next drone, maybe based on available airspace (system resources) or just a simple count limit. | How to handle potential delays or issues (like rate limiting from a specific website). | . In Crawl4AI, the BaseDispatcher acts as this Traffic Controller or Task Scheduler for concurrent crawling operations, primarily when using arun_many(). It manages how multiple crawl tasks are executed concurrently, ensuring the process is efficient without overwhelming your system or the target websites. ",
    "url": "/Crawl4AI/10_basedispatcher.html#what-problem-does-basedispatcher-solve",
    
    "relUrl": "/Crawl4AI/10_basedispatcher.html#what-problem-does-basedispatcher-solve"
  },"336": {
    "doc": "BaseDispatcher",
    "title": "What is BaseDispatcher?",
    "content": "BaseDispatcher is an abstract concept (a blueprint or job description) in Crawl4AI. It defines that we need a system for managing the execution of multiple, concurrent crawling tasks. It specifies the interface for how the main AsyncWebCrawler interacts with such a system, but the specific logic for managing concurrency can vary. Think of it as the control panel for our drone fleet – the panel exists, but the specific rules programmed into it determine how drones are dispatched. ",
    "url": "/Crawl4AI/10_basedispatcher.html#what-is-basedispatcher",
    
    "relUrl": "/Crawl4AI/10_basedispatcher.html#what-is-basedispatcher"
  },"337": {
    "doc": "BaseDispatcher",
    "title": "The Different Controllers: Ways to Dispatch Tasks",
    "content": "Crawl4AI provides concrete implementations (the actual traffic control systems) based on the BaseDispatcher blueprint: . | SemaphoreDispatcher (The Simple Counter): . | Analogy: A parking garage with a fixed number of spots (e.g., 10). A gate (asyncio.Semaphore) only lets a new car in if one of the 10 spots is free. | How it works: You tell it the maximum number of crawls that can run at the same time (e.g., semaphore_count=10). It uses a simple counter (a semaphore) to ensure that no more than this number of crawls are active simultaneously. When one crawl finishes, it allows another one from the queue to start. | Good for: Simple, direct control over concurrency when you know a specific limit works well for your system and the target sites. | . | MemoryAdaptiveDispatcher (The Resource-Aware Controller - Default): . | Analogy: A smart parking garage attendant who checks not just the number of cars, but also the total space they occupy (system memory). They might stop letting cars in if the garage is nearing its memory capacity, even if some numbered spots are technically free. | How it works: This dispatcher monitors your system’s available memory. It tries to run multiple crawls concurrently (up to a configurable maximum like max_session_permit), but it will pause launching new crawls if the system memory usage exceeds a certain threshold (e.g., memory_threshold_percent=90.0). It adapts the concurrency level based on available resources. | Good for: Automatically adjusting concurrency to prevent out-of-memory errors, especially when crawl tasks vary significantly in resource usage. This is the default dispatcher used by arun_many if you don’t specify one. | . | . These dispatchers can also optionally work with a RateLimiter component, which adds politeness rules for specific websites (e.g., slowing down requests to a domain if it returns “429 Too Many Requests”). ",
    "url": "/Crawl4AI/10_basedispatcher.html#the-different-controllers-ways-to-dispatch-tasks",
    
    "relUrl": "/Crawl4AI/10_basedispatcher.html#the-different-controllers-ways-to-dispatch-tasks"
  },"338": {
    "doc": "BaseDispatcher",
    "title": "How arun_many Uses the Dispatcher",
    "content": "When you call crawler.arun_many(urls=...), here’s the basic flow involving the dispatcher: . | Get URLs: arun_many receives the list of URLs you want to crawl. | Select Dispatcher: It checks if you provided a specific dispatcher instance. If not, it creates an instance of the default MemoryAdaptiveDispatcher. | Delegate Execution: It hands over the list of URLs and the CrawlerRunConfig to the chosen dispatcher’s run_urls (or run_urls_stream) method. | Manage Tasks: The dispatcher takes charge: . | It iterates through the URLs. | For each URL, it decides when to start the actual crawl based on its rules (semaphore count, memory usage, rate limits). | When ready, it typically calls the single-page crawler.arun(url, config) method internally for that specific URL, wrapped within its concurrency control mechanism. | It manages the running tasks (e.g., using asyncio.create_task and asyncio.wait). | . | Collect Results: As individual arun calls complete, the dispatcher collects their CrawlResult objects. | Return: Once all URLs are processed, the dispatcher returns the list of results (or yields them if streaming). | . sequenceDiagram participant User participant AWC as AsyncWebCrawler participant Dispatcher as BaseDispatcher (e.g., MemoryAdaptive) participant TaskPool as Concurrency Manager User-&gt;&gt;AWC: arun_many(urls, config, dispatcher?) AWC-&gt;&gt;Dispatcher: run_urls(crawler=AWC, urls, config) Dispatcher-&gt;&gt;TaskPool: Initialize (e.g., set max concurrency) loop For each URL in urls Dispatcher-&gt;&gt;TaskPool: Can I start a new task? (Checks limits) alt Yes TaskPool--&gt;&gt;Dispatcher: OK Note over Dispatcher: Create task: call AWC.arun(url, config) internally Dispatcher-&gt;&gt;TaskPool: Add new task else No TaskPool--&gt;&gt;Dispatcher: Wait Note over Dispatcher: Waits for a running task to finish end end Note over Dispatcher: Manages running tasks, collects results Dispatcher--&gt;&gt;AWC: List of CrawlResults AWC--&gt;&gt;User: List of CrawlResults . ",
    "url": "/Crawl4AI/10_basedispatcher.html#how-arun_many-uses-the-dispatcher",
    
    "relUrl": "/Crawl4AI/10_basedispatcher.html#how-arun_many-uses-the-dispatcher"
  },"339": {
    "doc": "BaseDispatcher",
    "title": "Using the Dispatcher (Often Implicitly!)",
    "content": "Most of the time, you don’t need to think about the dispatcher explicitly. When you use arun_many, the default MemoryAdaptiveDispatcher handles things automatically. # chapter10_example_1.py import asyncio from crawl4ai import AsyncWebCrawler, CrawlerRunConfig async def main(): urls_to_crawl = [ \"https://httpbin.org/html\", \"https://httpbin.org/links/5/0\", # Page with 5 links \"https://httpbin.org/robots.txt\", \"https://httpbin.org/status/200\", ] # We DON'T specify a dispatcher here. # arun_many will use the default MemoryAdaptiveDispatcher. async with AsyncWebCrawler() as crawler: print(f\"Crawling {len(urls_to_crawl)} URLs using the default dispatcher...\") config = CrawlerRunConfig(stream=False) # Get results as a list at the end # The MemoryAdaptiveDispatcher manages concurrency behind the scenes. results = await crawler.arun_many(urls=urls_to_crawl, config=config) print(f\"\\nFinished! Got {len(results)} results.\") for result in results: status = \"✅\" if result.success else \"❌\" url_short = result.url.split('/')[-1] print(f\" {status} {url_short:&lt;15} | Title: {result.metadata.get('title', 'N/A')}\") if __name__ == \"__main__\": asyncio.run(main()) . Explanation: . | We call crawler.arun_many without passing a dispatcher argument. | Crawl4AI automatically creates and uses a MemoryAdaptiveDispatcher. | This dispatcher runs the crawls concurrently, adapting to your system’s memory, and returns all the results once completed (because stream=False). You benefit from concurrency without explicit setup. | . ",
    "url": "/Crawl4AI/10_basedispatcher.html#using-the-dispatcher-often-implicitly",
    
    "relUrl": "/Crawl4AI/10_basedispatcher.html#using-the-dispatcher-often-implicitly"
  },"340": {
    "doc": "BaseDispatcher",
    "title": "Explicitly Choosing a Dispatcher",
    "content": "What if you want simpler, fixed concurrency? You can explicitly create and pass a SemaphoreDispatcher. # chapter10_example_2.py import asyncio from crawl4ai import ( AsyncWebCrawler, CrawlerRunConfig, SemaphoreDispatcher # 1. Import the specific dispatcher ) async def main(): urls_to_crawl = [ \"https://httpbin.org/delay/1\", # Takes 1 second \"https://httpbin.org/delay/1\", \"https://httpbin.org/delay/1\", \"https://httpbin.org/delay/1\", \"https://httpbin.org/delay/1\", ] # 2. Create an instance of the SemaphoreDispatcher # Allow only 2 crawls to run at the same time. semaphore_controller = SemaphoreDispatcher(semaphore_count=2) print(f\"Using SemaphoreDispatcher with limit: {semaphore_controller.semaphore_count}\") async with AsyncWebCrawler() as crawler: print(f\"Crawling {len(urls_to_crawl)} URLs with explicit dispatcher...\") config = CrawlerRunConfig(stream=False) # 3. Pass the dispatcher instance to arun_many results = await crawler.arun_many( urls=urls_to_crawl, config=config, dispatcher=semaphore_controller # Pass our controller ) print(f\"\\nFinished! Got {len(results)} results.\") # This crawl likely took around 3 seconds (5 tasks, 1s each, 2 concurrent = ceil(5/2)*1s) for result in results: status = \"✅\" if result.success else \"❌\" print(f\" {status} {result.url}\") if __name__ == \"__main__\": asyncio.run(main()) . Explanation: . | Import: We import SemaphoreDispatcher. | Instantiate: We create SemaphoreDispatcher(semaphore_count=2), limiting concurrency to 2 simultaneous crawls. | Pass Dispatcher: We pass our semaphore_controller instance directly to the dispatcher parameter of arun_many. | Execution: Now, arun_many uses our SemaphoreDispatcher. It will start the first two crawls. As one finishes, it will start the next one from the list, always ensuring no more than two are running concurrently. | . ",
    "url": "/Crawl4AI/10_basedispatcher.html#explicitly-choosing-a-dispatcher",
    
    "relUrl": "/Crawl4AI/10_basedispatcher.html#explicitly-choosing-a-dispatcher"
  },"341": {
    "doc": "BaseDispatcher",
    "title": "A Glimpse Under the Hood",
    "content": "Where are these dispatchers defined? In crawl4ai/async_dispatcher.py. The Blueprint (BaseDispatcher): . # Simplified from crawl4ai/async_dispatcher.py from abc import ABC, abstractmethod from typing import List, Optional # ... other imports like CrawlerRunConfig, CrawlerTaskResult, AsyncWebCrawler ... class BaseDispatcher(ABC): def __init__( self, rate_limiter: Optional[RateLimiter] = None, monitor: Optional[CrawlerMonitor] = None, ): self.crawler = None # Will be set by arun_many self.rate_limiter = rate_limiter self.monitor = monitor # ... other common state ... @abstractmethod async def crawl_url( self, url: str, config: CrawlerRunConfig, task_id: str, # ... maybe other internal params ... ) -&gt; CrawlerTaskResult: \"\"\"Crawls a single URL, potentially handling concurrency primitives.\"\"\" # This is often the core worker method called by run_urls pass @abstractmethod async def run_urls( self, urls: List[str], crawler: \"AsyncWebCrawler\", config: CrawlerRunConfig, ) -&gt; List[CrawlerTaskResult]: \"\"\"Manages the concurrent execution of crawl_url for multiple URLs.\"\"\" # This is the main entry point called by arun_many pass async def run_urls_stream( self, urls: List[str], crawler: \"AsyncWebCrawler\", config: CrawlerRunConfig, ) -&gt; AsyncGenerator[CrawlerTaskResult, None]: \"\"\" Streaming version of run_urls (might be implemented in base or subclasses) \"\"\" # Example default implementation (subclasses might override) results = await self.run_urls(urls, crawler, config) for res in results: yield res # Naive stream, real one is more complex # ... other potential helper methods ... Example Implementation (SemaphoreDispatcher): . # Simplified from crawl4ai/async_dispatcher.py import asyncio import uuid import psutil # For memory tracking in crawl_url import time # For timing in crawl_url # ... other imports ... class SemaphoreDispatcher(BaseDispatcher): def __init__( self, semaphore_count: int = 5, # ... other params like rate_limiter, monitor ... ): super().__init__(...) # Pass rate_limiter, monitor to base self.semaphore_count = semaphore_count async def crawl_url( self, url: str, config: CrawlerRunConfig, task_id: str, semaphore: asyncio.Semaphore = None, # Takes the semaphore ) -&gt; CrawlerTaskResult: # ... (Code to track start time, memory usage - similar to MemoryAdaptiveDispatcher's version) start_time = time.time() error_message = \"\" memory_usage = peak_memory = 0.0 result = None try: # Update monitor state if used if self.monitor: self.monitor.update_task(task_id, status=CrawlStatus.IN_PROGRESS) # Wait for rate limiter if used if self.rate_limiter: await self.rate_limiter.wait_if_needed(url) # --- Core Semaphore Logic --- async with semaphore: # Acquire a spot from the semaphore # Now that we have a spot, run the actual crawl process = psutil.Process() start_memory = process.memory_info().rss / (1024 * 1024) # Call the single-page crawl method of the main crawler result = await self.crawler.arun(url, config=config, session_id=task_id) end_memory = process.memory_info().rss / (1024 * 1024) memory_usage = peak_memory = end_memory - start_memory # --- Semaphore spot is released automatically on exiting 'async with' --- # Update rate limiter based on result status if used if self.rate_limiter and result.status_code: if not self.rate_limiter.update_delay(url, result.status_code): # Handle retry limit exceeded error_message = \"Rate limit retry count exceeded\" # ... update monitor, prepare error result ... # Update monitor status (success/fail) if result and not result.success: error_message = result.error_message if self.monitor: self.monitor.update_task(task_id, status=CrawlStatus.COMPLETED if result.success else CrawlStatus.FAILED) except Exception as e: # Handle unexpected errors during the crawl error_message = str(e) if self.monitor: self.monitor.update_task(task_id, status=CrawlStatus.FAILED) # Create a failed CrawlResult if needed if not result: result = CrawlResult(url=url, html=\"\", success=False, error_message=error_message) finally: # Final monitor update with timing, memory etc. end_time = time.time() if self.monitor: self.monitor.update_task(...) # Package everything into CrawlerTaskResult return CrawlerTaskResult(...) async def run_urls( self, crawler: \"AsyncWebCrawler\", urls: List[str], config: CrawlerRunConfig, ) -&gt; List[CrawlerTaskResult]: self.crawler = crawler # Store the crawler instance if self.monitor: self.monitor.start() try: # Create the semaphore with the specified count semaphore = asyncio.Semaphore(self.semaphore_count) tasks = [] # Create a crawl task for each URL, passing the semaphore for url in urls: task_id = str(uuid.uuid4()) if self.monitor: self.monitor.add_task(task_id, url) # Create an asyncio task to run crawl_url task = asyncio.create_task( self.crawl_url(url, config, task_id, semaphore=semaphore) ) tasks.append(task) # Wait for all created tasks to complete # asyncio.gather runs them concurrently, respecting the semaphore limit results = await asyncio.gather(*tasks, return_exceptions=True) # Process results (handle potential exceptions returned by gather) final_results = [] for res in results: if isinstance(res, Exception): # Handle case where gather caught an exception from a task # You might create a failed CrawlerTaskResult here pass elif isinstance(res, CrawlerTaskResult): final_results.append(res) return final_results finally: if self.monitor: self.monitor.stop() # run_urls_stream would have similar logic but use asyncio.as_completed # or manage tasks manually to yield results as they finish. The key takeaway is that the Dispatcher orchestrates calls to the single-page crawler.arun method, wrapping them with concurrency controls (like the async with semaphore: block) before running them using asyncio’s concurrency tools (asyncio.create_task, asyncio.gather, etc.). ",
    "url": "/Crawl4AI/10_basedispatcher.html#a-glimpse-under-the-hood",
    
    "relUrl": "/Crawl4AI/10_basedispatcher.html#a-glimpse-under-the-hood"
  },"342": {
    "doc": "BaseDispatcher",
    "title": "Conclusion",
    "content": "You’ve learned about BaseDispatcher, the crucial “Traffic Controller” that manages concurrent crawls in Crawl4AI, especially for arun_many. | It solves the problem of efficiently running many crawls without overloading systems or websites. | It acts as a blueprint for managing concurrency. | Key implementations: . | SemaphoreDispatcher: Uses a simple count limit. | MemoryAdaptiveDispatcher: Adjusts concurrency based on system memory (the default for arun_many). | . | The dispatcher is used automatically by arun_many, but you can provide a specific instance if needed. | It orchestrates the execution of individual crawl tasks, respecting defined limits. | . Understanding the dispatcher helps appreciate how Crawl4AI handles large-scale crawling tasks responsibly and efficiently. This concludes our tour of the core concepts in Crawl4AI! We’ve covered how pages are fetched, how the process is managed, how content is cleaned, filtered, and extracted, how deep crawls are performed, how caching optimizes fetches, and finally, how concurrency is managed. You now have a solid foundation to start building powerful web data extraction and processing applications with Crawl4AI. Happy crawling! . Generated by AI Codebase Knowledge Builder . ",
    "url": "/Crawl4AI/10_basedispatcher.html#conclusion",
    
    "relUrl": "/Crawl4AI/10_basedispatcher.html#conclusion"
  },"343": {
    "doc": "BaseDispatcher",
    "title": "BaseDispatcher",
    "content": " ",
    "url": "/Crawl4AI/10_basedispatcher.html",
    
    "relUrl": "/Crawl4AI/10_basedispatcher.html"
  },"344": {
    "doc": "System Design",
    "title": "System Design: Codebase Knowledge Builder",
    "content": "Please DON’T remove notes for AI . ",
    "url": "/design.html#system-design-codebase-knowledge-builder",
    
    "relUrl": "/design.html#system-design-codebase-knowledge-builder"
  },"345": {
    "doc": "System Design",
    "title": "Requirements",
    "content": "Notes for AI: Keep it simple and clear. If the requirements are abstract, write concrete user stories . User Story: As a developer onboarding to a new codebase, I want a tutorial automatically generated from its GitHub repository or local directory, optionally in a specific language. This tutorial should explain the core abstractions, their relationships (visualized), and how they work together, using beginner-friendly language, analogies, and multi-line descriptions where needed, so I can understand the project structure and key concepts quickly without manually digging through all the code. Input: . | A publicly accessible GitHub repository URL or a local directory path. | A project name (optional, will be derived from the URL/directory if not provided). | Desired language for the tutorial (optional, defaults to English). | . Output: . | A directory named after the project containing: . | An index.md file with: . | A high-level project summary (potentially translated). | A Mermaid flowchart diagram visualizing relationships between abstractions (using potentially translated names/labels). | An ordered list of links to chapter files (using potentially translated names). | . | Individual Markdown files for each chapter (01_chapter_one.md, 02_chapter_two.md, etc.) detailing core abstractions in a logical order (potentially translated content). | . | . ",
    "url": "/design.html#requirements",
    
    "relUrl": "/design.html#requirements"
  },"346": {
    "doc": "System Design",
    "title": "Flow Design",
    "content": "Notes for AI: . | Consider the design patterns of agent, map-reduce, rag, and workflow. Apply them if they fit. | Present a concise, high-level description of the workflow. | . Applicable Design Pattern: . This project primarily uses a Workflow pattern to decompose the tutorial generation process into sequential steps. The chapter writing step utilizes a BatchNode (a form of MapReduce) to process each abstraction individually. | Workflow: The overall process follows a defined sequence: fetch code -&gt; identify abstractions -&gt; analyze relationships -&gt; determine order -&gt; write chapters -&gt; combine tutorial into files. | Batch Processing: The WriteChapters node processes each identified abstraction independently (map) before the final tutorial files are structured (reduce). | . Flow high-level Design: . | FetchRepo: Crawls the specified GitHub repository URL or local directory using appropriate utility (crawl_github_files or crawl_local_files), retrieving relevant source code file contents. | IdentifyAbstractions: Analyzes the codebase using an LLM to identify up to 10 core abstractions, generate beginner-friendly descriptions (potentially translated if language != English), and list the indices of files related to each abstraction. | AnalyzeRelationships: Uses an LLM to analyze the identified abstractions (referenced by index) and their related code to generate a high-level project summary and describe the relationships/interactions between these abstractions (summary and labels potentially translated if language != English), specifying source and target abstraction indices and a concise label for each interaction. | OrderChapters: Determines the most logical order (as indices) to present the abstractions in the tutorial, considering input context which might be translated. The output order itself is language-independent. | WriteChapters (BatchNode): Iterates through the ordered list of abstraction indices. For each abstraction, it calls an LLM to write a detailed, beginner-friendly chapter (content potentially fully translated if language != English), using the relevant code files (accessed via indices) and summaries of previously generated chapters (potentially translated) as context. | CombineTutorial: Creates an output directory, generates a Mermaid diagram from the relationship data (using potentially translated names/labels), and writes the project summary (potentially translated), relationship diagram, chapter links (using potentially translated names), and individually generated chapter files (potentially translated content) into it. Fixed text like “Chapters”, “Source Repository”, and the attribution footer remain in English. | . flowchart TD A[FetchRepo] --&gt; B[IdentifyAbstractions]; B --&gt; C[AnalyzeRelationships]; C --&gt; D[OrderChapters]; D --&gt; E[Batch WriteChapters]; E --&gt; F[CombineTutorial]; . ",
    "url": "/design.html#flow-design",
    
    "relUrl": "/design.html#flow-design"
  },"347": {
    "doc": "System Design",
    "title": "Utility Functions",
    "content": "Notes for AI: . | Understand the utility function definition thoroughly by reviewing the doc. | Include only the necessary utility functions, based on nodes in the flow. | . | crawl_github_files (utils/crawl_github_files.py) - External Dependency: requests, gitpython (optional for SSH) . | Input: repo_url (str), token (str, optional), max_file_size (int, optional), use_relative_paths (bool, optional), include_patterns (set, optional), exclude_patterns (set, optional) | Output: dict containing files (dict[str, str]) and stats. | Necessity: Required by FetchRepo to download and read source code from GitHub if a repo_url is provided. Handles API calls or SSH cloning, filtering, and file reading. | . | crawl_local_files (utils/crawl_local_files.py) - External Dependency: None . | Input: directory (str), max_file_size (int, optional), use_relative_paths (bool, optional), include_patterns (set, optional), exclude_patterns (set, optional) | Output: dict containing files (dict[str, str]). | Necessity: Required by FetchRepo to read source code from a local directory if a local_dir path is provided. Handles directory walking, filtering, and file reading. | . | call_llm (utils/call_llm.py) - External Dependency: LLM Provider API (e.g., Google GenAI) . | Input: prompt (str), use_cache (bool, optional) | Output: response (str) | Necessity: Used by IdentifyAbstractions, AnalyzeRelationships, OrderChapters, and WriteChapters for code analysis and content generation. Needs careful prompt engineering and YAML validation (implicit via yaml.safe_load which raises errors). | . | . ",
    "url": "/design.html#utility-functions",
    
    "relUrl": "/design.html#utility-functions"
  },"348": {
    "doc": "System Design",
    "title": "Node Design",
    "content": "Shared Store . Notes for AI: Try to minimize data redundancy . The shared Store structure is organized as follows: . shared = { # --- Inputs --- \"repo_url\": None, # Provided by the user/main script if using GitHub \"local_dir\": None, # Provided by the user/main script if using local directory \"project_name\": None, # Optional, derived from repo_url/local_dir if not provided \"github_token\": None, # Optional, from argument or environment variable \"output_dir\": \"output\", # Default or user-specified base directory for output \"include_patterns\": set(), # File patterns to include \"exclude_patterns\": set(), # File patterns to exclude \"max_file_size\": 100000, # Default or user-specified max file size \"language\": \"english\", # Default or user-specified language for the tutorial # --- Intermediate/Output Data --- \"files\": [], # Output of FetchRepo: List of tuples (file_path: str, file_content: str) \"abstractions\": [], # Output of IdentifyAbstractions: List of {\"name\": str (potentially translated), \"description\": str (potentially translated), \"files\": [int]} (indices into shared[\"files\"]) \"relationships\": { # Output of AnalyzeRelationships \"summary\": None, # Overall project summary (potentially translated) \"details\": [] # List of {\"from\": int, \"to\": int, \"label\": str (potentially translated)} describing relationships between abstraction indices. }, \"chapter_order\": [], # Output of OrderChapters: List of indices into shared[\"abstractions\"], determining tutorial order \"chapters\": [], # Output of WriteChapters: List of chapter content strings (Markdown, potentially translated), ordered according to chapter_order \"final_output_dir\": None # Output of CombineTutorial: Path to the final generated tutorial directory (e.g., \"output/my_project\") } . Node Steps . Notes for AI: Carefully decide whether to use Batch/Async Node/Flow. Removed explicit try/except in exec, relying on Node’s built-in fault tolerance. | FetchRepo . | Purpose: Download the repository code (from GitHub) or read from a local directory, loading relevant files into memory using the appropriate crawler utility. | Type: Regular | Steps: . | prep: Read repo_url, local_dir, project_name, github_token, output_dir, include_patterns, exclude_patterns, max_file_size from shared store. Determine project_name from repo_url or local_dir if not present in shared. Set use_relative_paths flag. | exec: If repo_url is present, call crawl_github_files(...). Otherwise, call crawl_local_files(...). Convert the resulting files dictionary into a list of (path, content) tuples. | post: Write the list of files tuples and the derived project_name (if applicable) to the shared store. | . | . | IdentifyAbstractions . | Purpose: Analyze the code to identify key concepts/abstractions using indices. Generates potentially translated names and descriptions if language is not English. | Type: Regular | Steps: . | prep: Read files (list of tuples), project_name, and language from shared store. Create context using create_llm_context helper which adds file indices. Format the list of index # path for the prompt. | exec: Construct a prompt for call_llm. If language is not English, add instructions to generate name and description in the target language. Ask LLM to identify ~5-10 core abstractions, provide a simple description for each, and list the relevant file indices (e.g., - 0 # path/to/file.py). Request YAML list output. Parse and validate the YAML, ensuring indices are within bounds and converting entries like 0 # path... to just the integer 0. | post: Write the validated list of abstractions (e.g., [{\"name\": \"Node\", \"description\": \"...\", \"files\": [0, 3, 5]}, ...]) containing file indices and potentially translated name/description to the shared store. | . | . | AnalyzeRelationships . | Purpose: Generate a project summary and describe how the identified abstractions interact using indices and concise labels. Generates potentially translated summary and labels if language is not English. | Type: Regular | Steps: . | prep: Read abstractions, files, project_name, and language from shared store. Format context for the LLM, including potentially translated abstraction names and indices, potentially translated descriptions, and content snippets from related files (referenced by index # path using get_content_for_indices helper). Prepare the list of index # AbstractionName (potentially translated) for the prompt. | exec: Construct a prompt for call_llm. If language is not English, add instructions to generate summary and label in the target language, and note that input names might be translated. Ask for (1) a high-level summary and (2) a list of relationships, each specifying from_abstraction (e.g., 0 # Abstraction1), to_abstraction (e.g., 1 # Abstraction2), and a concise label. Request structured YAML output. Parse and validate, converting referenced abstractions to indices (from: 0, to: 1). | post: Parse the LLM response and write the relationships dictionary ({\"summary\": \"...\", \"details\": [{\"from\": 0, \"to\": 1, \"label\": \"...\"}, ...]}) with indices and potentially translated summary/label to the shared store. | . | . | OrderChapters . | Purpose: Determine the sequence (as indices) in which abstractions should be presented. Considers potentially translated input context. | Type: Regular | Steps: . | prep: Read abstractions, relationships, project_name, and language from the shared store. Prepare context including the list of index # AbstractionName (potentially translated) and textual descriptions of relationships referencing indices and using the potentially translated label. Note in context if summary/names might be translated. | exec: Construct a prompt for call_llm asking it to order the abstractions based on importance, foundational concepts, or dependencies. Request output as an ordered YAML list of index # AbstractionName. Parse and validate, extracting only the indices and ensuring all are present exactly once. | post: Write the validated ordered list of indices (chapter_order) to the shared store. | . | . | WriteChapters . | Purpose: Generate the detailed content for each chapter of the tutorial. Generates potentially fully translated chapter content if language is not English. | Type: BatchNode | Steps: . | prep: Read chapter_order (indices), abstractions, files, project_name, and language from shared store. Initialize an empty instance variable self.chapters_written_so_far. Return an iterable list where each item corresponds to an abstraction index from chapter_order. Each item should contain chapter number, potentially translated abstraction details, a map of related file content ({ \"idx # path\": content }), full chapter listing (potentially translated names), chapter filename map, previous/next chapter info (potentially translated names), and language. | exec(item): Construct a prompt for call_llm. If language is not English, add detailed instructions to write the entire chapter in the target language, translating explanations, examples, etc., while noting which input context might already be translated. Ask LLM to write a beginner-friendly Markdown chapter. Provide potentially translated concept details. Include a summary of previously written chapters (potentially translated). Provide relevant code snippets. Add the generated (potentially translated) chapter content to self.chapters_written_so_far for the next iteration’s context. Return the chapter content. | post(shared, prep_res, exec_res_list): exec_res_list contains the generated chapter Markdown content strings (potentially translated), ordered correctly. Assign this list directly to shared[\"chapters\"]. Clean up self.chapters_written_so_far. | . | . | CombineTutorial . | Purpose: Assemble the final tutorial files, including a Mermaid diagram using potentially translated labels/names. Fixed text remains English. | Type: Regular | Steps: . | prep: Read project_name, relationships (potentially translated summary/labels), chapter_order (indices), abstractions (potentially translated name/desc), chapters (list of potentially translated content), repo_url, and output_dir from shared store. Generate a Mermaid flowchart TD string based on relationships[\"details\"], using indices to identify nodes (potentially translated names) and the concise label (potentially translated) for edges. Construct the content for index.md (including potentially translated summary, Mermaid diagram, and ordered links to chapters using potentially translated names derived using chapter_order and abstractions). Define the output directory path (e.g., ./output_dir/project_name). Prepare a list of { \"filename\": \"01_...\", \"content\": \"...\" } for chapters, adding the English attribution footer to each chapter’s content. Add the English attribution footer to the index content. | exec: Create the output directory. Write the generated index.md content. Iterate through the prepared chapter file list and write each chapter’s content to its corresponding .md file in the output directory. | post: Write the final output_path to shared[\"final_output_dir\"]. Log completion. | . | . | . ",
    "url": "/design.html#node-design",
    
    "relUrl": "/design.html#node-design"
  },"349": {
    "doc": "System Design",
    "title": "System Design",
    "content": " ",
    "url": "/design.html",
    
    "relUrl": "/design.html"
  },"350": {
    "doc": "Crawl4AI",
    "title": "Tutorial: Crawl4AI",
    "content": "This tutorial is AI-generated! To learn more, check out AI Codebase Knowledge Builder . Crawl4AIView Repo is a flexible Python library for asynchronously crawling websites and extracting structured content, specifically designed for AI use cases. You primarily interact with the AsyncWebCrawler, which acts as the main coordinator. You provide it with URLs and a CrawlerRunConfig detailing how to crawl (e.g., using specific strategies for fetching, scraping, filtering, and extraction). It can handle single pages or multiple URLs concurrently using a BaseDispatcher, optionally crawl deeper by following links via DeepCrawlStrategy, manage CacheMode, and apply RelevantContentFilter before finally returning a CrawlResult containing all the gathered data. flowchart TD A0[\"AsyncWebCrawler\"] A1[\"CrawlerRunConfig\"] A2[\"AsyncCrawlerStrategy\"] A3[\"ContentScrapingStrategy\"] A4[\"ExtractionStrategy\"] A5[\"CrawlResult\"] A6[\"BaseDispatcher\"] A7[\"DeepCrawlStrategy\"] A8[\"CacheContext / CacheMode\"] A9[\"RelevantContentFilter\"] A0 -- \"Configured by\" --&gt; A1 A0 -- \"Uses Fetching Strategy\" --&gt; A2 A0 -- \"Uses Scraping Strategy\" --&gt; A3 A0 -- \"Uses Extraction Strategy\" --&gt; A4 A0 -- \"Produces\" --&gt; A5 A0 -- \"Uses Dispatcher for `arun_m...\" --&gt; A6 A0 -- \"Uses Caching Logic\" --&gt; A8 A6 -- \"Calls Crawler's `arun`\" --&gt; A0 A1 -- \"Specifies Deep Crawl Strategy\" --&gt; A7 A7 -- \"Processes Links from\" --&gt; A5 A3 -- \"Provides Cleaned HTML to\" --&gt; A9 A1 -- \"Specifies Content Filter\" --&gt; A9 . ",
    "url": "/Crawl4AI/#tutorial-crawl4ai",
    
    "relUrl": "/Crawl4AI/#tutorial-crawl4ai"
  },"351": {
    "doc": "Crawl4AI",
    "title": "Crawl4AI",
    "content": " ",
    "url": "/Crawl4AI/",
    
    "relUrl": "/Crawl4AI/"
  },"352": {
    "doc": "My Tutorial for Mojo v2",
    "title": "Tutorial: mojo",
    "content": "The mojo project provides a core data structure called NDBuffer, which is a non-owning, multi-dimensional view into a block of memory, similar to a tensor. It’s designed for high-performance computing by allowing direct memory manipulation via UnsafePointers and supporting efficient element access through Strides and Offset Computation. NDBuffer can handle both statically known and dynamically determined dimension sizes using DimList and Dim abstractions, and it facilitates SIMD Data Access for accelerated, vectorized operations on data. Source Repository: None . flowchart TD A0[\"NDBuffer \"] A1[\"DimList and Dim \"] A2[\"UnsafePointer (as used by NDBuffer) \"] A3[\"Strides and Offset Computation \"] A4[\"SIMD Data Access \"] A0 -- \"Uses for shape/strides\" --&gt; A1 A0 -- \"References memory via\" --&gt; A2 A0 -- \"Locates elements using\" --&gt; A3 A0 -- \"Offers\" --&gt; A4 A4 -- \"Performs loads/stores on\" --&gt; A2 . ",
    "url": "/mojo-v2/#tutorial-mojo",
    
    "relUrl": "/mojo-v2/#tutorial-mojo"
  },"353": {
    "doc": "My Tutorial for Mojo v2",
    "title": "Chapters",
    "content": ". | UnsafePointer (as used by NDBuffer) | DimList and Dim | NDBuffer | Strides and Offset Computation | SIMD Data Access | . Generated by AI Codebase Knowledge Builder . ",
    "url": "/mojo-v2/#chapters",
    
    "relUrl": "/mojo-v2/#chapters"
  },"354": {
    "doc": "My Tutorial for Mojo v2",
    "title": "My Tutorial for Mojo v2",
    "content": " ",
    "url": "/mojo-v2/",
    
    "relUrl": "/mojo-v2/"
  },"355": {
    "doc": "My Tutorial for Crawl4ai",
    "title": "Tutorial: My Crawl4ai",
    "content": "Crawl4ai is an intelligent web crawling framework designed specifically for AI applications. It provides asynchronous web crawling capabilities with configurable strategies for content extraction, deep website exploration, and smart URL prioritization. The library manages resources efficiently through its dispatcher framework and caching system, while offering flexible logging and content filtering. It can be deployed as a service via Docker with API endpoints, making it suitable for both simple data collection and complex distributed crawling tasks. Source Repository: https://github.com/unclecode/crawl4ai . flowchart TD A0[\"AsyncWebCrawler\"] A1[\"Configuration System\"] A2[\"Content Extraction Pipeline\"] A3[\"Deep Crawling System\"] A4[\"Dispatcher Framework\"] A5[\"Caching System\"] A6[\"URL Filtering &amp; Scoring\"] A7[\"Async Logging Infrastructure\"] A8[\"API &amp; Docker Integration\"] A0 -- \"Initializes with and uses\" --&gt; A1 A0 -- \"Processes HTML with\" --&gt; A2 A0 -- \"Delegates concurrent crawli...\" --&gt; A4 A0 -- \"Stores and retrieves conten...\" --&gt; A5 A0 -- \"Records operations through\" --&gt; A7 A3 -- \"Uses for page processing\" --&gt; A0 A3 -- \"Prioritizes URLs with\" --&gt; A6 A4 -- \"Coordinates multiple instan...\" --&gt; A0 A8 -- \"Exposes functionality of\" --&gt; A0 A1 -- \"Configures strategies for\" --&gt; A2 . ",
    "url": "/my-crawl4AI/#tutorial-my-crawl4ai",
    
    "relUrl": "/my-crawl4AI/#tutorial-my-crawl4ai"
  },"356": {
    "doc": "My Tutorial for Crawl4ai",
    "title": "Chapters",
    "content": ". | Configuration System | AsyncWebCrawler | Content Extraction Pipeline | URL Filtering &amp; Scoring | Deep Crawling System | Caching System | Dispatcher Framework | Async Logging Infrastructure | API &amp; Docker Integration | . Generated by AI Codebase Knowledge Builder . ",
    "url": "/my-crawl4AI/#chapters",
    
    "relUrl": "/my-crawl4AI/#chapters"
  },"357": {
    "doc": "My Tutorial for Crawl4ai",
    "title": "My Tutorial for Crawl4ai",
    "content": " ",
    "url": "/my-crawl4AI/",
    
    "relUrl": "/my-crawl4AI/"
  },"358": {
    "doc": "My Tutorial for Mojo v1",
    "title": "Tutorial: mojo",
    "content": "The project mojo (as represented by these abstractions) provides a system for high-performance, multi-dimensional array (tensor) operations. It features NDBuffer as a central data structure that acts as a smart view over raw memory, pointed to by an UnsafePointer. The NDBuffer’s structure (shape and memory layout strides) is defined using DimList. Accessing elements in an NDBuffer is done using IndexList for coordinates, which are then converted to a 1D memory offset by strided N-D to 1D indexing logic. The AddressSpace parameter allows these operations to be tailored for different memory systems, like CPU or GPU, for optimal performance. Source Repository: None . flowchart TD A0[\"NDBuffer \"] A1[\"DimList \"] A2[\"UnsafePointer \"] A3[\"IndexList \"] A4[\"N-D to 1D Indexing Logic (Strided Memory Access) \"] A5[\"AddressSpace \"] A0 -- \"Uses for shape/stride defin...\" --&gt; A1 A0 -- \"Points to data via\" --&gt; A2 A0 -- \"Applies for element access\" --&gt; A4 A4 -- \"Takes coordinates as\" --&gt; A3 A2 -- \"Parameterized by memory type\" --&gt; A5 . ",
    "url": "/mojo-v1/#tutorial-mojo",
    
    "relUrl": "/mojo-v1/#tutorial-mojo"
  },"359": {
    "doc": "My Tutorial for Mojo v1",
    "title": "Chapters",
    "content": ". | AddressSpace | UnsafePointer | IndexList | DimList | NDBuffer | N-D to 1D Indexing Logic (Strided Memory Access) | . Generated by AI Codebase Knowledge Builder . ",
    "url": "/mojo-v1/#chapters",
    
    "relUrl": "/mojo-v1/#chapters"
  },"360": {
    "doc": "My Tutorial for Mojo v1",
    "title": "My Tutorial for Mojo v1",
    "content": " ",
    "url": "/mojo-v1/",
    
    "relUrl": "/mojo-v1/"
  },"361": {
    "doc": "Home",
    "title": "Turns Codebase into Easy Tutorial - Pocket Flow",
    "content": "Ever stared at a new codebase written by others feeling completely lost? This project analyzes GitHub repositories and creates beginner-friendly tutorials explaining exactly how the code works - all powered by AI! Our intelligent system automatically breaks down complex codebases into digestible explanations that even beginners can understand. ",
    "url": "/#turns-codebase-into-easy-tutorial---pocket-flow",
    
    "relUrl": "/#turns-codebase-into-easy-tutorial---pocket-flow"
  },"362": {
    "doc": "Home",
    "title": "PF-Understand Documentation",
    "content": "Welcome to the documentation for PF-Understand, a tool for generating comprehensive tutorials from codebases. ",
    "url": "/#pf-understand-documentation",
    
    "relUrl": "/#pf-understand-documentation"
  },"363": {
    "doc": "Home",
    "title": "Tutorials",
    "content": "All in all, I did 4 fairly complete runs of Pocket Flow tutorializing abstractions. | Crawl4AI - The original tutorial already done in Pocket Flow’s repo | My Crawl4AI - My sanity check run for an that repo | Modular Max - When doing the entire Modular repo, it chose to do Max Engine, ignoring Mojo almost entirely. | Mojo v1 - When focusing Pocket Flow on just the mojo folder in Modular’s repo, a tutorial on its high-performance multi-dimensional array operation | Mojo v2 - The final run of Pocket Flow which ran smoothly (no errors in about 1.75 hours) on resulted in a different tutorial for Mojo, this time focusing on Mojo’s N-dimensional data structures | . Using gemini-2.5-pro in Cursor, I did the following two comparisons: . | TBD: the first run above, sanity check on Crawl4AI in the exisitng repo in output/crawl4ai/. | TBD: the last two runs above, the two different versions of tutorials for Mojo. I also did a comparison | . ",
    "url": "/#tutorials",
    
    "relUrl": "/#tutorials"
  },"364": {
    "doc": "Home",
    "title": "Other notes TBD",
    "content": " ",
    "url": "/#other-notes-tbd",
    
    "relUrl": "/#other-notes-tbd"
  },"365": {
    "doc": "Home",
    "title": "Home",
    "content": " ",
    "url": "/",
    
    "relUrl": "/"
  },"366": {
    "doc": "My Tutorial for Modular's Max",
    "title": "Tutorial: modular",
    "content": "The project, modular, is a high-performance serving framework for Large Language Models (LLMs). It allows users to easily deploy and run LLMs, handling incoming requests, orchestrating the generation process, and streaming back responses. Key features include efficient request batching, advanced KV cache management for speed, and a modular architecture to manage different parts of the serving stack like API endpoints, model execution, and telemetry. Source Repository: None . flowchart TD A0[\"Serving API Layer (FastAPI App &amp; Routers) \"] A1[\"LLM Pipeline Orchestrator (`TokenGeneratorPipeline`) \"] A2[\"Model Worker \"] A3[\"Scheduler (`TokenGenerationScheduler`, `EmbeddingsScheduler`) \"] A4[\"EngineQueue \"] A5[\"Settings (`Settings` class) \"] A6[\"Telemetry and Metrics (`METRICS`, `MetricClient`) \"] A7[\"KV Cache Management \"] A0 -- \"Forwards requests\" --&gt; A1 A0 -- \"Reads config\" --&gt; A5 A1 -- \"Sends tasks to\" --&gt; A4 A1 -- \"Reports metrics\" --&gt; A6 A2 -- \"Uses\" --&gt; A3 A2 -- \"Uses for inference\" --&gt; A7 A3 -- \"Uses for tasks\" --&gt; A4 A3 -- \"Manages cache via\" --&gt; A7 A4 -- \"Delivers tasks to\" --&gt; A2 A5 -- \"Configures\" --&gt; A0 A5 -- \"Configures telemetry\" --&gt; A6 A7 -- \"Agent started by\" --&gt; A0 . ",
    "url": "/modular_max/#tutorial-modular",
    
    "relUrl": "/modular_max/#tutorial-modular"
  },"367": {
    "doc": "My Tutorial for Modular's Max",
    "title": "Chapters",
    "content": ". | Settings (Settings class) | Serving API Layer (FastAPI App &amp; Routers) | LLM Pipeline Orchestrator (TokenGeneratorPipeline) | Model Worker | Scheduler (TokenGenerationScheduler, EmbeddingsScheduler) | KV Cache Management | EngineQueue | Telemetry and Metrics (METRICS, MetricClient) | . Generated by AI Codebase Knowledge Builder . ",
    "url": "/modular_max/#chapters",
    
    "relUrl": "/modular_max/#chapters"
  },"368": {
    "doc": "My Tutorial for Modular's Max",
    "title": "My Tutorial for Modular's Max",
    "content": " ",
    "url": "/modular_max/",
    
    "relUrl": "/modular_max/"
  }
}
