{"0": {
    "doc": "Chapter 1: AddressSpace",
    "title": "Chapter 1: Understanding Memory Neighborhoods with AddressSpace",
    "content": "Welcome to your first step into the world of Mojo! In this series, we’ll explore some fundamental concepts that help Mojo achieve its impressive performance, especially when dealing with different kinds of hardware like CPUs and GPUs. Our first topic is AddressSpace. It might sound a bit technical, but we’ll break it down with simple analogies. ",
    "url": "/mojo-v1/01_addressspace_.html#chapter-1-understanding-memory-neighborhoods-with-addressspace",
    
    "relUrl": "/mojo-v1/01_addressspace_.html#chapter-1-understanding-memory-neighborhoods-with-addressspace"
  },"1": {
    "doc": "Chapter 1: AddressSpace",
    "title": "What is AddressSpace?",
    "content": "Imagine a big city. This city has many different neighborhoods: . | Downtown (CPU RAM): This is your general-purpose area. Lots of things happen here, it’s accessible to everyone, and it’s where most everyday tasks are carried out. | Industrial Zone (GPU Global Memory): A large area on specialized hardware (like a Graphics Processing Unit, or GPU) designed for heavy-duty work. | Specialized Workshops (GPU Shared/Constant Memory): Smaller, super-fast areas within the Industrial Zone, designed for specific tasks or for workers (GPU threads) to collaborate very quickly. | . In Mojo, AddressSpace is a way to tell the system which “neighborhood” in memory a piece of data lives in. The official description you saw earlier puts it nicely: . AddressSpace is a parameter that specifies the type or region of memory where the data an NDBuffer (a type we’ll see later) points to is located. Think of it like different neighborhoods in a city, each with its own characteristics and accessibility rules. For instance, AddressSpace.GENERIC might refer to standard CPU RAM, while other address spaces like _GPUAddressSpace.SHARED or _GPUAddressSpace.CONSTANT would indicate memory on a GPU with specific properties (e.g., shared among GPU threads or read-only constant data). The choice of AddressSpace is critical for performance and data management in systems with diverse memory types, particularly when programming for GPUs. So, AddressSpace helps Mojo understand the properties and location of memory. ",
    "url": "/mojo-v1/01_addressspace_.html#what-is-addressspace",
    
    "relUrl": "/mojo-v1/01_addressspace_.html#what-is-addressspace"
  },"2": {
    "doc": "Chapter 1: AddressSpace",
    "title": "Why Do We Need Different Memory “Neighborhoods”?",
    "content": "Why not just have one giant memory area for everything? . | Speed and Performance: Different types of memory have different speeds. | CPU RAM (our “Downtown”) is generally fast for a wide variety of tasks. | GPU memory can be incredibly fast for certain parallel computations but might have different access patterns. Specialized parts of GPU memory (like “Workshops”) are even faster for specific uses. Telling Mojo where data is allows it to optimize how that data is accessed. | . | Special Capabilities: Some memory regions have special properties. | For example, “shared memory” on a GPU allows multiple GPU processing units (threads) to share data very quickly, which is great for collaborative tasks. | “Constant memory” on a GPU is for data that doesn’t change; the GPU can often cache this aggressively for faster reads. | . | Hardware Differences: Modern computers often have both a CPU and one or more GPUs. These components have their own memory systems. AddressSpace helps manage data across these different pieces of hardware. | . Knowing the AddressSpace allows Mojo (and you, the programmer!) to make smart decisions about how to store and access data for optimal performance and correctness. ",
    "url": "/mojo-v1/01_addressspace_.html#why-do-we-need-different-memory-neighborhoods",
    
    "relUrl": "/mojo-v1/01_addressspace_.html#why-do-we-need-different-memory-neighborhoods"
  },"3": {
    "doc": "Chapter 1: AddressSpace",
    "title": "AddressSpace in Mojo: A Closer Look",
    "content": "Let’s peek at how AddressSpace is defined in Mojo’s standard library. You’ll find it in a file named stdlib/src/memory/pointer.mojo. Don’t worry about understanding all the code details yet; we’ll focus on the main ideas. // From stdlib/src/memory/pointer.mojo @value @register_passable(\"trivial\") struct AddressSpace(...): \"\"\"Address space of the pointer.\"\"\" var _value: Int // Each address space has an underlying integer ID alias GENERIC = AddressSpace(0) // The most common one! \"\"\"Generic address space.\"\"\" // ... other details and functions ... @always_inline(\"builtin\") fn __init__(out self, value: Int): \"\"\"Initializes the address space from the underlying integral value.\"\"\" self._value = value @always_inline(\"builtin\") fn __init__(out self, value: _GPUAddressSpace): // Can also be made from a _GPUAddressSpace type \"\"\"Initializes the address space from the underlying integral value.\"\"\" self._value = value._value // ... later in the same file ... @value @register_passable(\"trivial\") struct _GPUAddressSpace(EqualityComparable): var _value: Int // These are like pre-defined AddressSpace values for common GPU memory types alias GENERIC = AddressSpace(0) // GPUs can also access generic memory alias GLOBAL = AddressSpace(1) \"\"\"Global address space (typically main GPU memory).\"\"\" alias SHARED = AddressSpace(3) \"\"\"Shared address space (fast memory for GPU thread groups).\"\"\" alias CONSTANT = AddressSpace(4) \"\"\"Constant address space (read-only, often cached).\"\"\" alias LOCAL = AddressSpace(5) \"\"\"Local address space (private to a single GPU thread).\"\"\" // ... other details ... Here’s what this means for a beginner: . | struct AddressSpace: This defines the AddressSpace type. | The decorators @value and @register_passable(\"trivial\") tell Mojo that AddressSpace is a simple value type that can be handled very efficiently. Think of it like a number. | var _value: Int: Internally, each AddressSpace is represented by an integer. For example, 0 means generic, 1 might mean global GPU memory, and so on. | alias GENERIC = AddressSpace(0): This is a very important line! AddressSpace.GENERIC is the most common address space. It usually refers to standard CPU memory. If you don’t specify an address space, this is often the default. | . | struct _GPUAddressSpace: This struct helps define common address spaces used with GPUs. | alias GLOBAL = AddressSpace(1): _GPUAddressSpace.GLOBAL is a convenient name for AddressSpace(1), which typically represents the main memory on a GPU. | alias SHARED = AddressSpace(3): _GPUAddressSpace.SHARED refers to AddressSpace(3), a special, fast memory region that groups of GPU threads can use to share data. | alias CONSTANT = AddressSpace(4): _GPUAddressSpace.CONSTANT refers to AddressSpace(4), used for data that won’t change during a computation. GPUs can optimize access to constant memory. | alias LOCAL = AddressSpace(5): _GPUAddressSpace.LOCAL refers to AddressSpace(5), representing memory private to individual GPU threads. | . | . So, you have AddressSpace.GENERIC for general CPU memory. For GPU-specific memory, you can use handy aliases like _GPUAddressSpace.GLOBAL or _GPUAddressSpace.SHARED. These aliases are actually AddressSpace values themselves. ",
    "url": "/mojo-v1/01_addressspace_.html#addressspace-in-mojo-a-closer-look",
    
    "relUrl": "/mojo-v1/01_addressspace_.html#addressspace-in-mojo-a-closer-look"
  },"4": {
    "doc": "Chapter 1: AddressSpace",
    "title": "How is AddressSpace Used?",
    "content": "You’ll encounter AddressSpace as a parameter when you define types that manage memory, like Pointer (which represents a direct memory address) and NDBuffer (which represents an N-dimensional array of data, like a tensor or matrix). Look at how Pointer and NDBuffer use it (simplified from their definitions): . // From stdlib/src/memory/pointer.mojo struct Pointer[ // ... other parameters ... type: AnyType, address_space: AddressSpace = AddressSpace.GENERIC, // Default is GENERIC! ]: // ... // From stdlib/src/buffer/buffer.mojo struct NDBuffer[ // ... other parameters ... type: DType, address_space: AddressSpace = AddressSpace.GENERIC, // Default is GENERIC! // ... ]: // ... Notice the address_space: AddressSpace = AddressSpace.GENERIC part. This means: . | Both Pointer and NDBuffer need to know the AddressSpace of the memory they are dealing with. | By default, if you don’t specify otherwise, they assume the memory is in AddressSpace.GENERIC (i.e., standard CPU RAM). | . Conceptual Example: . Imagine you’re creating a buffer for some calculations. // This is conceptual Mojo-like code to illustrate the idea // A buffer in standard CPU memory (GENERIC is the default) // let cpu_buffer = NDBuffer[Float32, ...]() // A buffer specifically in GPU's global memory // let gpu_global_buffer = NDBuffer[Float32, ..., address_space: _GPUAddressSpace.GLOBAL]() // A buffer in fast GPU shared memory // let gpu_shared_buffer = NDBuffer[Float32, ..., address_space: _GPUAddressSpace.SHARED]() . By specifying the address_space, you’re giving Mojo crucial information that it can use to manage and access the data efficiently. Real-world Impact in NDBuffer: . The NDBuffer code itself makes decisions based on AddressSpace. For example, there’s a helper function in stdlib/src/buffer/buffer.mojo: . @always_inline fn _use_32bit_indexing[address_space: AddressSpace]() -&gt; Bool: return is_gpu() and address_space in ( _GPUAddressSpace.SHARED, _GPUAddressSpace.LOCAL, _GPUAddressSpace.CONSTANT, ) . This function checks if the code is running on a GPU (is_gpu()) and if the NDBuffer’s data is in SHARED, LOCAL, or CONSTANT GPU memory. If so, it might decide to use 32-bit numbers for indexing into the buffer (which can sometimes be faster on GPUs for these memory types) instead of the usual 64-bit numbers. This is a direct example of AddressSpace influencing performance optimizations! . ",
    "url": "/mojo-v1/01_addressspace_.html#how-is-addressspace-used",
    
    "relUrl": "/mojo-v1/01_addressspace_.html#how-is-addressspace-used"
  },"5": {
    "doc": "Chapter 1: AddressSpace",
    "title": "Key Takeaways",
    "content": ". | AddressSpace tells Mojo where data lives in memory (like different neighborhoods in a city). | AddressSpace.GENERIC is the common default, usually meaning standard CPU RAM. | For GPUs, there are specific address spaces like _GPUAddressSpace.GLOBAL (main GPU memory), _GPUAddressSpace.SHARED (fast shared memory for GPU threads), and _GPUAddressSpace.CONSTANT (read-only cached memory). | Knowing the AddressSpace is crucial for performance and correct data handling, especially when working with diverse hardware like GPUs. | You’ll see AddressSpace as a parameter in memory-related types like Pointer and NDBuffer. | . Understanding AddressSpace is a foundational piece in learning how Mojo manages memory and achieves high performance. It’s all about giving the system enough information to make smart choices! . ",
    "url": "/mojo-v1/01_addressspace_.html#key-takeaways",
    
    "relUrl": "/mojo-v1/01_addressspace_.html#key-takeaways"
  },"6": {
    "doc": "Chapter 1: AddressSpace",
    "title": "Next Steps",
    "content": "Now that you have a basic understanding of AddressSpace, we’re ready to explore how Mojo represents direct memory locations. In the next chapter, we’ll dive into UnsafePointer, a fundamental building block for memory operations in Mojo. Generated by AI Codebase Knowledge Builder . ",
    "url": "/mojo-v1/01_addressspace_.html#next-steps",
    
    "relUrl": "/mojo-v1/01_addressspace_.html#next-steps"
  },"7": {
    "doc": "Chapter 1: AddressSpace",
    "title": "Chapter 1: AddressSpace",
    "content": " ",
    "url": "/mojo-v1/01_addressspace_.html",
    
    "relUrl": "/mojo-v1/01_addressspace_.html"
  },"8": {
    "doc": "AsyncCrawlerStrategy",
    "title": "Chapter 1: How We Fetch Webpages - AsyncCrawlerStrategy",
    "content": "Welcome to the Crawl4AI tutorial series! Our goal is to build intelligent agents that can understand and extract information from the web. The very first step in this process is actually getting the content from a webpage. This chapter explains how Crawl4AI handles that fundamental task. Imagine you need to pick up a package from a specific address. How do you get there and retrieve it? . | You could send a simple, fast drone that just grabs the package off the porch (if it’s easily accessible). This is quick but might fail if the package is inside or requires a signature. | Or, you could send a full delivery truck with a driver. The driver can ring the bell, wait, sign for the package, and even handle complex instructions. This is more versatile but takes more time and resources. | . In Crawl4AI, the AsyncCrawlerStrategy is like choosing your delivery vehicle. It defines how the crawler fetches the raw content (like the HTML, CSS, and maybe JavaScript results) of a webpage. ",
    "url": "/Crawl4AI/01_asynccrawlerstrategy.html#chapter-1-how-we-fetch-webpages---asynccrawlerstrategy",
    
    "relUrl": "/Crawl4AI/01_asynccrawlerstrategy.html#chapter-1-how-we-fetch-webpages---asynccrawlerstrategy"
  },"9": {
    "doc": "AsyncCrawlerStrategy",
    "title": "What Exactly is AsyncCrawlerStrategy?",
    "content": "AsyncCrawlerStrategy is a core concept in Crawl4AI that represents the method or technique used to download the content of a given URL. Think of it as a blueprint: it specifies that we need a way to fetch content, but the specific details of how it’s done can vary. This “blueprint” approach is powerful because it allows us to swap out the fetching mechanism depending on our needs, without changing the rest of our crawling logic. ",
    "url": "/Crawl4AI/01_asynccrawlerstrategy.html#what-exactly-is-asynccrawlerstrategy",
    
    "relUrl": "/Crawl4AI/01_asynccrawlerstrategy.html#what-exactly-is-asynccrawlerstrategy"
  },"10": {
    "doc": "AsyncCrawlerStrategy",
    "title": "The Default: AsyncPlaywrightCrawlerStrategy (The Delivery Truck)",
    "content": "By default, Crawl4AI uses AsyncPlaywrightCrawlerStrategy. This strategy uses a real, automated web browser engine (like Chrome, Firefox, or WebKit) behind the scenes. Why use a full browser? . | Handles JavaScript: Modern websites rely heavily on JavaScript to load content, change the layout, or fetch data after the initial page load. AsyncPlaywrightCrawlerStrategy runs this JavaScript, just like your normal browser does. | Simulates User Interaction: It can wait for elements to appear, handle dynamic content, and see the page after scripts have run. | Gets the “Final” View: It fetches the content as a user would see it in their browser. | . This is our “delivery truck” – powerful and capable of handling complex websites. However, like a real truck, it’s slower and uses more memory and CPU compared to simpler methods. You generally don’t need to do anything to use it, as it’s the default! When you start Crawl4AI, it picks this strategy automatically. ",
    "url": "/Crawl4AI/01_asynccrawlerstrategy.html#the-default-asyncplaywrightcrawlerstrategy-the-delivery-truck",
    
    "relUrl": "/Crawl4AI/01_asynccrawlerstrategy.html#the-default-asyncplaywrightcrawlerstrategy-the-delivery-truck"
  },"11": {
    "doc": "AsyncCrawlerStrategy",
    "title": "Another Option: AsyncHTTPCrawlerStrategy (The Delivery Drone)",
    "content": "Crawl4AI also offers AsyncHTTPCrawlerStrategy. This strategy is much simpler. It directly requests the URL and downloads the initial HTML source code that the web server sends back. Why use this simpler strategy? . | Speed: It’s significantly faster because it doesn’t need to start a browser, render the page, or execute JavaScript. | Efficiency: It uses much less memory and CPU. | . This is our “delivery drone” – super fast and efficient for simple tasks. What’s the catch? . | No JavaScript: It won’t run any JavaScript on the page. If content is loaded dynamically by scripts, this strategy will likely miss it. | Basic HTML Only: You get the raw HTML source, not necessarily what a user sees after the browser processes everything. | . This strategy is great for websites with simple, static HTML content or when you only need the basic structure and metadata very quickly. ",
    "url": "/Crawl4AI/01_asynccrawlerstrategy.html#another-option-asynchttpcrawlerstrategy-the-delivery-drone",
    
    "relUrl": "/Crawl4AI/01_asynccrawlerstrategy.html#another-option-asynchttpcrawlerstrategy-the-delivery-drone"
  },"12": {
    "doc": "AsyncCrawlerStrategy",
    "title": "Why Have Different Strategies? (The Power of Abstraction)",
    "content": "Having AsyncCrawlerStrategy as a distinct concept offers several advantages: . | Flexibility: You can choose the best tool for the job. Need to crawl complex, dynamic sites? Use the default AsyncPlaywrightCrawlerStrategy. Need to quickly fetch basic HTML from thousands of simple pages? Switch to AsyncHTTPCrawlerStrategy. | Maintainability: The logic for fetching content is kept separate from the logic for processing it. | Extensibility: Advanced users could even create their own custom strategies for specialized fetching needs (though that’s beyond this beginner tutorial). | . ",
    "url": "/Crawl4AI/01_asynccrawlerstrategy.html#why-have-different-strategies-the-power-of-abstraction",
    
    "relUrl": "/Crawl4AI/01_asynccrawlerstrategy.html#why-have-different-strategies-the-power-of-abstraction"
  },"13": {
    "doc": "AsyncCrawlerStrategy",
    "title": "How It Works Conceptually",
    "content": "When you ask Crawl4AI to crawl a URL, the main AsyncWebCrawler doesn’t fetch the content itself. Instead, it delegates the task to the currently selected AsyncCrawlerStrategy. Here’s a simplified flow: . sequenceDiagram participant C as AsyncWebCrawler participant S as AsyncCrawlerStrategy participant W as Website C-&gt;&gt;S: Please crawl(\"https://example.com\") Note over S: I'm using my method (e.g., Browser or HTTP) S-&gt;&gt;W: Request Page Content W--&gt;&gt;S: Return Raw Content (HTML, etc.) S--&gt;&gt;C: Here's the result (AsyncCrawlResponse) . The AsyncWebCrawler only needs to know how to talk to any strategy through a common interface (the crawl method). The strategy handles the specific details of the fetching process. ",
    "url": "/Crawl4AI/01_asynccrawlerstrategy.html#how-it-works-conceptually",
    
    "relUrl": "/Crawl4AI/01_asynccrawlerstrategy.html#how-it-works-conceptually"
  },"14": {
    "doc": "AsyncCrawlerStrategy",
    "title": "Using the Default Strategy (You’re Already Doing It!)",
    "content": "Let’s see how you use the default AsyncPlaywrightCrawlerStrategy without even needing to specify it. # main_example.py import asyncio from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode async def main(): # When you create AsyncWebCrawler without specifying a strategy, # it automatically uses AsyncPlaywrightCrawlerStrategy! async with AsyncWebCrawler() as crawler: print(\"Crawler is ready using the default strategy (Playwright).\") # Let's crawl a simple page that just returns HTML # We use CacheMode.BYPASS to ensure we fetch it fresh each time for this demo. config = CrawlerRunConfig(cache_mode=CacheMode.BYPASS) result = await crawler.arun( url=\"https://httpbin.org/html\", config=config ) if result.success: print(\"\\nSuccessfully fetched content!\") # The strategy fetched the raw HTML. # AsyncWebCrawler then processes it (more on that later). print(f\"First 100 chars of fetched HTML: {result.html[:100]}...\") else: print(f\"\\nFailed to fetch content: {result.error_message}\") if __name__ == \"__main__\": asyncio.run(main()) . Explanation: . | We import AsyncWebCrawler and supporting classes. | We create an instance of AsyncWebCrawler() inside an async with block (this handles setup and cleanup). Since we didn’t tell it which strategy to use, it defaults to AsyncPlaywrightCrawlerStrategy. | We call crawler.arun() to crawl the URL. Under the hood, the AsyncPlaywrightCrawlerStrategy starts a browser, navigates to the page, gets the content, and returns it. | We print the first part of the fetched HTML from the result. | . ",
    "url": "/Crawl4AI/01_asynccrawlerstrategy.html#using-the-default-strategy-youre-already-doing-it",
    
    "relUrl": "/Crawl4AI/01_asynccrawlerstrategy.html#using-the-default-strategy-youre-already-doing-it"
  },"15": {
    "doc": "AsyncCrawlerStrategy",
    "title": "Explicitly Choosing the HTTP Strategy",
    "content": "What if you know the page is simple and want the speed of the “delivery drone”? You can explicitly tell AsyncWebCrawler to use AsyncHTTPCrawlerStrategy. # http_strategy_example.py import asyncio from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode # Import the specific strategies we want to use from crawl4ai.async_crawler_strategy import AsyncHTTPCrawlerStrategy async def main(): # 1. Create an instance of the strategy you want http_strategy = AsyncHTTPCrawlerStrategy() # 2. Pass the strategy instance when creating the AsyncWebCrawler async with AsyncWebCrawler(crawler_strategy=http_strategy) as crawler: print(\"Crawler is ready using the explicit HTTP strategy.\") # Crawl the same simple page config = CrawlerRunConfig(cache_mode=CacheMode.BYPASS) result = await crawler.arun( url=\"https://httpbin.org/html\", config=config ) if result.success: print(\"\\nSuccessfully fetched content using HTTP strategy!\") print(f\"First 100 chars of fetched HTML: {result.html[:100]}...\") else: print(f\"\\nFailed to fetch content: {result.error_message}\") if __name__ == \"__main__\": asyncio.run(main()) . Explanation: . | We now also import AsyncHTTPCrawlerStrategy. | We create an instance: http_strategy = AsyncHTTPCrawlerStrategy(). | We pass this instance to the AsyncWebCrawler constructor: AsyncWebCrawler(crawler_strategy=http_strategy). | The rest of the code is the same, but now crawler.arun() will use the faster, simpler HTTP GET request method defined by AsyncHTTPCrawlerStrategy. | . For a simple page like httpbin.org/html, both strategies will likely return the same HTML content, but the HTTP strategy would generally be faster and use fewer resources. On a complex JavaScript-heavy site, the HTTP strategy might fail to get the full content, while the Playwright strategy would handle it correctly. ",
    "url": "/Crawl4AI/01_asynccrawlerstrategy.html#explicitly-choosing-the-http-strategy",
    
    "relUrl": "/Crawl4AI/01_asynccrawlerstrategy.html#explicitly-choosing-the-http-strategy"
  },"16": {
    "doc": "AsyncCrawlerStrategy",
    "title": "A Glimpse Under the Hood",
    "content": "You don’t need to know the deep internals to use the strategies, but it helps to understand the structure. Inside the crawl4ai library, you’d find a file like async_crawler_strategy.py. It defines the “blueprint” (an Abstract Base Class): . # Simplified from async_crawler_strategy.py from abc import ABC, abstractmethod from .models import AsyncCrawlResponse # Defines the structure of the result class AsyncCrawlerStrategy(ABC): \"\"\" Abstract base class for crawler strategies. \"\"\" @abstractmethod async def crawl(self, url: str, **kwargs) -&gt; AsyncCrawlResponse: \"\"\"Fetch content from the URL.\"\"\" pass # Each specific strategy must implement this . And then the specific implementations: . # Simplified from async_crawler_strategy.py from playwright.async_api import Page # Playwright library for browser automation # ... other imports class AsyncPlaywrightCrawlerStrategy(AsyncCrawlerStrategy): # ... (Initialization code to manage browsers) async def crawl(self, url: str, config: CrawlerRunConfig, **kwargs) -&gt; AsyncCrawlResponse: # Uses Playwright to: # 1. Get a browser page # 2. Navigate to the url (page.goto(url)) # 3. Wait for content, run JS, etc. # 4. Get the final HTML (page.content()) # 5. Optionally take screenshots, etc. # 6. Return an AsyncCrawlResponse # ... implementation details ... pass . # Simplified from async_crawler_strategy.py import aiohttp # Library for making HTTP requests asynchronously # ... other imports class AsyncHTTPCrawlerStrategy(AsyncCrawlerStrategy): # ... (Initialization code to manage HTTP sessions) async def crawl(self, url: str, config: CrawlerRunConfig, **kwargs) -&gt; AsyncCrawlResponse: # Uses aiohttp to: # 1. Make an HTTP GET (or other method) request to the url # 2. Read the response body (HTML) # 3. Get response headers and status code # 4. Return an AsyncCrawlResponse # ... implementation details ... pass . The key takeaway is that both strategies implement the same crawl method, allowing AsyncWebCrawler to use them interchangeably. ",
    "url": "/Crawl4AI/01_asynccrawlerstrategy.html#a-glimpse-under-the-hood",
    
    "relUrl": "/Crawl4AI/01_asynccrawlerstrategy.html#a-glimpse-under-the-hood"
  },"17": {
    "doc": "AsyncCrawlerStrategy",
    "title": "Conclusion",
    "content": "You’ve learned about AsyncCrawlerStrategy, the core concept defining how Crawl4AI fetches webpage content. | It’s like choosing a vehicle: a powerful browser (AsyncPlaywrightCrawlerStrategy, the default) or a fast, simple HTTP request (AsyncHTTPCrawlerStrategy). | This abstraction gives you flexibility to choose the right fetching method for your task. | You usually don’t need to worry about it, as the default handles most modern websites well. | . Now that we understand how the raw content is fetched, the next step is to look at the main class that orchestrates the entire crawling process. Next: Let’s dive into the AsyncWebCrawler itself! . Generated by AI Codebase Knowledge Builder . ",
    "url": "/Crawl4AI/01_asynccrawlerstrategy.html#conclusion",
    
    "relUrl": "/Crawl4AI/01_asynccrawlerstrategy.html#conclusion"
  },"18": {
    "doc": "AsyncCrawlerStrategy",
    "title": "AsyncCrawlerStrategy",
    "content": " ",
    "url": "/Crawl4AI/01_asynccrawlerstrategy.html",
    
    "relUrl": "/Crawl4AI/01_asynccrawlerstrategy.html"
  },"19": {
    "doc": "Chapter 1: Settings Class",
    "title": "Chapter 1: Settings (Settings class)",
    "content": "Welcome to the modular tutorial! We’re excited to help you understand how this powerful serving application works, piece by piece. In this first chapter, we’ll dive into one of the most fundamental concepts: the Settings class. Imagine you’re about to launch a new web application. You might need to decide: . | “What web address (host) and port number should it use?” | “How much detail should its logs show?” | “Should it offer an OpenAI-compatible API, or perhaps a SageMaker one?” | . Changing these kinds of parameters directly in the main code can be messy and error-prone, especially as the application grows. This is where the Settings class comes in! . ",
    "url": "/modular_max/01_settings___settings__class__.html#chapter-1-settings-settings-class",
    
    "relUrl": "/modular_max/01_settings___settings__class__.html#chapter-1-settings-settings-class"
  },"20": {
    "doc": "Chapter 1: Settings Class",
    "title": "What Problem Do Settings Solve?",
    "content": "The Settings class acts as the central control panel for the entire modular serving application. Think of it like the main dashboard of a complex machine, like a spaceship. This dashboard has various dials and switches that allow an operator (that’s you!) to adjust how the machine runs without needing to open up the engine and rewire things. For example, you might want to: . | Run the server on port 8080 instead of the default 8000. | Tell the application to expose only an OpenAI-compatible API. | Increase the logging detail to help debug an issue. | . The Settings class allows you to make these kinds of adjustments easily and safely. ",
    "url": "/modular_max/01_settings___settings__class__.html#what-problem-do-settings-solve",
    
    "relUrl": "/modular_max/01_settings___settings__class__.html#what-problem-do-settings-solve"
  },"21": {
    "doc": "Chapter 1: Settings Class",
    "title": "Meet the Settings Class",
    "content": "At its core, the Settings class (found in src/max/serve/config.py) uses a popular Python library called Pydantic. Pydantic helps define what settings are available, what types they should be (e.g., a number, text, a list), and ensures that any values you provide are valid. These settings can be loaded from a few places: . | Environment Variables: These are variables set in your system’s environment (e.g., MAX_SERVE_PORT=8080). | A .env file: This is a special file (literally named .env) you can create in your project directory to list your settings. | Directly in code: When the application starts, it can provide default values or override others. | . Let’s look at some key aspects. Defining a Setting . Here’s a simplified peek at how settings like the server host and port are defined within the Settings class: . # From: src/max/serve/config.py from pydantic import Field from pydantic_settings import BaseSettings, SettingsConfigDict class Settings(BaseSettings): # This tells Pydantic where to look for a .env file # and to allow other environment variables not explicitly defined here. model_config = SettingsConfigDict( env_file=\".env\", extra=\"allow\", ) # Server host configuration host: str = Field( description=\"Hostname to use\", default=\"0.0.0.0\", # Default if not specified elsewhere alias=\"MAX_SERVE_HOST\" # Environment variable to look for ) # Server port configuration port: int = Field( description=\"Port to use\", default=8000, # Default if not specified elsewhere alias=\"MAX_SERVE_PORT\" # Environment variable to look for ) # ... many other settings for logging, model workers, etc... In this snippet: . | host: str means the host setting is expected to be a string (text). | default=\"0.0.0.0\" means if you don’t specify a host, it will use “0.0.0.0”. | alias=\"MAX_SERVE_HOST\" means Pydantic will look for an environment variable named MAX_SERVE_HOST to get the value for host. The Settings class internally uses host, but externally (like in environment variables), you’d use MAX_SERVE_HOST. | . Automatic Validation . Pydantic is smart! If you try to set port to a text value like \"eight-thousand\", Pydantic will raise an error because it expects an integer (int). The Settings class can also have custom validation. For example, it checks if the chosen port is already in use: . # From: src/max/serve/config.py # (Inside the Settings class) @field_validator(\"port\") # This function validates the 'port' field def validate_port(cls, port: int): # (Simplified: code to check if port is available) # If port is in use, it raises a ValueError. # Otherwise, it returns the port. # ... return port . This helps catch configuration mistakes early on. Different Types of Settings . Settings aren’t just numbers and text. They can be more complex, like a list of API types to enable: . # From: src/max/serve/config.py from enum import Enum class APIType(Enum): # Defines allowed API types KSERVE = \"kserve\" OPENAI = \"openai\" SAGEMAKER = \"sagemaker\" class Settings(BaseSettings): # ... other settings ... api_types: list[APIType] = Field( description=\"List of exposed API types.\", default=[APIType.OPENAI, APIType.SAGEMAKER] # Default is a list ) # ... Here, api_types is expected to be a list, and each item in the list must be one of the predefined APIType values. ",
    "url": "/modular_max/01_settings___settings__class__.html#meet-the-settings-class",
    
    "relUrl": "/modular_max/01_settings___settings__class__.html#meet-the-settings-class"
  },"22": {
    "doc": "Chapter 1: Settings Class",
    "title": "How to Use and Change Settings",
    "content": "Let’s say you want to change the server port from the default 8000 to 8080. You have a few ways to do this: . 1. Using Environment Variables: Before running your application, you can set an environment variable in your terminal: . export MAX_SERVE_PORT=8080 # Now run your modular application . When the Settings class is initialized, it will pick up MAX_SERVE_PORT and use 8080 for the port. 2. Using a .env file: Create a file named .env in the root directory of your modular project and add the line: . MAX_SERVE_PORT=8080 . or you can also use the attribute name directly: . port=8080 . The Settings class will automatically read this file when the application starts. 3. Initializing Settings in Code (Less Common for End-Users): Sometimes, settings are specified when the Settings object is created directly in the code. This is often used for internal defaults or specific configurations within the application’s entrypoints. # Simplified from: src/max/entrypoints/cli/serve.py from max.serve.config import Settings # When starting the server, 'modular' might initialize settings like this: # This particular setting 'MAX_SERVE_USE_HEARTBEAT' is an example; # it would override any environment or .env file setting for this specific run. settings_object = Settings(MAX_SERVE_USE_HEARTBEAT=False) print(f\"Host: {settings_object.host}\") # Will show host from env/.env/default print(f\"Port: {settings_object.port}\") # Will show port from env/.env/default print(f\"Use Heartbeat: {settings_object.use_heartbeat}\") # Will show False . Accessing Settings in the Application: Once the Settings object is created and populated, other parts of the modular application can access these values. For example, when the API server is being set up: . # Simplified from: src/max/serve/api_server.py from max.serve.config import Settings # Assume Settings class is defined def configure_my_server(current_settings: Settings): # The application uses the loaded settings host_to_use = current_settings.host port_to_use = current_settings.port print(f\"Server will run on: http://{host_to_use}:{port_to_use}\") # ... actual server setup using these values ... # When the application starts: app_settings = Settings() # Loads from .env, environment variables, or defaults configure_my_server(app_settings) . If you had set MAX_SERVE_PORT=8080, the output would be: . Server will run on: http://0.0.0.0:8080 . (Assuming host remained its default “0.0.0.0”). ",
    "url": "/modular_max/01_settings___settings__class__.html#how-to-use-and-change-settings",
    
    "relUrl": "/modular_max/01_settings___settings__class__.html#how-to-use-and-change-settings"
  },"23": {
    "doc": "Chapter 1: Settings Class",
    "title": "Under the Hood: How Settings Are Loaded",
    "content": "When you create an instance of the Settings class (e.g., my_settings = Settings()), Pydantic performs a sequence of steps to determine the final value for each setting: . | Direct Initialization: If values are passed directly when creating the Settings object (e.g., Settings(MAX_SERVE_PORT=9000)), these take the highest precedence for those specific settings. | Environment Variables: Pydantic then checks for environment variables that match the alias defined for each field (e.g., MAX_SERVE_HOST, MAX_SERVE_PORT). If found, these override values from the .env file and defaults. | .env File: If not found in environment variables, Pydantic looks for the setting in the .env file (specified by model_config = SettingsConfigDict(env_file=\".env\", ...)). It can match either the alias (e.g., MAX_SERVE_PORT=8001) or the field name directly (e.g., port=8001). | Default Values: If the setting isn’t found in any of the above, the default value specified in the class definition is used (e.g., default=8000 for port). | Validation: After a value is determined, any associated validators (like validate_port) are run. | . Here’s a simplified diagram illustrating this loading process: . sequenceDiagram participant App as Application Code participant SettingsObj as Settings() Instance participant Pydantic as Pydantic Logic participant InitArgs as Direct Arguments participant EnvVars as Environment Variables participant DotEnv as .env File participant Defaults as Default Values (in class) App-&gt;&gt;SettingsObj: Create (e.g., Settings(MAX_SERVE_PORT=9000)) SettingsObj-&gt;&gt;Pydantic: Process settings fields Pydantic-&gt;&gt;InitArgs: Check for 'MAX_SERVE_PORT' alt Value provided in InitArgs (e.g., 9000) InitArgs--&gt;&gt;Pydantic: Use 9000 else No value in InitArgs Pydantic-&gt;&gt;EnvVars: Check for 'MAX_SERVE_PORT' alt EnvVar 'MAX_SERVE_PORT' exists (e.g., 8080) EnvVars--&gt;&gt;Pydantic: Use 8080 else EnvVar not found Pydantic-&gt;&gt;DotEnv: Check '.env' for 'MAX_SERVE_PORT' or 'port' alt Value in '.env' (e.g., 8001) DotEnv--&gt;&gt;Pydantic: Use 8001 else Value not in '.env' Pydantic-&gt;&gt;Defaults: Use default for 'port' (e.g., 8000) Defaults--&gt;&gt;Pydantic: Use 8000 end end end Pydantic--&gt;&gt;SettingsObj: Populated 'port' field SettingsObj--&gt;&gt;App: Ready-to-use settings . The actual Settings class is defined in src/max/serve/config.py. It inherits from BaseSettings provided by pydantic-settings, which gives it all this powerful loading and validation capability. # From: src/max/serve/config.py from pydantic_settings import BaseSettings, SettingsConfigDict class Settings(BaseSettings): # This 'model_config' is crucial. # It tells Pydantic to look for a \".env\" file and # to use an empty prefix for environment variables by default # (so 'MAX_SERVE_HOST' maps to 'host', not 'SOMETHING_host'). model_config = SettingsConfigDict( env_file=\".env\", env_prefix=\"\", # For aliases like MAX_SERVE_HOST extra=\"allow\", # Allow other env vars not listed here populate_by_name=False, # Important for alias behavior ) # Example of a setting: host: str = Field( default=\"0.0.0.0\", alias=\"MAX_SERVE_HOST\" # Matches env var MAX_SERVE_HOST ) # ... other settings definitions . When code like settings = Settings() is executed (as seen in src/max/serve/api_server.py’s main function or src/max/entrypoints/cli/serve.py’s serve_pipeline), Pydantic does its magic to collect all the values. ",
    "url": "/modular_max/01_settings___settings__class__.html#under-the-hood-how-settings-are-loaded",
    
    "relUrl": "/modular_max/01_settings___settings__class__.html#under-the-hood-how-settings-are-loaded"
  },"24": {
    "doc": "Chapter 1: Settings Class",
    "title": "Common Settings You Might Tweak",
    "content": "While there are many settings, here are a few common ones you might interact with: . | host (Env: MAX_SERVE_HOST): The IP address the server listens on (e.g., “0.0.0.0” for all available interfaces, “127.0.0.1” for localhost). | port (Env: MAX_SERVE_PORT): The port number the server uses (e.g., 8000). | api_types: A list specifying which API styles to enable (e.g., [APIType.OPENAI, APIType.SAGEMAKER]). You could set this via an environment variable like MAX_SERVE_API_TYPES='[\"openai\"]' (note the string representation of a list). | logs_console_level (Env: MAX_SERVE_LOGS_CONSOLE_LEVEL): Controls the verbosity of logs printed to your console (e.g., “INFO”, “DEBUG”, “WARNING”). | disable_telemetry (Env: MAX_SERVE_DISABLE_TELEMETRY): A boolean (True or False) to turn off remote telemetry. | . ",
    "url": "/modular_max/01_settings___settings__class__.html#common-settings-you-might-tweak",
    
    "relUrl": "/modular_max/01_settings___settings__class__.html#common-settings-you-might-tweak"
  },"25": {
    "doc": "Chapter 1: Settings Class",
    "title": "Conclusion",
    "content": "You’ve now learned about the Settings class, the central control panel for the modular application! You’ve seen how it allows for flexible configuration through environment variables, .env files, or code defaults, all validated by the power of Pydantic. This system ensures that you can tailor the application’s behavior to your needs without modifying its core code. With this foundation in how the application is configured, you’re ready to explore how these settings are put to use. In the next chapter, we’ll look at the Serving API Layer (FastAPI App &amp; Routers), which is responsible for handling incoming requests based on these configurations. Generated by AI Codebase Knowledge Builder . ",
    "url": "/modular_max/01_settings___settings__class__.html#conclusion",
    
    "relUrl": "/modular_max/01_settings___settings__class__.html#conclusion"
  },"26": {
    "doc": "Chapter 1: Settings Class",
    "title": "Chapter 1: Settings Class",
    "content": " ",
    "url": "/modular_max/01_settings___settings__class__.html",
    
    "relUrl": "/modular_max/01_settings___settings__class__.html"
  },"27": {
    "doc": "Chapter 1: UnsafePointer",
    "title": "Chapter 1: Meet UnsafePointer - Your Direct Line to Memory",
    "content": "Welcome to your Mojo journey! In this first chapter, we’ll explore a fundamental concept that’s crucial for understanding how Mojo can work with data efficiently, especially when dealing with complex structures like NDBuffer. We’re talking about UnsafePointer. If you’re new to programming concepts like pointers, don’t worry! We’ll break it down step by step. ",
    "url": "/mojo-v2/01_unsafepointer__as_used_by_ndbuffer__.html#chapter-1-meet-unsafepointer---your-direct-line-to-memory",
    
    "relUrl": "/mojo-v2/01_unsafepointer__as_used_by_ndbuffer__.html#chapter-1-meet-unsafepointer---your-direct-line-to-memory"
  },"28": {
    "doc": "Chapter 1: UnsafePointer",
    "title": "What’s an NDBuffer Anyway? (A Quick Peek)",
    "content": "Imagine you’re working with a grid of numbers, like: . | A list of scores: [10, 20, 30] | A spreadsheet or a simple image: 1, 2, 3 4, 5, 6 . | Or even more complex, multi-dimensional data (like a video, which is a sequence of images). | . In Mojo, NDBuffer (which stands for N-Dimensional Buffer) is a powerful structure designed to handle such collections of data. It helps you organize and access elements in these structures. But for an NDBuffer to know where its actual data (the numbers, pixels, etc.) lives in your computer’s memory, it needs an address. And that’s precisely where UnsafePointer comes into the picture. ",
    "url": "/mojo-v2/01_unsafepointer__as_used_by_ndbuffer__.html#whats-an-ndbuffer-anyway-a-quick-peek",
    
    "relUrl": "/mojo-v2/01_unsafepointer__as_used_by_ndbuffer__.html#whats-an-ndbuffer-anyway-a-quick-peek"
  },"29": {
    "doc": "Chapter 1: UnsafePointer",
    "title": "UnsafePointer: The Street Address for Your Data",
    "content": "Let’s use an analogy. Think of your computer’s memory as a giant city full of warehouses. Each warehouse can store some data. An UnsafePointer is like the exact street address of one of these warehouses. The NDBuffer holds this “street address” (the UnsafePointer) to know exactly where its data begins. Here’s a key description to keep in mind: . NDBuffer contains a field data which is an UnsafePointer. This pointer directly references the starting address of the memory region that the NDBuffer describes. So, inside every NDBuffer, there’s a special variable, typically named data. The type of this data variable is UnsafePointer. This data variable doesn’t hold the numbers themselves, but rather the memory location where the first number (or element) of the NDBuffer is stored. Familiar Territory? (For C/C++ Users) . If you’ve ever worked with pointers in languages like C or C++, the idea of an UnsafePointer will seem quite familiar. It provides direct memory access. This means you’re operating very close to the computer’s hardware, which can be very powerful. Why “Unsafe”? The Power and Responsibility . The “unsafe” part of UnsafePointer is a crucial distinction. Mojo, by default, is designed with many safety features to help you avoid common programming errors. For example: . | Bounds Checking: If you have an array of 3 items, Mojo usually stops you if you try to access a 4th item (which doesn’t exist). | Lifetime Management: Mojo often helps manage when memory is no longer in use and can be cleared up or reused, preventing “memory leaks” or using “stale” memory. | . UnsafePointer bypasses these built-in safety nets. Why? . | Performance: Sometimes, these safety checks can add a tiny bit of overhead. For very performance-critical tasks, removing this overhead can be beneficial. | Interoperability: When Mojo needs to work with C/C++ code or low-level hardware, it needs a way to handle raw memory addresses. | . However, this power comes with responsibility. When you use an UnsafePointer, you, the programmer, are responsible for: . | Ensuring the memory address is actually valid (it points to a real, allocated piece of memory). | Making sure you don’t read or write past the end of the allocated memory (no automatic bounds checking). | Knowing if the memory is still “alive” or if it has been “freed” (deallocated). Using a pointer to freed memory can lead to crashes or weird behavior. | . Let’s revisit our warehouse analogy: . Think of UnsafePointer as the exact street address of a warehouse where your data is stored. The NDBuffer holds this address to know where its data begins. However, the NDBuffer doesn’t own the warehouse or manage its contents; it just has the key to the front door and relies on the address being correct. This is a very important concept: the NDBuffer uses the UnsafePointer to find its data, but it typically doesn’t “own” or “manage” the lifecycle of that memory. It trusts that the UnsafePointer it’s given is correct and valid. Anatomy of an UnsafePointer (Its Parameters) . When you declare or use an UnsafePointer in Mojo code, it’s often parameterized. These parameters give more specific information about the pointer and the data it points to: . | type: AnyType: This specifies the kind of data that the pointer points to. Is it an integer (Int), a floating-point number (Float64), a character, or some other custom type? . | Example: UnsafePointer[Int] points to a memory location that is expected to hold an integer. | . | mut: Bool: This stands for “mutability.” It indicates whether the data at the memory location can be changed (mutated) through this pointer. | mut=True: The data can be modified. | mut=False: The data is read-only through this pointer. | Example: UnsafePointer[Int, mut=True] points to an integer that you are allowed to change. | . | address_space: AddressSpace: This tells Mojo about the memory system where the data resides. For instance, data could be in the main CPU memory, or on a GPU, or in some other special memory region. | Example: AddressSpace.GENERIC is a common default, referring to general-purpose memory. | . | Origin: This is a more advanced Mojo concept related to its ownership and borrowing system. It essentially helps track “where did the permission to access this memory come from?” We won’t dive deep into Origin in this chapter, but it’s good to know it exists. | alignment: Int: This specifies memory alignment, which can be important for performance, especially on certain hardware. It ensures the data starts at a memory address that’s a multiple of a certain number. | . So, a declaration like UnsafePointer[Float64, mut=True, address_space=AddressSpace.GENERIC] describes a pointer that: . | Points to a Float64 value. | Allows modification of that Float64 value. | Resides in the generic memory address space. | . ",
    "url": "/mojo-v2/01_unsafepointer__as_used_by_ndbuffer__.html#unsafepointer-the-street-address-for-your-data",
    
    "relUrl": "/mojo-v2/01_unsafepointer__as_used_by_ndbuffer__.html#unsafepointer-the-street-address-for-your-data"
  },"30": {
    "doc": "Chapter 1: UnsafePointer",
    "title": "A Glimpse into the Mojo Standard Library Code",
    "content": "You don’t need to understand all the internal details now, but let’s peek at how UnsafePointer is defined. This code lives in a file named unsafe_pointer.mojo within Mojo’s standard library. // This is a *simplified* look at the definition from: // stdlib/src/memory/unsafe_pointer.mojo @register_passable(\"trivial\") struct UnsafePointer[ type: AnyType, // The data type it points to *, // Indicates subsequent parameters are keyword-only address_space: AddressSpace = AddressSpace.GENERIC, // Default to generic memory alignment: Int = _default_alignment[type](), // Default alignment mut: Bool = True, // Default to mutable origin: Origin[mut] = Origin[mut].cast_from[MutableAnyOrigin].result, // ... other traits it conforms to ... ]{ // This is the core: it holds the actual memory address! var address: Self._mlir_type; // --- Life cycle methods --- // fn __init__(out self): // Creates a null (empty) pointer // fn __init__(out self, *, ref to: type): // Points to an existing variable // --- Factory methods --- // @staticmethod // fn alloc(count: Int) -&gt; UnsafePointer[type, ...]: // Allocates new memory // --- Operator dunders --- // fn __getitem__(self) -&gt; ref type: // Access data (dereference) // fn offset[I: Indexer](self, idx: I) -&gt; Self: // Pointer arithmetic // --- Methods --- // fn load[...](self) -&gt; SIMD[...]: // Reads data from memory // fn store[...](self, ..., val: SIMD[...]): // Writes data to memory // fn free(self): // Deallocates memory this pointer might manage // ... and many other helpful (but unsafe!) methods ... } . Key things for a beginner to notice: . | The parameters we just discussed (type, mut, address_space, alignment, origin) are all there in the struct UnsafePointer[...] definition. | The line var address: Self._mlir_type; is the crucial field. This is where the actual raw memory address is stored. (_mlir_type is an internal representation detail). | There are many methods (functions associated with the struct) like alloc (to request new memory), free (to release memory), load (to read data from the memory location), and store (to write data). Using these methods correctly is part of your “unsafe” responsibility. | . ",
    "url": "/mojo-v2/01_unsafepointer__as_used_by_ndbuffer__.html#a-glimpse-into-the-mojo-standard-library-code",
    
    "relUrl": "/mojo-v2/01_unsafepointer__as_used_by_ndbuffer__.html#a-glimpse-into-the-mojo-standard-library-code"
  },"31": {
    "doc": "Chapter 1: UnsafePointer",
    "title": "How NDBuffer Uses UnsafePointer",
    "content": "Now, let’s connect this back to NDBuffer. Here’s a simplified look at the NDBuffer structure, found in stdlib/src/buffer/buffer.mojo: . // This is a *simplified* look at the definition from: // stdlib/src/buffer/buffer.mojo @value @register_passable(\"trivial\") struct NDBuffer[ mut: Bool, // Is the NDBuffer's data mutable? type: DType, // The data type of elements (e.g., DType.float64) rank: Int, // Number of dimensions (e.g., 2 for a 2D matrix) origin: Origin[mut], // Memory origin, tied to the UnsafePointer's origin shape: DimList = DimList.create_unknown[rank](), // Static shape info (optional) strides: DimList = DimList.create_unknown[rank](), // Static stride info (optional) *, alignment: Int = 1, address_space: AddressSpace = AddressSpace.GENERIC, // ... other traits ... ]{ // This is the star of our show for this chapter! // The NDBuffer's direct link to its underlying data. var data: UnsafePointer[ Scalar[type], // It points to individual elements of the NDBuffer's 'type' address_space=address_space, // Inherits address space mut=mut, // Inherits mutability origin=origin // Inherits origin ]; // These fields help interpret the data pointed to by `data` var dynamic_shape: IndexList[rank, ...]; // How large each dimension is (e.g., [3, 4] for 3x4) var dynamic_stride: IndexList[rank, ...]; // How to \"jump\" in memory to get to the next element // in each dimension. // ... other methods for initialization, access, etc... } . Do you see the var data: UnsafePointer[...] line within NDBuffer? That’s it! When an NDBuffer is created or used, its data field holds an UnsafePointer. This pointer tells the NDBuffer the starting memory location of all its elements. The NDBuffer then uses its other information, like dynamic_shape (e.g., “this is a 3x4 matrix”) and dynamic_stride (e.g., “to get to the next row, jump forward 4 elements in memory”), to correctly access and interpret the block of memory that starts at the address held by data. We’ll explore shape and strides in much more detail in later chapters. For now, the key is that UnsafePointer provides the starting point. ",
    "url": "/mojo-v2/01_unsafepointer__as_used_by_ndbuffer__.html#how-ndbuffer-uses-unsafepointer",
    
    "relUrl": "/mojo-v2/01_unsafepointer__as_used_by_ndbuffer__.html#how-ndbuffer-uses-unsafepointer"
  },"32": {
    "doc": "Chapter 1: UnsafePointer",
    "title": "Key Takeaways for Chapter 1",
    "content": "That was a deep dive for a first chapter, but you’ve learned some very important Mojo concepts! Here are the main takeaways: . | UnsafePointer is like a raw, direct memory address. It tells Mojo precisely where some data is stored in the computer’s memory. | It’s called “unsafe” because it operates outside of Mojo’s usual safety mechanisms (like automatic bounds checking or memory lifetime management). This gives you power and performance but requires you, the programmer, to be very careful and responsible for using it correctly. | NDBuffer, Mojo’s structure for handling N-dimensional data (like arrays and matrices), has an important field named data. This data field is an UnsafePointer. | This UnsafePointer within NDBuffer points to the starting memory location of the NDBuffer’s elements. | Typically, an NDBuffer itself doesn’t “own” or manage the memory it points to via its UnsafePointer. It simply holds the “key” (the address) and trusts that the memory is valid and correctly managed elsewhere. | . Understanding UnsafePointer is a fundamental step in seeing how Mojo can achieve high performance and interact with memory at a low level, especially for data-intensive structures like NDBuffer. ",
    "url": "/mojo-v2/01_unsafepointer__as_used_by_ndbuffer__.html#key-takeaways-for-chapter-1",
    
    "relUrl": "/mojo-v2/01_unsafepointer__as_used_by_ndbuffer__.html#key-takeaways-for-chapter-1"
  },"33": {
    "doc": "Chapter 1: UnsafePointer",
    "title": "What’s Next?",
    "content": "Now that you know how an NDBuffer finds the start of its data using an UnsafePointer, you might be curious about: . | How does it know the dimensions (like 3 rows, 4 columns)? | How are the elements arranged in memory? | How does it use this information to find a specific element, say, at row 2, column 1? | . These questions lead us directly to concepts like DimList, shape, and strides. We’ll start unraveling these in the next chapter! Keep up the great work! . Generated by AI Codebase Knowledge Builder . ",
    "url": "/mojo-v2/01_unsafepointer__as_used_by_ndbuffer__.html#whats-next",
    
    "relUrl": "/mojo-v2/01_unsafepointer__as_used_by_ndbuffer__.html#whats-next"
  },"34": {
    "doc": "Chapter 1: UnsafePointer",
    "title": "Chapter 1: UnsafePointer",
    "content": " ",
    "url": "/mojo-v2/01_unsafepointer__as_used_by_ndbuffer__.html",
    
    "relUrl": "/mojo-v2/01_unsafepointer__as_used_by_ndbuffer__.html"
  },"35": {
    "doc": "AsyncWebCrawler",
    "title": "Chapter 2: Meet the General Manager - AsyncWebCrawler",
    "content": "In Chapter 1: How We Fetch Webpages - AsyncCrawlerStrategy, we learned about the different ways Crawl4AI can fetch the raw content of a webpage, like choosing between a fast drone (AsyncHTTPCrawlerStrategy) or a versatile delivery truck (AsyncPlaywrightCrawlerStrategy). But who decides which delivery vehicle to use? Who tells it which address (URL) to go to? And who takes the delivered package (the raw HTML) and turns it into something useful? . That’s where the AsyncWebCrawler comes in. Think of it as the General Manager of the entire crawling operation. ",
    "url": "/Crawl4AI/02_asyncwebcrawler.html#chapter-2-meet-the-general-manager---asyncwebcrawler",
    
    "relUrl": "/Crawl4AI/02_asyncwebcrawler.html#chapter-2-meet-the-general-manager---asyncwebcrawler"
  },"36": {
    "doc": "AsyncWebCrawler",
    "title": "What Problem Does AsyncWebCrawler Solve?",
    "content": "Imagine you want to get information from a website. You need to: . | Decide how to fetch the page (like choosing the drone or truck from Chapter 1). | Actually fetch the page content. | Maybe clean up the messy HTML. | Perhaps extract specific pieces of information (like product prices or article titles). | Maybe save the results so you don’t have to fetch them again immediately (caching). | Finally, give you the final, processed result. | . Doing all these steps manually for every URL would be tedious and complex. AsyncWebCrawler acts as the central coordinator, managing all these steps for you. You just tell it what URL to crawl and maybe some preferences, and it handles the rest. ",
    "url": "/Crawl4AI/02_asyncwebcrawler.html#what-problem-does-asyncwebcrawler-solve",
    
    "relUrl": "/Crawl4AI/02_asyncwebcrawler.html#what-problem-does-asyncwebcrawler-solve"
  },"37": {
    "doc": "AsyncWebCrawler",
    "title": "What is AsyncWebCrawler?",
    "content": "AsyncWebCrawler is the main class you’ll interact with when using Crawl4AI. It’s the primary entry point for starting any crawling task. Key Responsibilities: . | Initialization: Sets up the necessary components, like the browser (if needed). | Coordination: Takes your request (a URL and configuration) and orchestrates the different parts: . | Delegates fetching to an AsyncCrawlerStrategy. | Manages caching using CacheContext / CacheMode. | Uses a ContentScrapingStrategy to clean and parse HTML. | Applies a RelevantContentFilter if configured. | Uses an ExtractionStrategy to pull out specific data if needed. | . | Result Packaging: Bundles everything up into a neat CrawlResult object. | Resource Management: Handles starting and stopping resources (like browsers) cleanly. | . It’s the “conductor” making sure all the different instruments play together harmoniously. ",
    "url": "/Crawl4AI/02_asyncwebcrawler.html#what-is-asyncwebcrawler",
    
    "relUrl": "/Crawl4AI/02_asyncwebcrawler.html#what-is-asyncwebcrawler"
  },"38": {
    "doc": "AsyncWebCrawler",
    "title": "Your First Crawl: Using arun",
    "content": "Let’s see the AsyncWebCrawler in action. The most common way to use it is with an async with block, which automatically handles setup and cleanup. The main method to crawl a single URL is arun. # chapter2_example_1.py import asyncio from crawl4ai import AsyncWebCrawler # Import the General Manager async def main(): # Create the General Manager instance using 'async with' # This handles setup (like starting a browser if needed) # and cleanup (closing the browser). async with AsyncWebCrawler() as crawler: print(\"Crawler is ready!\") # Tell the manager to crawl a specific URL url_to_crawl = \"https://httpbin.org/html\" # A simple example page print(f\"Asking the crawler to fetch: {url_to_crawl}\") result = await crawler.arun(url=url_to_crawl) # Check if the crawl was successful if result.success: print(\"\\nSuccess! Crawler got the content.\") # The result object contains the processed data # We'll learn more about CrawlResult in Chapter 7 print(f\"Page Title: {result.metadata.get('title', 'N/A')}\") print(f\"First 100 chars of Markdown: {result.markdown.raw_markdown[:100]}...\") else: print(f\"\\nFailed to crawl: {result.error_message}\") if __name__ == \"__main__\": asyncio.run(main()) . Explanation: . | import AsyncWebCrawler: We import the main class. | async def main():: Crawl4AI uses Python’s asyncio for efficiency, so our code needs to be in an async function. | async with AsyncWebCrawler() as crawler:: This is the standard way to create and manage the crawler. The async with statement ensures that resources (like the underlying browser used by the default AsyncPlaywrightCrawlerStrategy) are properly started and stopped, even if errors occur. | crawler.arun(url=url_to_crawl): This is the core command. We tell our crawler instance (the General Manager) to run (arun) the crawling process for the specified url. await is used because fetching webpages takes time, and asyncio allows other tasks to run while waiting. | result: The arun method returns a CrawlResult object. This object contains all the information gathered during the crawl (HTML, cleaned text, metadata, etc.). We’ll explore this object in detail in Chapter 7: Understanding the Results - CrawlResult. | result.success: We check this boolean flag to see if the crawl completed without critical errors. | Accessing Data: If successful, we can access processed information like the page title (result.metadata['title']) or the content formatted as Markdown (result.markdown.raw_markdown). | . ",
    "url": "/Crawl4AI/02_asyncwebcrawler.html#your-first-crawl-using-arun",
    
    "relUrl": "/Crawl4AI/02_asyncwebcrawler.html#your-first-crawl-using-arun"
  },"39": {
    "doc": "AsyncWebCrawler",
    "title": "Configuring the Crawl",
    "content": "Sometimes, the default behavior isn’t quite what you need. Maybe you want to use the faster “drone” strategy from Chapter 1, or perhaps you want to ensure you always fetch a fresh copy of the page, ignoring any saved cache. You can customize the behavior of a specific arun call by passing a CrawlerRunConfig object. Think of this as giving specific instructions to the General Manager for this particular job. # chapter2_example_2.py import asyncio from crawl4ai import AsyncWebCrawler from crawl4ai import CrawlerRunConfig # Import configuration class from crawl4ai import CacheMode # Import cache options async def main(): async with AsyncWebCrawler() as crawler: print(\"Crawler is ready!\") url_to_crawl = \"https://httpbin.org/html\" # Create a specific configuration for this run # Tell the crawler to BYPASS the cache (fetch fresh) run_config = CrawlerRunConfig( cache_mode=CacheMode.BYPASS ) print(\"Configuration: Bypass cache for this run.\") # Pass the config object to the arun method result = await crawler.arun( url=url_to_crawl, config=run_config # Pass the specific instructions ) if result.success: print(\"\\nSuccess! Crawler got fresh content (cache bypassed).\") print(f\"Page Title: {result.metadata.get('title', 'N/A')}\") else: print(f\"\\nFailed to crawl: {result.error_message}\") if __name__ == \"__main__\": asyncio.run(main()) . Explanation: . | from crawl4ai import CrawlerRunConfig, CacheMode: We import the necessary classes for configuration. | run_config = CrawlerRunConfig(...): We create an instance of CrawlerRunConfig. This object holds various settings for a specific crawl job. | cache_mode=CacheMode.BYPASS: We set the cache_mode. CacheMode.BYPASS tells the crawler to ignore any previously saved results for this URL and fetch it directly from the web server. We’ll learn all about caching options in Chapter 9: Smart Fetching with Caching - CacheContext / CacheMode. | crawler.arun(..., config=run_config): We pass our custom run_config object to the arun method using the config parameter. | . The CrawlerRunConfig is very powerful and lets you control many aspects of the crawl, including which scraping or extraction methods to use. We’ll dive deep into it in the next chapter: Chapter 3: Giving Instructions - CrawlerRunConfig. ",
    "url": "/Crawl4AI/02_asyncwebcrawler.html#configuring-the-crawl",
    
    "relUrl": "/Crawl4AI/02_asyncwebcrawler.html#configuring-the-crawl"
  },"40": {
    "doc": "AsyncWebCrawler",
    "title": "What Happens When You Call arun? (The Flow)",
    "content": "When you call crawler.arun(url=\"...\"), the AsyncWebCrawler (our General Manager) springs into action and coordinates several steps behind the scenes: . sequenceDiagram participant U as User participant AWC as AsyncWebCrawler (Manager) participant CC as Cache Check participant CS as AsyncCrawlerStrategy (Fetcher) participant SP as Scraping/Processing participant CR as CrawlResult (Final Report) U-&gt;&gt;AWC: arun(\"https://example.com\", config) AWC-&gt;&gt;CC: Need content for \"https://example.com\"? (Respect CacheMode in config) alt Cache Hit &amp; Cache Mode allows reading CC--&gt;&gt;AWC: Yes, here's the cached result. AWC--&gt;&gt;CR: Package cached result. AWC--&gt;&gt;U: Here is the CrawlResult else Cache Miss or Cache Mode prevents reading CC--&gt;&gt;AWC: No cached result / Cannot read cache. AWC-&gt;&gt;CS: Please fetch \"https://example.com\" (using configured strategy) CS--&gt;&gt;AWC: Here's the raw response (HTML, etc.) AWC-&gt;&gt;SP: Process this raw content (Scrape, Filter, Extract based on config) SP--&gt;&gt;AWC: Here's the processed data (Markdown, Metadata, etc.) AWC-&gt;&gt;CC: Cache this result? (Respect CacheMode in config) CC--&gt;&gt;AWC: OK, cached. AWC--&gt;&gt;CR: Package new result. AWC--&gt;&gt;U: Here is the CrawlResult end . Simplified Steps: . | Receive Request: The AsyncWebCrawler gets the URL and configuration from your arun call. | Check Cache: It checks if a valid result for this URL is already saved (cached) and if the CacheMode allows using it. (See Chapter 9). | Fetch (if needed): If no valid cached result exists or caching is bypassed, it asks the configured AsyncCrawlerStrategy (e.g., Playwright or HTTP) to fetch the raw page content. | Process Content: It takes the raw HTML and passes it through various processing steps based on the configuration: . | Scraping: Cleaning up HTML, extracting basic structure using a ContentScrapingStrategy. | Filtering: Optionally filtering content for relevance using a RelevantContentFilter. | Extraction: Optionally extracting specific structured data using an ExtractionStrategy. | . | Cache Result (if needed): If caching is enabled for writing, it saves the final processed result. | Return Result: It bundles everything into a CrawlResult object and returns it to you. | . ",
    "url": "/Crawl4AI/02_asyncwebcrawler.html#what-happens-when-you-call-arun-the-flow",
    
    "relUrl": "/Crawl4AI/02_asyncwebcrawler.html#what-happens-when-you-call-arun-the-flow"
  },"41": {
    "doc": "AsyncWebCrawler",
    "title": "Crawling Many Pages: arun_many",
    "content": "What if you have a whole list of URLs to crawl? Calling arun in a loop works, but it might not be the most efficient way. AsyncWebCrawler provides the arun_many method designed for this. # chapter2_example_3.py import asyncio from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode async def main(): async with AsyncWebCrawler() as crawler: urls_to_crawl = [ \"https://httpbin.org/html\", \"https://httpbin.org/links/10/0\", \"https://httpbin.org/robots.txt\" ] print(f\"Asking crawler to fetch {len(urls_to_crawl)} URLs.\") # Use arun_many for multiple URLs # We can still pass a config that applies to all URLs in the batch config = CrawlerRunConfig(cache_mode=CacheMode.BYPASS) results = await crawler.arun_many(urls=urls_to_crawl, config=config) print(f\"\\nFinished crawling! Got {len(results)} results.\") for result in results: status = \"Success\" if result.success else \"Failed\" url_short = result.url.split('/')[-1] # Get last part of URL print(f\"- URL: {url_short:&lt;10} | Status: {status:&lt;7} | Title: {result.metadata.get('title', 'N/A')}\") if __name__ == \"__main__\": asyncio.run(main()) . Explanation: . | urls_to_crawl = [...]: We define a list of URLs. | await crawler.arun_many(urls=urls_to_crawl, config=config): We call arun_many, passing the list of URLs. It handles crawling them concurrently (like dispatching multiple delivery trucks or drones efficiently). | results: arun_many returns a list where each item is a CrawlResult object corresponding to one of the input URLs. | . arun_many is much more efficient for batch processing as it leverages asyncio to handle multiple fetches and processing tasks concurrently. It uses a BaseDispatcher internally to manage this concurrency. ",
    "url": "/Crawl4AI/02_asyncwebcrawler.html#crawling-many-pages-arun_many",
    
    "relUrl": "/Crawl4AI/02_asyncwebcrawler.html#crawling-many-pages-arun_many"
  },"42": {
    "doc": "AsyncWebCrawler",
    "title": "Under the Hood (A Peek at the Code)",
    "content": "You don’t need to know the internal details to use AsyncWebCrawler, but seeing the structure can help. Inside the crawl4ai library, the file async_webcrawler.py defines this class. # Simplified from async_webcrawler.py # ... imports ... from .async_crawler_strategy import AsyncCrawlerStrategy, AsyncPlaywrightCrawlerStrategy from .async_configs import BrowserConfig, CrawlerRunConfig from .models import CrawlResult from .cache_context import CacheContext, CacheMode # ... other strategy imports ... class AsyncWebCrawler: def __init__( self, crawler_strategy: AsyncCrawlerStrategy = None, # You can provide a strategy... config: BrowserConfig = None, # Configuration for the browser # ... other parameters like logger, base_directory ... ): # If no strategy is given, it defaults to Playwright (the 'truck') self.crawler_strategy = crawler_strategy or AsyncPlaywrightCrawlerStrategy(...) self.browser_config = config or BrowserConfig() # ... setup logger, directories, etc... self.ready = False # Flag to track if setup is complete async def __aenter__(self): # This is called when you use 'async with'. It starts the strategy. await self.crawler_strategy.__aenter__() await self.awarmup() # Perform internal setup self.ready = True return self async def __aexit__(self, exc_type, exc_val, exc_tb): # This is called when exiting 'async with'. It cleans up. await self.crawler_strategy.__aexit__(exc_type, exc_val, exc_tb) self.ready = False async def arun(self, url: str, config: CrawlerRunConfig = None) -&gt; CrawlResult: # 1. Ensure config exists, set defaults (like CacheMode.ENABLED) crawler_config = config or CrawlerRunConfig() if crawler_config.cache_mode is None: crawler_config.cache_mode = CacheMode.ENABLED # 2. Create CacheContext to manage caching logic cache_context = CacheContext(url, crawler_config.cache_mode) # 3. Try reading from cache if allowed cached_result = None if cache_context.should_read(): cached_result = await async_db_manager.aget_cached_url(url) # 4. If cache hit and valid, return cached result if cached_result and self._is_cache_valid(cached_result, crawler_config): # ... log cache hit ... return cached_result # 5. If no cache hit or cache invalid/bypassed: Fetch fresh content # Delegate to the configured AsyncCrawlerStrategy async_response = await self.crawler_strategy.crawl(url, config=crawler_config) # 6. Process the HTML (scrape, filter, extract) # This involves calling other strategies based on config crawl_result = await self.aprocess_html( url=url, html=async_response.html, config=crawler_config, # ... other details from async_response ... ) # 7. Write to cache if allowed if cache_context.should_write(): await async_db_manager.acache_url(crawl_result) # 8. Return the final CrawlResult return crawl_result async def aprocess_html(self, url: str, html: str, config: CrawlerRunConfig, ...) -&gt; CrawlResult: # This internal method handles: # - Getting the configured ContentScrapingStrategy # - Calling its 'scrap' method # - Getting the configured MarkdownGenerationStrategy # - Calling its 'generate_markdown' method # - Getting the configured ExtractionStrategy (if any) # - Calling its 'run' method # - Packaging everything into a CrawlResult # ... implementation details ... pass # Simplified async def arun_many(self, urls: List[str], config: Optional[CrawlerRunConfig] = None, ...) -&gt; List[CrawlResult]: # Uses a Dispatcher (like MemoryAdaptiveDispatcher) # to run self.arun for each URL concurrently. # ... implementation details using a dispatcher ... pass # Simplified # ... other methods like awarmup, close, caching helpers ... The key takeaway is that AsyncWebCrawler doesn’t do the fetching or detailed processing itself. It acts as the central hub, coordinating calls to the various specialized Strategy classes based on the provided configuration. ",
    "url": "/Crawl4AI/02_asyncwebcrawler.html#under-the-hood-a-peek-at-the-code",
    
    "relUrl": "/Crawl4AI/02_asyncwebcrawler.html#under-the-hood-a-peek-at-the-code"
  },"43": {
    "doc": "AsyncWebCrawler",
    "title": "Conclusion",
    "content": "You’ve met the General Manager: AsyncWebCrawler! . | It’s the main entry point for using Crawl4AI. | It coordinates all the steps: fetching, caching, scraping, extracting. | You primarily interact with it using async with and the arun() (single URL) or arun_many() (multiple URLs) methods. | It takes a URL and an optional CrawlerRunConfig object to customize the crawl. | It returns a comprehensive CrawlResult object. | . Now that you understand the central role of AsyncWebCrawler, let’s explore how to give it detailed instructions for each crawling job. Next: Let’s dive into the specifics of configuration with Chapter 3: Giving Instructions - CrawlerRunConfig. Generated by AI Codebase Knowledge Builder . ",
    "url": "/Crawl4AI/02_asyncwebcrawler.html#conclusion",
    
    "relUrl": "/Crawl4AI/02_asyncwebcrawler.html#conclusion"
  },"44": {
    "doc": "AsyncWebCrawler",
    "title": "AsyncWebCrawler",
    "content": " ",
    "url": "/Crawl4AI/02_asyncwebcrawler.html",
    
    "relUrl": "/Crawl4AI/02_asyncwebcrawler.html"
  },"45": {
    "doc": "CrawlerRunConfig",
    "title": "Chapter 3: Giving Instructions - CrawlerRunConfig",
    "content": "In Chapter 2: Meet the General Manager - AsyncWebCrawler, we met the AsyncWebCrawler, the central coordinator for our web crawling tasks. We saw how to tell it what URL to crawl using the arun method. But what if we want to tell the crawler how to crawl that URL? Maybe we want it to take a picture (screenshot) of the page? Or perhaps we only care about a specific section of the page? Or maybe we want to ignore the cache and get the very latest version? . Passing all these different instructions individually every time we call arun could get complicated and messy. # Imagine doing this every time - it gets long! # result = await crawler.arun( # url=\"https://example.com\", # take_screenshot=True, # ignore_cache=True, # only_look_at_this_part=\"#main-content\", # wait_for_this_element=\"#data-table\", # # ... maybe many more settings ... # ) . That’s where CrawlerRunConfig comes in! . ",
    "url": "/Crawl4AI/03_crawlerrunconfig.html#chapter-3-giving-instructions---crawlerrunconfig",
    
    "relUrl": "/Crawl4AI/03_crawlerrunconfig.html#chapter-3-giving-instructions---crawlerrunconfig"
  },"46": {
    "doc": "CrawlerRunConfig",
    "title": "What Problem Does CrawlerRunConfig Solve?",
    "content": "Think of CrawlerRunConfig as the Instruction Manual for a specific crawl job. Instead of giving the AsyncWebCrawler manager lots of separate instructions each time, you bundle them all neatly into a single CrawlerRunConfig object. This object tells the AsyncWebCrawler exactly how to handle a particular URL or set of URLs for that specific run. It makes your code cleaner and easier to manage. ",
    "url": "/Crawl4AI/03_crawlerrunconfig.html#what-problem-does-crawlerrunconfig-solve",
    
    "relUrl": "/Crawl4AI/03_crawlerrunconfig.html#what-problem-does-crawlerrunconfig-solve"
  },"47": {
    "doc": "CrawlerRunConfig",
    "title": "What is CrawlerRunConfig?",
    "content": "CrawlerRunConfig is a configuration class that holds all the settings for a single crawl operation initiated by AsyncWebCrawler.arun() or arun_many(). It allows you to customize various aspects of the crawl, such as: . | Taking Screenshots: Should the crawler capture an image of the page? (screenshot) | Waiting: How long should the crawler wait for the page or specific elements to load? (page_timeout, wait_for) | Focusing Content: Should the crawler only process a specific part of the page? (css_selector) | Extracting Data: Should the crawler use a specific method to pull out structured data? (ExtractionStrategy) | Caching: How should the crawler interact with previously saved results? (CacheMode) | And much more! (like handling JavaScript, filtering links, etc.) | . ",
    "url": "/Crawl4AI/03_crawlerrunconfig.html#what-is-crawlerrunconfig",
    
    "relUrl": "/Crawl4AI/03_crawlerrunconfig.html#what-is-crawlerrunconfig"
  },"48": {
    "doc": "CrawlerRunConfig",
    "title": "Using CrawlerRunConfig",
    "content": "Let’s see how to use it. Remember our basic crawl from Chapter 2? . # chapter3_example_1.py import asyncio from crawl4ai import AsyncWebCrawler async def main(): async with AsyncWebCrawler() as crawler: url_to_crawl = \"https://httpbin.org/html\" print(f\"Crawling {url_to_crawl} with default settings...\") # This uses the default behavior (no specific config) result = await crawler.arun(url=url_to_crawl) if result.success: print(\"Success! Got the content.\") print(f\"Screenshot taken? {'Yes' if result.screenshot else 'No'}\") # Likely No # We'll learn about CacheMode later, but it defaults to using the cache else: print(f\"Failed: {result.error_message}\") if __name__ == \"__main__\": asyncio.run(main()) . Now, let’s say for this specific crawl, we want to bypass the cache (fetch fresh) and also take a screenshot. We create a CrawlerRunConfig instance and pass it to arun: . # chapter3_example_2.py import asyncio from crawl4ai import AsyncWebCrawler from crawl4ai import CrawlerRunConfig # 1. Import the config class from crawl4ai import CacheMode # Import cache options async def main(): async with AsyncWebCrawler() as crawler: url_to_crawl = \"https://httpbin.org/html\" print(f\"Crawling {url_to_crawl} with custom settings...\") # 2. Create an instance of CrawlerRunConfig with our desired settings my_instructions = CrawlerRunConfig( cache_mode=CacheMode.BYPASS, # Don't use the cache, fetch fresh screenshot=True # Take a screenshot ) print(\"Instructions: Bypass cache, take screenshot.\") # 3. Pass the config object to arun() result = await crawler.arun( url=url_to_crawl, config=my_instructions # Pass our instruction manual ) if result.success: print(\"\\nSuccess! Got the content with custom config.\") print(f\"Screenshot taken? {'Yes' if result.screenshot else 'No'}\") # Should be Yes # Check if the screenshot file path exists in result.screenshot if result.screenshot: print(f\"Screenshot saved to: {result.screenshot}\") else: print(f\"\\nFailed: {result.error_message}\") if __name__ == \"__main__\": asyncio.run(main()) . Explanation: . | Import: We import CrawlerRunConfig and CacheMode. | Create Config: We create an instance: my_instructions = CrawlerRunConfig(...). We set cache_mode to CacheMode.BYPASS and screenshot to True. All other settings remain at their defaults. | Pass Config: We pass this my_instructions object to crawler.arun using the config= parameter. | . Now, when AsyncWebCrawler runs this job, it will look inside my_instructions and follow those specific settings for this run only. ",
    "url": "/Crawl4AI/03_crawlerrunconfig.html#using-crawlerrunconfig",
    
    "relUrl": "/Crawl4AI/03_crawlerrunconfig.html#using-crawlerrunconfig"
  },"49": {
    "doc": "CrawlerRunConfig",
    "title": "Some Common CrawlerRunConfig Parameters",
    "content": "CrawlerRunConfig has many options, but here are a few common ones you might use: . | cache_mode: Controls caching behavior. | CacheMode.ENABLED (Default): Use the cache if available, otherwise fetch and save. | CacheMode.BYPASS: Always fetch fresh, ignoring any cached version (but still save the new result). | CacheMode.DISABLED: Never read from or write to the cache. | (More details in Chapter 9: Smart Fetching with Caching - CacheContext / CacheMode) | . | screenshot (bool): If True, takes a screenshot of the fully rendered page. The path to the screenshot file will be in CrawlResult.screenshot. Default: False. | pdf (bool): If True, generates a PDF of the page. The path to the PDF file will be in CrawlResult.pdf. Default: False. | css_selector (str): If provided (e.g., \"#main-content\" or .article-body), the crawler will try to extract only the HTML content within the element(s) matching this CSS selector. This is great for focusing on the important part of a page. Default: None (process the whole page). | wait_for (str): A CSS selector (e.g., \"#data-loaded-indicator\"). The crawler will wait until an element matching this selector appears on the page before proceeding. Useful for pages that load content dynamically with JavaScript. Default: None. | page_timeout (int): Maximum time in milliseconds to wait for page navigation or certain operations. Default: 60000 (60 seconds). | extraction_strategy: An object that defines how to extract specific, structured data (like product names and prices) from the page. Default: None. (See Chapter 6: Getting Specific Data - ExtractionStrategy) | scraping_strategy: An object defining how the raw HTML is cleaned and basic content (like text and links) is extracted. Default: WebScrapingStrategy(). (See Chapter 4: Cleaning Up the Mess - ContentScrapingStrategy) | . Let’s try combining a few: focus on a specific part of the page and wait for something to appear. # chapter3_example_3.py import asyncio from crawl4ai import AsyncWebCrawler, CrawlerRunConfig async def main(): # This example site has a heading 'H1' inside a 'body' tag. url_to_crawl = \"https://httpbin.org/html\" async with AsyncWebCrawler() as crawler: print(f\"Crawling {url_to_crawl}, focusing on the H1 tag...\") # Instructions: Only get the H1 tag, wait max 10s for it specific_config = CrawlerRunConfig( css_selector=\"h1\", # Only grab content inside &lt;h1&gt; tags page_timeout=10000 # Set page timeout to 10 seconds # We could also add wait_for=\"h1\" if needed for dynamic loading ) result = await crawler.arun(url=url_to_crawl, config=specific_config) if result.success: print(\"\\nSuccess! Focused crawl completed.\") # The markdown should now ONLY contain the H1 content print(f\"Markdown content:\\n---\\n{result.markdown.raw_markdown.strip()}\\n---\") else: print(f\"\\nFailed: {result.error_message}\") if __name__ == \"__main__\": asyncio.run(main()) . This time, the result.markdown should only contain the text from the &lt;h1&gt; tag on that page, because we used css_selector=\"h1\" in our CrawlerRunConfig. ",
    "url": "/Crawl4AI/03_crawlerrunconfig.html#some-common-crawlerrunconfig-parameters",
    
    "relUrl": "/Crawl4AI/03_crawlerrunconfig.html#some-common-crawlerrunconfig-parameters"
  },"50": {
    "doc": "CrawlerRunConfig",
    "title": "How AsyncWebCrawler Uses the Config (Under the Hood)",
    "content": "You don’t need to know the exact internal code, but it helps to understand the flow. When you call crawler.arun(url, config=my_config), the AsyncWebCrawler essentially does this: . | Receives the url and the my_config object. | Before fetching, it checks my_config.cache_mode to see if it should look in the cache first. | If fetching is needed, it passes my_config to the underlying AsyncCrawlerStrategy. | The strategy uses settings from my_config like page_timeout, wait_for, and whether to take a screenshot. | After getting the raw HTML, AsyncWebCrawler uses the my_config.scraping_strategy and my_config.css_selector to process the content. | If my_config.extraction_strategy is set, it uses that to extract structured data. | Finally, it bundles everything into a CrawlResult and returns it. | . Here’s a simplified view: . sequenceDiagram participant User participant AWC as AsyncWebCrawler participant Config as CrawlerRunConfig participant Fetcher as AsyncCrawlerStrategy participant Processor as Scraping/Extraction User-&gt;&gt;AWC: arun(url, config=my_config) AWC-&gt;&gt;Config: Check my_config.cache_mode alt Need to Fetch AWC-&gt;&gt;Fetcher: crawl(url, config=my_config) Note over Fetcher: Uses my_config settings (timeout, wait_for, screenshot...) Fetcher--&gt;&gt;AWC: Raw Response (HTML, screenshot?) AWC-&gt;&gt;Processor: Process HTML (using my_config.css_selector, my_config.extraction_strategy...) Processor--&gt;&gt;AWC: Processed Data else Use Cache AWC-&gt;&gt;AWC: Retrieve from Cache end AWC--&gt;&gt;User: Return CrawlResult . The CrawlerRunConfig acts as a messenger carrying your specific instructions throughout the crawling process. Inside the crawl4ai library, in the file async_configs.py, you’ll find the definition of the CrawlerRunConfig class. It looks something like this (simplified): . # Simplified from crawl4ai/async_configs.py from .cache_context import CacheMode from .extraction_strategy import ExtractionStrategy from .content_scraping_strategy import ContentScrapingStrategy, WebScrapingStrategy # ... other imports ... class CrawlerRunConfig(): \"\"\" Configuration class for controlling how the crawler runs each crawl operation. \"\"\" def __init__( self, # Caching cache_mode: CacheMode = CacheMode.BYPASS, # Default behavior if not specified # Content Selection / Waiting css_selector: str = None, wait_for: str = None, page_timeout: int = 60000, # 60 seconds # Media screenshot: bool = False, pdf: bool = False, # Processing Strategies scraping_strategy: ContentScrapingStrategy = None, # Defaults internally if None extraction_strategy: ExtractionStrategy = None, # ... many other parameters omitted for clarity ... **kwargs # Allows for flexibility ): self.cache_mode = cache_mode self.css_selector = css_selector self.wait_for = wait_for self.page_timeout = page_timeout self.screenshot = screenshot self.pdf = pdf # Assign scraping strategy, ensuring a default if None is provided self.scraping_strategy = scraping_strategy or WebScrapingStrategy() self.extraction_strategy = extraction_strategy # ... initialize other attributes ... # Helper methods like 'clone', 'to_dict', 'from_kwargs' might exist too # ... The key idea is that it’s a class designed to hold various settings together. When you create an instance CrawlerRunConfig(...), you’re essentially creating an object that stores your choices for these parameters. ",
    "url": "/Crawl4AI/03_crawlerrunconfig.html#how-asyncwebcrawler-uses-the-config-under-the-hood",
    
    "relUrl": "/Crawl4AI/03_crawlerrunconfig.html#how-asyncwebcrawler-uses-the-config-under-the-hood"
  },"51": {
    "doc": "CrawlerRunConfig",
    "title": "Conclusion",
    "content": "You’ve learned about CrawlerRunConfig, the “Instruction Manual” for individual crawl jobs in Crawl4AI! . | It solves the problem of passing many settings individually to AsyncWebCrawler. | You create an instance of CrawlerRunConfig and set the parameters you want to customize (like cache_mode, screenshot, css_selector, wait_for). | You pass this config object to crawler.arun(url, config=your_config). | This makes your code cleaner and gives you fine-grained control over how each crawl is performed. | . Now that we know how to fetch content (AsyncCrawlerStrategy), manage the overall process (AsyncWebCrawler), and give specific instructions (CrawlerRunConfig), let’s look at how the raw, messy HTML fetched from the web is initially cleaned up and processed. Next: Let’s explore Chapter 4: Cleaning Up the Mess - ContentScrapingStrategy. Generated by AI Codebase Knowledge Builder . ",
    "url": "/Crawl4AI/03_crawlerrunconfig.html#conclusion",
    
    "relUrl": "/Crawl4AI/03_crawlerrunconfig.html#conclusion"
  },"52": {
    "doc": "CrawlerRunConfig",
    "title": "CrawlerRunConfig",
    "content": " ",
    "url": "/Crawl4AI/03_crawlerrunconfig.html",
    
    "relUrl": "/Crawl4AI/03_crawlerrunconfig.html"
  },"53": {
    "doc": "ContentScrapingStrategy",
    "title": "Chapter 4: Cleaning Up the Mess - ContentScrapingStrategy",
    "content": "In Chapter 3: Giving Instructions - CrawlerRunConfig, we learned how to give specific instructions to our AsyncWebCrawler using CrawlerRunConfig. This included telling it how to fetch the page and potentially take screenshots or PDFs. Now, imagine the crawler has successfully fetched the raw HTML content of a webpage. What’s next? Raw HTML is often messy! It contains not just the main article or product description you might care about, but also: . | Navigation menus | Advertisements | Headers and footers | Hidden code like JavaScript (&lt;script&gt;) and styling information (&lt;style&gt;) | Comments left by developers | . Before we can really understand the meaning of the page or extract specific important information, we need to clean up this mess and get a basic understanding of its structure. ",
    "url": "/Crawl4AI/04_contentscrapingstrategy.html#chapter-4-cleaning-up-the-mess---contentscrapingstrategy",
    
    "relUrl": "/Crawl4AI/04_contentscrapingstrategy.html#chapter-4-cleaning-up-the-mess---contentscrapingstrategy"
  },"54": {
    "doc": "ContentScrapingStrategy",
    "title": "What Problem Does ContentScrapingStrategy Solve?",
    "content": "Think of the raw HTML fetched by the crawler as a very rough first draft of a book manuscript. It has the core story, but it’s full of editor’s notes, coffee stains, layout instructions for the printer, and maybe even doodles in the margins. Before the main editor (who focuses on plot and character) can work on it, someone needs to do an initial cleanup. This “First Pass Editor” would: . | Remove the coffee stains and doodles (irrelevant stuff like ads, scripts, styles). | Identify the basic structure: chapter headings (like the page title), paragraph text, image captions (image alt text), and maybe a list of illustrations (links). | Produce a tidier version of the manuscript, ready for more detailed analysis. | . In Crawl4AI, the ContentScrapingStrategy acts as this First Pass Editor. It takes the raw HTML and performs an initial cleanup and structure extraction. Its job is to transform the messy HTML into a more manageable format, identifying key elements like text content, links, images, and basic page metadata (like the title). ",
    "url": "/Crawl4AI/04_contentscrapingstrategy.html#what-problem-does-contentscrapingstrategy-solve",
    
    "relUrl": "/Crawl4AI/04_contentscrapingstrategy.html#what-problem-does-contentscrapingstrategy-solve"
  },"55": {
    "doc": "ContentScrapingStrategy",
    "title": "What is ContentScrapingStrategy?",
    "content": "ContentScrapingStrategy is an abstract concept (like a job description) in Crawl4AI that defines how the initial processing of raw HTML should happen. It specifies that we need a method to clean HTML and extract basic structure, but the specific tools and techniques used can vary. This allows Crawl4AI to be flexible. Different strategies might use different underlying libraries or have different performance characteristics. ",
    "url": "/Crawl4AI/04_contentscrapingstrategy.html#what-is-contentscrapingstrategy",
    
    "relUrl": "/Crawl4AI/04_contentscrapingstrategy.html#what-is-contentscrapingstrategy"
  },"56": {
    "doc": "ContentScrapingStrategy",
    "title": "The Implementations: Meet the Editors",
    "content": "Crawl4AI provides concrete implementations (the actual editors doing the work) of this strategy: . | WebScrapingStrategy (The Default Editor): . | This is the strategy used by default if you don’t specify otherwise. | It uses a popular Python library called BeautifulSoup behind the scenes to parse and manipulate the HTML. | It’s generally robust and good at handling imperfect HTML. | Think of it as a reliable, experienced editor who does a thorough job. | . | LXMLWebScrapingStrategy (The Speedy Editor): . | This strategy uses another powerful library called lxml. | lxml is often faster than BeautifulSoup, especially on large or complex pages. | Think of it as a very fast editor who might be slightly stricter about the manuscript’s format but gets the job done quickly. | . | . For most beginners, the default WebScrapingStrategy works perfectly fine! You usually don’t need to worry about switching unless you encounter performance issues on very large-scale crawls (which is a more advanced topic). ",
    "url": "/Crawl4AI/04_contentscrapingstrategy.html#the-implementations-meet-the-editors",
    
    "relUrl": "/Crawl4AI/04_contentscrapingstrategy.html#the-implementations-meet-the-editors"
  },"57": {
    "doc": "ContentScrapingStrategy",
    "title": "How It Works Conceptually",
    "content": "Here’s the flow: . | The AsyncWebCrawler receives the raw HTML from the AsyncCrawlerStrategy (the fetcher). | It looks at the CrawlerRunConfig to see which ContentScrapingStrategy to use (defaulting to WebScrapingStrategy if none is specified). | It hands the raw HTML over to the chosen strategy’s scrap method. | The strategy parses the HTML, removes unwanted tags (like &lt;script&gt;, &lt;style&gt;, &lt;nav&gt;, &lt;aside&gt;, etc., based on its internal rules), extracts all links (&lt;a&gt; tags), images (&lt;img&gt; tags with their alt text), and metadata (like the &lt;title&gt; tag). | It returns the results packaged in a ScrapingResult object, containing the cleaned HTML, lists of links and media items, and extracted metadata. | The AsyncWebCrawler then takes this ScrapingResult and uses its contents (along with other info) to build the final CrawlResult. | . sequenceDiagram participant AWC as AsyncWebCrawler (Manager) participant Fetcher as AsyncCrawlerStrategy participant HTML as Raw HTML participant CSS as ContentScrapingStrategy (Editor) participant SR as ScrapingResult (Cleaned Draft) participant CR as CrawlResult (Final Report) AWC-&gt;&gt;Fetcher: Fetch(\"https://example.com\") Fetcher--&gt;&gt;AWC: Here's the Raw HTML AWC-&gt;&gt;CSS: Please scrap this Raw HTML (using config) Note over CSS: Parsing HTML... Removing scripts, styles, ads... Extracting links, images, title... CSS--&gt;&gt;AWC: Here's the ScrapingResult (Cleaned HTML, Links, Media, Metadata) AWC-&gt;&gt;CR: Combine ScrapingResult with other info AWC--&gt;&gt;User: Return final CrawlResult . ",
    "url": "/Crawl4AI/04_contentscrapingstrategy.html#how-it-works-conceptually",
    
    "relUrl": "/Crawl4AI/04_contentscrapingstrategy.html#how-it-works-conceptually"
  },"58": {
    "doc": "ContentScrapingStrategy",
    "title": "Using the Default Strategy (WebScrapingStrategy)",
    "content": "You’re likely already using it without realizing it! When you run a basic crawl, AsyncWebCrawler automatically employs WebScrapingStrategy. # chapter4_example_1.py import asyncio from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode async def main(): # Uses the default AsyncPlaywrightCrawlerStrategy (fetching) # AND the default WebScrapingStrategy (scraping/cleaning) async with AsyncWebCrawler() as crawler: url_to_crawl = \"https://httpbin.org/html\" # A very simple HTML page # We don't specify a scraping_strategy in the config, so it uses the default config = CrawlerRunConfig(cache_mode=CacheMode.BYPASS) # Fetch fresh print(f\"Crawling {url_to_crawl} using default scraping strategy...\") result = await crawler.arun(url=url_to_crawl, config=config) if result.success: print(\"\\nSuccess! Content fetched and scraped.\") # The 'result' object now contains info processed by WebScrapingStrategy # 1. Metadata extracted (e.g., page title) print(f\"Page Title: {result.metadata.get('title', 'N/A')}\") # 2. Links extracted print(f\"Found {len(result.links.internal)} internal links and {len(result.links.external)} external links.\") # Example: print first external link if exists if result.links.external: print(f\" Example external link: {result.links.external[0].href}\") # 3. Media extracted (images, videos, etc.) print(f\"Found {len(result.media.images)} images.\") # Example: print first image alt text if exists if result.media.images: print(f\" Example image alt text: '{result.media.images[0].alt}'\") # 4. Cleaned HTML (scripts, styles etc. removed) - might still be complex # print(f\"\\nCleaned HTML snippet:\\n---\\n{result.cleaned_html[:200]}...\\n---\") # 5. Markdown representation (generated AFTER scraping) print(f\"\\nMarkdown snippet:\\n---\\n{result.markdown.raw_markdown[:200]}...\\n---\") else: print(f\"\\nFailed: {result.error_message}\") if __name__ == \"__main__\": asyncio.run(main()) . Explanation: . | We create AsyncWebCrawler and CrawlerRunConfig as usual. | We don’t set the scraping_strategy parameter in CrawlerRunConfig. Crawl4AI automatically picks WebScrapingStrategy. | When crawler.arun executes, after fetching the HTML, it internally calls WebScrapingStrategy.scrap(). | The result (a CrawlResult object) contains fields populated by the scraping strategy: . | result.metadata: Contains things like the page title found in &lt;title&gt; tags. | result.links: Contains lists of internal and external links found (&lt;a&gt; tags). | result.media: Contains lists of images (&lt;img&gt;), videos (&lt;video&gt;), etc. | result.cleaned_html: The HTML after the strategy removed unwanted tags and attributes (this is then used to generate the Markdown). | result.markdown: While not directly created by the scraping strategy, the cleaned HTML it produces is the input for generating the Markdown representation. | . | . ",
    "url": "/Crawl4AI/04_contentscrapingstrategy.html#using-the-default-strategy-webscrapingstrategy",
    
    "relUrl": "/Crawl4AI/04_contentscrapingstrategy.html#using-the-default-strategy-webscrapingstrategy"
  },"59": {
    "doc": "ContentScrapingStrategy",
    "title": "Explicitly Choosing a Strategy (e.g., LXMLWebScrapingStrategy)",
    "content": "What if you want to try the potentially faster LXMLWebScrapingStrategy? You can specify it in the CrawlerRunConfig. # chapter4_example_2.py import asyncio from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode # 1. Import the specific strategy you want to use from crawl4ai import LXMLWebScrapingStrategy async def main(): # 2. Create an instance of the desired scraping strategy lxml_editor = LXMLWebScrapingStrategy() print(f\"Using scraper: {lxml_editor.__class__.__name__}\") async with AsyncWebCrawler() as crawler: url_to_crawl = \"https://httpbin.org/html\" # 3. Create a CrawlerRunConfig and pass the strategy instance config = CrawlerRunConfig( cache_mode=CacheMode.BYPASS, scraping_strategy=lxml_editor # Tell the config which strategy to use ) print(f\"Crawling {url_to_crawl} with explicit LXML scraping strategy...\") result = await crawler.arun(url=url_to_crawl, config=config) if result.success: print(\"\\nSuccess! Content fetched and scraped using LXML.\") print(f\"Page Title: {result.metadata.get('title', 'N/A')}\") print(f\"Found {len(result.links.external)} external links.\") # Output should be largely the same as the default strategy for simple pages else: print(f\"\\nFailed: {result.error_message}\") if __name__ == \"__main__\": asyncio.run(main()) . Explanation: . | Import: We import LXMLWebScrapingStrategy alongside the other classes. | Instantiate: We create an instance: lxml_editor = LXMLWebScrapingStrategy(). | Configure: We create CrawlerRunConfig and pass our instance to the scraping_strategy parameter: CrawlerRunConfig(..., scraping_strategy=lxml_editor). | Run: Now, when crawler.arun is called with this config, it will use LXMLWebScrapingStrategy instead of the default WebScrapingStrategy for the initial HTML processing step. | . For simple pages, the results from both strategies will often be very similar. The choice typically comes down to performance considerations in more advanced scenarios. ",
    "url": "/Crawl4AI/04_contentscrapingstrategy.html#explicitly-choosing-a-strategy-eg-lxmlwebscrapingstrategy",
    
    "relUrl": "/Crawl4AI/04_contentscrapingstrategy.html#explicitly-choosing-a-strategy-eg-lxmlwebscrapingstrategy"
  },"60": {
    "doc": "ContentScrapingStrategy",
    "title": "A Glimpse Under the Hood",
    "content": "Inside the crawl4ai library, the file content_scraping_strategy.py defines the blueprint and the implementations. The Blueprint (Abstract Base Class): . # Simplified from crawl4ai/content_scraping_strategy.py from abc import ABC, abstractmethod from .models import ScrapingResult # Defines the structure of the result class ContentScrapingStrategy(ABC): \"\"\"Abstract base class for content scraping strategies.\"\"\" @abstractmethod def scrap(self, url: str, html: str, **kwargs) -&gt; ScrapingResult: \"\"\" Synchronous method to scrape content. Takes raw HTML, returns structured ScrapingResult. \"\"\" pass @abstractmethod async def ascrap(self, url: str, html: str, **kwargs) -&gt; ScrapingResult: \"\"\" Asynchronous method to scrape content. Takes raw HTML, returns structured ScrapingResult. \"\"\" pass . The Implementations: . # Simplified from crawl4ai/content_scraping_strategy.py from bs4 import BeautifulSoup # Library used by WebScrapingStrategy # ... other imports like models ... class WebScrapingStrategy(ContentScrapingStrategy): def __init__(self, logger=None): self.logger = logger # ... potentially other setup ... def scrap(self, url: str, html: str, **kwargs) -&gt; ScrapingResult: # 1. Parse HTML using BeautifulSoup soup = BeautifulSoup(html, 'lxml') # Or another parser # 2. Find the main content area (maybe using kwargs['css_selector']) # 3. Remove unwanted tags (scripts, styles, nav, footer, ads...) # 4. Extract metadata (title, description...) # 5. Extract all links (&lt;a&gt; tags) # 6. Extract all images (&lt;img&gt; tags) and other media # 7. Get the remaining cleaned HTML text content # ... complex cleaning and extraction logic using BeautifulSoup methods ... # 8. Package results into a ScrapingResult object cleaned_html_content = \"&lt;html&gt;&lt;body&gt;Cleaned content...&lt;/body&gt;&lt;/html&gt;\" # Placeholder links_data = Links(...) media_data = Media(...) metadata_dict = {\"title\": \"Page Title\"} return ScrapingResult( cleaned_html=cleaned_html_content, links=links_data, media=media_data, metadata=metadata_dict, success=True ) async def ascrap(self, url: str, html: str, **kwargs) -&gt; ScrapingResult: # Often delegates to the synchronous version for CPU-bound tasks return await asyncio.to_thread(self.scrap, url, html, **kwargs) . # Simplified from crawl4ai/content_scraping_strategy.py from lxml import html as lhtml # Library used by LXMLWebScrapingStrategy # ... other imports like models ... class LXMLWebScrapingStrategy(WebScrapingStrategy): # Often inherits for shared logic def __init__(self, logger=None): super().__init__(logger) # ... potentially LXML specific setup ... def scrap(self, url: str, html: str, **kwargs) -&gt; ScrapingResult: # 1. Parse HTML using lxml doc = lhtml.document_fromstring(html) # 2. Find main content, remove unwanted tags, extract info # ... complex cleaning and extraction logic using lxml's XPath or CSS selectors ... # 3. Package results into a ScrapingResult object cleaned_html_content = \"&lt;html&gt;&lt;body&gt;Cleaned LXML content...&lt;/body&gt;&lt;/html&gt;\" # Placeholder links_data = Links(...) media_data = Media(...) metadata_dict = {\"title\": \"Page Title LXML\"} return ScrapingResult( cleaned_html=cleaned_html_content, links=links_data, media=media_data, metadata=metadata_dict, success=True ) # ascrap might also delegate or have specific async optimizations . The key takeaway is that both strategies implement the scrap (and ascrap) method, taking raw HTML and returning a structured ScrapingResult. The AsyncWebCrawler can use either one thanks to this common interface. ",
    "url": "/Crawl4AI/04_contentscrapingstrategy.html#a-glimpse-under-the-hood",
    
    "relUrl": "/Crawl4AI/04_contentscrapingstrategy.html#a-glimpse-under-the-hood"
  },"61": {
    "doc": "ContentScrapingStrategy",
    "title": "Conclusion",
    "content": "You’ve learned about ContentScrapingStrategy, Crawl4AI’s “First Pass Editor” for raw HTML. | It tackles the problem of messy HTML by cleaning it and extracting basic structure. | It acts as a blueprint, with WebScrapingStrategy (default, using BeautifulSoup) and LXMLWebScrapingStrategy (using lxml) as concrete implementations. | It’s used automatically by AsyncWebCrawler after fetching content. | You can specify which strategy to use via CrawlerRunConfig. | Its output (cleaned HTML, links, media, metadata) is packaged into a ScrapingResult and contributes significantly to the final CrawlResult. | . Now that we have this initially cleaned and structured content, we might want to further filter it. What if we only care about the parts of the page that are relevant to a specific topic? . Next: Let’s explore how to filter content for relevance with Chapter 5: Focusing on What Matters - RelevantContentFilter. Generated by AI Codebase Knowledge Builder . ",
    "url": "/Crawl4AI/04_contentscrapingstrategy.html#conclusion",
    
    "relUrl": "/Crawl4AI/04_contentscrapingstrategy.html#conclusion"
  },"62": {
    "doc": "ContentScrapingStrategy",
    "title": "ContentScrapingStrategy",
    "content": " ",
    "url": "/Crawl4AI/04_contentscrapingstrategy.html",
    
    "relUrl": "/Crawl4AI/04_contentscrapingstrategy.html"
  },"63": {
    "doc": "RelevantContentFilter",
    "title": "Chapter 5: Focusing on What Matters - RelevantContentFilter",
    "content": "In Chapter 4: Cleaning Up the Mess - ContentScrapingStrategy, we learned how Crawl4AI takes the raw, messy HTML from a webpage and cleans it up using a ContentScrapingStrategy. This gives us a tidier version of the HTML (cleaned_html) and extracts basic elements like links and images. But even after this initial cleanup, the page might still contain a lot of “noise” relative to what we actually care about. Imagine a news article page: the ContentScrapingStrategy might remove scripts and styles, but it could still leave the main article text, plus related article links, user comments, sidebars with ads, and maybe a lengthy footer. If our goal is just to get the main article content (e.g., to summarize it or feed it to an AI), all that extra stuff is just noise. How can we filter the cleaned content even further to keep only the truly relevant parts? . ",
    "url": "/Crawl4AI/05_relevantcontentfilter.html#chapter-5-focusing-on-what-matters---relevantcontentfilter",
    
    "relUrl": "/Crawl4AI/05_relevantcontentfilter.html#chapter-5-focusing-on-what-matters---relevantcontentfilter"
  },"64": {
    "doc": "RelevantContentFilter",
    "title": "What Problem Does RelevantContentFilter Solve?",
    "content": "Think of the cleaned_html from the previous step like flour that’s been roughly sifted – the biggest lumps are gone, but there might still be smaller clumps or bran mixed in. If you want super fine flour for a delicate cake, you need a finer sieve. RelevantContentFilter acts as this finer sieve or a Relevance Sieve. It’s a strategy applied after the initial cleaning by ContentScrapingStrategy but before the final processing (like generating the final Markdown output or using an AI for extraction). Its job is to go through the cleaned content and decide which parts are truly relevant to our goal, removing the rest. This helps us: . | Reduce Noise: Eliminate irrelevant sections like comments, footers, navigation bars, or tangential “related content” blocks. | Focus AI: If we’re sending the content to a Large Language Model (LLM), feeding it only the most relevant parts saves processing time (and potentially money) and can lead to better results. | Improve Accuracy: By removing distracting noise, subsequent steps like data extraction are less likely to grab the wrong information. | . ",
    "url": "/Crawl4AI/05_relevantcontentfilter.html#what-problem-does-relevantcontentfilter-solve",
    
    "relUrl": "/Crawl4AI/05_relevantcontentfilter.html#what-problem-does-relevantcontentfilter-solve"
  },"65": {
    "doc": "RelevantContentFilter",
    "title": "What is RelevantContentFilter?",
    "content": "RelevantContentFilter is an abstract concept (a blueprint) in Crawl4AI representing a method for identifying and retaining only the relevant portions of cleaned HTML content. It defines that we need a way to filter for relevance, but the specific technique used can vary. This allows us to choose different filtering approaches depending on the task and the type of content. ",
    "url": "/Crawl4AI/05_relevantcontentfilter.html#what-is-relevantcontentfilter",
    
    "relUrl": "/Crawl4AI/05_relevantcontentfilter.html#what-is-relevantcontentfilter"
  },"66": {
    "doc": "RelevantContentFilter",
    "title": "The Different Filters: Tools for Sieving",
    "content": "Crawl4AI provides several concrete implementations (the actual sieves) of RelevantContentFilter: . | BM25ContentFilter (The Keyword Sieve): . | Analogy: Like a mini search engine operating within the webpage. | How it Works: You give it (or it figures out) some keywords related to what you’re looking for (e.g., from a user query like “product specifications” or derived from the page title). It then uses a search algorithm called BM25 to score different chunks of the cleaned HTML based on how relevant they are to those keywords. Only the chunks scoring above a certain threshold are kept. | Good For: Finding specific sections about a known topic within a larger page (e.g., finding only the paragraphs discussing “climate change impact” on a long environmental report page). | . | PruningContentFilter (The Structural Sieve): . | Analogy: Like a gardener pruning a bush, removing weak or unnecessary branches based on their structure. | How it Works: This filter doesn’t care about keywords. Instead, it looks at the structure and characteristics of the HTML elements. It removes elements that often represent noise, such as those with very little text compared to the number of links (low text density), elements with common “noise” words in their CSS classes or IDs (like sidebar, comments, footer), or elements deemed structurally insignificant. | Good For: Removing common boilerplate sections (like headers, footers, simple sidebars, navigation) based purely on layout and density clues, even if you don’t have a specific topic query. | . | LLMContentFilter (The AI Sieve): . | Analogy: Asking a smart assistant to read the cleaned content and pick out only the parts relevant to your request. | How it Works: This filter sends the cleaned HTML (often broken into manageable chunks) to a Large Language Model (like GPT). You provide an instruction (e.g., “Extract only the main article content, removing all comments and related links” or “Keep only the sections discussing financial results”). The AI uses its understanding of language and context to identify and return only the relevant parts, often already formatted nicely (like in Markdown). | Good For: Handling complex relevance decisions that require understanding meaning and context, following nuanced natural language instructions. (Note: Requires configuring LLM access, like API keys, and can be slower and potentially costlier than other methods). | . | . ",
    "url": "/Crawl4AI/05_relevantcontentfilter.html#the-different-filters-tools-for-sieving",
    
    "relUrl": "/Crawl4AI/05_relevantcontentfilter.html#the-different-filters-tools-for-sieving"
  },"67": {
    "doc": "RelevantContentFilter",
    "title": "How RelevantContentFilter is Used (Via Markdown Generation)",
    "content": "In Crawl4AI, the RelevantContentFilter is typically integrated into the Markdown generation step. The standard markdown generator (DefaultMarkdownGenerator) can accept a RelevantContentFilter instance. When configured this way: . | The AsyncWebCrawler fetches the page and uses the ContentScrapingStrategy to get cleaned_html. | It then calls the DefaultMarkdownGenerator to produce the Markdown output. | The generator first creates the standard, “raw” Markdown from the entire cleaned_html. | If a RelevantContentFilter was provided to the generator, it then uses this filter on the cleaned_html to select only the relevant HTML fragments. | It converts these filtered fragments into Markdown. This becomes the fit_markdown. | . So, the CrawlResult will contain both: . | result.markdown.raw_markdown: Markdown based on the full cleaned_html. | result.markdown.fit_markdown: Markdown based only on the parts deemed relevant by the filter. | . Let’s see how to configure this. Example 1: Using BM25ContentFilter to find specific content . Imagine we crawled a page about renewable energy, but we only want the parts specifically discussing solar power. # chapter5_example_1.py import asyncio from crawl4ai import ( AsyncWebCrawler, CrawlerRunConfig, DefaultMarkdownGenerator, # The standard markdown generator BM25ContentFilter # The keyword-based filter ) async def main(): # 1. Create the BM25 filter with our query solar_filter = BM25ContentFilter(user_query=\"solar power technology\") print(f\"Filter created for query: '{solar_filter.user_query}'\") # 2. Create a Markdown generator that USES this filter markdown_generator_with_filter = DefaultMarkdownGenerator( content_filter=solar_filter ) print(\"Markdown generator configured with BM25 filter.\") # 3. Create CrawlerRunConfig using this specific markdown generator run_config = CrawlerRunConfig( markdown_generator=markdown_generator_with_filter ) # 4. Run the crawl async with AsyncWebCrawler() as crawler: # Example URL (replace with a real page having relevant content) url_to_crawl = \"https://en.wikipedia.org/wiki/Renewable_energy\" print(f\"\\nCrawling {url_to_crawl}...\") result = await crawler.arun(url=url_to_crawl, config=run_config) if result.success: print(\"\\nCrawl successful!\") print(f\"Raw Markdown length: {len(result.markdown.raw_markdown)}\") print(f\"Fit Markdown length: {len(result.markdown.fit_markdown)}\") # The fit_markdown should be shorter and focused on solar power print(\"\\n--- Start of Fit Markdown (Solar Power Focus) ---\") # Print first 500 chars of the filtered markdown print(result.markdown.fit_markdown[:500] + \"...\") print(\"--- End of Fit Markdown Snippet ---\") else: print(f\"\\nCrawl failed: {result.error_message}\") if __name__ == \"__main__\": asyncio.run(main()) . Explanation: . | Create Filter: We make an instance of BM25ContentFilter, telling it we’re interested in “solar power technology”. | Create Generator: We make an instance of DefaultMarkdownGenerator and pass our solar_filter to its content_filter parameter. | Configure Run: We create CrawlerRunConfig and tell it to use our special markdown_generator_with_filter for this run. | Crawl &amp; Check: We run the crawl as usual. In the result, result.markdown.raw_markdown will have the markdown for the whole page, while result.markdown.fit_markdown will only contain markdown derived from the HTML parts that the BM25ContentFilter scored highly for relevance to “solar power technology”. You’ll likely see the fit_markdown is significantly shorter. | . Example 2: Using PruningContentFilter to remove boilerplate . Now, let’s try removing common noise like sidebars or footers based on structure, without needing a specific query. # chapter5_example_2.py import asyncio from crawl4ai import ( AsyncWebCrawler, CrawlerRunConfig, DefaultMarkdownGenerator, PruningContentFilter # The structural filter ) async def main(): # 1. Create the Pruning filter (no query needed) pruning_filter = PruningContentFilter() print(\"Filter created: PruningContentFilter (structural)\") # 2. Create a Markdown generator that uses this filter markdown_generator_with_filter = DefaultMarkdownGenerator( content_filter=pruning_filter ) print(\"Markdown generator configured with Pruning filter.\") # 3. Create CrawlerRunConfig using this generator run_config = CrawlerRunConfig( markdown_generator=markdown_generator_with_filter ) # 4. Run the crawl async with AsyncWebCrawler() as crawler: # Example URL (replace with a real page that has boilerplate) url_to_crawl = \"https://www.python.org/\" # Python homepage likely has headers/footers print(f\"\\nCrawling {url_to_crawl}...\") result = await crawler.arun(url=url_to_crawl, config=run_config) if result.success: print(\"\\nCrawl successful!\") print(f\"Raw Markdown length: {len(result.markdown.raw_markdown)}\") print(f\"Fit Markdown length: {len(result.markdown.fit_markdown)}\") # fit_markdown should have less header/footer/sidebar content print(\"\\n--- Start of Fit Markdown (Pruned) ---\") print(result.markdown.fit_markdown[:500] + \"...\") print(\"--- End of Fit Markdown Snippet ---\") else: print(f\"\\nCrawl failed: {result.error_message}\") if __name__ == \"__main__\": asyncio.run(main()) . Explanation: . The structure is the same as the BM25 example, but: . | We instantiate PruningContentFilter(), which doesn’t require a user_query. | We pass this filter to the DefaultMarkdownGenerator. | The resulting result.markdown.fit_markdown should contain Markdown primarily from the main content areas of the page, with structurally identified boilerplate removed. | . Example 3: Using LLMContentFilter (Conceptual) . Using LLMContentFilter follows the same pattern, but requires setting up LLM provider details. # chapter5_example_3_conceptual.py import asyncio from crawl4ai import ( AsyncWebCrawler, CrawlerRunConfig, DefaultMarkdownGenerator, LLMContentFilter, # Assume LlmConfig is set up correctly (see LLM-specific docs) # from crawl4ai.async_configs import LlmConfig ) # Assume llm_config is properly configured with API keys, provider, etc. # Example: llm_config = LlmConfig(provider=\"openai\", api_token=\"env:OPENAI_API_KEY\") # For this example, we'll pretend it's ready. class MockLlmConfig: # Mock for demonstration provider = \"mock_provider\" api_token = \"mock_token\" base_url = None llm_config = MockLlmConfig() async def main(): # 1. Create the LLM filter with an instruction instruction = \"Extract only the main news article content. Remove headers, footers, ads, comments, and related links.\" llm_filter = LLMContentFilter( instruction=instruction, llmConfig=llm_config # Pass the LLM configuration ) print(f\"Filter created: LLMContentFilter\") print(f\"Instruction: '{llm_filter.instruction}'\") # 2. Create a Markdown generator using this filter markdown_generator_with_filter = DefaultMarkdownGenerator( content_filter=llm_filter ) print(\"Markdown generator configured with LLM filter.\") # 3. Create CrawlerRunConfig run_config = CrawlerRunConfig( markdown_generator=markdown_generator_with_filter ) # 4. Run the crawl async with AsyncWebCrawler() as crawler: # Example URL (replace with a real news article) url_to_crawl = \"https://httpbin.org/html\" # Using simple page for demo print(f\"\\nCrawling {url_to_crawl}...\") # In a real scenario, this would call the LLM API result = await crawler.arun(url=url_to_crawl, config=run_config) if result.success: print(\"\\nCrawl successful!\") # The fit_markdown would contain the AI-filtered content print(\"\\n--- Start of Fit Markdown (AI Filtered - Conceptual) ---\") # Because we used a mock LLM/simple page, fit_markdown might be empty or simple. # On a real page with a real LLM, it would ideally contain just the main article. print(result.markdown.fit_markdown[:500] + \"...\") print(\"--- End of Fit Markdown Snippet ---\") else: print(f\"\\nCrawl failed: {result.error_message}\") if __name__ == \"__main__\": asyncio.run(main()) . Explanation: . | We create LLMContentFilter, providing our natural language instruction and the necessary llmConfig (which holds provider details and API keys - mocked here for simplicity). | We integrate it into DefaultMarkdownGenerator and CrawlerRunConfig as before. | When arun is called, the LLMContentFilter would (in a real scenario) interact with the configured LLM API, sending chunks of the cleaned_html and the instruction, then assembling the AI’s response into the fit_markdown. | . ",
    "url": "/Crawl4AI/05_relevantcontentfilter.html#how-relevantcontentfilter-is-used-via-markdown-generation",
    
    "relUrl": "/Crawl4AI/05_relevantcontentfilter.html#how-relevantcontentfilter-is-used-via-markdown-generation"
  },"68": {
    "doc": "RelevantContentFilter",
    "title": "Under the Hood: How Filtering Fits In",
    "content": "The RelevantContentFilter doesn’t run on its own; it’s invoked by another component, typically the DefaultMarkdownGenerator. Here’s the sequence: . sequenceDiagram participant User participant AWC as AsyncWebCrawler participant Config as CrawlerRunConfig participant Scraper as ContentScrapingStrategy participant MDGen as DefaultMarkdownGenerator participant Filter as RelevantContentFilter participant Result as CrawlResult User-&gt;&gt;AWC: arun(url, config=my_config) Note over AWC: Config includes Markdown Generator with a Filter AWC-&gt;&gt;Scraper: scrap(raw_html) Scraper--&gt;&gt;AWC: cleaned_html, links, etc. AWC-&gt;&gt;MDGen: generate_markdown(cleaned_html, config=my_config) Note over MDGen: Uses html2text for raw markdown MDGen--&gt;&gt;MDGen: raw_markdown = html2text(cleaned_html) Note over MDGen: Now, check for content_filter alt Filter Provided in MDGen MDGen-&gt;&gt;Filter: filter_content(cleaned_html) Filter--&gt;&gt;MDGen: filtered_html_fragments Note over MDGen: Uses html2text on filtered fragments MDGen--&gt;&gt;MDGen: fit_markdown = html2text(filtered_html_fragments) else No Filter Provided MDGen--&gt;&gt;MDGen: fit_markdown = \"\" (or None) end Note over MDGen: Generate citations if needed MDGen--&gt;&gt;AWC: MarkdownGenerationResult (raw, fit, references) AWC-&gt;&gt;Result: Package everything AWC--&gt;&gt;User: Return CrawlResult . Code Glimpse: . Inside crawl4ai/markdown_generation_strategy.py, the DefaultMarkdownGenerator’s generate_markdown method has logic like this (simplified): . # Simplified from markdown_generation_strategy.py from .models import MarkdownGenerationResult from .html2text import CustomHTML2Text from .content_filter_strategy import RelevantContentFilter # Import filter base class class DefaultMarkdownGenerator(MarkdownGenerationStrategy): # ... __init__ stores self.content_filter ... def generate_markdown( self, cleaned_html: str, # ... other params like base_url, options ... content_filter: Optional[RelevantContentFilter] = None, **kwargs, ) -&gt; MarkdownGenerationResult: h = CustomHTML2Text(...) # Setup html2text converter # ... apply options ... # 1. Generate raw markdown from the full cleaned_html raw_markdown = h.handle(cleaned_html) # ... post-process raw_markdown ... # 2. Convert links to citations (if enabled) markdown_with_citations, references_markdown = self.convert_links_to_citations(...) # 3. Generate fit markdown IF a filter is available fit_markdown = \"\" filtered_html = \"\" # Use the filter passed directly, or the one stored during initialization active_filter = content_filter or self.content_filter if active_filter: try: # Call the filter's main method filtered_html_fragments = active_filter.filter_content(cleaned_html) # Join fragments (assuming filter returns list of HTML strings) filtered_html = \"\\n\".join(filtered_html_fragments) # Convert ONLY the filtered HTML to markdown fit_markdown = h.handle(filtered_html) except Exception as e: fit_markdown = f\"Error during filtering: {e}\" # Log error... return MarkdownGenerationResult( raw_markdown=raw_markdown, markdown_with_citations=markdown_with_citations, references_markdown=references_markdown, fit_markdown=fit_markdown, # Contains the filtered result fit_html=filtered_html, # The HTML fragments kept by the filter ) . And inside crawl4ai/content_filter_strategy.py, you find the blueprint and implementations: . # Simplified from content_filter_strategy.py from abc import ABC, abstractmethod from typing import List # ... other imports like BeautifulSoup, BM25Okapi ... class RelevantContentFilter(ABC): \"\"\"Abstract base class for content filtering strategies\"\"\" def __init__(self, user_query: str = None, ...): self.user_query = user_query # ... common setup ... @abstractmethod def filter_content(self, html: str) -&gt; List[str]: \"\"\" Takes cleaned HTML, returns a list of HTML fragments deemed relevant by the specific strategy. \"\"\" pass # ... common helper methods like extract_page_query, is_excluded ... class BM25ContentFilter(RelevantContentFilter): def __init__(self, user_query: str = None, bm25_threshold: float = 1.0, ...): super().__init__(user_query) self.bm25_threshold = bm25_threshold # ... BM25 specific setup ... def filter_content(self, html: str) -&gt; List[str]: # 1. Parse HTML (e.g., with BeautifulSoup) # 2. Extract text chunks (candidates) # 3. Determine query (user_query or extracted) # 4. Tokenize query and chunks # 5. Calculate BM25 scores for chunks vs query # 6. Filter chunks based on score and threshold # 7. Return the HTML string of the selected chunks # ... implementation details ... relevant_html_fragments = [\"&lt;p&gt;Relevant paragraph 1...&lt;/p&gt;\", \"&lt;h2&gt;Relevant Section&lt;/h2&gt;...\"] # Placeholder return relevant_html_fragments # ... Implementations for PruningContentFilter and LLMContentFilter ... The key is that each filter implements the filter_content method, returning the list of HTML fragments it considers relevant. The DefaultMarkdownGenerator then uses these fragments to create the fit_markdown. ",
    "url": "/Crawl4AI/05_relevantcontentfilter.html#under-the-hood-how-filtering-fits-in",
    
    "relUrl": "/Crawl4AI/05_relevantcontentfilter.html#under-the-hood-how-filtering-fits-in"
  },"69": {
    "doc": "RelevantContentFilter",
    "title": "Conclusion",
    "content": "You’ve learned about RelevantContentFilter, Crawl4AI’s “Relevance Sieve”! . | It addresses the problem that even cleaned HTML can contain noise relative to a specific goal. | It acts as a strategy to filter cleaned HTML, keeping only the relevant parts. | Different filter types exist: BM25ContentFilter (keywords), PruningContentFilter (structure), and LLMContentFilter (AI/semantic). | It’s typically used within the DefaultMarkdownGenerator to produce a focused fit_markdown output in the CrawlResult, alongside the standard raw_markdown. | You configure it by passing the chosen filter instance to the DefaultMarkdownGenerator and then passing that generator to the CrawlerRunConfig. | . By using RelevantContentFilter, you can significantly improve the signal-to-noise ratio of the content you get from webpages, making downstream tasks like summarization or analysis more effective. But what if just getting relevant text isn’t enough? What if you need specific, structured data like product names, prices, and ratings from an e-commerce page, or names and affiliations from a list of conference speakers? . Next: Let’s explore how to extract structured data with Chapter 6: Getting Specific Data - ExtractionStrategy. Generated by AI Codebase Knowledge Builder . ",
    "url": "/Crawl4AI/05_relevantcontentfilter.html#conclusion",
    
    "relUrl": "/Crawl4AI/05_relevantcontentfilter.html#conclusion"
  },"70": {
    "doc": "RelevantContentFilter",
    "title": "RelevantContentFilter",
    "content": " ",
    "url": "/Crawl4AI/05_relevantcontentfilter.html",
    
    "relUrl": "/Crawl4AI/05_relevantcontentfilter.html"
  },"71": {
    "doc": "ExtractionStrategy",
    "title": "Chapter 6: Getting Specific Data - ExtractionStrategy",
    "content": "In the previous chapter, Chapter 5: Focusing on What Matters - RelevantContentFilter, we learned how to sift through the cleaned webpage content to keep only the parts relevant to our query or goal, producing a focused fit_markdown. This is great for tasks like summarization or getting the main gist of an article. But sometimes, we need more than just relevant text. Imagine you’re analyzing an e-commerce website listing products. You don’t just want the description; you need the exact product name, the specific price, the customer rating, and maybe the SKU number, all neatly organized. How do we tell Crawl4AI to find these specific pieces of information and return them in a structured format, like a JSON object? . ",
    "url": "/Crawl4AI/06_extractionstrategy.html#chapter-6-getting-specific-data---extractionstrategy",
    
    "relUrl": "/Crawl4AI/06_extractionstrategy.html#chapter-6-getting-specific-data---extractionstrategy"
  },"72": {
    "doc": "ExtractionStrategy",
    "title": "What Problem Does ExtractionStrategy Solve?",
    "content": "Think of the content we’ve processed so far (like the cleaned HTML or the generated Markdown) as a detailed report delivered by a researcher. RelevantContentFilter helped trim the report down to the most relevant pages. Now, we need to give specific instructions to an Analyst to go through that focused report and pull out precise data points. We don’t just want the report; we want a filled-in spreadsheet with columns for “Product Name,” “Price,” and “Rating.” . ExtractionStrategy is the set of instructions we give to this Analyst. It defines how to locate and extract specific, structured information (like fields in a database or keys in a JSON object) from the content. ",
    "url": "/Crawl4AI/06_extractionstrategy.html#what-problem-does-extractionstrategy-solve",
    
    "relUrl": "/Crawl4AI/06_extractionstrategy.html#what-problem-does-extractionstrategy-solve"
  },"73": {
    "doc": "ExtractionStrategy",
    "title": "What is ExtractionStrategy?",
    "content": "ExtractionStrategy is a core concept (a blueprint) in Crawl4AI that represents the method used to extract structured data from the processed content (which could be HTML or Markdown). It specifies that we need a way to find specific fields, but the actual technique used to find them can vary. This allows us to choose the best “Analyst” for the job, depending on the complexity of the website and the data we need. ",
    "url": "/Crawl4AI/06_extractionstrategy.html#what-is-extractionstrategy",
    
    "relUrl": "/Crawl4AI/06_extractionstrategy.html#what-is-extractionstrategy"
  },"74": {
    "doc": "ExtractionStrategy",
    "title": "The Different Analysts: Ways to Extract Data",
    "content": "Crawl4AI offers several concrete implementations (the different Analysts) for extracting structured data: . | The Precise Locator (JsonCssExtractionStrategy &amp; JsonXPathExtractionStrategy) . | Analogy: An analyst who uses very precise map coordinates (CSS Selectors or XPath expressions) to find information on a page. They need to be told exactly where to look. “The price is always in the HTML element with the ID #product-price.” | How it works: You define a schema (a Python dictionary) that maps the names of the fields you want (e.g., “product_name”, “price”) to the specific CSS selector (JsonCssExtractionStrategy) or XPath expression (JsonXPathExtractionStrategy) that locates that information within the HTML structure. | Pros: Very fast and reliable if the website structure is consistent and predictable. Doesn’t require external AI services. | Cons: Can break easily if the website changes its layout (selectors become invalid). Requires you to inspect the HTML and figure out the correct selectors. | Input: Typically works directly on the raw or cleaned HTML. | . | The Smart Interpreter (LLMExtractionStrategy) . | Analogy: A highly intelligent analyst who can read and understand the content. You give them a list of fields you need (a schema) or even just natural language instructions (“Find the product name, its price, and a short description”). They read the content (usually Markdown) and use their understanding of language and context to figure out the values, even if the layout isn’t perfectly consistent. | How it works: You provide a desired output schema (e.g., a Pydantic model or a dictionary structure) or a natural language instruction. The strategy sends the content (often the generated Markdown, possibly split into chunks) along with your schema/instruction to a configured Large Language Model (LLM) like GPT or Llama. The LLM reads the text and generates the structured data (usually JSON) according to your request. | Pros: Much more resilient to website layout changes. Can understand context and handle variations. Can extract data based on meaning, not just location. | Cons: Requires setting up access to an LLM (API keys, potentially costs). Can be significantly slower than selector-based methods. The quality of extraction depends on the LLM’s capabilities and the clarity of your instructions/schema. | Input: Often works best on the cleaned Markdown representation of the content, but can sometimes use HTML. | . | . ",
    "url": "/Crawl4AI/06_extractionstrategy.html#the-different-analysts-ways-to-extract-data",
    
    "relUrl": "/Crawl4AI/06_extractionstrategy.html#the-different-analysts-ways-to-extract-data"
  },"75": {
    "doc": "ExtractionStrategy",
    "title": "How to Use an ExtractionStrategy",
    "content": "You tell the AsyncWebCrawler which extraction strategy to use (if any) by setting the extraction_strategy parameter within the CrawlerRunConfig object you pass to arun or arun_many. Example 1: Extracting Data with JsonCssExtractionStrategy . Let’s imagine we want to extract the title (from the &lt;h1&gt; tag) and the main heading (from the &lt;h1&gt; tag) of the simple httpbin.org/html page. # chapter6_example_1.py import asyncio import json from crawl4ai import ( AsyncWebCrawler, CrawlerRunConfig, JsonCssExtractionStrategy # Import the CSS strategy ) async def main(): # 1. Define the extraction schema (Field Name -&gt; CSS Selector) extraction_schema = { \"baseSelector\": \"body\", # Operate within the body tag \"fields\": [ {\"name\": \"page_title\", \"selector\": \"title\", \"type\": \"text\"}, {\"name\": \"main_heading\", \"selector\": \"h1\", \"type\": \"text\"} ] } print(\"Extraction Schema defined using CSS selectors.\") # 2. Create an instance of the strategy with the schema css_extractor = JsonCssExtractionStrategy(schema=extraction_schema) print(f\"Using strategy: {css_extractor.__class__.__name__}\") # 3. Create CrawlerRunConfig and set the extraction_strategy run_config = CrawlerRunConfig( extraction_strategy=css_extractor ) # 4. Run the crawl async with AsyncWebCrawler() as crawler: url_to_crawl = \"https://httpbin.org/html\" print(f\"\\nCrawling {url_to_crawl} to extract structured data...\") result = await crawler.arun(url=url_to_crawl, config=run_config) if result.success and result.extracted_content: print(\"\\nExtraction successful!\") # The extracted data is stored as a JSON string in result.extracted_content # Parse the JSON string to work with the data as a Python object extracted_data = json.loads(result.extracted_content) print(\"Extracted Data:\") # Print the extracted data nicely formatted print(json.dumps(extracted_data, indent=2)) elif result.success: print(\"\\nCrawl successful, but no structured data extracted.\") else: print(f\"\\nCrawl failed: {result.error_message}\") if __name__ == \"__main__\": asyncio.run(main()) . Explanation: . | Schema Definition: We create a Python dictionary extraction_schema. | baseSelector: \"body\" tells the strategy to look for items within the &lt;body&gt; tag of the HTML. | fields is a list of dictionaries, each defining a field to extract: . | name: The key for this field in the output JSON (e.g., “page_title”). | selector: The CSS selector to find the element containing the data (e.g., “title” finds the &lt;title&gt; tag, “h1” finds the &lt;h1&gt; tag). | type: How to get the data from the selected element (\"text\" means get the text content). | . | . | Instantiate Strategy: We create an instance of JsonCssExtractionStrategy, passing our extraction_schema. This strategy knows its input format should be HTML. | Configure Run: We create a CrawlerRunConfig and assign our css_extractor instance to the extraction_strategy parameter. | Crawl: We run crawler.arun. After fetching and basic scraping, the AsyncWebCrawler will see the extraction_strategy in the config and call our css_extractor. | Result: The CrawlResult object now contains a field called extracted_content. This field holds the structured data found by the strategy, formatted as a JSON string. We use json.loads() to convert this string back into a Python list/dictionary. | . Expected Output (Conceptual): . Extraction Schema defined using CSS selectors. Using strategy: JsonCssExtractionStrategy Crawling https://httpbin.org/html to extract structured data... Extraction successful! Extracted Data: [ { \"page_title\": \"Herman Melville - Moby-Dick\", \"main_heading\": \"Moby Dick\" } ] . (Note: The actual output is a list containing one dictionary because baseSelector: \"body\" matches one element, and we extract fields relative to that.) . Example 2: Extracting Data with LLMExtractionStrategy (Conceptual) . Now, let’s imagine we want the same information (title, heading) but using an AI. We’ll provide a schema describing what we want. (Note: This requires setting up LLM access separately, e.g., API keys). # chapter6_example_2.py import asyncio import json from crawl4ai import ( AsyncWebCrawler, CrawlerRunConfig, LLMExtractionStrategy, # Import the LLM strategy LlmConfig # Import LLM configuration helper ) # Assume llm_config is properly configured with provider, API key, etc. # This is just a placeholder - replace with your actual LLM setup # E.g., llm_config = LlmConfig(provider=\"openai\", api_token=\"env:OPENAI_API_KEY\") class MockLlmConfig: provider=\"mock\"; api_token=\"mock\"; base_url=None llm_config = MockLlmConfig() async def main(): # 1. Define the desired output schema (what fields we want) # This helps guide the LLM. output_schema = { \"page_title\": \"string\", \"main_heading\": \"string\" } print(\"Extraction Schema defined for LLM.\") # 2. Create an instance of the LLM strategy # We pass the schema and the LLM configuration. # We also specify input_format='markdown' (common for LLMs). llm_extractor = LLMExtractionStrategy( schema=output_schema, llmConfig=llm_config, # Pass the LLM provider details input_format=\"markdown\" # Tell it to read the Markdown content ) print(f\"Using strategy: {llm_extractor.__class__.__name__}\") print(f\"LLM Provider (mocked): {llm_config.provider}\") # 3. Create CrawlerRunConfig with the strategy run_config = CrawlerRunConfig( extraction_strategy=llm_extractor ) # 4. Run the crawl async with AsyncWebCrawler() as crawler: url_to_crawl = \"https://httpbin.org/html\" print(f\"\\nCrawling {url_to_crawl} using LLM to extract...\") # This would make calls to the configured LLM API result = await crawler.arun(url=url_to_crawl, config=run_config) if result.success and result.extracted_content: print(\"\\nExtraction successful (using LLM)!\") # Extracted data is a JSON string try: extracted_data = json.loads(result.extracted_content) print(\"Extracted Data:\") print(json.dumps(extracted_data, indent=2)) except json.JSONDecodeError: print(\"Could not parse LLM output as JSON:\") print(result.extracted_content) elif result.success: print(\"\\nCrawl successful, but no structured data extracted by LLM.\") # This might happen if the mock LLM doesn't return valid JSON # or if the content was too small/irrelevant for extraction. else: print(f\"\\nCrawl failed: {result.error_message}\") if __name__ == \"__main__\": asyncio.run(main()) . Explanation: . | Schema Definition: We define a simple dictionary output_schema telling the LLM we want fields named “page_title” and “main_heading”, both expected to be strings. | Instantiate Strategy: We create LLMExtractionStrategy, passing: . | schema=output_schema: Our desired output structure. | llmConfig=llm_config: The configuration telling the strategy which LLM to use and how to authenticate (here, it’s mocked). | input_format=\"markdown\": Instructs the strategy to feed the generated Markdown content (from result.markdown.raw_markdown) to the LLM, which is often easier for LLMs to parse than raw HTML. | . | Configure Run &amp; Crawl: Same as before, we set the extraction_strategy in CrawlerRunConfig and run the crawl. | Result: The AsyncWebCrawler calls the llm_extractor. The strategy sends the Markdown content and the schema instructions to the configured LLM. The LLM analyzes the text and (hopefully) returns a JSON object matching the schema. This JSON is stored as a string in result.extracted_content. | . Expected Output (Conceptual, with a real LLM): . Extraction Schema defined for LLM. Using strategy: LLMExtractionStrategy LLM Provider (mocked): mock Crawling https://httpbin.org/html using LLM to extract... Extraction successful (using LLM)! Extracted Data: [ { \"page_title\": \"Herman Melville - Moby-Dick\", \"main_heading\": \"Moby Dick\" } ] . (Note: LLM output format might vary slightly, but it aims to match the requested schema based on the content it reads.) . ",
    "url": "/Crawl4AI/06_extractionstrategy.html#how-to-use-an-extractionstrategy",
    
    "relUrl": "/Crawl4AI/06_extractionstrategy.html#how-to-use-an-extractionstrategy"
  },"76": {
    "doc": "ExtractionStrategy",
    "title": "How It Works Inside (Under the Hood)",
    "content": "When you provide an extraction_strategy in the CrawlerRunConfig, how does AsyncWebCrawler use it? . | Fetch &amp; Scrape: The crawler fetches the raw HTML (AsyncCrawlerStrategy) and performs initial cleaning/scraping (ContentScrapingStrategy) to get cleaned_html, links, etc. | Markdown Generation: It usually generates Markdown representation (DefaultMarkdownGenerator). | Check for Strategy: The AsyncWebCrawler (specifically in its internal aprocess_html method) checks if config.extraction_strategy is set. | Execute Strategy: If a strategy exists: . | It determines the required input format (e.g., “html” for JsonCssExtractionStrategy, “markdown” for LLMExtractionStrategy based on its input_format attribute). | It retrieves the corresponding content (e.g., result.cleaned_html or result.markdown.raw_markdown). | If the content is long and the strategy supports chunking (like LLMExtractionStrategy), it might first split the content into smaller chunks. | It calls the strategy’s run method, passing the content chunk(s). | The strategy performs its logic (applying selectors, calling LLM API). | The strategy returns the extracted data (typically as a list of dictionaries). | . | Store Result: The AsyncWebCrawler converts the returned structured data into a JSON string and stores it in CrawlResult.extracted_content. | . Here’s a simplified view: . sequenceDiagram participant User participant AWC as AsyncWebCrawler participant Config as CrawlerRunConfig participant Processor as HTML Processing participant Extractor as ExtractionStrategy participant Result as CrawlResult User-&gt;&gt;AWC: arun(url, config=my_config) Note over AWC: Config includes an Extraction Strategy AWC-&gt;&gt;Processor: Process HTML (scrape, generate markdown) Processor--&gt;&gt;AWC: Processed Content (HTML, Markdown) AWC-&gt;&gt;Extractor: Run extraction on content (using Strategy's input format) Note over Extractor: Applying logic (CSS, XPath, LLM...) Extractor--&gt;&gt;AWC: Structured Data (List[Dict]) AWC-&gt;&gt;AWC: Convert data to JSON String AWC-&gt;&gt;Result: Store JSON String in extracted_content AWC--&gt;&gt;User: Return CrawlResult . Code Glimpse (extraction_strategy.py) . Inside the crawl4ai library, the file extraction_strategy.py defines the blueprint and the implementations. The Blueprint (Abstract Base Class): . # Simplified from crawl4ai/extraction_strategy.py from abc import ABC, abstractmethod from typing import List, Dict, Any class ExtractionStrategy(ABC): \"\"\"Abstract base class for all extraction strategies.\"\"\" def __init__(self, input_format: str = \"markdown\", **kwargs): self.input_format = input_format # e.g., 'html', 'markdown' # ... other common init ... @abstractmethod def extract(self, url: str, content_chunk: str, *q, **kwargs) -&gt; List[Dict[str, Any]]: \"\"\"Extract structured data from a single chunk of content.\"\"\" pass def run(self, url: str, sections: List[str], *q, **kwargs) -&gt; List[Dict[str, Any]]: \"\"\"Process content sections (potentially chunked) and call extract.\"\"\" # Default implementation might process sections in parallel or sequentially all_extracted_data = [] for section in sections: all_extracted_data.extend(self.extract(url, section, **kwargs)) return all_extracted_data . Example Implementation (JsonCssExtractionStrategy): . # Simplified from crawl4ai/extraction_strategy.py from bs4 import BeautifulSoup # Uses BeautifulSoup for CSS selectors class JsonCssExtractionStrategy(ExtractionStrategy): def __init__(self, schema: Dict[str, Any], **kwargs): # Force input format to HTML for CSS selectors super().__init__(input_format=\"html\", **kwargs) self.schema = schema # Store the user-defined schema def extract(self, url: str, html_content: str, *q, **kwargs) -&gt; List[Dict[str, Any]]: # Parse the HTML content chunk soup = BeautifulSoup(html_content, \"html.parser\") extracted_items = [] # Find base elements defined in the schema base_elements = soup.select(self.schema.get(\"baseSelector\", \"body\")) for element in base_elements: item = {} # Extract fields based on schema selectors and types fields_to_extract = self.schema.get(\"fields\", []) for field_def in fields_to_extract: try: # Find the specific sub-element using CSS selector target_element = element.select_one(field_def[\"selector\"]) if target_element: if field_def[\"type\"] == \"text\": item[field_def[\"name\"]] = target_element.get_text(strip=True) elif field_def[\"type\"] == \"attribute\": item[field_def[\"name\"]] = target_element.get(field_def[\"attribute\"]) # ... other types like 'html', 'list', 'nested' ... except Exception as e: # Handle errors, maybe log them if verbose pass if item: extracted_items.append(item) return extracted_items # run() method likely uses the default implementation from base class . Example Implementation (LLMExtractionStrategy): . # Simplified from crawl4ai/extraction_strategy.py # Needs imports for LLM interaction (e.g., perform_completion_with_backoff) from .utils import perform_completion_with_backoff, chunk_documents, escape_json_string from .prompts import PROMPT_EXTRACT_SCHEMA_WITH_INSTRUCTION # Example prompt class LLMExtractionStrategy(ExtractionStrategy): def __init__(self, schema: Dict = None, instruction: str = None, llmConfig=None, input_format=\"markdown\", **kwargs): super().__init__(input_format=input_format, **kwargs) self.schema = schema self.instruction = instruction self.llmConfig = llmConfig # Contains provider, API key, etc. # ... other LLM specific setup ... def extract(self, url: str, content_chunk: str, *q, **kwargs) -&gt; List[Dict[str, Any]]: # Prepare the prompt for the LLM prompt = self._build_llm_prompt(url, content_chunk) # Call the LLM API response = perform_completion_with_backoff( provider=self.llmConfig.provider, prompt_with_variables=prompt, api_token=self.llmConfig.api_token, base_url=self.llmConfig.base_url, json_response=True # Often expect JSON from LLM for extraction # ... pass other necessary args ... ) # Parse the LLM's response (which should ideally be JSON) try: extracted_data = json.loads(response.choices[0].message.content) # Ensure it's a list if isinstance(extracted_data, dict): extracted_data = [extracted_data] return extracted_data except Exception as e: # Handle LLM response parsing errors print(f\"Error parsing LLM response: {e}\") return [{\"error\": \"Failed to parse LLM output\", \"raw_output\": response.choices[0].message.content}] def _build_llm_prompt(self, url: str, content_chunk: str) -&gt; str: # Logic to construct the prompt using self.schema or self.instruction # and the content_chunk. Example: prompt_template = PROMPT_EXTRACT_SCHEMA_WITH_INSTRUCTION # Choose appropriate prompt variable_values = { \"URL\": url, \"CONTENT\": escape_json_string(content_chunk), # Send Markdown or HTML chunk \"SCHEMA\": json.dumps(self.schema) if self.schema else \"{}\", \"REQUEST\": self.instruction if self.instruction else \"Extract relevant data based on the schema.\" } prompt = prompt_template for var, val in variable_values.items(): prompt = prompt.replace(\"{\" + var + \"}\", str(val)) return prompt # run() method might override the base to handle chunking specifically for LLMs def run(self, url: str, sections: List[str], *q, **kwargs) -&gt; List[Dict[str, Any]]: # Potentially chunk sections based on token limits before calling extract # chunked_content = chunk_documents(sections, ...) # extracted_data = [] # for chunk in chunked_content: # extracted_data.extend(self.extract(url, chunk, **kwargs)) # return extracted_data # Simplified for now: return super().run(url, sections, *q, **kwargs) . ",
    "url": "/Crawl4AI/06_extractionstrategy.html#how-it-works-inside-under-the-hood",
    
    "relUrl": "/Crawl4AI/06_extractionstrategy.html#how-it-works-inside-under-the-hood"
  },"77": {
    "doc": "ExtractionStrategy",
    "title": "Conclusion",
    "content": "You’ve learned about ExtractionStrategy, Crawl4AI’s way of giving instructions to an “Analyst” to pull out specific, structured data from web content. | It solves the problem of needing precise data points (like product names, prices) in an organized format, not just blocks of text. | You can choose your “Analyst”: . | Precise Locators (JsonCssExtractionStrategy, JsonXPathExtractionStrategy): Use exact CSS/XPath selectors defined in a schema. Fast but brittle. | Smart Interpreter (LLMExtractionStrategy): Uses an AI (LLM) guided by a schema or instructions. More flexible but slower and needs setup. | . | You configure the desired strategy within the CrawlerRunConfig. | The extracted structured data is returned as a JSON string in the CrawlResult.extracted_content field. | . Now that we understand how to fetch, clean, filter, and extract data, let’s put it all together and look at the final package that Crawl4AI delivers after a crawl. Next: Let’s dive into the details of the output with Chapter 7: Understanding the Results - CrawlResult. Generated by AI Codebase Knowledge Builder . ",
    "url": "/Crawl4AI/06_extractionstrategy.html#conclusion",
    
    "relUrl": "/Crawl4AI/06_extractionstrategy.html#conclusion"
  },"78": {
    "doc": "ExtractionStrategy",
    "title": "ExtractionStrategy",
    "content": " ",
    "url": "/Crawl4AI/06_extractionstrategy.html",
    
    "relUrl": "/Crawl4AI/06_extractionstrategy.html"
  },"79": {
    "doc": "CrawlResult",
    "title": "Chapter 7: Understanding the Results - CrawlResult",
    "content": "In the previous chapter, Chapter 6: Getting Specific Data - ExtractionStrategy, we learned how to teach Crawl4AI to act like an analyst, extracting specific, structured data points from a webpage using an ExtractionStrategy. We’ve seen how Crawl4AI can fetch pages, clean them, filter them, and even extract precise information. But after all that work, where does all the gathered information go? When you ask the AsyncWebCrawler to crawl a URL using arun(), what do you actually get back? . ",
    "url": "/Crawl4AI/07_crawlresult.html#chapter-7-understanding-the-results---crawlresult",
    
    "relUrl": "/Crawl4AI/07_crawlresult.html#chapter-7-understanding-the-results---crawlresult"
  },"80": {
    "doc": "CrawlResult",
    "title": "What Problem Does CrawlResult Solve?",
    "content": "Imagine you sent a research assistant to the library (a website) with a set of instructions: “Find this book (URL), make a clean copy of the relevant chapter (clean HTML/Markdown), list all the cited references (links), take photos of the illustrations (media), find the author and publication date (metadata), and maybe extract specific quotes (structured data).” . When the assistant returns, they wouldn’t just hand you a single piece of paper. They’d likely give you a folder containing everything you asked for: the clean copy, the list of references, the photos, the metadata notes, and the extracted quotes, all neatly organized. They might also include a note if they encountered any problems (errors). CrawlResult is exactly this final report folder or delivery package. It’s a single object that neatly contains all the information Crawl4AI gathered and processed for a specific URL during a crawl operation. Instead of getting lots of separate pieces of data back, you get one convenient container. ",
    "url": "/Crawl4AI/07_crawlresult.html#what-problem-does-crawlresult-solve",
    
    "relUrl": "/Crawl4AI/07_crawlresult.html#what-problem-does-crawlresult-solve"
  },"81": {
    "doc": "CrawlResult",
    "title": "What is CrawlResult?",
    "content": "CrawlResult is a Python object (specifically, a Pydantic model, which is like a super-powered dictionary) that acts as a data container. It holds the results of a single crawl task performed by AsyncWebCrawler.arun() or one of the results from arun_many(). Think of it as a toolbox filled with different tools and information related to the crawled page. Key Information Stored in CrawlResult: . | url (string): The original URL that was requested. | success (boolean): Did the crawl complete without critical errors? True if successful, False otherwise. Always check this first! | html (string): The raw, original HTML source code fetched from the page. | cleaned_html (string): The HTML after initial cleaning by the ContentScrapingStrategy (e.g., scripts, styles removed). | markdown (object): An object containing different Markdown representations of the content. | markdown.raw_markdown: Basic Markdown generated from cleaned_html. | markdown.fit_markdown: Markdown generated only from content deemed relevant by a RelevantContentFilter (if one was used). Might be empty if no filter was applied. | (Other fields like markdown_with_citations might exist) | . | extracted_content (string): If you used an ExtractionStrategy, this holds the extracted structured data, usually formatted as a JSON string. None if no extraction was performed or nothing was found. | metadata (dictionary): Information extracted from the page’s metadata tags, like the page title (metadata['title']), description, keywords, etc. | links (object): Contains lists of links found on the page. | links.internal: List of links pointing to the same website. | links.external: List of links pointing to other websites. | . | media (object): Contains lists of media items found. | media.images: List of images (&lt;img&gt; tags). | media.videos: List of videos (&lt;video&gt; tags). | (Other media types might be included) | . | screenshot (string): If you requested a screenshot (screenshot=True in CrawlerRunConfig), this holds the file path to the saved image. None otherwise. | pdf (bytes): If you requested a PDF (pdf=True in CrawlerRunConfig), this holds the PDF data as bytes. None otherwise. (Note: Previously might have been a path, now often bytes). | error_message (string): If success is False, this field usually contains details about what went wrong. | status_code (integer): The HTTP status code received from the server (e.g., 200 for OK, 404 for Not Found). | response_headers (dictionary): The HTTP response headers sent by the server. | redirected_url (string): If the original URL redirected, this shows the final URL the crawler landed on. | . ",
    "url": "/Crawl4AI/07_crawlresult.html#what-is-crawlresult",
    
    "relUrl": "/Crawl4AI/07_crawlresult.html#what-is-crawlresult"
  },"82": {
    "doc": "CrawlResult",
    "title": "Accessing the CrawlResult",
    "content": "You get a CrawlResult object back every time you await a call to crawler.arun(): . # chapter7_example_1.py import asyncio from crawl4ai import AsyncWebCrawler async def main(): async with AsyncWebCrawler() as crawler: url = \"https://httpbin.org/html\" print(f\"Crawling {url}...\") # The 'arun' method returns a CrawlResult object result: CrawlResult = await crawler.arun(url=url) # Type hint optional print(\"Crawl finished!\") # Now 'result' holds all the information print(f\"Result object type: {type(result)}\") if __name__ == \"__main__\": asyncio.run(main()) . Explanation: . | We call crawler.arun(url=url). | The await keyword pauses execution until the crawl is complete. | The value returned by arun is assigned to the result variable. | This result variable is our CrawlResult object. | . If you use crawler.arun_many(), it returns a list where each item is a CrawlResult object for one of the requested URLs (or an async generator if stream=True). ",
    "url": "/Crawl4AI/07_crawlresult.html#accessing-the-crawlresult",
    
    "relUrl": "/Crawl4AI/07_crawlresult.html#accessing-the-crawlresult"
  },"83": {
    "doc": "CrawlResult",
    "title": "Exploring the Attributes: Using the Toolbox",
    "content": "Once you have the result object, you can access its attributes using dot notation (e.g., result.success, result.markdown). 1. Checking for Success (Most Important!) . Before you try to use any data, always check if the crawl was successful: . # chapter7_example_2.py import asyncio from crawl4ai import AsyncWebCrawler, CrawlResult # Import CrawlResult for type hint async def main(): async with AsyncWebCrawler() as crawler: url = \"https://httpbin.org/html\" # A working URL # url = \"https://httpbin.org/status/404\" # Try this URL to see failure result: CrawlResult = await crawler.arun(url=url) # --- ALWAYS CHECK 'success' FIRST! --- if result.success: print(f\"✅ Successfully crawled: {result.url}\") # Now it's safe to access other attributes print(f\" Page Title: {result.metadata.get('title', 'N/A')}\") else: print(f\"❌ Failed to crawl: {result.url}\") print(f\" Error: {result.error_message}\") print(f\" Status Code: {result.status_code}\") if __name__ == \"__main__\": asyncio.run(main()) . Explanation: . | We use an if result.success: block. | If True, we proceed to access other data like result.metadata. | If False, we print the result.error_message and result.status_code to understand why it failed. | . 2. Accessing Content (HTML, Markdown) . # chapter7_example_3.py import asyncio from crawl4ai import AsyncWebCrawler, CrawlResult async def main(): async with AsyncWebCrawler() as crawler: url = \"https://httpbin.org/html\" result: CrawlResult = await crawler.arun(url=url) if result.success: print(\"--- Content ---\") # Print the first 150 chars of raw HTML print(f\"Raw HTML snippet: {result.html[:150]}...\") # Access the raw markdown if result.markdown: # Check if markdown object exists print(f\"Markdown snippet: {result.markdown.raw_markdown[:150]}...\") else: print(\"Markdown not generated.\") else: print(f\"Crawl failed: {result.error_message}\") if __name__ == \"__main__\": asyncio.run(main()) . Explanation: . | We access result.html for the original HTML. | We access result.markdown.raw_markdown for the main Markdown content. Note the two dots: result.markdown gives the MarkdownGenerationResult object, and .raw_markdown accesses the specific string within it. We also check if result.markdown: first, just in case markdown generation failed for some reason. | . 3. Getting Metadata, Links, and Media . # chapter7_example_4.py import asyncio from crawl4ai import AsyncWebCrawler, CrawlResult async def main(): async with AsyncWebCrawler() as crawler: url = \"https://httpbin.org/links/10/0\" # A page with links result: CrawlResult = await crawler.arun(url=url) if result.success: print(\"--- Metadata &amp; Links ---\") print(f\"Title: {result.metadata.get('title', 'N/A')}\") print(f\"Found {len(result.links.internal)} internal links.\") print(f\"Found {len(result.links.external)} external links.\") if result.links.internal: print(f\" First internal link text: '{result.links.internal[0].text}'\") # Similarly access result.media.images etc. else: print(f\"Crawl failed: {result.error_message}\") if __name__ == \"__main__\": asyncio.run(main()) . Explanation: . | result.metadata is a dictionary; use .get() for safe access. | result.links and result.media are objects containing lists (internal, external, images, etc.). We can check their lengths (len()) and access individual items by index (e.g., [0]). | . 4. Checking for Extracted Data, Screenshots, PDFs . # chapter7_example_5.py import asyncio import json from crawl4ai import ( AsyncWebCrawler, CrawlResult, CrawlerRunConfig, JsonCssExtractionStrategy # Example extractor ) async def main(): # Define a simple extraction strategy (from Chapter 6) schema = {\"baseSelector\": \"body\", \"fields\": [{\"name\": \"heading\", \"selector\": \"h1\", \"type\": \"text\"}]} extractor = JsonCssExtractionStrategy(schema=schema) # Configure the run to extract and take a screenshot config = CrawlerRunConfig( extraction_strategy=extractor, screenshot=True ) async with AsyncWebCrawler() as crawler: url = \"https://httpbin.org/html\" result: CrawlResult = await crawler.arun(url=url, config=config) if result.success: print(\"--- Extracted Data &amp; Media ---\") # Check if structured data was extracted if result.extracted_content: print(\"Extracted Data found:\") data = json.loads(result.extracted_content) # Parse the JSON string print(json.dumps(data, indent=2)) else: print(\"No structured data extracted.\") # Check if a screenshot was taken if result.screenshot: print(f\"Screenshot saved to: {result.screenshot}\") else: print(\"Screenshot not taken.\") # Check for PDF (would be bytes if requested and successful) if result.pdf: print(f\"PDF data captured ({len(result.pdf)} bytes).\") else: print(\"PDF not generated.\") else: print(f\"Crawl failed: {result.error_message}\") if __name__ == \"__main__\": asyncio.run(main()) . Explanation: . | We check if result.extracted_content is not None or empty before trying to parse it as JSON. | We check if result.screenshot is not None to see if the file path exists. | We check if result.pdf is not None to see if the PDF data (bytes) was captured. | . ",
    "url": "/Crawl4AI/07_crawlresult.html#exploring-the-attributes-using-the-toolbox",
    
    "relUrl": "/Crawl4AI/07_crawlresult.html#exploring-the-attributes-using-the-toolbox"
  },"84": {
    "doc": "CrawlResult",
    "title": "How is CrawlResult Created? (Under the Hood)",
    "content": "You don’t interact with the CrawlResult constructor directly. The AsyncWebCrawler creates it for you at the very end of the arun process, typically inside its internal aprocess_html method (or just before returning if fetching from cache). Here’s a simplified sequence: . | Fetch: AsyncWebCrawler calls the AsyncCrawlerStrategy to get the raw html, status_code, response_headers, etc. | Scrape: It passes the html to the ContentScrapingStrategy to get cleaned_html, links, media, metadata. | Markdown: It generates Markdown using the configured generator, possibly involving a RelevantContentFilter, resulting in a MarkdownGenerationResult object. | Extract (Optional): If an ExtractionStrategy is configured, it runs it on the appropriate content (HTML or Markdown) to get extracted_content. | Screenshot/PDF (Optional): If requested, the fetching strategy captures the screenshot path or pdf data. | Package: AsyncWebCrawler gathers all these pieces (url, html, cleaned_html, the markdown object, links, media, metadata, extracted_content, screenshot, pdf, success status, error_message, etc.). | Instantiate: It creates the CrawlResult object, passing all the gathered data into its constructor. | Return: It returns this fully populated CrawlResult object to your code. | . ",
    "url": "/Crawl4AI/07_crawlresult.html#how-is-crawlresult-created-under-the-hood",
    
    "relUrl": "/Crawl4AI/07_crawlresult.html#how-is-crawlresult-created-under-the-hood"
  },"85": {
    "doc": "CrawlResult",
    "title": "Code Glimpse (models.py)",
    "content": "The CrawlResult is defined in the crawl4ai/models.py file. It uses Pydantic, a library that helps define data structures with type hints and validation. Here’s a simplified view: . # Simplified from crawl4ai/models.py from pydantic import BaseModel, HttpUrl from typing import List, Dict, Optional, Any # Other related models (simplified) class MarkdownGenerationResult(BaseModel): raw_markdown: str fit_markdown: Optional[str] = None # ... other markdown fields ... class Links(BaseModel): internal: List[Dict] = [] external: List[Dict] = [] class Media(BaseModel): images: List[Dict] = [] videos: List[Dict] = [] # The main CrawlResult model class CrawlResult(BaseModel): url: str html: str success: bool cleaned_html: Optional[str] = None media: Media = Media() # Use the Media model links: Links = Links() # Use the Links model screenshot: Optional[str] = None pdf: Optional[bytes] = None # Uses a private attribute and property for markdown for compatibility _markdown: Optional[MarkdownGenerationResult] = None # Actual storage extracted_content: Optional[str] = None # JSON string metadata: Optional[Dict[str, Any]] = None error_message: Optional[str] = None status_code: Optional[int] = None response_headers: Optional[Dict[str, str]] = None redirected_url: Optional[str] = None # ... other fields like session_id, ssl_certificate ... # Custom property to access markdown data @property def markdown(self) -&gt; Optional[MarkdownGenerationResult]: return self._markdown # Configuration for Pydantic class Config: arbitrary_types_allowed = True # Custom init and model_dump might exist for backward compatibility handling # ... (omitted for simplicity) ... Explanation: . | It’s defined as a class CrawlResult(BaseModel):. | Each attribute (like url, html, success) is defined with a type hint (like str, bool, Optional[str]). Optional[str] means the field can be a string or None. | Some attributes are themselves complex objects defined by other Pydantic models (like media: Media, links: Links). | The markdown field uses a common pattern (property wrapping a private attribute) to provide the MarkdownGenerationResult object while maintaining some backward compatibility. You access it simply as result.markdown. | . ",
    "url": "/Crawl4AI/07_crawlresult.html#code-glimpse-modelspy",
    
    "relUrl": "/Crawl4AI/07_crawlresult.html#code-glimpse-modelspy"
  },"86": {
    "doc": "CrawlResult",
    "title": "Conclusion",
    "content": "You’ve now met the CrawlResult object – the final, comprehensive report delivered by Crawl4AI after processing a URL. | It acts as a container holding all gathered information (HTML, Markdown, metadata, links, media, extracted data, errors, etc.). | It’s the return value of AsyncWebCrawler.arun() and arun_many(). | The most crucial attribute is success (boolean), which you should always check first. | You can easily access all the different pieces of information using dot notation (e.g., result.metadata['title'], result.markdown.raw_markdown, result.links.external). | . Understanding the CrawlResult is key to effectively using the information Crawl4AI provides. So far, we’ve focused on crawling single pages or lists of specific URLs. But what if you want to start at one page and automatically discover and crawl linked pages, exploring a website more deeply? . Next: Let’s explore how to perform multi-page crawls with Chapter 8: Exploring Websites - DeepCrawlStrategy. Generated by AI Codebase Knowledge Builder . ",
    "url": "/Crawl4AI/07_crawlresult.html#conclusion",
    
    "relUrl": "/Crawl4AI/07_crawlresult.html#conclusion"
  },"87": {
    "doc": "CrawlResult",
    "title": "CrawlResult",
    "content": " ",
    "url": "/Crawl4AI/07_crawlresult.html",
    
    "relUrl": "/Crawl4AI/07_crawlresult.html"
  },"88": {
    "doc": "DeepCrawlStrategy",
    "title": "Chapter 8: Exploring Websites - DeepCrawlStrategy",
    "content": "In Chapter 7: Understanding the Results - CrawlResult, we saw the final report (CrawlResult) that Crawl4AI gives us after processing a single URL. This report contains cleaned content, links, metadata, and maybe even extracted data. But what if you want to explore a website beyond just the first page? Imagine you land on a blog’s homepage. You don’t just want the homepage content; you want to automatically discover and crawl all the individual blog posts linked from it. How can you tell Crawl4AI to act like an explorer, following links and venturing deeper into the website? . ",
    "url": "/Crawl4AI/08_deepcrawlstrategy.html#chapter-8-exploring-websites---deepcrawlstrategy",
    
    "relUrl": "/Crawl4AI/08_deepcrawlstrategy.html#chapter-8-exploring-websites---deepcrawlstrategy"
  },"89": {
    "doc": "DeepCrawlStrategy",
    "title": "What Problem Does DeepCrawlStrategy Solve?",
    "content": "Think of the AsyncWebCrawler.arun() method we’ve used so far like visiting just the entrance hall of a vast library. You get information about that specific hall, but you don’t automatically explore the adjoining rooms or different floors. What if you want to systematically explore the library? You need a plan: . | Do you explore room by room on the current floor before going upstairs? (Level by level) | Do you pick one wing and explore all its rooms down to the very end before exploring another wing? (Go deep first) | Do you have a map highlighting potentially interesting sections and prioritize visiting those first? (Prioritize promising paths) | . DeepCrawlStrategy provides this exploration plan. It defines the logic for how Crawl4AI should discover and crawl new URLs starting from the initial one(s) by following the links it finds on each page. It turns the crawler from a single-page visitor into a website explorer. ",
    "url": "/Crawl4AI/08_deepcrawlstrategy.html#what-problem-does-deepcrawlstrategy-solve",
    
    "relUrl": "/Crawl4AI/08_deepcrawlstrategy.html#what-problem-does-deepcrawlstrategy-solve"
  },"90": {
    "doc": "DeepCrawlStrategy",
    "title": "What is DeepCrawlStrategy?",
    "content": "DeepCrawlStrategy is a concept (a blueprint) in Crawl4AI that represents the method or logic used to navigate and crawl multiple pages by following links. It tells the crawler which links to follow and in what order to visit them. It essentially takes over the process when you call arun() if a deep crawl is requested, managing a queue or list of URLs to visit and coordinating the crawling of those URLs, potentially up to a certain depth or number of pages. ",
    "url": "/Crawl4AI/08_deepcrawlstrategy.html#what-is-deepcrawlstrategy",
    
    "relUrl": "/Crawl4AI/08_deepcrawlstrategy.html#what-is-deepcrawlstrategy"
  },"91": {
    "doc": "DeepCrawlStrategy",
    "title": "Different Exploration Plans: The Strategies",
    "content": "Crawl4AI provides several concrete exploration plans (implementations) for DeepCrawlStrategy: . | BFSDeepCrawlStrategy (Level-by-Level Explorer): . | Analogy: Like ripples spreading in a pond. | How it works: It first crawls the starting URL (Level 0). Then, it crawls all the valid links found on that page (Level 1). Then, it crawls all the valid links found on those pages (Level 2), and so on. It explores the website layer by layer. | Good for: Finding the shortest path to all reachable pages, getting a broad overview quickly near the start page. | . | DFSDeepCrawlStrategy (Deep Path Explorer): . | Analogy: Like exploring one specific corridor in a maze all the way to the end before backtracking and trying another corridor. | How it works: It starts at the initial URL, follows one link, then follows a link from that page, and continues going deeper down one path as far as possible (or until a specified depth limit). Only when it hits a dead end or the limit does it backtrack and try another path. | Good for: Exploring specific branches of a website thoroughly, potentially reaching deeper pages faster than BFS (if the target is down a specific path). | . | BestFirstCrawlingStrategy (Priority Explorer): . | Analogy: Like using a treasure map where some paths are marked as more promising than others. | How it works: This strategy uses a scoring system. It looks at all the discovered (but not yet visited) links and assigns a score to each one based on how “promising” it seems (e.g., does the URL contain relevant keywords? Is it from a trusted domain?). It then crawls the link with the best score first, regardless of its depth. | Good for: Focusing the crawl on the most relevant or important pages first, especially useful when you can’t crawl the entire site and need to prioritize. | . | . Guiding the Explorer: Filters and Scorers . Deep crawl strategies often work together with: . | Filters: Rules that decide if a discovered link should even be considered for crawling. Examples: . | DomainFilter: Only follow links within the starting website’s domain. | URLPatternFilter: Only follow links matching a specific pattern (e.g., /blog/posts/...). | ContentTypeFilter: Avoid following links to non-HTML content like PDFs or images. | . | Scorers: (Used mainly by BestFirstCrawlingStrategy) Rules that assign a score to a potential link to help prioritize it. Examples: . | KeywordRelevanceScorer: Scores links higher if the URL contains certain keywords. | PathDepthScorer: Might score links differently based on how deep they are. | . | . These act like instructions for the explorer: “Only explore rooms on this floor (filter),” “Ignore corridors marked ‘Staff Only’ (filter),” or “Check rooms marked with a star first (scorer).” . ",
    "url": "/Crawl4AI/08_deepcrawlstrategy.html#different-exploration-plans-the-strategies",
    
    "relUrl": "/Crawl4AI/08_deepcrawlstrategy.html#different-exploration-plans-the-strategies"
  },"92": {
    "doc": "DeepCrawlStrategy",
    "title": "How to Use a DeepCrawlStrategy",
    "content": "You enable deep crawling by adding a DeepCrawlStrategy instance to your CrawlerRunConfig. Let’s try exploring a website layer by layer using BFSDeepCrawlStrategy, going only one level deep from the start page. # chapter8_example_1.py import asyncio from crawl4ai import ( AsyncWebCrawler, CrawlerRunConfig, BFSDeepCrawlStrategy, # 1. Import the desired strategy DomainFilter # Import a filter to stay on the same site ) async def main(): # 2. Create an instance of the strategy # - max_depth=1: Crawl start URL (depth 0) + links found (depth 1) # - filter_chain: Use DomainFilter to only follow links on the same website bfs_explorer = BFSDeepCrawlStrategy( max_depth=1, filter_chain=[DomainFilter()] # Stay within the initial domain ) print(f\"Strategy: BFS, Max Depth: {bfs_explorer.max_depth}\") # 3. Create CrawlerRunConfig and set the deep_crawl_strategy # Also set stream=True to get results as they come in. run_config = CrawlerRunConfig( deep_crawl_strategy=bfs_explorer, stream=True # Get results one by one using async for ) # 4. Run the crawl - arun now handles the deep crawl! async with AsyncWebCrawler() as crawler: start_url = \"https://httpbin.org/links/10/0\" # A page with 10 internal links print(f\"\\nStarting deep crawl from: {start_url}...\") crawl_results_generator = await crawler.arun(url=start_url, config=run_config) crawled_count = 0 # Iterate over the results as they are yielded async for result in crawl_results_generator: crawled_count += 1 status = \"✅\" if result.success else \"❌\" depth = result.metadata.get(\"depth\", \"N/A\") parent = result.metadata.get(\"parent_url\", \"Start\") url_short = result.url.split('/')[-1] # Show last part of URL print(f\" {status} Crawled: {url_short:&lt;6} (Depth: {depth})\") print(f\"\\nFinished deep crawl. Total pages processed: {crawled_count}\") # Expecting 1 (start URL) + 10 (links) = 11 results if __name__ == \"__main__\": asyncio.run(main()) . Explanation: . | Import: We import AsyncWebCrawler, CrawlerRunConfig, BFSDeepCrawlStrategy, and DomainFilter. | Instantiate Strategy: We create BFSDeepCrawlStrategy. | max_depth=1: We tell it to crawl the starting URL (depth 0) and any valid links it finds on that page (depth 1), but not to go any further. | filter_chain=[DomainFilter()]: We provide a list containing DomainFilter. This tells the strategy to only consider following links that point to the same domain as the start_url. Links to external sites will be ignored. | . | Configure Run: We create a CrawlerRunConfig and pass our bfs_explorer instance to the deep_crawl_strategy parameter. We also set stream=True so we can process results as soon as they are ready, rather than waiting for the entire crawl to finish. | Crawl: We call await crawler.arun(url=start_url, config=run_config). Because the config contains a deep_crawl_strategy, arun doesn’t just crawl the single start_url. Instead, it activates the deep crawl logic defined by BFSDeepCrawlStrategy. | Process Results: Since we used stream=True, the return value is an asynchronous generator. We use async for result in crawl_results_generator: to loop through the CrawlResult objects as they are produced by the deep crawl. For each result, we print its status and depth. | . You’ll see the output showing the crawl starting, then processing the initial page (links/10/0 at depth 0), followed by the 10 linked pages (e.g., 9, 8, … 0 at depth 1). ",
    "url": "/Crawl4AI/08_deepcrawlstrategy.html#how-to-use-a-deepcrawlstrategy",
    
    "relUrl": "/Crawl4AI/08_deepcrawlstrategy.html#how-to-use-a-deepcrawlstrategy"
  },"93": {
    "doc": "DeepCrawlStrategy",
    "title": "How It Works (Under the Hood)",
    "content": "How does simply putting a strategy in the config change arun’s behavior? It involves a bit of Python magic called a decorator. | Decorator: When you create an AsyncWebCrawler, its arun method is automatically wrapped by a DeepCrawlDecorator. | Check Config: When you call await crawler.arun(url=..., config=...), this decorator checks if config.deep_crawl_strategy is set. | Delegate or Run Original: . | If a strategy is set, the decorator doesn’t run the original single-page crawl logic. Instead, it calls the arun method of your chosen DeepCrawlStrategy instance (e.g., bfs_explorer.arun(...)), passing it the crawler itself, the start_url, and the config. | If no strategy is set, the decorator simply calls the original arun logic to crawl the single page. | . | Strategy Takes Over: The DeepCrawlStrategy’s arun method now manages the crawl. | It maintains a list or queue of URLs to visit (e.g., current_level in BFS, a stack in DFS, a priority queue in BestFirst). | It repeatedly takes batches of URLs from its list/queue. | For each batch, it calls crawler.arun_many(urls=batch_urls, config=batch_config) (with deep crawling disabled in batch_config to avoid infinite loops!). | As results come back from arun_many, the strategy processes them: . | It yields the CrawlResult if running in stream mode. | It extracts links using its link_discovery method. | link_discovery uses can_process_url (which applies filters) to validate links. | Valid new links are added to the list/queue for future crawling. | . | This continues until the list/queue is empty, the max depth/pages limit is reached, or it’s cancelled. | . | . sequenceDiagram participant User participant Decorator as DeepCrawlDecorator participant Strategy as DeepCrawlStrategy (e.g., BFS) participant AWC as AsyncWebCrawler User-&gt;&gt;Decorator: arun(start_url, config_with_strategy) Decorator-&gt;&gt;Strategy: arun(start_url, crawler=AWC, config) Note over Strategy: Initialize queue/level with start_url loop Until Queue Empty or Limits Reached Strategy-&gt;&gt;Strategy: Get next batch of URLs from queue Note over Strategy: Create batch_config (deep_crawl=None) Strategy-&gt;&gt;AWC: arun_many(batch_urls, config=batch_config) AWC--&gt;&gt;Strategy: batch_results (List/Stream of CrawlResult) loop For each result in batch_results Strategy-&gt;&gt;Strategy: Process result (yield if streaming) Strategy-&gt;&gt;Strategy: Discover links (apply filters) Strategy-&gt;&gt;Strategy: Add valid new links to queue end end Strategy--&gt;&gt;Decorator: Final result (List or Generator) Decorator--&gt;&gt;User: Final result . ",
    "url": "/Crawl4AI/08_deepcrawlstrategy.html#how-it-works-under-the-hood",
    
    "relUrl": "/Crawl4AI/08_deepcrawlstrategy.html#how-it-works-under-the-hood"
  },"94": {
    "doc": "DeepCrawlStrategy",
    "title": "Code Glimpse",
    "content": "Let’s peek at the simplified structure: . 1. The Decorator (deep_crawling/base_strategy.py) . # Simplified from deep_crawling/base_strategy.py from contextvars import ContextVar from functools import wraps # ... other imports class DeepCrawlDecorator: deep_crawl_active = ContextVar(\"deep_crawl_active\", default=False) def __init__(self, crawler: AsyncWebCrawler): self.crawler = crawler def __call__(self, original_arun): @wraps(original_arun) async def wrapped_arun(url: str, config: CrawlerRunConfig = None, **kwargs): # Is a strategy present AND not already inside a deep crawl? if config and config.deep_crawl_strategy and not self.deep_crawl_active.get(): # Mark that we are starting a deep crawl token = self.deep_crawl_active.set(True) try: # Call the STRATEGY's arun method instead of the original strategy_result = await config.deep_crawl_strategy.arun( crawler=self.crawler, start_url=url, config=config ) # Handle streaming if needed if config.stream: # Return an async generator that resets the context var on exit async def result_wrapper(): try: async for result in strategy_result: yield result finally: self.deep_crawl_active.reset(token) return result_wrapper() else: return strategy_result # Return the list of results directly finally: # Reset the context var if not streaming (or handled in wrapper) if not config.stream: self.deep_crawl_active.reset(token) else: # No strategy or already deep crawling, call the original single-page arun return await original_arun(url, config=config, **kwargs) return wrapped_arun . 2. The Strategy Blueprint (deep_crawling/base_strategy.py) . # Simplified from deep_crawling/base_strategy.py from abc import ABC, abstractmethod # ... other imports class DeepCrawlStrategy(ABC): @abstractmethod async def _arun_batch(self, start_url, crawler, config) -&gt; List[CrawlResult]: # Implementation for non-streaming mode pass @abstractmethod async def _arun_stream(self, start_url, crawler, config) -&gt; AsyncGenerator[CrawlResult, None]: # Implementation for streaming mode pass async def arun(self, start_url, crawler, config) -&gt; RunManyReturn: # Decides whether to call _arun_batch or _arun_stream if config.stream: return self._arun_stream(start_url, crawler, config) else: return await self._arun_batch(start_url, crawler, config) @abstractmethod async def can_process_url(self, url: str, depth: int) -&gt; bool: # Applies filters to decide if a URL is valid to crawl pass @abstractmethod async def link_discovery(self, result, source_url, current_depth, visited, next_level, depths): # Extracts, validates, and prepares links for the next step pass @abstractmethod async def shutdown(self): # Cleanup logic pass . 3. Example: BFS Implementation (deep_crawling/bfs_strategy.py) . # Simplified from deep_crawling/bfs_strategy.py # ... imports ... from .base_strategy import DeepCrawlStrategy # Import the base class class BFSDeepCrawlStrategy(DeepCrawlStrategy): def __init__(self, max_depth, filter_chain=None, url_scorer=None, ...): self.max_depth = max_depth self.filter_chain = filter_chain or FilterChain() # Use default if none self.url_scorer = url_scorer # ... other init ... self._pages_crawled = 0 async def can_process_url(self, url: str, depth: int) -&gt; bool: # ... (validation logic using self.filter_chain) ... is_valid = True # Placeholder if depth != 0 and not await self.filter_chain.apply(url): is_valid = False return is_valid async def link_discovery(self, result, source_url, current_depth, visited, next_level, depths): # ... (logic to get links from result.links) ... links = result.links.get(\"internal\", []) # Example: only internal for link_data in links: url = link_data.get(\"href\") if url and url not in visited: if await self.can_process_url(url, current_depth + 1): # Check scoring, max_pages limit etc. depths[url] = current_depth + 1 next_level.append((url, source_url)) # Add (url, parent) tuple async def _arun_batch(self, start_url, crawler, config) -&gt; List[CrawlResult]: visited = set() current_level = [(start_url, None)] # List of (url, parent_url) depths = {start_url: 0} all_results = [] while current_level: # While there are pages in the current level next_level = [] urls_in_level = [url for url, parent in current_level] visited.update(urls_in_level) # Create config for this batch (no deep crawl recursion) batch_config = config.clone(deep_crawl_strategy=None, stream=False) # Crawl all URLs in the current level batch_results = await crawler.arun_many(urls=urls_in_level, config=batch_config) for result in batch_results: # Add metadata (depth, parent) depth = depths.get(result.url, 0) result.metadata = result.metadata or {} result.metadata[\"depth\"] = depth # ... find parent ... all_results.append(result) # Discover links for the *next* level if result.success: await self.link_discovery(result, result.url, depth, visited, next_level, depths) current_level = next_level # Move to the next level return all_results async def _arun_stream(self, start_url, crawler, config) -&gt; AsyncGenerator[CrawlResult, None]: # Similar logic to _arun_batch, but uses 'yield result' # and processes results as they come from arun_many stream visited = set() current_level = [(start_url, None)] # List of (url, parent_url) depths = {start_url: 0} while current_level: next_level = [] urls_in_level = [url for url, parent in current_level] visited.update(urls_in_level) # Use stream=True for arun_many batch_config = config.clone(deep_crawl_strategy=None, stream=True) batch_results_gen = await crawler.arun_many(urls=urls_in_level, config=batch_config) async for result in batch_results_gen: # Add metadata depth = depths.get(result.url, 0) result.metadata = result.metadata or {} result.metadata[\"depth\"] = depth # ... find parent ... yield result # Yield result immediately # Discover links for the next level if result.success: await self.link_discovery(result, result.url, depth, visited, next_level, depths) current_level = next_level # ... shutdown method ... ",
    "url": "/Crawl4AI/08_deepcrawlstrategy.html#code-glimpse",
    
    "relUrl": "/Crawl4AI/08_deepcrawlstrategy.html#code-glimpse"
  },"95": {
    "doc": "DeepCrawlStrategy",
    "title": "Conclusion",
    "content": "You’ve learned about DeepCrawlStrategy, the component that turns Crawl4AI into a website explorer! . | It solves the problem of crawling beyond a single starting page by following links. | It defines the exploration plan: . | BFSDeepCrawlStrategy: Level by level. | DFSDeepCrawlStrategy: Deep paths first. | BestFirstCrawlingStrategy: Prioritized by score. | . | Filters and Scorers help guide the exploration. | You enable it by setting deep_crawl_strategy in the CrawlerRunConfig. | A decorator mechanism intercepts arun calls to activate the strategy. | The strategy manages the queue of URLs and uses crawler.arun_many to crawl them in batches. | . Deep crawling allows you to gather information from multiple related pages automatically. But how does Crawl4AI avoid re-fetching the same page over and over again, especially during these deeper crawls? The answer lies in caching. Next: Let’s explore how Crawl4AI smartly caches results with Chapter 9: Smart Fetching with Caching - CacheContext / CacheMode. Generated by AI Codebase Knowledge Builder . ",
    "url": "/Crawl4AI/08_deepcrawlstrategy.html#conclusion",
    
    "relUrl": "/Crawl4AI/08_deepcrawlstrategy.html#conclusion"
  },"96": {
    "doc": "DeepCrawlStrategy",
    "title": "DeepCrawlStrategy",
    "content": " ",
    "url": "/Crawl4AI/08_deepcrawlstrategy.html",
    
    "relUrl": "/Crawl4AI/08_deepcrawlstrategy.html"
  },"97": {
    "doc": "CacheContext & CacheMode",
    "title": "Chapter 9: Smart Fetching with Caching - CacheContext / CacheMode",
    "content": "In the previous chapter, Chapter 8: Exploring Websites - DeepCrawlStrategy, we saw how Crawl4AI can explore websites by following links, potentially visiting many pages. During such explorations, or even when you run the same crawl multiple times, the crawler might try to fetch the exact same webpage again and again. This can be slow and might unnecessarily put a load on the website you’re crawling. Wouldn’t it be smarter to remember the result from the first time and just reuse it? . ",
    "url": "/Crawl4AI/09_cachecontext___cachemode.html#chapter-9-smart-fetching-with-caching---cachecontext--cachemode",
    
    "relUrl": "/Crawl4AI/09_cachecontext___cachemode.html#chapter-9-smart-fetching-with-caching---cachecontext--cachemode"
  },"98": {
    "doc": "CacheContext & CacheMode",
    "title": "What Problem Does Caching Solve?",
    "content": "Imagine you need to download a large instruction manual (a webpage) from the internet. | Without Caching: Every single time you need the manual, you download the entire file again. This takes time and uses bandwidth every time. | With Caching: The first time you download it, you save a copy on your computer (the “cache”). The next time you need it, you first check your local copy. If it’s there, you use it instantly! You only download it again if you specifically want the absolute latest version or if your local copy is missing. | . Caching in Crawl4AI works the same way. It’s a mechanism to store the results of crawling a webpage locally (in a database file). When asked to crawl a URL again, Crawl4AI can check its cache first. If a valid result is already stored, it can return that saved result almost instantly, saving time and resources. ",
    "url": "/Crawl4AI/09_cachecontext___cachemode.html#what-problem-does-caching-solve",
    
    "relUrl": "/Crawl4AI/09_cachecontext___cachemode.html#what-problem-does-caching-solve"
  },"99": {
    "doc": "CacheContext & CacheMode",
    "title": "Introducing CacheMode and CacheContext",
    "content": "Crawl4AI uses two key concepts to manage this caching behavior: . | CacheMode (The Cache Policy): . | Think of this like setting the rules for how you interact with your saved instruction manuals. | It’s an instruction you give the crawler for a specific run, telling it how to use the cache. | Analogy: Should you always use your saved copy if you have one? (ENABLED) Should you ignore your saved copies and always download a fresh one? (BYPASS) Should you never save any copies? (DISABLED) Should you save new copies but never reuse old ones? (WRITE_ONLY) | CacheMode lets you choose the caching behavior that best fits your needs for a particular task. | . | CacheContext (The Decision Maker): . | This is an internal helper that Crawl4AI uses during a crawl. You don’t usually interact with it directly. | It looks at the CacheMode you provided (the policy) and the type of URL being processed. | Analogy: Imagine a librarian who checks the library’s borrowing rules (CacheMode) and the type of item you’re requesting (e.g., a reference book that can’t be checked out, like raw: HTML which isn’t cached). Based on these, the librarian (CacheContext) decides if you can borrow an existing copy (read from cache) or if a new copy should be added to the library (write to cache). | It helps the main AsyncWebCrawler make the right decision about reading from or writing to the cache for each specific URL based on the active policy. | . | . ",
    "url": "/Crawl4AI/09_cachecontext___cachemode.html#introducing-cachemode-and-cachecontext",
    
    "relUrl": "/Crawl4AI/09_cachecontext___cachemode.html#introducing-cachemode-and-cachecontext"
  },"100": {
    "doc": "CacheContext & CacheMode",
    "title": "Setting the Cache Policy: Using CacheMode",
    "content": "You control the caching behavior by setting the cache_mode parameter within the CrawlerRunConfig object that you pass to crawler.arun() or crawler.arun_many(). Let’s explore the most common CacheMode options: . 1. CacheMode.ENABLED (The Default Behavior - If not specified) . | Policy: “Use the cache if a valid result exists. If not, fetch the page, save the result to the cache, and then return it.” | This is the standard, balanced approach. It saves time on repeated crawls but ensures you get the content eventually. | Note: In recent versions, the default if cache_mode is left completely unspecified might be CacheMode.BYPASS. Always check the documentation or explicitly set the mode for clarity. For this tutorial, let’s assume we explicitly set it. | . # chapter9_example_1.py import asyncio from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode async def main(): url = \"https://httpbin.org/html\" async with AsyncWebCrawler() as crawler: # Explicitly set the mode to ENABLED config_enabled = CrawlerRunConfig(cache_mode=CacheMode.ENABLED) print(f\"Running with CacheMode: {config_enabled.cache_mode.name}\") # First run: Fetches, caches, and returns result print(\"First run (ENABLED)...\") result1 = await crawler.arun(url=url, config=config_enabled) print(f\"Got result 1? {'Yes' if result1.success else 'No'}\") # Second run: Finds result in cache and returns it instantly print(\"Second run (ENABLED)...\") result2 = await crawler.arun(url=url, config=config_enabled) print(f\"Got result 2? {'Yes' if result2.success else 'No'}\") # This second run should be much faster! if __name__ == \"__main__\": asyncio.run(main()) . Explanation: . | We create a CrawlerRunConfig with cache_mode=CacheMode.ENABLED. | The first arun call fetches the page from the web and saves the result in the cache. | The second arun call (for the same URL and config affecting cache key) finds the saved result in the cache and returns it immediately, skipping the web fetch. | . 2. CacheMode.BYPASS . | Policy: “Ignore any existing saved copy. Always fetch a fresh copy from the web. After fetching, save this new result to the cache (overwriting any old one).” | Useful when you always need the absolute latest version of the page, but you still want to update the cache for potential future use with CacheMode.ENABLED. | . # chapter9_example_2.py import asyncio from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode import time async def main(): url = \"https://httpbin.org/html\" async with AsyncWebCrawler() as crawler: # Set the mode to BYPASS config_bypass = CrawlerRunConfig(cache_mode=CacheMode.BYPASS) print(f\"Running with CacheMode: {config_bypass.cache_mode.name}\") # First run: Fetches, caches, and returns result print(\"First run (BYPASS)...\") start_time = time.perf_counter() result1 = await crawler.arun(url=url, config=config_bypass) duration1 = time.perf_counter() - start_time print(f\"Got result 1? {'Yes' if result1.success else 'No'} (took {duration1:.2f}s)\") # Second run: Ignores cache, fetches again, updates cache, returns result print(\"Second run (BYPASS)...\") start_time = time.perf_counter() result2 = await crawler.arun(url=url, config=config_bypass) duration2 = time.perf_counter() - start_time print(f\"Got result 2? {'Yes' if result2.success else 'No'} (took {duration2:.2f}s)\") # Both runs should take a similar amount of time (fetching time) if __name__ == \"__main__\": asyncio.run(main()) . Explanation: . | We set cache_mode=CacheMode.BYPASS. | Both the first and second arun calls will fetch the page directly from the web, ignoring any previously cached result. They will still write the newly fetched result to the cache. Notice both runs take roughly the same amount of time (network fetch time). | . 3. CacheMode.DISABLED . | Policy: “Completely ignore the cache. Never read from it, never write to it.” | Useful when you don’t want Crawl4AI to interact with the cache files at all, perhaps for debugging or if you have storage constraints. | . # chapter9_example_3.py import asyncio from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode import time async def main(): url = \"https://httpbin.org/html\" async with AsyncWebCrawler() as crawler: # Set the mode to DISABLED config_disabled = CrawlerRunConfig(cache_mode=CacheMode.DISABLED) print(f\"Running with CacheMode: {config_disabled.cache_mode.name}\") # First run: Fetches, returns result (does NOT cache) print(\"First run (DISABLED)...\") start_time = time.perf_counter() result1 = await crawler.arun(url=url, config=config_disabled) duration1 = time.perf_counter() - start_time print(f\"Got result 1? {'Yes' if result1.success else 'No'} (took {duration1:.2f}s)\") # Second run: Fetches again, returns result (does NOT cache) print(\"Second run (DISABLED)...\") start_time = time.perf_counter() result2 = await crawler.arun(url=url, config=config_disabled) duration2 = time.perf_counter() - start_time print(f\"Got result 2? {'Yes' if result2.success else 'No'} (took {duration2:.2f}s)\") # Both runs fetch fresh, and nothing is ever saved to the cache. if __name__ == \"__main__\": asyncio.run(main()) . Explanation: . | We set cache_mode=CacheMode.DISABLED. | Both arun calls fetch fresh content from the web. Crucially, neither run reads from nor writes to the cache database. | . Other Modes (READ_ONLY, WRITE_ONLY): . | CacheMode.READ_ONLY: Only uses existing cached results. If a result isn’t in the cache, it will fail or return an empty result rather than fetching it. Never saves anything new. | CacheMode.WRITE_ONLY: Never reads from the cache (always fetches fresh). It only writes the newly fetched result to the cache. | . ",
    "url": "/Crawl4AI/09_cachecontext___cachemode.html#setting-the-cache-policy-using-cachemode",
    
    "relUrl": "/Crawl4AI/09_cachecontext___cachemode.html#setting-the-cache-policy-using-cachemode"
  },"101": {
    "doc": "CacheContext & CacheMode",
    "title": "How Caching Works Internally",
    "content": "When you call crawler.arun(url=\"...\", config=...): . | Create Context: The AsyncWebCrawler creates a CacheContext instance using the url and the config.cache_mode. | Check Read: It asks the CacheContext, “Should I read from the cache?” (cache_context.should_read()). | Try Reading: If should_read() is True, it asks the database manager (AsyncDatabaseManager) to look for a cached result for the url. | Cache Hit? . | If a valid cached result is found: The AsyncWebCrawler returns this cached CrawlResult immediately. Done! | If no cached result is found (or if should_read() was False): Proceed to fetching. | . | Fetch: The AsyncWebCrawler calls the appropriate AsyncCrawlerStrategy to fetch the content from the web. | Process: It processes the fetched HTML (scraping, filtering, extracting) to create a new CrawlResult. | Check Write: It asks the CacheContext, “Should I write this result to the cache?” (cache_context.should_write()). | Write Cache: If should_write() is True, it tells the database manager to save the new CrawlResult into the cache database. | Return: The AsyncWebCrawler returns the newly created CrawlResult. | . sequenceDiagram participant User participant AWC as AsyncWebCrawler participant Ctx as CacheContext participant DB as DatabaseManager participant Fetcher as AsyncCrawlerStrategy User-&gt;&gt;AWC: arun(url, config) AWC-&gt;&gt;Ctx: Create CacheContext(url, config.cache_mode) AWC-&gt;&gt;Ctx: should_read()? alt Cache Read Allowed Ctx--&gt;&gt;AWC: Yes AWC-&gt;&gt;DB: aget_cached_url(url) DB--&gt;&gt;AWC: Cached Result (or None) alt Cache Hit &amp; Valid AWC--&gt;&gt;User: Return Cached CrawlResult else Cache Miss or Invalid AWC-&gt;&gt;AWC: Proceed to Fetch end else Cache Read Not Allowed Ctx--&gt;&gt;AWC: No AWC-&gt;&gt;AWC: Proceed to Fetch end Note over AWC: Fetching Required AWC-&gt;&gt;Fetcher: crawl(url, config) Fetcher--&gt;&gt;AWC: Raw Response AWC-&gt;&gt;AWC: Process HTML -&gt; New CrawlResult AWC-&gt;&gt;Ctx: should_write()? alt Cache Write Allowed Ctx--&gt;&gt;AWC: Yes AWC-&gt;&gt;DB: acache_url(New CrawlResult) DB--&gt;&gt;AWC: OK else Cache Write Not Allowed Ctx--&gt;&gt;AWC: No end AWC--&gt;&gt;User: Return New CrawlResult . ",
    "url": "/Crawl4AI/09_cachecontext___cachemode.html#how-caching-works-internally",
    
    "relUrl": "/Crawl4AI/09_cachecontext___cachemode.html#how-caching-works-internally"
  },"102": {
    "doc": "CacheContext & CacheMode",
    "title": "Code Glimpse",
    "content": "Let’s look at simplified code snippets. Inside async_webcrawler.py (where arun uses caching): . # Simplified from crawl4ai/async_webcrawler.py from .cache_context import CacheContext, CacheMode from .async_database import async_db_manager from .models import CrawlResult # ... other imports class AsyncWebCrawler: # ... (init, other methods) ... async def arun(self, url: str, config: CrawlerRunConfig = None) -&gt; CrawlResult: # ... (ensure config exists, set defaults) ... if config.cache_mode is None: config.cache_mode = CacheMode.ENABLED # Example default # 1. Create CacheContext cache_context = CacheContext(url, config.cache_mode) cached_result = None # 2. Check if cache read is allowed if cache_context.should_read(): # 3. Try reading from database cached_result = await async_db_manager.aget_cached_url(url) # 4. If cache hit and valid, return it if cached_result and self._is_cache_valid(cached_result, config): self.logger.info(\"Cache hit for: %s\", url) # Example log return cached_result # Return early # 5. Fetch fresh content (if no cache hit or read disabled) async_response = await self.crawler_strategy.crawl(url, config=config) html = async_response.html # ... and other data ... # 6. Process the HTML to get a new CrawlResult crawl_result = await self.aprocess_html( url=url, html=html, config=config, # ... other params ... ) # 7. Check if cache write is allowed if cache_context.should_write(): # 8. Write the new result to the database await async_db_manager.acache_url(crawl_result) # 9. Return the new result return crawl_result def _is_cache_valid(self, cached_result: CrawlResult, config: CrawlerRunConfig) -&gt; bool: # Internal logic to check if cached result meets current needs # (e.g., was screenshot requested now but not cached?) if config.screenshot and not cached_result.screenshot: return False if config.pdf and not cached_result.pdf: return False # ... other checks ... return True . Inside cache_context.py (defining the concepts): . # Simplified from crawl4ai/cache_context.py from enum import Enum class CacheMode(Enum): \"\"\"Defines the caching behavior for web crawling operations.\"\"\" ENABLED = \"enabled\" # Read and Write DISABLED = \"disabled\" # No Read, No Write READ_ONLY = \"read_only\" # Read Only, No Write WRITE_ONLY = \"write_only\" # Write Only, No Read BYPASS = \"bypass\" # No Read, Write Only (similar to WRITE_ONLY but explicit intention) class CacheContext: \"\"\"Encapsulates cache-related decisions and URL handling.\"\"\" def __init__(self, url: str, cache_mode: CacheMode, always_bypass: bool = False): self.url = url self.cache_mode = cache_mode self.always_bypass = always_bypass # Usually False # Determine if URL type is cacheable (e.g., not 'raw:') self.is_cacheable = url.startswith((\"http://\", \"https://\", \"file://\")) # ... other URL type checks ... def should_read(self) -&gt; bool: \"\"\"Determines if cache should be read based on context.\"\"\" if self.always_bypass or not self.is_cacheable: return False # Allow read if mode is ENABLED or READ_ONLY return self.cache_mode in [CacheMode.ENABLED, CacheMode.READ_ONLY] def should_write(self) -&gt; bool: \"\"\"Determines if cache should be written based on context.\"\"\" if self.always_bypass or not self.is_cacheable: return False # Allow write if mode is ENABLED, WRITE_ONLY, or BYPASS return self.cache_mode in [CacheMode.ENABLED, CacheMode.WRITE_ONLY, CacheMode.BYPASS] @property def display_url(self) -&gt; str: \"\"\"Returns the URL in display format.\"\"\" return self.url if not self.url.startswith(\"raw:\") else \"Raw HTML\" # Helper for backward compatibility (may be removed later) def _legacy_to_cache_mode(...) -&gt; CacheMode: # ... logic to convert old boolean flags ... pass . ",
    "url": "/Crawl4AI/09_cachecontext___cachemode.html#code-glimpse",
    
    "relUrl": "/Crawl4AI/09_cachecontext___cachemode.html#code-glimpse"
  },"103": {
    "doc": "CacheContext & CacheMode",
    "title": "Conclusion",
    "content": "You’ve learned how Crawl4AI uses caching to avoid redundant work and speed up repeated crawls! . | Caching stores results locally to reuse them later. | CacheMode is the policy you set in CrawlerRunConfig to control how the cache is used (ENABLED, BYPASS, DISABLED, etc.). | CacheContext is an internal helper that makes decisions based on the CacheMode and URL type. | Using the cache effectively (especially CacheMode.ENABLED) can significantly speed up your crawling tasks, particularly during development or when dealing with many URLs, including deep crawls. | . We’ve seen how Crawl4AI can crawl single pages, lists of pages (arun_many), and even explore websites (DeepCrawlStrategy). But how does arun_many or a deep crawl manage running potentially hundreds or thousands of individual crawl tasks efficiently without overwhelming your system or the target website? . Next: Let’s explore the component responsible for managing concurrent tasks: Chapter 10: Orchestrating the Crawl - BaseDispatcher. Generated by AI Codebase Knowledge Builder . ",
    "url": "/Crawl4AI/09_cachecontext___cachemode.html#conclusion",
    
    "relUrl": "/Crawl4AI/09_cachecontext___cachemode.html#conclusion"
  },"104": {
    "doc": "CacheContext & CacheMode",
    "title": "CacheContext & CacheMode",
    "content": " ",
    "url": "/Crawl4AI/09_cachecontext___cachemode.html",
    
    "relUrl": "/Crawl4AI/09_cachecontext___cachemode.html"
  },"105": {
    "doc": "BaseDispatcher",
    "title": "Chapter 10: Orchestrating the Crawl - BaseDispatcher",
    "content": "In Chapter 9: Smart Fetching with Caching - CacheContext / CacheMode, we learned how Crawl4AI uses caching to cleverly avoid re-fetching the same webpage multiple times, which is especially helpful when crawling many URLs. We’ve also seen how methods like arun_many() (Chapter 2: Meet the General Manager - AsyncWebCrawler) or strategies like DeepCrawlStrategy can lead to potentially hundreds or thousands of individual URLs needing to be crawled. This raises a question: if we have 1000 URLs to crawl, does Crawl4AI try to crawl all 1000 simultaneously? That would likely overwhelm your computer’s resources (like memory and CPU) and could also flood the target website with too many requests, potentially getting you blocked! How does Crawl4AI manage running many crawls efficiently and responsibly? . ",
    "url": "/Crawl4AI/10_basedispatcher.html#chapter-10-orchestrating-the-crawl---basedispatcher",
    
    "relUrl": "/Crawl4AI/10_basedispatcher.html#chapter-10-orchestrating-the-crawl---basedispatcher"
  },"106": {
    "doc": "BaseDispatcher",
    "title": "What Problem Does BaseDispatcher Solve?",
    "content": "Imagine you’re managing a fleet of delivery drones (AsyncWebCrawler tasks) that need to pick up packages from many different addresses (URLs). If you launch all 1000 drones at the exact same moment: . | Your control station (your computer) might crash due to the processing load. | The central warehouse (the target website) might get overwhelmed by simultaneous arrivals. | Some drones might collide or interfere with each other. | . You need a Traffic Controller or a Dispatch Center to manage the fleet. This controller decides: . | How many drones can be active in the air at any one time. | When to launch the next drone, maybe based on available airspace (system resources) or just a simple count limit. | How to handle potential delays or issues (like rate limiting from a specific website). | . In Crawl4AI, the BaseDispatcher acts as this Traffic Controller or Task Scheduler for concurrent crawling operations, primarily when using arun_many(). It manages how multiple crawl tasks are executed concurrently, ensuring the process is efficient without overwhelming your system or the target websites. ",
    "url": "/Crawl4AI/10_basedispatcher.html#what-problem-does-basedispatcher-solve",
    
    "relUrl": "/Crawl4AI/10_basedispatcher.html#what-problem-does-basedispatcher-solve"
  },"107": {
    "doc": "BaseDispatcher",
    "title": "What is BaseDispatcher?",
    "content": "BaseDispatcher is an abstract concept (a blueprint or job description) in Crawl4AI. It defines that we need a system for managing the execution of multiple, concurrent crawling tasks. It specifies the interface for how the main AsyncWebCrawler interacts with such a system, but the specific logic for managing concurrency can vary. Think of it as the control panel for our drone fleet – the panel exists, but the specific rules programmed into it determine how drones are dispatched. ",
    "url": "/Crawl4AI/10_basedispatcher.html#what-is-basedispatcher",
    
    "relUrl": "/Crawl4AI/10_basedispatcher.html#what-is-basedispatcher"
  },"108": {
    "doc": "BaseDispatcher",
    "title": "The Different Controllers: Ways to Dispatch Tasks",
    "content": "Crawl4AI provides concrete implementations (the actual traffic control systems) based on the BaseDispatcher blueprint: . | SemaphoreDispatcher (The Simple Counter): . | Analogy: A parking garage with a fixed number of spots (e.g., 10). A gate (asyncio.Semaphore) only lets a new car in if one of the 10 spots is free. | How it works: You tell it the maximum number of crawls that can run at the same time (e.g., semaphore_count=10). It uses a simple counter (a semaphore) to ensure that no more than this number of crawls are active simultaneously. When one crawl finishes, it allows another one from the queue to start. | Good for: Simple, direct control over concurrency when you know a specific limit works well for your system and the target sites. | . | MemoryAdaptiveDispatcher (The Resource-Aware Controller - Default): . | Analogy: A smart parking garage attendant who checks not just the number of cars, but also the total space they occupy (system memory). They might stop letting cars in if the garage is nearing its memory capacity, even if some numbered spots are technically free. | How it works: This dispatcher monitors your system’s available memory. It tries to run multiple crawls concurrently (up to a configurable maximum like max_session_permit), but it will pause launching new crawls if the system memory usage exceeds a certain threshold (e.g., memory_threshold_percent=90.0). It adapts the concurrency level based on available resources. | Good for: Automatically adjusting concurrency to prevent out-of-memory errors, especially when crawl tasks vary significantly in resource usage. This is the default dispatcher used by arun_many if you don’t specify one. | . | . These dispatchers can also optionally work with a RateLimiter component, which adds politeness rules for specific websites (e.g., slowing down requests to a domain if it returns “429 Too Many Requests”). ",
    "url": "/Crawl4AI/10_basedispatcher.html#the-different-controllers-ways-to-dispatch-tasks",
    
    "relUrl": "/Crawl4AI/10_basedispatcher.html#the-different-controllers-ways-to-dispatch-tasks"
  },"109": {
    "doc": "BaseDispatcher",
    "title": "How arun_many Uses the Dispatcher",
    "content": "When you call crawler.arun_many(urls=...), here’s the basic flow involving the dispatcher: . | Get URLs: arun_many receives the list of URLs you want to crawl. | Select Dispatcher: It checks if you provided a specific dispatcher instance. If not, it creates an instance of the default MemoryAdaptiveDispatcher. | Delegate Execution: It hands over the list of URLs and the CrawlerRunConfig to the chosen dispatcher’s run_urls (or run_urls_stream) method. | Manage Tasks: The dispatcher takes charge: . | It iterates through the URLs. | For each URL, it decides when to start the actual crawl based on its rules (semaphore count, memory usage, rate limits). | When ready, it typically calls the single-page crawler.arun(url, config) method internally for that specific URL, wrapped within its concurrency control mechanism. | It manages the running tasks (e.g., using asyncio.create_task and asyncio.wait). | . | Collect Results: As individual arun calls complete, the dispatcher collects their CrawlResult objects. | Return: Once all URLs are processed, the dispatcher returns the list of results (or yields them if streaming). | . sequenceDiagram participant User participant AWC as AsyncWebCrawler participant Dispatcher as BaseDispatcher (e.g., MemoryAdaptive) participant TaskPool as Concurrency Manager User-&gt;&gt;AWC: arun_many(urls, config, dispatcher?) AWC-&gt;&gt;Dispatcher: run_urls(crawler=AWC, urls, config) Dispatcher-&gt;&gt;TaskPool: Initialize (e.g., set max concurrency) loop For each URL in urls Dispatcher-&gt;&gt;TaskPool: Can I start a new task? (Checks limits) alt Yes TaskPool--&gt;&gt;Dispatcher: OK Note over Dispatcher: Create task: call AWC.arun(url, config) internally Dispatcher-&gt;&gt;TaskPool: Add new task else No TaskPool--&gt;&gt;Dispatcher: Wait Note over Dispatcher: Waits for a running task to finish end end Note over Dispatcher: Manages running tasks, collects results Dispatcher--&gt;&gt;AWC: List of CrawlResults AWC--&gt;&gt;User: List of CrawlResults . ",
    "url": "/Crawl4AI/10_basedispatcher.html#how-arun_many-uses-the-dispatcher",
    
    "relUrl": "/Crawl4AI/10_basedispatcher.html#how-arun_many-uses-the-dispatcher"
  },"110": {
    "doc": "BaseDispatcher",
    "title": "Using the Dispatcher (Often Implicitly!)",
    "content": "Most of the time, you don’t need to think about the dispatcher explicitly. When you use arun_many, the default MemoryAdaptiveDispatcher handles things automatically. # chapter10_example_1.py import asyncio from crawl4ai import AsyncWebCrawler, CrawlerRunConfig async def main(): urls_to_crawl = [ \"https://httpbin.org/html\", \"https://httpbin.org/links/5/0\", # Page with 5 links \"https://httpbin.org/robots.txt\", \"https://httpbin.org/status/200\", ] # We DON'T specify a dispatcher here. # arun_many will use the default MemoryAdaptiveDispatcher. async with AsyncWebCrawler() as crawler: print(f\"Crawling {len(urls_to_crawl)} URLs using the default dispatcher...\") config = CrawlerRunConfig(stream=False) # Get results as a list at the end # The MemoryAdaptiveDispatcher manages concurrency behind the scenes. results = await crawler.arun_many(urls=urls_to_crawl, config=config) print(f\"\\nFinished! Got {len(results)} results.\") for result in results: status = \"✅\" if result.success else \"❌\" url_short = result.url.split('/')[-1] print(f\" {status} {url_short:&lt;15} | Title: {result.metadata.get('title', 'N/A')}\") if __name__ == \"__main__\": asyncio.run(main()) . Explanation: . | We call crawler.arun_many without passing a dispatcher argument. | Crawl4AI automatically creates and uses a MemoryAdaptiveDispatcher. | This dispatcher runs the crawls concurrently, adapting to your system’s memory, and returns all the results once completed (because stream=False). You benefit from concurrency without explicit setup. | . ",
    "url": "/Crawl4AI/10_basedispatcher.html#using-the-dispatcher-often-implicitly",
    
    "relUrl": "/Crawl4AI/10_basedispatcher.html#using-the-dispatcher-often-implicitly"
  },"111": {
    "doc": "BaseDispatcher",
    "title": "Explicitly Choosing a Dispatcher",
    "content": "What if you want simpler, fixed concurrency? You can explicitly create and pass a SemaphoreDispatcher. # chapter10_example_2.py import asyncio from crawl4ai import ( AsyncWebCrawler, CrawlerRunConfig, SemaphoreDispatcher # 1. Import the specific dispatcher ) async def main(): urls_to_crawl = [ \"https://httpbin.org/delay/1\", # Takes 1 second \"https://httpbin.org/delay/1\", \"https://httpbin.org/delay/1\", \"https://httpbin.org/delay/1\", \"https://httpbin.org/delay/1\", ] # 2. Create an instance of the SemaphoreDispatcher # Allow only 2 crawls to run at the same time. semaphore_controller = SemaphoreDispatcher(semaphore_count=2) print(f\"Using SemaphoreDispatcher with limit: {semaphore_controller.semaphore_count}\") async with AsyncWebCrawler() as crawler: print(f\"Crawling {len(urls_to_crawl)} URLs with explicit dispatcher...\") config = CrawlerRunConfig(stream=False) # 3. Pass the dispatcher instance to arun_many results = await crawler.arun_many( urls=urls_to_crawl, config=config, dispatcher=semaphore_controller # Pass our controller ) print(f\"\\nFinished! Got {len(results)} results.\") # This crawl likely took around 3 seconds (5 tasks, 1s each, 2 concurrent = ceil(5/2)*1s) for result in results: status = \"✅\" if result.success else \"❌\" print(f\" {status} {result.url}\") if __name__ == \"__main__\": asyncio.run(main()) . Explanation: . | Import: We import SemaphoreDispatcher. | Instantiate: We create SemaphoreDispatcher(semaphore_count=2), limiting concurrency to 2 simultaneous crawls. | Pass Dispatcher: We pass our semaphore_controller instance directly to the dispatcher parameter of arun_many. | Execution: Now, arun_many uses our SemaphoreDispatcher. It will start the first two crawls. As one finishes, it will start the next one from the list, always ensuring no more than two are running concurrently. | . ",
    "url": "/Crawl4AI/10_basedispatcher.html#explicitly-choosing-a-dispatcher",
    
    "relUrl": "/Crawl4AI/10_basedispatcher.html#explicitly-choosing-a-dispatcher"
  },"112": {
    "doc": "BaseDispatcher",
    "title": "A Glimpse Under the Hood",
    "content": "Where are these dispatchers defined? In crawl4ai/async_dispatcher.py. The Blueprint (BaseDispatcher): . # Simplified from crawl4ai/async_dispatcher.py from abc import ABC, abstractmethod from typing import List, Optional # ... other imports like CrawlerRunConfig, CrawlerTaskResult, AsyncWebCrawler ... class BaseDispatcher(ABC): def __init__( self, rate_limiter: Optional[RateLimiter] = None, monitor: Optional[CrawlerMonitor] = None, ): self.crawler = None # Will be set by arun_many self.rate_limiter = rate_limiter self.monitor = monitor # ... other common state ... @abstractmethod async def crawl_url( self, url: str, config: CrawlerRunConfig, task_id: str, # ... maybe other internal params ... ) -&gt; CrawlerTaskResult: \"\"\"Crawls a single URL, potentially handling concurrency primitives.\"\"\" # This is often the core worker method called by run_urls pass @abstractmethod async def run_urls( self, urls: List[str], crawler: \"AsyncWebCrawler\", config: CrawlerRunConfig, ) -&gt; List[CrawlerTaskResult]: \"\"\"Manages the concurrent execution of crawl_url for multiple URLs.\"\"\" # This is the main entry point called by arun_many pass async def run_urls_stream( self, urls: List[str], crawler: \"AsyncWebCrawler\", config: CrawlerRunConfig, ) -&gt; AsyncGenerator[CrawlerTaskResult, None]: \"\"\" Streaming version of run_urls (might be implemented in base or subclasses) \"\"\" # Example default implementation (subclasses might override) results = await self.run_urls(urls, crawler, config) for res in results: yield res # Naive stream, real one is more complex # ... other potential helper methods ... Example Implementation (SemaphoreDispatcher): . # Simplified from crawl4ai/async_dispatcher.py import asyncio import uuid import psutil # For memory tracking in crawl_url import time # For timing in crawl_url # ... other imports ... class SemaphoreDispatcher(BaseDispatcher): def __init__( self, semaphore_count: int = 5, # ... other params like rate_limiter, monitor ... ): super().__init__(...) # Pass rate_limiter, monitor to base self.semaphore_count = semaphore_count async def crawl_url( self, url: str, config: CrawlerRunConfig, task_id: str, semaphore: asyncio.Semaphore = None, # Takes the semaphore ) -&gt; CrawlerTaskResult: # ... (Code to track start time, memory usage - similar to MemoryAdaptiveDispatcher's version) start_time = time.time() error_message = \"\" memory_usage = peak_memory = 0.0 result = None try: # Update monitor state if used if self.monitor: self.monitor.update_task(task_id, status=CrawlStatus.IN_PROGRESS) # Wait for rate limiter if used if self.rate_limiter: await self.rate_limiter.wait_if_needed(url) # --- Core Semaphore Logic --- async with semaphore: # Acquire a spot from the semaphore # Now that we have a spot, run the actual crawl process = psutil.Process() start_memory = process.memory_info().rss / (1024 * 1024) # Call the single-page crawl method of the main crawler result = await self.crawler.arun(url, config=config, session_id=task_id) end_memory = process.memory_info().rss / (1024 * 1024) memory_usage = peak_memory = end_memory - start_memory # --- Semaphore spot is released automatically on exiting 'async with' --- # Update rate limiter based on result status if used if self.rate_limiter and result.status_code: if not self.rate_limiter.update_delay(url, result.status_code): # Handle retry limit exceeded error_message = \"Rate limit retry count exceeded\" # ... update monitor, prepare error result ... # Update monitor status (success/fail) if result and not result.success: error_message = result.error_message if self.monitor: self.monitor.update_task(task_id, status=CrawlStatus.COMPLETED if result.success else CrawlStatus.FAILED) except Exception as e: # Handle unexpected errors during the crawl error_message = str(e) if self.monitor: self.monitor.update_task(task_id, status=CrawlStatus.FAILED) # Create a failed CrawlResult if needed if not result: result = CrawlResult(url=url, html=\"\", success=False, error_message=error_message) finally: # Final monitor update with timing, memory etc. end_time = time.time() if self.monitor: self.monitor.update_task(...) # Package everything into CrawlerTaskResult return CrawlerTaskResult(...) async def run_urls( self, crawler: \"AsyncWebCrawler\", urls: List[str], config: CrawlerRunConfig, ) -&gt; List[CrawlerTaskResult]: self.crawler = crawler # Store the crawler instance if self.monitor: self.monitor.start() try: # Create the semaphore with the specified count semaphore = asyncio.Semaphore(self.semaphore_count) tasks = [] # Create a crawl task for each URL, passing the semaphore for url in urls: task_id = str(uuid.uuid4()) if self.monitor: self.monitor.add_task(task_id, url) # Create an asyncio task to run crawl_url task = asyncio.create_task( self.crawl_url(url, config, task_id, semaphore=semaphore) ) tasks.append(task) # Wait for all created tasks to complete # asyncio.gather runs them concurrently, respecting the semaphore limit results = await asyncio.gather(*tasks, return_exceptions=True) # Process results (handle potential exceptions returned by gather) final_results = [] for res in results: if isinstance(res, Exception): # Handle case where gather caught an exception from a task # You might create a failed CrawlerTaskResult here pass elif isinstance(res, CrawlerTaskResult): final_results.append(res) return final_results finally: if self.monitor: self.monitor.stop() # run_urls_stream would have similar logic but use asyncio.as_completed # or manage tasks manually to yield results as they finish. The key takeaway is that the Dispatcher orchestrates calls to the single-page crawler.arun method, wrapping them with concurrency controls (like the async with semaphore: block) before running them using asyncio’s concurrency tools (asyncio.create_task, asyncio.gather, etc.). ",
    "url": "/Crawl4AI/10_basedispatcher.html#a-glimpse-under-the-hood",
    
    "relUrl": "/Crawl4AI/10_basedispatcher.html#a-glimpse-under-the-hood"
  },"113": {
    "doc": "BaseDispatcher",
    "title": "Conclusion",
    "content": "You’ve learned about BaseDispatcher, the crucial “Traffic Controller” that manages concurrent crawls in Crawl4AI, especially for arun_many. | It solves the problem of efficiently running many crawls without overloading systems or websites. | It acts as a blueprint for managing concurrency. | Key implementations: . | SemaphoreDispatcher: Uses a simple count limit. | MemoryAdaptiveDispatcher: Adjusts concurrency based on system memory (the default for arun_many). | . | The dispatcher is used automatically by arun_many, but you can provide a specific instance if needed. | It orchestrates the execution of individual crawl tasks, respecting defined limits. | . Understanding the dispatcher helps appreciate how Crawl4AI handles large-scale crawling tasks responsibly and efficiently. This concludes our tour of the core concepts in Crawl4AI! We’ve covered how pages are fetched, how the process is managed, how content is cleaned, filtered, and extracted, how deep crawls are performed, how caching optimizes fetches, and finally, how concurrency is managed. You now have a solid foundation to start building powerful web data extraction and processing applications with Crawl4AI. Happy crawling! . Generated by AI Codebase Knowledge Builder . ",
    "url": "/Crawl4AI/10_basedispatcher.html#conclusion",
    
    "relUrl": "/Crawl4AI/10_basedispatcher.html#conclusion"
  },"114": {
    "doc": "BaseDispatcher",
    "title": "BaseDispatcher",
    "content": " ",
    "url": "/Crawl4AI/10_basedispatcher.html",
    
    "relUrl": "/Crawl4AI/10_basedispatcher.html"
  },"115": {
    "doc": "System Design",
    "title": "System Design: Codebase Knowledge Builder",
    "content": "Please DON’T remove notes for AI . ",
    "url": "/design.html#system-design-codebase-knowledge-builder",
    
    "relUrl": "/design.html#system-design-codebase-knowledge-builder"
  },"116": {
    "doc": "System Design",
    "title": "Requirements",
    "content": "Notes for AI: Keep it simple and clear. If the requirements are abstract, write concrete user stories . User Story: As a developer onboarding to a new codebase, I want a tutorial automatically generated from its GitHub repository or local directory, optionally in a specific language. This tutorial should explain the core abstractions, their relationships (visualized), and how they work together, using beginner-friendly language, analogies, and multi-line descriptions where needed, so I can understand the project structure and key concepts quickly without manually digging through all the code. Input: . | A publicly accessible GitHub repository URL or a local directory path. | A project name (optional, will be derived from the URL/directory if not provided). | Desired language for the tutorial (optional, defaults to English). | . Output: . | A directory named after the project containing: . | An index.md file with: . | A high-level project summary (potentially translated). | A Mermaid flowchart diagram visualizing relationships between abstractions (using potentially translated names/labels). | An ordered list of links to chapter files (using potentially translated names). | . | Individual Markdown files for each chapter (01_chapter_one.md, 02_chapter_two.md, etc.) detailing core abstractions in a logical order (potentially translated content). | . | . ",
    "url": "/design.html#requirements",
    
    "relUrl": "/design.html#requirements"
  },"117": {
    "doc": "System Design",
    "title": "Flow Design",
    "content": "Notes for AI: . | Consider the design patterns of agent, map-reduce, rag, and workflow. Apply them if they fit. | Present a concise, high-level description of the workflow. | . Applicable Design Pattern: . This project primarily uses a Workflow pattern to decompose the tutorial generation process into sequential steps. The chapter writing step utilizes a BatchNode (a form of MapReduce) to process each abstraction individually. | Workflow: The overall process follows a defined sequence: fetch code -&gt; identify abstractions -&gt; analyze relationships -&gt; determine order -&gt; write chapters -&gt; combine tutorial into files. | Batch Processing: The WriteChapters node processes each identified abstraction independently (map) before the final tutorial files are structured (reduce). | . Flow high-level Design: . | FetchRepo: Crawls the specified GitHub repository URL or local directory using appropriate utility (crawl_github_files or crawl_local_files), retrieving relevant source code file contents. | IdentifyAbstractions: Analyzes the codebase using an LLM to identify up to 10 core abstractions, generate beginner-friendly descriptions (potentially translated if language != English), and list the indices of files related to each abstraction. | AnalyzeRelationships: Uses an LLM to analyze the identified abstractions (referenced by index) and their related code to generate a high-level project summary and describe the relationships/interactions between these abstractions (summary and labels potentially translated if language != English), specifying source and target abstraction indices and a concise label for each interaction. | OrderChapters: Determines the most logical order (as indices) to present the abstractions in the tutorial, considering input context which might be translated. The output order itself is language-independent. | WriteChapters (BatchNode): Iterates through the ordered list of abstraction indices. For each abstraction, it calls an LLM to write a detailed, beginner-friendly chapter (content potentially fully translated if language != English), using the relevant code files (accessed via indices) and summaries of previously generated chapters (potentially translated) as context. | CombineTutorial: Creates an output directory, generates a Mermaid diagram from the relationship data (using potentially translated names/labels), and writes the project summary (potentially translated), relationship diagram, chapter links (using potentially translated names), and individually generated chapter files (potentially translated content) into it. Fixed text like “Chapters”, “Source Repository”, and the attribution footer remain in English. | . flowchart TD A[FetchRepo] --&gt; B[IdentifyAbstractions]; B --&gt; C[AnalyzeRelationships]; C --&gt; D[OrderChapters]; D --&gt; E[Batch WriteChapters]; E --&gt; F[CombineTutorial]; . ",
    "url": "/design.html#flow-design",
    
    "relUrl": "/design.html#flow-design"
  },"118": {
    "doc": "System Design",
    "title": "Utility Functions",
    "content": "Notes for AI: . | Understand the utility function definition thoroughly by reviewing the doc. | Include only the necessary utility functions, based on nodes in the flow. | . | crawl_github_files (utils/crawl_github_files.py) - External Dependency: requests, gitpython (optional for SSH) . | Input: repo_url (str), token (str, optional), max_file_size (int, optional), use_relative_paths (bool, optional), include_patterns (set, optional), exclude_patterns (set, optional) | Output: dict containing files (dict[str, str]) and stats. | Necessity: Required by FetchRepo to download and read source code from GitHub if a repo_url is provided. Handles API calls or SSH cloning, filtering, and file reading. | . | crawl_local_files (utils/crawl_local_files.py) - External Dependency: None . | Input: directory (str), max_file_size (int, optional), use_relative_paths (bool, optional), include_patterns (set, optional), exclude_patterns (set, optional) | Output: dict containing files (dict[str, str]). | Necessity: Required by FetchRepo to read source code from a local directory if a local_dir path is provided. Handles directory walking, filtering, and file reading. | . | call_llm (utils/call_llm.py) - External Dependency: LLM Provider API (e.g., Google GenAI) . | Input: prompt (str), use_cache (bool, optional) | Output: response (str) | Necessity: Used by IdentifyAbstractions, AnalyzeRelationships, OrderChapters, and WriteChapters for code analysis and content generation. Needs careful prompt engineering and YAML validation (implicit via yaml.safe_load which raises errors). | . | . ",
    "url": "/design.html#utility-functions",
    
    "relUrl": "/design.html#utility-functions"
  },"119": {
    "doc": "System Design",
    "title": "Node Design",
    "content": "Shared Store . Notes for AI: Try to minimize data redundancy . The shared Store structure is organized as follows: . shared = { # --- Inputs --- \"repo_url\": None, # Provided by the user/main script if using GitHub \"local_dir\": None, # Provided by the user/main script if using local directory \"project_name\": None, # Optional, derived from repo_url/local_dir if not provided \"github_token\": None, # Optional, from argument or environment variable \"output_dir\": \"output\", # Default or user-specified base directory for output \"include_patterns\": set(), # File patterns to include \"exclude_patterns\": set(), # File patterns to exclude \"max_file_size\": 100000, # Default or user-specified max file size \"language\": \"english\", # Default or user-specified language for the tutorial # --- Intermediate/Output Data --- \"files\": [], # Output of FetchRepo: List of tuples (file_path: str, file_content: str) \"abstractions\": [], # Output of IdentifyAbstractions: List of {\"name\": str (potentially translated), \"description\": str (potentially translated), \"files\": [int]} (indices into shared[\"files\"]) \"relationships\": { # Output of AnalyzeRelationships \"summary\": None, # Overall project summary (potentially translated) \"details\": [] # List of {\"from\": int, \"to\": int, \"label\": str (potentially translated)} describing relationships between abstraction indices. }, \"chapter_order\": [], # Output of OrderChapters: List of indices into shared[\"abstractions\"], determining tutorial order \"chapters\": [], # Output of WriteChapters: List of chapter content strings (Markdown, potentially translated), ordered according to chapter_order \"final_output_dir\": None # Output of CombineTutorial: Path to the final generated tutorial directory (e.g., \"output/my_project\") } . Node Steps . Notes for AI: Carefully decide whether to use Batch/Async Node/Flow. Removed explicit try/except in exec, relying on Node’s built-in fault tolerance. | FetchRepo . | Purpose: Download the repository code (from GitHub) or read from a local directory, loading relevant files into memory using the appropriate crawler utility. | Type: Regular | Steps: . | prep: Read repo_url, local_dir, project_name, github_token, output_dir, include_patterns, exclude_patterns, max_file_size from shared store. Determine project_name from repo_url or local_dir if not present in shared. Set use_relative_paths flag. | exec: If repo_url is present, call crawl_github_files(...). Otherwise, call crawl_local_files(...). Convert the resulting files dictionary into a list of (path, content) tuples. | post: Write the list of files tuples and the derived project_name (if applicable) to the shared store. | . | . | IdentifyAbstractions . | Purpose: Analyze the code to identify key concepts/abstractions using indices. Generates potentially translated names and descriptions if language is not English. | Type: Regular | Steps: . | prep: Read files (list of tuples), project_name, and language from shared store. Create context using create_llm_context helper which adds file indices. Format the list of index # path for the prompt. | exec: Construct a prompt for call_llm. If language is not English, add instructions to generate name and description in the target language. Ask LLM to identify ~5-10 core abstractions, provide a simple description for each, and list the relevant file indices (e.g., - 0 # path/to/file.py). Request YAML list output. Parse and validate the YAML, ensuring indices are within bounds and converting entries like 0 # path... to just the integer 0. | post: Write the validated list of abstractions (e.g., [{\"name\": \"Node\", \"description\": \"...\", \"files\": [0, 3, 5]}, ...]) containing file indices and potentially translated name/description to the shared store. | . | . | AnalyzeRelationships . | Purpose: Generate a project summary and describe how the identified abstractions interact using indices and concise labels. Generates potentially translated summary and labels if language is not English. | Type: Regular | Steps: . | prep: Read abstractions, files, project_name, and language from shared store. Format context for the LLM, including potentially translated abstraction names and indices, potentially translated descriptions, and content snippets from related files (referenced by index # path using get_content_for_indices helper). Prepare the list of index # AbstractionName (potentially translated) for the prompt. | exec: Construct a prompt for call_llm. If language is not English, add instructions to generate summary and label in the target language, and note that input names might be translated. Ask for (1) a high-level summary and (2) a list of relationships, each specifying from_abstraction (e.g., 0 # Abstraction1), to_abstraction (e.g., 1 # Abstraction2), and a concise label. Request structured YAML output. Parse and validate, converting referenced abstractions to indices (from: 0, to: 1). | post: Parse the LLM response and write the relationships dictionary ({\"summary\": \"...\", \"details\": [{\"from\": 0, \"to\": 1, \"label\": \"...\"}, ...]}) with indices and potentially translated summary/label to the shared store. | . | . | OrderChapters . | Purpose: Determine the sequence (as indices) in which abstractions should be presented. Considers potentially translated input context. | Type: Regular | Steps: . | prep: Read abstractions, relationships, project_name, and language from the shared store. Prepare context including the list of index # AbstractionName (potentially translated) and textual descriptions of relationships referencing indices and using the potentially translated label. Note in context if summary/names might be translated. | exec: Construct a prompt for call_llm asking it to order the abstractions based on importance, foundational concepts, or dependencies. Request output as an ordered YAML list of index # AbstractionName. Parse and validate, extracting only the indices and ensuring all are present exactly once. | post: Write the validated ordered list of indices (chapter_order) to the shared store. | . | . | WriteChapters . | Purpose: Generate the detailed content for each chapter of the tutorial. Generates potentially fully translated chapter content if language is not English. | Type: BatchNode | Steps: . | prep: Read chapter_order (indices), abstractions, files, project_name, and language from shared store. Initialize an empty instance variable self.chapters_written_so_far. Return an iterable list where each item corresponds to an abstraction index from chapter_order. Each item should contain chapter number, potentially translated abstraction details, a map of related file content ({ \"idx # path\": content }), full chapter listing (potentially translated names), chapter filename map, previous/next chapter info (potentially translated names), and language. | exec(item): Construct a prompt for call_llm. If language is not English, add detailed instructions to write the entire chapter in the target language, translating explanations, examples, etc., while noting which input context might already be translated. Ask LLM to write a beginner-friendly Markdown chapter. Provide potentially translated concept details. Include a summary of previously written chapters (potentially translated). Provide relevant code snippets. Add the generated (potentially translated) chapter content to self.chapters_written_so_far for the next iteration’s context. Return the chapter content. | post(shared, prep_res, exec_res_list): exec_res_list contains the generated chapter Markdown content strings (potentially translated), ordered correctly. Assign this list directly to shared[\"chapters\"]. Clean up self.chapters_written_so_far. | . | . | CombineTutorial . | Purpose: Assemble the final tutorial files, including a Mermaid diagram using potentially translated labels/names. Fixed text remains English. | Type: Regular | Steps: . | prep: Read project_name, relationships (potentially translated summary/labels), chapter_order (indices), abstractions (potentially translated name/desc), chapters (list of potentially translated content), repo_url, and output_dir from shared store. Generate a Mermaid flowchart TD string based on relationships[\"details\"], using indices to identify nodes (potentially translated names) and the concise label (potentially translated) for edges. Construct the content for index.md (including potentially translated summary, Mermaid diagram, and ordered links to chapters using potentially translated names derived using chapter_order and abstractions). Define the output directory path (e.g., ./output_dir/project_name). Prepare a list of { \"filename\": \"01_...\", \"content\": \"...\" } for chapters, adding the English attribution footer to each chapter’s content. Add the English attribution footer to the index content. | exec: Create the output directory. Write the generated index.md content. Iterate through the prepared chapter file list and write each chapter’s content to its corresponding .md file in the output directory. | post: Write the final output_path to shared[\"final_output_dir\"]. Log completion. | . | . | . ",
    "url": "/design.html#node-design",
    
    "relUrl": "/design.html#node-design"
  },"120": {
    "doc": "System Design",
    "title": "System Design",
    "content": " ",
    "url": "/design.html",
    
    "relUrl": "/design.html"
  },"121": {
    "doc": "My Tutorial for Modular's Max",
    "title": "Tutorial: modular",
    "content": "The project, modular, is a high-performance serving framework for Large Language Models (LLMs). It allows users to easily deploy and run LLMs, handling incoming requests, orchestrating the generation process, and streaming back responses. Key features include efficient request batching, advanced KV cache management for speed, and a modular architecture to manage different parts of the serving stack like API endpoints, model execution, and telemetry. Source Repository: None . flowchart TD A0[\"Serving API Layer (FastAPI App &amp; Routers) \"] A1[\"LLM Pipeline Orchestrator (`TokenGeneratorPipeline`) \"] A2[\"Model Worker \"] A3[\"Scheduler (`TokenGenerationScheduler`, `EmbeddingsScheduler`) \"] A4[\"EngineQueue \"] A5[\"Settings (`Settings` class) \"] A6[\"Telemetry and Metrics (`METRICS`, `MetricClient`) \"] A7[\"KV Cache Management \"] A0 -- \"Forwards requests\" --&gt; A1 A0 -- \"Reads config\" --&gt; A5 A1 -- \"Sends tasks to\" --&gt; A4 A1 -- \"Reports metrics\" --&gt; A6 A2 -- \"Uses\" --&gt; A3 A2 -- \"Uses for inference\" --&gt; A7 A3 -- \"Uses for tasks\" --&gt; A4 A3 -- \"Manages cache via\" --&gt; A7 A4 -- \"Delivers tasks to\" --&gt; A2 A5 -- \"Configures\" --&gt; A0 A5 -- \"Configures telemetry\" --&gt; A6 A7 -- \"Agent started by\" --&gt; A0 . ",
    "url": "/modular_max/#tutorial-modular",
    
    "relUrl": "/modular_max/#tutorial-modular"
  },"122": {
    "doc": "My Tutorial for Modular's Max",
    "title": "Chapters",
    "content": ". | Settings (Settings class) | Serving API Layer (FastAPI App &amp; Routers) | LLM Pipeline Orchestrator (TokenGeneratorPipeline) | Model Worker | Scheduler (TokenGenerationScheduler, EmbeddingsScheduler) | KV Cache Management | EngineQueue | Telemetry and Metrics (METRICS, MetricClient) | . Generated by AI Codebase Knowledge Builder . ",
    "url": "/modular_max/#chapters",
    
    "relUrl": "/modular_max/#chapters"
  },"123": {
    "doc": "My Tutorial for Modular's Max",
    "title": "My Tutorial for Modular's Max",
    "content": " ",
    "url": "/modular_max/",
    
    "relUrl": "/modular_max/"
  },"124": {
    "doc": "My Tutorial for Mojo v1",
    "title": "Tutorial: mojo",
    "content": "The project mojo (as represented by these abstractions) provides a system for high-performance, multi-dimensional array (tensor) operations. It features NDBuffer as a central data structure that acts as a smart view over raw memory, pointed to by an UnsafePointer. The NDBuffer’s structure (shape and memory layout strides) is defined using DimList. Accessing elements in an NDBuffer is done using IndexList for coordinates, which are then converted to a 1D memory offset by strided N-D to 1D indexing logic. The AddressSpace parameter allows these operations to be tailored for different memory systems, like CPU or GPU, for optimal performance. Source Repository: None . flowchart TD A0[\"NDBuffer \"] A1[\"DimList \"] A2[\"UnsafePointer \"] A3[\"IndexList \"] A4[\"N-D to 1D Indexing Logic (Strided Memory Access) \"] A5[\"AddressSpace \"] A0 -- \"Uses for shape/stride defin...\" --&gt; A1 A0 -- \"Points to data via\" --&gt; A2 A0 -- \"Applies for element access\" --&gt; A4 A4 -- \"Takes coordinates as\" --&gt; A3 A2 -- \"Parameterized by memory type\" --&gt; A5 . ",
    "url": "/mojo-v1/#tutorial-mojo",
    
    "relUrl": "/mojo-v1/#tutorial-mojo"
  },"125": {
    "doc": "My Tutorial for Mojo v1",
    "title": "Chapters",
    "content": ". | AddressSpace | UnsafePointer | IndexList | DimList | NDBuffer | N-D to 1D Indexing Logic (Strided Memory Access) | . Generated by AI Codebase Knowledge Builder . ",
    "url": "/mojo-v1/#chapters",
    
    "relUrl": "/mojo-v1/#chapters"
  },"126": {
    "doc": "My Tutorial for Mojo v1",
    "title": "My Tutorial for Mojo v1",
    "content": " ",
    "url": "/mojo-v1/",
    
    "relUrl": "/mojo-v1/"
  },"127": {
    "doc": "My Tutorial for Crawl4ai",
    "title": "Tutorial: My Crawl4ai",
    "content": "Crawl4ai is an intelligent web crawling framework designed specifically for AI applications. It provides asynchronous web crawling capabilities with configurable strategies for content extraction, deep website exploration, and smart URL prioritization. The library manages resources efficiently through its dispatcher framework and caching system, while offering flexible logging and content filtering. It can be deployed as a service via Docker with API endpoints, making it suitable for both simple data collection and complex distributed crawling tasks. Source Repository: https://github.com/unclecode/crawl4ai . flowchart TD A0[\"AsyncWebCrawler\"] A1[\"Configuration System\"] A2[\"Content Extraction Pipeline\"] A3[\"Deep Crawling System\"] A4[\"Dispatcher Framework\"] A5[\"Caching System\"] A6[\"URL Filtering &amp; Scoring\"] A7[\"Async Logging Infrastructure\"] A8[\"API &amp; Docker Integration\"] A0 -- \"Initializes with and uses\" --&gt; A1 A0 -- \"Processes HTML with\" --&gt; A2 A0 -- \"Delegates concurrent crawli...\" --&gt; A4 A0 -- \"Stores and retrieves conten...\" --&gt; A5 A0 -- \"Records operations through\" --&gt; A7 A3 -- \"Uses for page processing\" --&gt; A0 A3 -- \"Prioritizes URLs with\" --&gt; A6 A4 -- \"Coordinates multiple instan...\" --&gt; A0 A8 -- \"Exposes functionality of\" --&gt; A0 A1 -- \"Configures strategies for\" --&gt; A2 . ",
    "url": "/my-crawl4AI/#tutorial-my-crawl4ai",
    
    "relUrl": "/my-crawl4AI/#tutorial-my-crawl4ai"
  },"128": {
    "doc": "My Tutorial for Crawl4ai",
    "title": "Chapters",
    "content": ". | Configuration System | AsyncWebCrawler | Content Extraction Pipeline | URL Filtering &amp; Scoring | Deep Crawling System | Caching System | Dispatcher Framework | Async Logging Infrastructure | API &amp; Docker Integration | . Generated by AI Codebase Knowledge Builder . ",
    "url": "/my-crawl4AI/#chapters",
    
    "relUrl": "/my-crawl4AI/#chapters"
  },"129": {
    "doc": "My Tutorial for Crawl4ai",
    "title": "My Tutorial for Crawl4ai",
    "content": " ",
    "url": "/my-crawl4AI/",
    
    "relUrl": "/my-crawl4AI/"
  },"130": {
    "doc": "Crawl4AI",
    "title": "Tutorial: Crawl4AI",
    "content": "This tutorial is AI-generated! To learn more, check out AI Codebase Knowledge Builder . Crawl4AIView Repo is a flexible Python library for asynchronously crawling websites and extracting structured content, specifically designed for AI use cases. You primarily interact with the AsyncWebCrawler, which acts as the main coordinator. You provide it with URLs and a CrawlerRunConfig detailing how to crawl (e.g., using specific strategies for fetching, scraping, filtering, and extraction). It can handle single pages or multiple URLs concurrently using a BaseDispatcher, optionally crawl deeper by following links via DeepCrawlStrategy, manage CacheMode, and apply RelevantContentFilter before finally returning a CrawlResult containing all the gathered data. flowchart TD A0[\"AsyncWebCrawler\"] A1[\"CrawlerRunConfig\"] A2[\"AsyncCrawlerStrategy\"] A3[\"ContentScrapingStrategy\"] A4[\"ExtractionStrategy\"] A5[\"CrawlResult\"] A6[\"BaseDispatcher\"] A7[\"DeepCrawlStrategy\"] A8[\"CacheContext / CacheMode\"] A9[\"RelevantContentFilter\"] A0 -- \"Configured by\" --&gt; A1 A0 -- \"Uses Fetching Strategy\" --&gt; A2 A0 -- \"Uses Scraping Strategy\" --&gt; A3 A0 -- \"Uses Extraction Strategy\" --&gt; A4 A0 -- \"Produces\" --&gt; A5 A0 -- \"Uses Dispatcher for `arun_m...\" --&gt; A6 A0 -- \"Uses Caching Logic\" --&gt; A8 A6 -- \"Calls Crawler's `arun`\" --&gt; A0 A1 -- \"Specifies Deep Crawl Strategy\" --&gt; A7 A7 -- \"Processes Links from\" --&gt; A5 A3 -- \"Provides Cleaned HTML to\" --&gt; A9 A1 -- \"Specifies Content Filter\" --&gt; A9 . ",
    "url": "/Crawl4AI/#tutorial-crawl4ai",
    
    "relUrl": "/Crawl4AI/#tutorial-crawl4ai"
  },"131": {
    "doc": "Crawl4AI",
    "title": "Crawl4AI",
    "content": " ",
    "url": "/Crawl4AI/",
    
    "relUrl": "/Crawl4AI/"
  },"132": {
    "doc": "My Tutorial for Mojo v2",
    "title": "Tutorial: mojo",
    "content": "The mojo project provides a core data structure called NDBuffer, which is a non-owning, multi-dimensional view into a block of memory, similar to a tensor. It’s designed for high-performance computing by allowing direct memory manipulation via UnsafePointers and supporting efficient element access through Strides and Offset Computation. NDBuffer can handle both statically known and dynamically determined dimension sizes using DimList and Dim abstractions, and it facilitates SIMD Data Access for accelerated, vectorized operations on data. Source Repository: None . flowchart TD A0[\"NDBuffer \"] A1[\"DimList and Dim \"] A2[\"UnsafePointer (as used by NDBuffer) \"] A3[\"Strides and Offset Computation \"] A4[\"SIMD Data Access \"] A0 -- \"Uses for shape/strides\" --&gt; A1 A0 -- \"References memory via\" --&gt; A2 A0 -- \"Locates elements using\" --&gt; A3 A0 -- \"Offers\" --&gt; A4 A4 -- \"Performs loads/stores on\" --&gt; A2 . ",
    "url": "/mojo-v2/#tutorial-mojo",
    
    "relUrl": "/mojo-v2/#tutorial-mojo"
  },"133": {
    "doc": "My Tutorial for Mojo v2",
    "title": "Chapters",
    "content": ". | UnsafePointer (as used by NDBuffer) | DimList and Dim | NDBuffer | Strides and Offset Computation | SIMD Data Access | . Generated by AI Codebase Knowledge Builder . ",
    "url": "/mojo-v2/#chapters",
    
    "relUrl": "/mojo-v2/#chapters"
  },"134": {
    "doc": "My Tutorial for Mojo v2",
    "title": "My Tutorial for Mojo v2",
    "content": " ",
    "url": "/mojo-v2/",
    
    "relUrl": "/mojo-v2/"
  },"135": {
    "doc": "Home",
    "title": "Turns Codebase into Easy Tutorial - Pocket Flow",
    "content": "Ever stared at a new codebase written by others feeling completely lost? This project analyzes GitHub repositories and creates beginner-friendly tutorials explaining exactly how the code works - all powered by AI! Our intelligent system automatically breaks down complex codebases into digestible explanations that even beginners can understand. ",
    "url": "/#turns-codebase-into-easy-tutorial---pocket-flow",
    
    "relUrl": "/#turns-codebase-into-easy-tutorial---pocket-flow"
  },"136": {
    "doc": "Home",
    "title": "PF-Understand Documentation",
    "content": "Welcome to the documentation for PF-Understand, a tool for generating comprehensive tutorials from codebases. ",
    "url": "/#pf-understand-documentation",
    
    "relUrl": "/#pf-understand-documentation"
  },"137": {
    "doc": "Home",
    "title": "Tutorials",
    "content": "All in all, I did 4 fairly complete runs of Pocket Flow tutorializing abstractions. | Crawl4AI - The original tutorial already done in Pocket Flow’s repo | My Crawl4AI - My sanity check run for an that repo | Modular Max - When doing the entire Modular repo, it chose to do Max Engine, ignoring Mojo almost entirely. | Mojo v1 - When focusing Pocket Flow on just the mojo folder in Modular’s repo, a tutorial on its high-performance multi-dimensional array operation | Mojo v2 - The final run of Pocket Flow which ran smoothly (no errors in about 1.75 hours) on resulted in a different tutorial for Mojo, this time focusing on Mojo’s N-dimensional data structures | . Using gemini-2.5-pro in Cursor, I did the following two comparisons: . | TBD: the first run above, sanity check on Crawl4AI in the exisitng repo in output/crawl4ai/. | TBD: the last two runs above, the two different versions of tutorials for Mojo. I also did a comparison | . ",
    "url": "/#tutorials",
    
    "relUrl": "/#tutorials"
  },"138": {
    "doc": "Home",
    "title": "Other notes TBD",
    "content": " ",
    "url": "/#other-notes-tbd",
    
    "relUrl": "/#other-notes-tbd"
  },"139": {
    "doc": "Home",
    "title": "Home",
    "content": " ",
    "url": "/",
    
    "relUrl": "/"
  }
}
