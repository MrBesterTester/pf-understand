<h1 id="chapter-5-scheduler-tokengenerationscheduler-embeddingsscheduler">Chapter 5: Scheduler (<code class="language-plaintext highlighter-rouge">TokenGenerationScheduler</code>, <code class="language-plaintext highlighter-rouge">EmbeddingsScheduler</code>)</h1>

<p>In <a href="04_model_worker_.md">Chapter 4: Model Worker</a>, we learned about the Model Worker, the dedicated workshop where the AI model does its heavy lifting. We saw that it receives requests from the <a href="07_enginequeue_.md">EngineQueue</a>. But how does the Model Worker decide <em>when</em> and <em>how</em> to feed these requests to the AI model? Does it process them one by one? Or is there a smarter way?</p>

<p>That’s where the <strong>Scheduler</strong> comes in! It’s like an intelligent traffic controller or a theme park ride operator working <em>inside</em> the Model Worker. Its main job is to make sure the AI model (our star attraction) is used as efficiently as possible.</p>

<h2 id="what-problem-does-the-scheduler-solve">What Problem Does the Scheduler Solve?</h2>

<p>Imagine our AI model is like a super-fast, super-powerful rollercoaster.</p>
<ul>
  <li>If we send only one person on the rollercoaster at a time, a lot of the seats will be empty. It’s not very efficient!</li>
  <li>If we wait too long to fill the rollercoaster, people in line get impatient.</li>
</ul>

<p>The Scheduler’s job is to gather individual requests (people wanting to ride the rollercoaster) and group them into <strong>optimal batches</strong> (filling up the rollercoaster carts efficiently) before sending them to the AI model for processing. This “batching” is crucial for:</p>
<ul>
  <li><strong>Maximizing GPU Utilization</strong>: Modern AI models, especially those running on GPUs, perform much better when they process multiple pieces of data at once. Running one request at a time can leave the GPU underutilized.</li>
  <li><strong>Improving Throughput</strong>: By processing requests in batches, the overall number of requests handled per second (throughput) increases significantly.</li>
</ul>

<p>Think of the Scheduler as a theme park ride operator. They don’t start the ride with just one person in a 20-seat cart. They try to fill the cart (or a good portion of it) to make each run worthwhile. Similarly, the Scheduler groups incoming requests based on things like how many requests are waiting, how long they’ve been waiting, or how “big” each request is (e.g., how many tokens need processing).</p>

<h2 id="meet-the-schedulers-tokengenerationscheduler-and-embeddingsscheduler">Meet the Schedulers: <code class="language-plaintext highlighter-rouge">TokenGenerationScheduler</code> and <code class="language-plaintext highlighter-rouge">EmbeddingsScheduler</code></h2>

<p><code class="language-plaintext highlighter-rouge">modular</code> has different types of AI tasks, and so it has specialized Schedulers:</p>

<ol>
  <li><strong><code class="language-plaintext highlighter-rouge">TokenGenerationScheduler</code></strong>: This scheduler is used when the AI model’s job is to generate text (like writing a story or answering a question). It has specific strategies for:
    <ul>
      <li><strong>Context Encoding (CE)</strong>: Processing the initial prompt (e.g., “Tell me a story about a dragon”). This often involves processing many tokens at once.</li>
      <li><strong>Token Generation (TG)</strong>: Generating the subsequent words or tokens of the story, often one or a few at a time, repeatedly.
The <code class="language-plaintext highlighter-rouge">TokenGenerationScheduler</code> is smart about forming batches for both these phases.</li>
    </ul>
  </li>
  <li><strong><code class="language-plaintext highlighter-rouge">EmbeddingsScheduler</code></strong>: This scheduler is used when the AI model’s job is to calculate embeddings (numerical representations of text, useful for tasks like semantic search). The requests are typically more straightforward: take some text, produce a list of numbers.</li>
</ol>

<p>Both schedulers operate on the same core principle: <strong>batch requests efficiently.</strong></p>

<h3 id="how-schedulers-are-created">How Schedulers are Created</h3>

<p>Remember in <a href="04_model_worker_.md">Chapter 4: Model Worker</a>, we saw the <code class="language-plaintext highlighter-rouge">model_worker_run_v3</code> function inside the Model Worker process? That’s where the appropriate scheduler is created.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Simplified from: src/max/serve/pipelines/model_worker.py
</span>
<span class="c1"># (Inside model_worker_run_v3)
# ...
# pipeline = model_factory() # This loads the AI model (TokenGenerator or EmbeddingsGenerator)
</span>
<span class="n">scheduler</span><span class="p">:</span> <span class="n">Scheduler</span> <span class="c1"># This will hold our chosen scheduler
</span><span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">pipeline</span><span class="p">,</span> <span class="n">TokenGenerator</span><span class="p">):</span>
    <span class="c1"># If it's a text-generating model, use TokenGenerationScheduler
</span>    <span class="n">scheduler</span> <span class="o">=</span> <span class="n">_create_token_generation_scheduler</span><span class="p">(</span>
        <span class="n">pipeline</span><span class="p">,</span> <span class="n">pc</span><span class="p">,</span> <span class="n">pipeline_config</span><span class="p">,</span> <span class="n">queues</span>
    <span class="p">)</span>
<span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">pipeline</span><span class="p">,</span> <span class="n">EmbeddingsGenerator</span><span class="p">):</span>
    <span class="c1"># If it's an embeddings model, use EmbeddingsScheduler
</span>    <span class="n">scheduler</span> <span class="o">=</span> <span class="n">_create_embeddings_scheduler</span><span class="p">(</span>
        <span class="n">pipeline</span><span class="p">,</span> <span class="n">pc</span><span class="p">,</span> <span class="n">pipeline_config</span><span class="p">,</span> <span class="n">queues</span>
    <span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="k">raise</span> <span class="nb">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s">"Invalid pipeline type: </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">pipeline</span><span class="p">)</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>

<span class="c1"># ...
# pc.set_started() # Signal that the worker is ready
# scheduler.run()  # Start the scheduler's main loop!
# ...
</span></code></pre></div></div>
<p>This code snippet shows that based on the type of AI model (<code class="language-plaintext highlighter-rouge">pipeline</code>) loaded, the Model Worker creates either a <code class="language-plaintext highlighter-rouge">TokenGenerationScheduler</code> or an <code class="language-plaintext highlighter-rouge">EmbeddingsScheduler</code>. The <code class="language-plaintext highlighter-rouge">pipeline_config</code> (which comes from <a href="01_settings___settings__class__.md">Settings (<code class="language-plaintext highlighter-rouge">Settings</code> class)</a>) and <code class="language-plaintext highlighter-rouge">queues</code> (parts of the <a href="07_enginequeue_.md">EngineQueue</a> for communication) are passed to configure the scheduler.</p>

<p>Once created, <code class="language-plaintext highlighter-rouge">scheduler.run()</code> is called. This starts the scheduler’s main job: continuously looking for requests, batching them, and sending them to the AI model.</p>

<h2 id="the-schedulers-main-loop-a-day-in-the-life">The Scheduler’s Main Loop: A Day in the Life</h2>

<p>Let’s imagine we’re the <code class="language-plaintext highlighter-rouge">TokenGenerationScheduler</code>. Here’s a simplified version of what our <code class="language-plaintext highlighter-rouge">run()</code> loop might do:</p>

<ol>
  <li>
    <p><strong>Check for New Riders (Requests)</strong>: We look at our <code class="language-plaintext highlighter-rouge">request_q</code> (the part of the <a href="07_enginequeue_.md">EngineQueue</a> where new requests arrive from the <a href="03_llm_pipeline_orchestrator___tokengeneratorpipeline___.md">LLM Pipeline Orchestrator (<code class="language-plaintext highlighter-rouge">TokenGeneratorPipeline</code>)</a>). Are there any people waiting in line?</p>
  </li>
  <li><strong>Decide on the Next Cart (Batch Formation)</strong>:
    <ul>
      <li>If there are requests for <strong>Context Encoding</strong> (processing new prompts), we might try to group several of them. We might wait a short time (a <code class="language-plaintext highlighter-rouge">batch_timeout</code>) to see if more arrive, or until we have enough to fill a “batch” up to <code class="language-plaintext highlighter-rouge">max_batch_size_ce</code>.</li>
      <li>If there are requests that are already part-way through <strong>Token Generation</strong> (generating the next word of an ongoing story), we gather these.</li>
      <li>The scheduler uses its configuration (like <code class="language-plaintext highlighter-rouge">TokenGenerationSchedulerConfig</code>) to make these decisions. For example, <code class="language-plaintext highlighter-rouge">target_tokens_per_batch_ce</code> helps decide how many total prompt tokens to aim for in a context encoding batch.</li>
    </ul>
  </li>
  <li>
    <p><strong>Load the Cart (Prepare the Batch)</strong>: We select a group of requests to form the current batch. This is done by methods like <code class="language-plaintext highlighter-rouge">_create_batch_to_execute()</code>.</p>
  </li>
  <li>
    <p><strong>Start the Ride (Send to Model)</strong>: We send this batch of requests to the actual AI model (<code class="language-plaintext highlighter-rouge">self.pipeline.next_token(...)</code> for text generation or <code class="language-plaintext highlighter-rouge">self.pipeline.encode(...)</code> for embeddings).</p>
  </li>
  <li>
    <p><strong>Collect Souvenirs (Get Results)</strong>: The AI model processes the batch and produces results (e.g., the next set of generated tokens for each request in the batch).</p>
  </li>
  <li>
    <p><strong>Guide Riders to Exit (Send Results Back)</strong>: We take these results and put them onto our <code class="language-plaintext highlighter-rouge">response_q</code> (another part of the <a href="07_enginequeue_.md">EngineQueue</a>), so they can be sent back to the <a href="03_llm_pipeline_orchestrator___tokengeneratorpipeline___.md">LLM Pipeline Orchestrator (<code class="language-plaintext highlighter-rouge">TokenGeneratorPipeline</code>)</a> and eventually to the user.</p>
  </li>
  <li><strong>Repeat!</strong>: We go back to step 1 and do this all over again, continuously.</li>
</ol>

<p>This loop ensures a steady flow of efficiently batched requests to the AI model.</p>

<h3 id="visualizing-the-schedulers-role">Visualizing the Scheduler’s Role</h3>

<p>Here’s a diagram showing the Scheduler at work inside the Model Worker:</p>

<pre><code class="language-mermaid">sequenceDiagram
    participant InQueue as Input Queue (from EngineQueue)
    participant Sched as Scheduler
    participant AIModel as AI Model Pipeline
    participant OutQueue as Output Queue (to EngineQueue)

    Note over Sched: Scheduler's main run() loop active
    loop Processing Cycle
        Sched-&gt;&gt;InQueue: Check for/Get incoming requests
        InQueue--&gt;&gt;Sched: Provides requests (e.g., prompt A, prompt B)
        Sched-&gt;&gt;Sched: Forms a batch (e.g., [A, B]) based on strategy
        Sched-&gt;&gt;AIModel: Sends batch for processing
        AIModel-&gt;&gt;AIModel: Processes batch (e.g., generates text for A &amp; B)
        AIModel--&gt;&gt;Sched: Returns results for the batch
        Sched-&gt;&gt;OutQueue: Puts results onto output queue
    end
</code></pre>

<h2 id="under-the-hood-a-peek-into-tokengenerationscheduler">Under the Hood: A Peek into <code class="language-plaintext highlighter-rouge">TokenGenerationScheduler</code></h2>

<p>Let’s look at a simplified version of how the <code class="language-plaintext highlighter-rouge">TokenGenerationScheduler</code> might work, focusing on its <code class="language-plaintext highlighter-rouge">run</code> method and how it creates and schedules batches. The actual code is in <code class="language-plaintext highlighter-rouge">src/max/serve/pipelines/scheduler.py</code>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Simplified from: src/max/serve/pipelines/scheduler.py
</span>
<span class="k">class</span> <span class="nc">TokenGenerationScheduler</span><span class="p">(</span><span class="n">Scheduler</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="c1"># ... process_control, scheduler_config, pipeline, queues ...
</span>        <span class="n">paged_manager</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">PagedKVCacheManager</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span> <span class="c1"># For KV Cache
</span>    <span class="p">):</span>
        <span class="c1"># self.scheduler_config = scheduler_config (stores max_batch_size_tg, etc.)
</span>        <span class="c1"># self.pipeline = pipeline (the AI model like TokenGenerator)
</span>        <span class="c1"># self.request_q = queues["REQUEST"] (where new requests arrive)
</span>        <span class="c1"># self.response_q = queues["RESPONSE"] (where results are sent)
</span>        <span class="c1"># self.active_batch = {} (requests currently being processed for TG)
</span>        <span class="c1"># self.paged_manager = paged_manager (manages KV Cache, see Chapter 6)
</span>        <span class="c1"># ...
</span>        <span class="k">pass</span>

    <span class="k">def</span> <span class="nf">run</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># Main loop: continuously process requests
</span>        <span class="k">while</span> <span class="bp">True</span><span class="p">:</span> <span class="c1"># Simplified; actual loop checks for cancellation
</span>            <span class="c1"># self.pc.beat() # Heartbeat to show it's alive
</span>
            <span class="c1"># 1. Decide what kind of batch to make next (CE or TG)
</span>            <span class="c1"># and gather requests for it.
</span>            <span class="n">batch_to_execute</span><span class="p">:</span> <span class="n">SchedulerOutput</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">_create_batch_to_execute</span><span class="p">()</span>
            <span class="c1"># SchedulerOutput contains the batch_inputs, batch_type (CE/TG), etc.
</span>
            <span class="k">if</span> <span class="n">batch_to_execute</span><span class="p">.</span><span class="n">batch_size</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="c1"># No requests to process right now, maybe sleep briefly
</span>                <span class="n">time</span><span class="p">.</span><span class="n">sleep</span><span class="p">(</span><span class="mf">0.001</span><span class="p">)</span> <span class="c1"># Small delay
</span>                <span class="k">continue</span>

            <span class="c1"># 2. Schedule the batch on the AI model
</span>            <span class="bp">self</span><span class="p">.</span><span class="n">_schedule</span><span class="p">(</span><span class="n">batch_to_execute</span><span class="p">)</span> <span class="c1"># This calls the model
</span>
            <span class="c1"># ... (handle cancelled requests, logging, etc.) ...
</span></code></pre></div></div>
<ul>
  <li>The <code class="language-plaintext highlighter-rouge">__init__</code> method sets up the scheduler with its configuration, the AI model pipeline it will use, the communication queues, and a reference to the <code class="language-plaintext highlighter-rouge">paged_manager</code> for <a href="06_kv_cache_management_.md">KV Cache Management</a> (more on this in the next chapter!).</li>
  <li>The <code class="language-plaintext highlighter-rouge">run()</code> method is the heart.
    <ul>
      <li><code class="language-plaintext highlighter-rouge">_create_batch_to_execute()</code>: This internal method is responsible for looking at waiting requests and current <code class="language-plaintext highlighter-rouge">active_batch</code> (for ongoing token generation) and deciding what to run next. It considers <code class="language-plaintext highlighter-rouge">scheduler_config</code> parameters like <code class="language-plaintext highlighter-rouge">max_batch_size_ce</code>, <code class="language-plaintext highlighter-rouge">max_batch_size_tg</code>, <code class="language-plaintext highlighter-rouge">batch_timeout</code>, and <code class="language-plaintext highlighter-rouge">target_tokens_per_batch_ce</code>. It also interacts with the <code class="language-plaintext highlighter-rouge">paged_manager</code> to ensure there’s enough KV Cache space.</li>
      <li><code class="language-plaintext highlighter-rouge">_schedule()</code>: This method takes the <code class="language-plaintext highlighter-rouge">SchedulerOutput</code> (which contains the batch) and actually sends it to the AI model.</li>
    </ul>
  </li>
</ul>

<p>Let’s look at a simplified <code class="language-plaintext highlighter-rouge">_schedule</code> method:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Simplified from: src/max/serve/pipelines/scheduler.py
# (Inside TokenGenerationScheduler class)
</span>
    <span class="k">def</span> <span class="nf">_schedule</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sch_output</span><span class="p">:</span> <span class="n">SchedulerOutput</span><span class="p">):</span>
        <span class="c1"># sch_output contains:
</span>        <span class="c1"># - sch_output.batch_inputs: the actual dictionary of requests for the model
</span>        <span class="c1"># - sch_output.batch_type: tells if it's ContextEncoding or TokenGeneration
</span>        <span class="c1"># - sch_output.num_steps: how many tokens to generate for TG
</span>
        <span class="n">batch_responses</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">if</span> <span class="n">sch_output</span><span class="p">.</span><span class="n">batch_type</span> <span class="o">==</span> <span class="n">BatchType</span><span class="p">.</span><span class="n">ContextEncoding</span><span class="p">:</span>
            <span class="c1"># Processing initial prompts
</span>            <span class="n">batch_responses</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">pipeline</span><span class="p">.</span><span class="n">next_token</span><span class="p">(</span>
                <span class="n">sch_output</span><span class="p">.</span><span class="n">batch_inputs</span><span class="p">,</span> <span class="c1"># The batch of new prompts
</span>                <span class="n">num_steps</span><span class="o">=</span><span class="n">sch_output</span><span class="p">.</span><span class="n">num_steps</span><span class="p">,</span> <span class="c1"># Usually 1 for CE
</span>            <span class="p">)</span>
            <span class="c1"># ... logic to add these requests to self.active_batch if not done ...
</span>        <span class="k">elif</span> <span class="n">sch_output</span><span class="p">.</span><span class="n">batch_type</span> <span class="o">==</span> <span class="n">BatchType</span><span class="p">.</span><span class="n">TokenGeneration</span><span class="p">:</span>
            <span class="c1"># Generating subsequent tokens for ongoing requests
</span>            <span class="n">batch_responses</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">pipeline</span><span class="p">.</span><span class="n">next_token</span><span class="p">(</span>
                <span class="n">sch_output</span><span class="p">.</span><span class="n">batch_inputs</span><span class="p">,</span> <span class="c1"># The batch of active requests
</span>                <span class="n">num_steps</span><span class="o">=</span><span class="n">sch_output</span><span class="p">.</span><span class="n">num_steps</span><span class="p">,</span> <span class="c1"># How many tokens to generate
</span>            <span class="p">)</span>
        
        <span class="c1"># ... logic to handle terminated responses (remove from active_batch) ...
</span>
        <span class="c1"># Send all responses back to the main process via the response_q
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">_stream_responses_to_frontend</span><span class="p">(</span><span class="n">batch_responses</span><span class="p">)</span>
</code></pre></div></div>
<p>This shows that <code class="language-plaintext highlighter-rouge">_schedule</code> calls <code class="language-plaintext highlighter-rouge">self.pipeline.next_token()</code> with the batch. The <code class="language-plaintext highlighter-rouge">pipeline</code> is the <code class="language-plaintext highlighter-rouge">TokenGenerator</code> instance loaded in the Model Worker. The results (<code class="language-plaintext highlighter-rouge">batch_responses</code>) are then sent back using <code class="language-plaintext highlighter-rouge">_stream_responses_to_frontend()</code>, which puts them on the <code class="language-plaintext highlighter-rouge">response_q</code>.</p>

<p>The <code class="language-plaintext highlighter-rouge">EmbeddingsScheduler</code> has a simpler structure because it typically just encodes a batch of texts without the ongoing generation state:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Simplified from: src/max/serve/pipelines/scheduler.py
</span>
<span class="k">class</span> <span class="nc">EmbeddingsScheduler</span><span class="p">(</span><span class="n">Scheduler</span><span class="p">):</span>
    <span class="c1"># ... __init__ similar to TokenGenerationScheduler but with EmbeddingsSchedulerConfig ...
</span>
    <span class="k">def</span> <span class="nf">run</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">while</span> <span class="bp">True</span><span class="p">:</span> <span class="c1"># Simplified
</span>            <span class="c1"># ...
</span>            <span class="n">batch_to_execute</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">_create_batch_to_execute</span><span class="p">()</span> <span class="c1"># Gets requests from queue
</span>            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">batch_to_execute</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">time</span><span class="p">.</span><span class="n">sleep</span><span class="p">(</span><span class="mf">0.001</span><span class="p">)</span>
                <span class="k">continue</span>

            <span class="c1"># For embeddings, it's usually one call to encode the batch
</span>            <span class="n">batch_responses</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">pipeline</span><span class="p">.</span><span class="n">encode</span><span class="p">(</span><span class="n">batch_to_execute</span><span class="p">)</span>
            
            <span class="c1"># Send responses back
</span>            <span class="bp">self</span><span class="p">.</span><span class="n">response_q</span><span class="p">.</span><span class="n">put_nowait</span><span class="p">([</span><span class="n">batch_responses</span><span class="p">])</span> <span class="c1"># Note the list format
</span>            <span class="c1"># ...
</span></code></pre></div></div>
<p>Here, <code class="language-plaintext highlighter-rouge">_create_batch_to_execute</code> would pull items from <code class="language-plaintext highlighter-rouge">request_q</code> up to <code class="language-plaintext highlighter-rouge">max_batch_size</code> from <code class="language-plaintext highlighter-rouge">EmbeddingsSchedulerConfig</code>. Then, <code class="language-plaintext highlighter-rouge">self.pipeline.encode()</code> processes the whole batch.</p>

<h3 id="batching-configuration">Batching Configuration</h3>

<p>The behavior of these schedulers is heavily influenced by their configuration objects:</p>
<ul>
  <li><code class="language-plaintext highlighter-rouge">TokenGenerationSchedulerConfig</code>: Defines things like <code class="language-plaintext highlighter-rouge">max_batch_size_tg</code> (max requests in a token generation batch), <code class="language-plaintext highlighter-rouge">max_batch_size_ce</code> (max requests in a context encoding batch), <code class="language-plaintext highlighter-rouge">batch_timeout</code> (how long to wait for more CE requests), and <code class="language-plaintext highlighter-rouge">target_tokens_per_batch_ce</code> (aim for this many total prompt tokens in a CE batch).</li>
  <li><code class="language-plaintext highlighter-rouge">EmbeddingsSchedulerConfig</code>: Simpler, mainly <code class="language-plaintext highlighter-rouge">max_batch_size</code>.</li>
</ul>

<p>These configurations are derived from the overall application <a href="01_settings___settings__class__.md">Settings (<code class="language-plaintext highlighter-rouge">Settings</code> class)</a> and <code class="language-plaintext highlighter-rouge">TokenGeneratorPipelineConfig</code>.</p>

<h2 id="why-is-this-batching-so-important-for-performance">Why is This Batching So Important for Performance?</h2>

<p>AI models, especially large ones running on GPUs, are like massive parallel processors.</p>
<ul>
  <li><strong>High Latency, High Throughput</strong>: Sending a single request to a GPU might have some unavoidable startup cost (latency). But once it’s going, the GPU can perform calculations on many pieces of data simultaneously very quickly.</li>
  <li><strong>Filling the Pipes</strong>: Batching helps “fill the pipes” of the GPU, ensuring that its many processing units are kept busy. If you only send one piece of data, most of the GPU is idle.</li>
</ul>

<p>The Scheduler’s intelligent batching strategies aim to find the sweet spot: creating batches large enough to keep the GPU busy, but not waiting so long to form a batch that users experience high latency.</p>

<p>For <code class="language-plaintext highlighter-rouge">TokenGenerationScheduler</code>, it’s even more nuanced. Context encoding (processing the initial prompt) can be very different from generating subsequent tokens. The scheduler tries to optimize both. For example, it might prioritize getting ongoing token generation (TG) requests through quickly if the KV cache (memory for past tokens) is getting full, as TG requests are generally smaller and faster per step.</p>

<h2 id="conclusion">Conclusion</h2>

<p>The Scheduler (<code class="language-plaintext highlighter-rouge">TokenGenerationScheduler</code> and <code class="language-plaintext highlighter-rouge">EmbeddingsScheduler</code>) is the unsung hero of performance within the <a href="04_model_worker_.md">Model Worker</a>. It acts as an intelligent traffic controller:</p>
<ul>
  <li>It <strong>gathers</strong> individual requests from the <a href="07_enginequeue_.md">EngineQueue</a>.</li>
  <li>It <strong>groups</strong> them into optimal batches based on strategies and configuration.</li>
  <li>It <strong>dispatches</strong> these batches to the AI model pipeline.</li>
  <li>This batching is key to <strong>maximizing GPU utilization</strong> and overall application <strong>throughput</strong>.</li>
</ul>

<p>By efficiently managing how requests are fed to the AI model, the Scheduler ensures that <code class="language-plaintext highlighter-rouge">modular</code> can serve many users quickly and make the most of its powerful hardware.</p>

<p>One crucial aspect that the <code class="language-plaintext highlighter-rouge">TokenGenerationScheduler</code> often interacts with is how the model remembers previous parts of a conversation or text. This involves something called a KV Cache. In the next chapter, we’ll explore <a href="06_kv_cache_management_.md">Chapter 6: KV Cache Management</a> and see how it plays a vital role in efficient text generation.</p>

<hr />

<p>Generated by <a href="https://github.com/The-Pocket/Tutorial-Codebase-Knowledge">AI Codebase Knowledge Builder</a></p>
