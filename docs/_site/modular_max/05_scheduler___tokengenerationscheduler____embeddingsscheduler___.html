<!DOCTYPE html> <html lang="en-US"> <head> <meta charset="UTF-8"> <meta http-equiv="X-UA-Compatible" content="IE=Edge"> <link rel="stylesheet" href="/assets/css/just-the-docs-default.css"> <script src="/assets/js/vendor/lunr.min.js"></script> <script src="/assets/js/just-the-docs.js"></script> <meta name="viewport" content="width=device-width, initial-scale=1"> <!-- Begin Jekyll SEO tag v2.8.0 --> <title>Chapter 5: Scheduler | Two Tutorials for Mojo using Pocket Flow</title> <meta name="generator" content="Jekyll v4.3.4" /> <meta property="og:title" content="Chapter 5: Scheduler" /> <meta name="author" content="Sam Kirk" /> <meta property="og:locale" content="en_US" /> <meta name="description" content="Documentation generated using AI to explain codebases" /> <meta property="og:description" content="Documentation generated using AI to explain codebases" /> <link rel="canonical" href="http://localhost:4000/modular_max/05_scheduler___tokengenerationscheduler____embeddingsscheduler___.html" /> <meta property="og:url" content="http://localhost:4000/modular_max/05_scheduler___tokengenerationscheduler____embeddingsscheduler___.html" /> <meta property="og:site_name" content="Two Tutorials for Mojo using Pocket Flow" /> <meta property="og:type" content="website" /> <meta name="twitter:card" content="summary" /> <meta property="twitter:title" content="Chapter 5: Scheduler" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"WebPage","author":{"@type":"Person","name":"Sam Kirk","url":"https://www.linkedin.com/in/samuelkirk"},"description":"Documentation generated using AI to explain codebases","headline":"Chapter 5: Scheduler","url":"http://localhost:4000/modular_max/05_scheduler___tokengenerationscheduler____embeddingsscheduler___.html"}</script> <!-- End Jekyll SEO tag --> <!-- Add Mermaid support --> <script src="https://cdn.jsdelivr.net/npm/mermaid@11.6.0/dist/mermaid.min.js"></script> <script> document.addEventListener("DOMContentLoaded", function() { mermaid.initialize({ startOnLoad: true, theme: "default" }); // Process code blocks document.querySelectorAll('pre code.language-mermaid').forEach(function(block) { // Create a div with class 'mermaid' var mermaidDiv = document.createElement('div'); mermaidDiv.className = 'mermaid'; mermaidDiv.innerHTML = block.textContent; // Replace the parent pre with the mermaid div block.parentNode.parentNode.replaceChild(mermaidDiv, block.parentNode); console.log("Processed Mermaid block:", mermaidDiv.innerHTML.substring(0, 50) + "..."); }); console.log("Mermaid initialization complete. Version:", mermaid.version()); }); </script> </head> <body> <a class="skip-to-main" href="#main-content">Skip to main content</a> <svg xmlns="http://www.w3.org/2000/svg" class="d-none"> <symbol id="svg-link" viewBox="0 0 24 24"> <title>Link</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-link"> <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path> </svg> </symbol> <symbol id="svg-menu" viewBox="0 0 24 24"> <title>Menu</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu"> <line x1="3" y1="12" x2="21" y2="12"></line><line x1="3" y1="6" x2="21" y2="6"></line><line x1="3" y1="18" x2="21" y2="18"></line> </svg> </symbol> <symbol id="svg-arrow-right" viewBox="0 0 24 24"> <title>Expand</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-chevron-right"> <polyline points="9 18 15 12 9 6"></polyline> </svg> </symbol> <!-- Feather. MIT License: https://github.com/feathericons/feather/blob/master/LICENSE --> <symbol id="svg-external-link" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-external-link"> <title id="svg-external-link-title">(external link)</title> <path d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"></path><polyline points="15 3 21 3 21 9"></polyline><line x1="10" y1="14" x2="21" y2="3"></line> </symbol> <symbol id="svg-doc" viewBox="0 0 24 24"> <title>Document</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file"> <path d="M13 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V9z"></path><polyline points="13 2 13 9 20 9"></polyline> </svg> </symbol> <symbol id="svg-search" viewBox="0 0 24 24"> <title>Search</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-search"> <circle cx="11" cy="11" r="8"></circle><line x1="21" y1="21" x2="16.65" y2="16.65"></line> </svg> </symbol> <!-- Bootstrap Icons. MIT License: https://github.com/twbs/icons/blob/main/LICENSE.md --> <symbol id="svg-copy" viewBox="0 0 16 16"> <title>Copy</title> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard" viewBox="0 0 16 16"> <path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1h1a1 1 0 0 1 1 1V14a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V3.5a1 1 0 0 1 1-1h1v-1z"/> <path d="M9.5 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3zm-3-1A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3z"/> </svg> </symbol> <symbol id="svg-copied" viewBox="0 0 16 16"> <title>Copied</title> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard-check-fill" viewBox="0 0 16 16"> <path d="M6.5 0A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3Zm3 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3Z"/> <path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1A2.5 2.5 0 0 1 9.5 5h-3A2.5 2.5 0 0 1 4 2.5v-1Zm6.854 7.354-3 3a.5.5 0 0 1-.708 0l-1.5-1.5a.5.5 0 0 1 .708-.708L7.5 10.793l2.646-2.647a.5.5 0 0 1 .708.708Z"/> </svg> </symbol> </svg> <div class="side-bar"> <div class="site-header" role="banner"> <a href="/" class="site-title lh-tight"> Two Tutorials for Mojo using Pocket Flow </a> <button id="menu-button" class="site-button btn-reset" aria-label="Toggle menu" aria-pressed="false"> <svg viewBox="0 0 24 24" class="icon" aria-hidden="true"><use xlink:href="#svg-menu"></use></svg> </a> </div> <nav aria-label="Main" id="site-nav" class="site-nav"> <ul class="nav-list"><li class="nav-list-item"><a href="/" class="nav-list-link">README</a></li><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Crawl4AI category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button><a href="/Crawl4AI/" class="nav-list-link">Crawl4AI</a><ul class="nav-list"><li class="nav-list-item "><a href="/Crawl4AI/01_asynccrawlerstrategy.html" class="nav-list-link">AsyncCrawlerStrategy</a></li><li class="nav-list-item "><a href="/Crawl4AI/01_configuration_system_.html" class="nav-list-link">Chapter 1: Configuration System</a></li><li class="nav-list-item "><a href="/Crawl4AI/02_asyncwebcrawler.html" class="nav-list-link">AsyncWebCrawler</a></li><li class="nav-list-item "><a href="/Crawl4AI/02_asyncwebcrawler_.html" class="nav-list-link">Chapter 2: AsyncWebCrawler</a></li><li class="nav-list-item "><a href="/Crawl4AI/03_content_extraction_pipeline_.html" class="nav-list-link">Chapter 3: Content Extraction Pipeline</a></li><li class="nav-list-item "><a href="/Crawl4AI/03_crawlerrunconfig.html" class="nav-list-link">CrawlerRunConfig</a></li><li class="nav-list-item "><a href="/Crawl4AI/04_contentscrapingstrategy.html" class="nav-list-link">ContentScrapingStrategy</a></li><li class="nav-list-item "><a href="/Crawl4AI/04_url_filtering___scoring_.html" class="nav-list-link">Chapter 4: URL Filtering & Scoring</a></li><li class="nav-list-item "><a href="/Crawl4AI/05_deep_crawling_system_.html" class="nav-list-link">Chapter 5: Deep Crawling System</a></li><li class="nav-list-item "><a href="/Crawl4AI/05_relevantcontentfilter.html" class="nav-list-link">RelevantContentFilter</a></li><li class="nav-list-item "><a href="/Crawl4AI/06_caching_system_.html" class="nav-list-link">Chapter 6: Caching System</a></li><li class="nav-list-item "><a href="/Crawl4AI/06_extractionstrategy.html" class="nav-list-link">ExtractionStrategy</a></li><li class="nav-list-item "><a href="/Crawl4AI/07_crawlresult.html" class="nav-list-link">CrawlResult</a></li><li class="nav-list-item "><a href="/Crawl4AI/07_dispatcher_framework_.html" class="nav-list-link">Chapter 7: Dispatcher Framework</a></li><li class="nav-list-item "><a href="/Crawl4AI/08_async_logging_infrastructure_.html" class="nav-list-link">Chapter 8: Async Logging Infrastructure</a></li><li class="nav-list-item "><a href="/Crawl4AI/08_deepcrawlstrategy.html" class="nav-list-link">DeepCrawlStrategy</a></li><li class="nav-list-item "><a href="/Crawl4AI/09_cachecontext___cachemode.html" class="nav-list-link">CacheContext & CacheMode</a></li><li class="nav-list-item "><a href="/Crawl4AI/09_api___docker_integration_.html" class="nav-list-link">Chapter 9: API & Docker Integration</a></li><li class="nav-list-item "><a href="/Crawl4AI/10_basedispatcher.html" class="nav-list-link">BaseDispatcher</a></li></ul></li><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in My Tutorial for Crawl4ai category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button><a href="/my-crawl4AI/" class="nav-list-link">My Tutorial for Crawl4ai</a><ul class="nav-list"><li class="nav-list-item "><a href="/my-crawl4ai/01_configuration_system_.html" class="nav-list-link">Chapter 1: Configuration System</a></li><li class="nav-list-item "><a href="/my-crawl4ai/02_asyncwebcrawler_.html" class="nav-list-link">Chapter 2: AsyncWebCrawler</a></li><li class="nav-list-item "><a href="/my-crawl4ai/03_content_extraction_pipeline_.html" class="nav-list-link">Chapter 3: Content Extraction Pipeline</a></li><li class="nav-list-item "><a href="/my-crawl4ai/04_url_filtering___scoring_.html" class="nav-list-link">Chapter 4: URL Filtering & Scoring</a></li><li class="nav-list-item "><a href="/my-crawl4ai/05_deep_crawling_system_.html" class="nav-list-link">Chapter 5: Deep Crawling System</a></li><li class="nav-list-item "><a href="/my-crawl4ai/06_caching_system_.html" class="nav-list-link">Chapter 6: Caching System</a></li><li class="nav-list-item "><a href="/my-crawl4ai/07_dispatcher_framework_.html" class="nav-list-link">Chapter 7: Dispatcher Framework</a></li><li class="nav-list-item "><a href="/my-crawl4ai/08_async_logging_infrastructure_.html" class="nav-list-link">Chapter 8: Async Logging Infrastructure</a></li><li class="nav-list-item "><a href="/my-crawl4ai/09_api___docker_integration_.html" class="nav-list-link">Chapter 9: API & Docker Integration</a></li></ul></li><li class="nav-list-item active"><button class="nav-list-expander btn-reset" aria-label="toggle items in My Tutorial for Modular's Max category" aria-pressed="true"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button><a href="/modular_max/" class="nav-list-link">My Tutorial for Modular's Max</a><ul class="nav-list"><li class="nav-list-item "><a href="/modular_max/01_settings___settings__class__.html" class="nav-list-link">Chapter 1: Settings Class</a></li><li class="nav-list-item "><a href="/modular_max/02_serving_api_layer__fastapi_app___routers__.html" class="nav-list-link">Chapter 2: Serving API Layer</a></li><li class="nav-list-item "><a href="/modular_max/03_llm_pipeline_orchestrator___tokengeneratorpipeline___.html" class="nav-list-link">Chapter 3: LLM Pipeline Orchestrator</a></li><li class="nav-list-item "><a href="/modular_max/04_model_worker_.html" class="nav-list-link">Chapter 4: Model Worker</a></li><li class="nav-list-item active"><a href="/modular_max/05_scheduler___tokengenerationscheduler____embeddingsscheduler___.html" class="nav-list-link active">Chapter 5: Scheduler</a></li><li class="nav-list-item "><a href="/modular_max/06_kv_cache_management_.html" class="nav-list-link">Chapter 6: KV Cache Management</a></li><li class="nav-list-item "><a href="/modular_max/07_enginequeue_.html" class="nav-list-link">Chapter 7: EngineQueue</a></li><li class="nav-list-item "><a href="/modular_max/08_telemetry_and_metrics___metrics____metricclient___.html" class="nav-list-link">Chapter 8: Telemetry and Metrics</a></li></ul></li><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in My Tutorial for Mojo v1 category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button><a href="/mojo-v1/" class="nav-list-link">My Tutorial for Mojo v1</a><ul class="nav-list"><li class="nav-list-item "><a href="/mojo-v1/01_addressspace_.html" class="nav-list-link">Chapter 1: AddressSpace</a></li><li class="nav-list-item "><a href="/mojo-v1/02_unsafepointer_.html" class="nav-list-link">Chapter 2: UnsafePointer</a></li><li class="nav-list-item "><a href="/mojo-v1/03_indexlist_.html" class="nav-list-link">Chapter 3: IndexList</a></li><li class="nav-list-item "><a href="/mojo-v1/04_dimlist_.html" class="nav-list-link">Chapter 4: DimList</a></li><li class="nav-list-item "><a href="/mojo-v1/05_ndbuffer_.html" class="nav-list-link">Chapter 5: NDBuffer</a></li><li class="nav-list-item "><a href="/mojo-v1/06_n_d_to_1d_indexing_logic__strided_memory_access__.html" class="nav-list-link">Chapter 6: N-D to 1D Indexing Logic</a></li></ul></li><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in My Tutorial for Mojo v2 category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button><a href="/mojo-v2/" class="nav-list-link">My Tutorial for Mojo v2</a><ul class="nav-list"><li class="nav-list-item "><a href="/mojo-v2/01_unsafepointer__as_used_by_ndbuffer__.html" class="nav-list-link">Chapter 1: UnsafePointer</a></li><li class="nav-list-item "><a href="/mojo-v2/02_dimlist_and_dim_.html" class="nav-list-link">Chapter 2: DimList and Dim</a></li><li class="nav-list-item "><a href="/mojo-v2/03_ndbuffer_.html" class="nav-list-link">Chapter 3: NDBuffer</a></li><li class="nav-list-item "><a href="/mojo-v2/04_strides_and_offset_computation_.html" class="nav-list-link">Chapter 4: Strides and Offset Computation</a></li><li class="nav-list-item "><a href="/mojo-v2/05_simd_data_access_.html" class="nav-list-link">Chapter 5: SIMD Data Access</a></li></ul></li><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Comparisons category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button><a href="/comparisons/" class="nav-list-link">Comparisons</a><ul class="nav-list"><li class="nav-list-item "><a href="/comparisons/crawl4ai-versions.html" class="nav-list-link">The original version of Crawl4AI vs. the sanity check I did</a></li><li class="nav-list-item "><a href="/comparisons/mojo-versions.html" class="nav-list-link">The 1st version of the Mojo Tutorial vs. The 2nd version of the Mojo Tutorial</a></li></ul></li><li class="nav-list-item"><a href="/generating/" class="nav-list-link">Generating & Preparing for the Mojo Tutorials using Pocket Flow</a></li><li class="nav-list-item"><a href="/design.html" class="nav-list-link">System Design</a></li></ul> <div class="nav-category">Crawl4AI</div> <ul class="nav-list"></ul> <div class="nav-category">Modular Max</div> <ul class="nav-list"></ul> <div class="nav-category">Mojo (v1)</div> <ul class="nav-list"></ul> <div class="nav-category">Mojo (v2)</div> <ul class="nav-list"></ul> <div class="nav-category">Generating</div> <ul class="nav-list"></ul> <div class="nav-category">AI Design</div> <ul class="nav-list"></ul> </nav> <footer class="site-footer"> This site uses <a href="https://github.com/just-the-docs/just-the-docs">Just the Docs</a>, a documentation theme for Jekyll. </footer> </div> <div class="main" id="top"> <div id="main-header" class="main-header"> <div class="search" role="search"> <div class="search-input-wrap"> <input type="text" id="search-input" class="search-input" tabindex="0" placeholder="Search Two Tutorials for Mojo using Pocket Flow" aria-label="Search Two Tutorials for Mojo using Pocket Flow" autocomplete="off"> <label for="search-input" class="search-label"><svg viewBox="0 0 24 24" class="search-icon"><use xlink:href="#svg-search"></use></svg></label> </div> <div id="search-results" class="search-results"></div> </div> <nav aria-label="Auxiliary" class="aux-nav"> <ul class="aux-nav-list"> <li class="aux-nav-list-item"> <a href="https://github.com/MrBesterTester/pf-understand" class="site-button" > View on GitHub </a> </li> </ul> </nav> </div> <div id="main-content-wrap" class="main-content-wrap"> <nav aria-label="Breadcrumb" class="breadcrumb-nav"> <ol class="breadcrumb-nav-list"> <li class="breadcrumb-nav-list-item"><a href="/modular_max/">My Tutorial for Modular's Max</a></li> <li class="breadcrumb-nav-list-item"><span>Chapter 5: Scheduler</span></li> </ol> </nav> <div id="main-content" class="main-content"> <main> <h1 id="chapter-5-scheduler-tokengenerationscheduler-embeddingsscheduler"> <a href="#chapter-5-scheduler-tokengenerationscheduler-embeddingsscheduler" class="anchor-heading" aria-labelledby="chapter-5-scheduler-tokengenerationscheduler-embeddingsscheduler"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Chapter 5: Scheduler (<code class="language-plaintext highlighter-rouge">TokenGenerationScheduler</code>, <code class="language-plaintext highlighter-rouge">EmbeddingsScheduler</code>) </h1> <p>In <a href="04_model_worker_.md">Chapter 4: Model Worker</a>, we learned about the Model Worker, the dedicated workshop where the AI model does its heavy lifting. We saw that it receives requests from the <a href="07_enginequeue_.md">EngineQueue</a>. But how does the Model Worker decide <em>when</em> and <em>how</em> to feed these requests to the AI model? Does it process them one by one? Or is there a smarter way?</p> <p>That’s where the <strong>Scheduler</strong> comes in! It’s like an intelligent traffic controller or a theme park ride operator working <em>inside</em> the Model Worker. Its main job is to make sure the AI model (our star attraction) is used as efficiently as possible.</p> <h2 id="what-problem-does-the-scheduler-solve"> <a href="#what-problem-does-the-scheduler-solve" class="anchor-heading" aria-labelledby="what-problem-does-the-scheduler-solve"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> What Problem Does the Scheduler Solve? </h2> <p>Imagine our AI model is like a super-fast, super-powerful rollercoaster.</p> <ul> <li>If we send only one person on the rollercoaster at a time, a lot of the seats will be empty. It’s not very efficient!</li> <li>If we wait too long to fill the rollercoaster, people in line get impatient.</li> </ul> <p>The Scheduler’s job is to gather individual requests (people wanting to ride the rollercoaster) and group them into <strong>optimal batches</strong> (filling up the rollercoaster carts efficiently) before sending them to the AI model for processing. This “batching” is crucial for:</p> <ul> <li><strong>Maximizing GPU Utilization</strong>: Modern AI models, especially those running on GPUs, perform much better when they process multiple pieces of data at once. Running one request at a time can leave the GPU underutilized.</li> <li><strong>Improving Throughput</strong>: By processing requests in batches, the overall number of requests handled per second (throughput) increases significantly.</li> </ul> <p>Think of the Scheduler as a theme park ride operator. They don’t start the ride with just one person in a 20-seat cart. They try to fill the cart (or a good portion of it) to make each run worthwhile. Similarly, the Scheduler groups incoming requests based on things like how many requests are waiting, how long they’ve been waiting, or how “big” each request is (e.g., how many tokens need processing).</p> <h2 id="meet-the-schedulers-tokengenerationscheduler-and-embeddingsscheduler"> <a href="#meet-the-schedulers-tokengenerationscheduler-and-embeddingsscheduler" class="anchor-heading" aria-labelledby="meet-the-schedulers-tokengenerationscheduler-and-embeddingsscheduler"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Meet the Schedulers: <code class="language-plaintext highlighter-rouge">TokenGenerationScheduler</code> and <code class="language-plaintext highlighter-rouge">EmbeddingsScheduler</code> </h2> <p><code class="language-plaintext highlighter-rouge">modular</code> has different types of AI tasks, and so it has specialized Schedulers:</p> <ol> <li><strong><code class="language-plaintext highlighter-rouge">TokenGenerationScheduler</code></strong>: This scheduler is used when the AI model’s job is to generate text (like writing a story or answering a question). It has specific strategies for: <ul> <li><strong>Context Encoding (CE)</strong>: Processing the initial prompt (e.g., “Tell me a story about a dragon”). This often involves processing many tokens at once.</li> <li><strong>Token Generation (TG)</strong>: Generating the subsequent words or tokens of the story, often one or a few at a time, repeatedly. The <code class="language-plaintext highlighter-rouge">TokenGenerationScheduler</code> is smart about forming batches for both these phases.</li> </ul> </li> <li><strong><code class="language-plaintext highlighter-rouge">EmbeddingsScheduler</code></strong>: This scheduler is used when the AI model’s job is to calculate embeddings (numerical representations of text, useful for tasks like semantic search). The requests are typically more straightforward: take some text, produce a list of numbers.</li> </ol> <p>Both schedulers operate on the same core principle: <strong>batch requests efficiently.</strong></p> <h3 id="how-schedulers-are-created"> <a href="#how-schedulers-are-created" class="anchor-heading" aria-labelledby="how-schedulers-are-created"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> How Schedulers are Created </h3> <p>Remember in <a href="04_model_worker_.md">Chapter 4: Model Worker</a>, we saw the <code class="language-plaintext highlighter-rouge">model_worker_run_v3</code> function inside the Model Worker process? That’s where the appropriate scheduler is created.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Simplified from: src/max/serve/pipelines/model_worker.py
</span>
<span class="c1"># (Inside model_worker_run_v3)
# ...
# pipeline = model_factory() # This loads the AI model (TokenGenerator or EmbeddingsGenerator)
</span>
<span class="n">scheduler</span><span class="p">:</span> <span class="n">Scheduler</span> <span class="c1"># This will hold our chosen scheduler
</span><span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">pipeline</span><span class="p">,</span> <span class="n">TokenGenerator</span><span class="p">):</span>
    <span class="c1"># If it's a text-generating model, use TokenGenerationScheduler
</span>    <span class="n">scheduler</span> <span class="o">=</span> <span class="n">_create_token_generation_scheduler</span><span class="p">(</span>
        <span class="n">pipeline</span><span class="p">,</span> <span class="n">pc</span><span class="p">,</span> <span class="n">pipeline_config</span><span class="p">,</span> <span class="n">queues</span>
    <span class="p">)</span>
<span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">pipeline</span><span class="p">,</span> <span class="n">EmbeddingsGenerator</span><span class="p">):</span>
    <span class="c1"># If it's an embeddings model, use EmbeddingsScheduler
</span>    <span class="n">scheduler</span> <span class="o">=</span> <span class="n">_create_embeddings_scheduler</span><span class="p">(</span>
        <span class="n">pipeline</span><span class="p">,</span> <span class="n">pc</span><span class="p">,</span> <span class="n">pipeline_config</span><span class="p">,</span> <span class="n">queues</span>
    <span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="k">raise</span> <span class="nb">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s">"Invalid pipeline type: </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">pipeline</span><span class="p">)</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>

<span class="c1"># ...
# pc.set_started() # Signal that the worker is ready
# scheduler.run()  # Start the scheduler's main loop!
# ...
</span></code></pre></div></div> <p>This code snippet shows that based on the type of AI model (<code class="language-plaintext highlighter-rouge">pipeline</code>) loaded, the Model Worker creates either a <code class="language-plaintext highlighter-rouge">TokenGenerationScheduler</code> or an <code class="language-plaintext highlighter-rouge">EmbeddingsScheduler</code>. The <code class="language-plaintext highlighter-rouge">pipeline_config</code> (which comes from <a href="01_settings___settings__class__.md">Settings (<code class="language-plaintext highlighter-rouge">Settings</code> class)</a>) and <code class="language-plaintext highlighter-rouge">queues</code> (parts of the <a href="07_enginequeue_.md">EngineQueue</a> for communication) are passed to configure the scheduler.</p> <p>Once created, <code class="language-plaintext highlighter-rouge">scheduler.run()</code> is called. This starts the scheduler’s main job: continuously looking for requests, batching them, and sending them to the AI model.</p> <h2 id="the-schedulers-main-loop-a-day-in-the-life"> <a href="#the-schedulers-main-loop-a-day-in-the-life" class="anchor-heading" aria-labelledby="the-schedulers-main-loop-a-day-in-the-life"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> The Scheduler’s Main Loop: A Day in the Life </h2> <p>Let’s imagine we’re the <code class="language-plaintext highlighter-rouge">TokenGenerationScheduler</code>. Here’s a simplified version of what our <code class="language-plaintext highlighter-rouge">run()</code> loop might do:</p> <ol> <li> <p><strong>Check for New Riders (Requests)</strong>: We look at our <code class="language-plaintext highlighter-rouge">request_q</code> (the part of the <a href="07_enginequeue_.md">EngineQueue</a> where new requests arrive from the <a href="03_llm_pipeline_orchestrator___tokengeneratorpipeline___.md">LLM Pipeline Orchestrator (<code class="language-plaintext highlighter-rouge">TokenGeneratorPipeline</code>)</a>). Are there any people waiting in line?</p> </li> <li><strong>Decide on the Next Cart (Batch Formation)</strong>: <ul> <li>If there are requests for <strong>Context Encoding</strong> (processing new prompts), we might try to group several of them. We might wait a short time (a <code class="language-plaintext highlighter-rouge">batch_timeout</code>) to see if more arrive, or until we have enough to fill a “batch” up to <code class="language-plaintext highlighter-rouge">max_batch_size_ce</code>.</li> <li>If there are requests that are already part-way through <strong>Token Generation</strong> (generating the next word of an ongoing story), we gather these.</li> <li>The scheduler uses its configuration (like <code class="language-plaintext highlighter-rouge">TokenGenerationSchedulerConfig</code>) to make these decisions. For example, <code class="language-plaintext highlighter-rouge">target_tokens_per_batch_ce</code> helps decide how many total prompt tokens to aim for in a context encoding batch.</li> </ul> </li> <li> <p><strong>Load the Cart (Prepare the Batch)</strong>: We select a group of requests to form the current batch. This is done by methods like <code class="language-plaintext highlighter-rouge">_create_batch_to_execute()</code>.</p> </li> <li> <p><strong>Start the Ride (Send to Model)</strong>: We send this batch of requests to the actual AI model (<code class="language-plaintext highlighter-rouge">self.pipeline.next_token(...)</code> for text generation or <code class="language-plaintext highlighter-rouge">self.pipeline.encode(...)</code> for embeddings).</p> </li> <li> <p><strong>Collect Souvenirs (Get Results)</strong>: The AI model processes the batch and produces results (e.g., the next set of generated tokens for each request in the batch).</p> </li> <li> <p><strong>Guide Riders to Exit (Send Results Back)</strong>: We take these results and put them onto our <code class="language-plaintext highlighter-rouge">response_q</code> (another part of the <a href="07_enginequeue_.md">EngineQueue</a>), so they can be sent back to the <a href="03_llm_pipeline_orchestrator___tokengeneratorpipeline___.md">LLM Pipeline Orchestrator (<code class="language-plaintext highlighter-rouge">TokenGeneratorPipeline</code>)</a> and eventually to the user.</p> </li> <li><strong>Repeat!</strong>: We go back to step 1 and do this all over again, continuously.</li> </ol> <p>This loop ensures a steady flow of efficiently batched requests to the AI model.</p> <h3 id="visualizing-the-schedulers-role"> <a href="#visualizing-the-schedulers-role" class="anchor-heading" aria-labelledby="visualizing-the-schedulers-role"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Visualizing the Scheduler’s Role </h3> <p>Here’s a diagram showing the Scheduler at work inside the Model Worker:</p><pre><code class="language-mermaid">sequenceDiagram
    participant InQueue as Input Queue (from EngineQueue)
    participant Sched as Scheduler
    participant AIModel as AI Model Pipeline
    participant OutQueue as Output Queue (to EngineQueue)

    Note over Sched: Scheduler's main run() loop active
    loop Processing Cycle
        Sched-&gt;&gt;InQueue: Check for/Get incoming requests
        InQueue--&gt;&gt;Sched: Provides requests (e.g., prompt A, prompt B)
        Sched-&gt;&gt;Sched: Forms a batch (e.g., [A, B]) based on strategy
        Sched-&gt;&gt;AIModel: Sends batch for processing
        AIModel-&gt;&gt;AIModel: Processes batch (e.g., generates text for A &amp; B)
        AIModel--&gt;&gt;Sched: Returns results for the batch
        Sched-&gt;&gt;OutQueue: Puts results onto output queue
    end
</code></pre><h2 id="under-the-hood-a-peek-into-tokengenerationscheduler"> <a href="#under-the-hood-a-peek-into-tokengenerationscheduler" class="anchor-heading" aria-labelledby="under-the-hood-a-peek-into-tokengenerationscheduler"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Under the Hood: A Peek into <code class="language-plaintext highlighter-rouge">TokenGenerationScheduler</code> </h2> <p>Let’s look at a simplified version of how the <code class="language-plaintext highlighter-rouge">TokenGenerationScheduler</code> might work, focusing on its <code class="language-plaintext highlighter-rouge">run</code> method and how it creates and schedules batches. The actual code is in <code class="language-plaintext highlighter-rouge">src/max/serve/pipelines/scheduler.py</code>.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Simplified from: src/max/serve/pipelines/scheduler.py
</span>
<span class="k">class</span> <span class="nc">TokenGenerationScheduler</span><span class="p">(</span><span class="n">Scheduler</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="c1"># ... process_control, scheduler_config, pipeline, queues ...
</span>        <span class="n">paged_manager</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">PagedKVCacheManager</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span> <span class="c1"># For KV Cache
</span>    <span class="p">):</span>
        <span class="c1"># self.scheduler_config = scheduler_config (stores max_batch_size_tg, etc.)
</span>        <span class="c1"># self.pipeline = pipeline (the AI model like TokenGenerator)
</span>        <span class="c1"># self.request_q = queues["REQUEST"] (where new requests arrive)
</span>        <span class="c1"># self.response_q = queues["RESPONSE"] (where results are sent)
</span>        <span class="c1"># self.active_batch = {} (requests currently being processed for TG)
</span>        <span class="c1"># self.paged_manager = paged_manager (manages KV Cache, see Chapter 6)
</span>        <span class="c1"># ...
</span>        <span class="k">pass</span>

    <span class="k">def</span> <span class="nf">run</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># Main loop: continuously process requests
</span>        <span class="k">while</span> <span class="bp">True</span><span class="p">:</span> <span class="c1"># Simplified; actual loop checks for cancellation
</span>            <span class="c1"># self.pc.beat() # Heartbeat to show it's alive
</span>
            <span class="c1"># 1. Decide what kind of batch to make next (CE or TG)
</span>            <span class="c1"># and gather requests for it.
</span>            <span class="n">batch_to_execute</span><span class="p">:</span> <span class="n">SchedulerOutput</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">_create_batch_to_execute</span><span class="p">()</span>
            <span class="c1"># SchedulerOutput contains the batch_inputs, batch_type (CE/TG), etc.
</span>
            <span class="k">if</span> <span class="n">batch_to_execute</span><span class="p">.</span><span class="n">batch_size</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="c1"># No requests to process right now, maybe sleep briefly
</span>                <span class="n">time</span><span class="p">.</span><span class="n">sleep</span><span class="p">(</span><span class="mf">0.001</span><span class="p">)</span> <span class="c1"># Small delay
</span>                <span class="k">continue</span>

            <span class="c1"># 2. Schedule the batch on the AI model
</span>            <span class="bp">self</span><span class="p">.</span><span class="n">_schedule</span><span class="p">(</span><span class="n">batch_to_execute</span><span class="p">)</span> <span class="c1"># This calls the model
</span>
            <span class="c1"># ... (handle cancelled requests, logging, etc.) ...
</span></code></pre></div></div> <ul> <li>The <code class="language-plaintext highlighter-rouge">__init__</code> method sets up the scheduler with its configuration, the AI model pipeline it will use, the communication queues, and a reference to the <code class="language-plaintext highlighter-rouge">paged_manager</code> for <a href="06_kv_cache_management_.md">KV Cache Management</a> (more on this in the next chapter!).</li> <li>The <code class="language-plaintext highlighter-rouge">run()</code> method is the heart. <ul> <li><code class="language-plaintext highlighter-rouge">_create_batch_to_execute()</code>: This internal method is responsible for looking at waiting requests and current <code class="language-plaintext highlighter-rouge">active_batch</code> (for ongoing token generation) and deciding what to run next. It considers <code class="language-plaintext highlighter-rouge">scheduler_config</code> parameters like <code class="language-plaintext highlighter-rouge">max_batch_size_ce</code>, <code class="language-plaintext highlighter-rouge">max_batch_size_tg</code>, <code class="language-plaintext highlighter-rouge">batch_timeout</code>, and <code class="language-plaintext highlighter-rouge">target_tokens_per_batch_ce</code>. It also interacts with the <code class="language-plaintext highlighter-rouge">paged_manager</code> to ensure there’s enough KV Cache space.</li> <li><code class="language-plaintext highlighter-rouge">_schedule()</code>: This method takes the <code class="language-plaintext highlighter-rouge">SchedulerOutput</code> (which contains the batch) and actually sends it to the AI model.</li> </ul> </li> </ul> <p>Let’s look at a simplified <code class="language-plaintext highlighter-rouge">_schedule</code> method:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Simplified from: src/max/serve/pipelines/scheduler.py
# (Inside TokenGenerationScheduler class)
</span>
    <span class="k">def</span> <span class="nf">_schedule</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sch_output</span><span class="p">:</span> <span class="n">SchedulerOutput</span><span class="p">):</span>
        <span class="c1"># sch_output contains:
</span>        <span class="c1"># - sch_output.batch_inputs: the actual dictionary of requests for the model
</span>        <span class="c1"># - sch_output.batch_type: tells if it's ContextEncoding or TokenGeneration
</span>        <span class="c1"># - sch_output.num_steps: how many tokens to generate for TG
</span>
        <span class="n">batch_responses</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">if</span> <span class="n">sch_output</span><span class="p">.</span><span class="n">batch_type</span> <span class="o">==</span> <span class="n">BatchType</span><span class="p">.</span><span class="n">ContextEncoding</span><span class="p">:</span>
            <span class="c1"># Processing initial prompts
</span>            <span class="n">batch_responses</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">pipeline</span><span class="p">.</span><span class="n">next_token</span><span class="p">(</span>
                <span class="n">sch_output</span><span class="p">.</span><span class="n">batch_inputs</span><span class="p">,</span> <span class="c1"># The batch of new prompts
</span>                <span class="n">num_steps</span><span class="o">=</span><span class="n">sch_output</span><span class="p">.</span><span class="n">num_steps</span><span class="p">,</span> <span class="c1"># Usually 1 for CE
</span>            <span class="p">)</span>
            <span class="c1"># ... logic to add these requests to self.active_batch if not done ...
</span>        <span class="k">elif</span> <span class="n">sch_output</span><span class="p">.</span><span class="n">batch_type</span> <span class="o">==</span> <span class="n">BatchType</span><span class="p">.</span><span class="n">TokenGeneration</span><span class="p">:</span>
            <span class="c1"># Generating subsequent tokens for ongoing requests
</span>            <span class="n">batch_responses</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">pipeline</span><span class="p">.</span><span class="n">next_token</span><span class="p">(</span>
                <span class="n">sch_output</span><span class="p">.</span><span class="n">batch_inputs</span><span class="p">,</span> <span class="c1"># The batch of active requests
</span>                <span class="n">num_steps</span><span class="o">=</span><span class="n">sch_output</span><span class="p">.</span><span class="n">num_steps</span><span class="p">,</span> <span class="c1"># How many tokens to generate
</span>            <span class="p">)</span>
        
        <span class="c1"># ... logic to handle terminated responses (remove from active_batch) ...
</span>
        <span class="c1"># Send all responses back to the main process via the response_q
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">_stream_responses_to_frontend</span><span class="p">(</span><span class="n">batch_responses</span><span class="p">)</span>
</code></pre></div></div> <p>This shows that <code class="language-plaintext highlighter-rouge">_schedule</code> calls <code class="language-plaintext highlighter-rouge">self.pipeline.next_token()</code> with the batch. The <code class="language-plaintext highlighter-rouge">pipeline</code> is the <code class="language-plaintext highlighter-rouge">TokenGenerator</code> instance loaded in the Model Worker. The results (<code class="language-plaintext highlighter-rouge">batch_responses</code>) are then sent back using <code class="language-plaintext highlighter-rouge">_stream_responses_to_frontend()</code>, which puts them on the <code class="language-plaintext highlighter-rouge">response_q</code>.</p> <p>The <code class="language-plaintext highlighter-rouge">EmbeddingsScheduler</code> has a simpler structure because it typically just encodes a batch of texts without the ongoing generation state:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Simplified from: src/max/serve/pipelines/scheduler.py
</span>
<span class="k">class</span> <span class="nc">EmbeddingsScheduler</span><span class="p">(</span><span class="n">Scheduler</span><span class="p">):</span>
    <span class="c1"># ... __init__ similar to TokenGenerationScheduler but with EmbeddingsSchedulerConfig ...
</span>
    <span class="k">def</span> <span class="nf">run</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">while</span> <span class="bp">True</span><span class="p">:</span> <span class="c1"># Simplified
</span>            <span class="c1"># ...
</span>            <span class="n">batch_to_execute</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">_create_batch_to_execute</span><span class="p">()</span> <span class="c1"># Gets requests from queue
</span>            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">batch_to_execute</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">time</span><span class="p">.</span><span class="n">sleep</span><span class="p">(</span><span class="mf">0.001</span><span class="p">)</span>
                <span class="k">continue</span>

            <span class="c1"># For embeddings, it's usually one call to encode the batch
</span>            <span class="n">batch_responses</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">pipeline</span><span class="p">.</span><span class="n">encode</span><span class="p">(</span><span class="n">batch_to_execute</span><span class="p">)</span>
            
            <span class="c1"># Send responses back
</span>            <span class="bp">self</span><span class="p">.</span><span class="n">response_q</span><span class="p">.</span><span class="n">put_nowait</span><span class="p">([</span><span class="n">batch_responses</span><span class="p">])</span> <span class="c1"># Note the list format
</span>            <span class="c1"># ...
</span></code></pre></div></div> <p>Here, <code class="language-plaintext highlighter-rouge">_create_batch_to_execute</code> would pull items from <code class="language-plaintext highlighter-rouge">request_q</code> up to <code class="language-plaintext highlighter-rouge">max_batch_size</code> from <code class="language-plaintext highlighter-rouge">EmbeddingsSchedulerConfig</code>. Then, <code class="language-plaintext highlighter-rouge">self.pipeline.encode()</code> processes the whole batch.</p> <h3 id="batching-configuration"> <a href="#batching-configuration" class="anchor-heading" aria-labelledby="batching-configuration"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Batching Configuration </h3> <p>The behavior of these schedulers is heavily influenced by their configuration objects:</p> <ul> <li><code class="language-plaintext highlighter-rouge">TokenGenerationSchedulerConfig</code>: Defines things like <code class="language-plaintext highlighter-rouge">max_batch_size_tg</code> (max requests in a token generation batch), <code class="language-plaintext highlighter-rouge">max_batch_size_ce</code> (max requests in a context encoding batch), <code class="language-plaintext highlighter-rouge">batch_timeout</code> (how long to wait for more CE requests), and <code class="language-plaintext highlighter-rouge">target_tokens_per_batch_ce</code> (aim for this many total prompt tokens in a CE batch).</li> <li><code class="language-plaintext highlighter-rouge">EmbeddingsSchedulerConfig</code>: Simpler, mainly <code class="language-plaintext highlighter-rouge">max_batch_size</code>.</li> </ul> <p>These configurations are derived from the overall application <a href="01_settings___settings__class__.md">Settings (<code class="language-plaintext highlighter-rouge">Settings</code> class)</a> and <code class="language-plaintext highlighter-rouge">TokenGeneratorPipelineConfig</code>.</p> <h2 id="why-is-this-batching-so-important-for-performance"> <a href="#why-is-this-batching-so-important-for-performance" class="anchor-heading" aria-labelledby="why-is-this-batching-so-important-for-performance"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Why is This Batching So Important for Performance? </h2> <p>AI models, especially large ones running on GPUs, are like massive parallel processors.</p> <ul> <li><strong>High Latency, High Throughput</strong>: Sending a single request to a GPU might have some unavoidable startup cost (latency). But once it’s going, the GPU can perform calculations on many pieces of data simultaneously very quickly.</li> <li><strong>Filling the Pipes</strong>: Batching helps “fill the pipes” of the GPU, ensuring that its many processing units are kept busy. If you only send one piece of data, most of the GPU is idle.</li> </ul> <p>The Scheduler’s intelligent batching strategies aim to find the sweet spot: creating batches large enough to keep the GPU busy, but not waiting so long to form a batch that users experience high latency.</p> <p>For <code class="language-plaintext highlighter-rouge">TokenGenerationScheduler</code>, it’s even more nuanced. Context encoding (processing the initial prompt) can be very different from generating subsequent tokens. The scheduler tries to optimize both. For example, it might prioritize getting ongoing token generation (TG) requests through quickly if the KV cache (memory for past tokens) is getting full, as TG requests are generally smaller and faster per step.</p> <h2 id="conclusion"> <a href="#conclusion" class="anchor-heading" aria-labelledby="conclusion"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Conclusion </h2> <p>The Scheduler (<code class="language-plaintext highlighter-rouge">TokenGenerationScheduler</code> and <code class="language-plaintext highlighter-rouge">EmbeddingsScheduler</code>) is the unsung hero of performance within the <a href="04_model_worker_.md">Model Worker</a>. It acts as an intelligent traffic controller:</p> <ul> <li>It <strong>gathers</strong> individual requests from the <a href="07_enginequeue_.md">EngineQueue</a>.</li> <li>It <strong>groups</strong> them into optimal batches based on strategies and configuration.</li> <li>It <strong>dispatches</strong> these batches to the AI model pipeline.</li> <li>This batching is key to <strong>maximizing GPU utilization</strong> and overall application <strong>throughput</strong>.</li> </ul> <p>By efficiently managing how requests are fed to the AI model, the Scheduler ensures that <code class="language-plaintext highlighter-rouge">modular</code> can serve many users quickly and make the most of its powerful hardware.</p> <p>One crucial aspect that the <code class="language-plaintext highlighter-rouge">TokenGenerationScheduler</code> often interacts with is how the model remembers previous parts of a conversation or text. This involves something called a KV Cache. In the next chapter, we’ll explore <a href="06_kv_cache_management_.md">Chapter 6: KV Cache Management</a> and see how it plays a vital role in efficient text generation.</p><hr /> <p>Generated by <a href="https://github.com/The-Pocket/Tutorial-Codebase-Knowledge">AI Codebase Knowledge Builder</a></p> </main> </div> </div> <div class="search-overlay"></div> </div> <script type="module"> import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.6.0/dist/mermaid.esm.min.mjs'; var config = {} ; mermaid.initialize(config); mermaid.run({ querySelector: '.language-mermaid', }); </script> </body> </html>
