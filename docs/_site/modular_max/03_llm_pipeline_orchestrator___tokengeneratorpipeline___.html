<!DOCTYPE html> <html lang="en-US"> <head> <meta charset="UTF-8"> <meta http-equiv="X-UA-Compatible" content="IE=Edge"> <link rel="stylesheet" href="/assets/css/just-the-docs-default.css"> <script src="/assets/js/vendor/lunr.min.js"></script> <script src="/assets/js/just-the-docs.js"></script> <meta name="viewport" content="width=device-width, initial-scale=1"> <!-- Begin Jekyll SEO tag v2.8.0 --> <title>Chapter 3: LLM Pipeline Orchestrator | Two Tutorials for Mojo using Pocket Flow</title> <meta name="generator" content="Jekyll v4.3.4" /> <meta property="og:title" content="Chapter 3: LLM Pipeline Orchestrator" /> <meta name="author" content="Sam Kirk" /> <meta property="og:locale" content="en_US" /> <meta name="description" content="Documentation generated using AI to explain codebases" /> <meta property="og:description" content="Documentation generated using AI to explain codebases" /> <link rel="canonical" href="http://localhost:4000/modular_max/03_llm_pipeline_orchestrator___tokengeneratorpipeline___.html" /> <meta property="og:url" content="http://localhost:4000/modular_max/03_llm_pipeline_orchestrator___tokengeneratorpipeline___.html" /> <meta property="og:site_name" content="Two Tutorials for Mojo using Pocket Flow" /> <meta property="og:type" content="website" /> <meta name="twitter:card" content="summary" /> <meta property="twitter:title" content="Chapter 3: LLM Pipeline Orchestrator" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"WebPage","author":{"@type":"Person","name":"Sam Kirk","url":"https://www.linkedin.com/in/samuelkirk"},"description":"Documentation generated using AI to explain codebases","headline":"Chapter 3: LLM Pipeline Orchestrator","url":"http://localhost:4000/modular_max/03_llm_pipeline_orchestrator___tokengeneratorpipeline___.html"}</script> <!-- End Jekyll SEO tag --> <!-- Add Mermaid support --> <script src="https://cdn.jsdelivr.net/npm/mermaid@11.6.0/dist/mermaid.min.js"></script> <script> document.addEventListener("DOMContentLoaded", function() { mermaid.initialize({ startOnLoad: true, theme: "default" }); // Process code blocks document.querySelectorAll('pre code.language-mermaid').forEach(function(block) { // Create a div with class 'mermaid' var mermaidDiv = document.createElement('div'); mermaidDiv.className = 'mermaid'; mermaidDiv.innerHTML = block.textContent; // Replace the parent pre with the mermaid div block.parentNode.parentNode.replaceChild(mermaidDiv, block.parentNode); console.log("Processed Mermaid block:", mermaidDiv.innerHTML.substring(0, 50) + "..."); }); console.log("Mermaid initialization complete. Version:", mermaid.version()); }); </script> </head> <body> <a class="skip-to-main" href="#main-content">Skip to main content</a> <svg xmlns="http://www.w3.org/2000/svg" class="d-none"> <symbol id="svg-link" viewBox="0 0 24 24"> <title>Link</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-link"> <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path> </svg> </symbol> <symbol id="svg-menu" viewBox="0 0 24 24"> <title>Menu</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu"> <line x1="3" y1="12" x2="21" y2="12"></line><line x1="3" y1="6" x2="21" y2="6"></line><line x1="3" y1="18" x2="21" y2="18"></line> </svg> </symbol> <symbol id="svg-arrow-right" viewBox="0 0 24 24"> <title>Expand</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-chevron-right"> <polyline points="9 18 15 12 9 6"></polyline> </svg> </symbol> <!-- Feather. MIT License: https://github.com/feathericons/feather/blob/master/LICENSE --> <symbol id="svg-external-link" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-external-link"> <title id="svg-external-link-title">(external link)</title> <path d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"></path><polyline points="15 3 21 3 21 9"></polyline><line x1="10" y1="14" x2="21" y2="3"></line> </symbol> <symbol id="svg-doc" viewBox="0 0 24 24"> <title>Document</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file"> <path d="M13 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V9z"></path><polyline points="13 2 13 9 20 9"></polyline> </svg> </symbol> <symbol id="svg-search" viewBox="0 0 24 24"> <title>Search</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-search"> <circle cx="11" cy="11" r="8"></circle><line x1="21" y1="21" x2="16.65" y2="16.65"></line> </svg> </symbol> <!-- Bootstrap Icons. MIT License: https://github.com/twbs/icons/blob/main/LICENSE.md --> <symbol id="svg-copy" viewBox="0 0 16 16"> <title>Copy</title> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard" viewBox="0 0 16 16"> <path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1h1a1 1 0 0 1 1 1V14a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V3.5a1 1 0 0 1 1-1h1v-1z"/> <path d="M9.5 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3zm-3-1A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3z"/> </svg> </symbol> <symbol id="svg-copied" viewBox="0 0 16 16"> <title>Copied</title> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard-check-fill" viewBox="0 0 16 16"> <path d="M6.5 0A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3Zm3 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3Z"/> <path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1A2.5 2.5 0 0 1 9.5 5h-3A2.5 2.5 0 0 1 4 2.5v-1Zm6.854 7.354-3 3a.5.5 0 0 1-.708 0l-1.5-1.5a.5.5 0 0 1 .708-.708L7.5 10.793l2.646-2.647a.5.5 0 0 1 .708.708Z"/> </svg> </symbol> </svg> <div class="side-bar"> <div class="site-header" role="banner"> <a href="/" class="site-title lh-tight"> Two Tutorials for Mojo using Pocket Flow </a> <button id="menu-button" class="site-button btn-reset" aria-label="Toggle menu" aria-pressed="false"> <svg viewBox="0 0 24 24" class="icon" aria-hidden="true"><use xlink:href="#svg-menu"></use></svg> </a> </div> <nav aria-label="Main" id="site-nav" class="site-nav"> <ul class="nav-list"><li class="nav-list-item"><a href="/" class="nav-list-link">README</a></li><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Crawl4AI category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button><a href="/Crawl4AI/" class="nav-list-link">Crawl4AI</a><ul class="nav-list"><li class="nav-list-item "><a href="/Crawl4AI/01_asynccrawlerstrategy.html" class="nav-list-link">AsyncCrawlerStrategy</a></li><li class="nav-list-item "><a href="/Crawl4AI/01_configuration_system_.html" class="nav-list-link">Chapter 1: Configuration System</a></li><li class="nav-list-item "><a href="/Crawl4AI/02_asyncwebcrawler.html" class="nav-list-link">AsyncWebCrawler</a></li><li class="nav-list-item "><a href="/Crawl4AI/02_asyncwebcrawler_.html" class="nav-list-link">Chapter 2: AsyncWebCrawler</a></li><li class="nav-list-item "><a href="/Crawl4AI/03_content_extraction_pipeline_.html" class="nav-list-link">Chapter 3: Content Extraction Pipeline</a></li><li class="nav-list-item "><a href="/Crawl4AI/03_crawlerrunconfig.html" class="nav-list-link">CrawlerRunConfig</a></li><li class="nav-list-item "><a href="/Crawl4AI/04_contentscrapingstrategy.html" class="nav-list-link">ContentScrapingStrategy</a></li><li class="nav-list-item "><a href="/Crawl4AI/04_url_filtering___scoring_.html" class="nav-list-link">Chapter 4: URL Filtering & Scoring</a></li><li class="nav-list-item "><a href="/Crawl4AI/05_deep_crawling_system_.html" class="nav-list-link">Chapter 5: Deep Crawling System</a></li><li class="nav-list-item "><a href="/Crawl4AI/05_relevantcontentfilter.html" class="nav-list-link">RelevantContentFilter</a></li><li class="nav-list-item "><a href="/Crawl4AI/06_caching_system_.html" class="nav-list-link">Chapter 6: Caching System</a></li><li class="nav-list-item "><a href="/Crawl4AI/06_extractionstrategy.html" class="nav-list-link">ExtractionStrategy</a></li><li class="nav-list-item "><a href="/Crawl4AI/07_crawlresult.html" class="nav-list-link">CrawlResult</a></li><li class="nav-list-item "><a href="/Crawl4AI/07_dispatcher_framework_.html" class="nav-list-link">Chapter 7: Dispatcher Framework</a></li><li class="nav-list-item "><a href="/Crawl4AI/08_async_logging_infrastructure_.html" class="nav-list-link">Chapter 8: Async Logging Infrastructure</a></li><li class="nav-list-item "><a href="/Crawl4AI/08_deepcrawlstrategy.html" class="nav-list-link">DeepCrawlStrategy</a></li><li class="nav-list-item "><a href="/Crawl4AI/09_cachecontext___cachemode.html" class="nav-list-link">CacheContext & CacheMode</a></li><li class="nav-list-item "><a href="/Crawl4AI/09_api___docker_integration_.html" class="nav-list-link">Chapter 9: API & Docker Integration</a></li><li class="nav-list-item "><a href="/Crawl4AI/10_basedispatcher.html" class="nav-list-link">BaseDispatcher</a></li></ul></li><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in My Tutorial for Crawl4ai category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button><a href="/my-crawl4AI/" class="nav-list-link">My Tutorial for Crawl4ai</a><ul class="nav-list"><li class="nav-list-item "><a href="/my-crawl4ai/01_configuration_system_.html" class="nav-list-link">Chapter 1: Configuration System</a></li><li class="nav-list-item "><a href="/my-crawl4ai/02_asyncwebcrawler_.html" class="nav-list-link">Chapter 2: AsyncWebCrawler</a></li><li class="nav-list-item "><a href="/my-crawl4ai/03_content_extraction_pipeline_.html" class="nav-list-link">Chapter 3: Content Extraction Pipeline</a></li><li class="nav-list-item "><a href="/my-crawl4ai/04_url_filtering___scoring_.html" class="nav-list-link">Chapter 4: URL Filtering & Scoring</a></li><li class="nav-list-item "><a href="/my-crawl4ai/05_deep_crawling_system_.html" class="nav-list-link">Chapter 5: Deep Crawling System</a></li><li class="nav-list-item "><a href="/my-crawl4ai/06_caching_system_.html" class="nav-list-link">Chapter 6: Caching System</a></li><li class="nav-list-item "><a href="/my-crawl4ai/07_dispatcher_framework_.html" class="nav-list-link">Chapter 7: Dispatcher Framework</a></li><li class="nav-list-item "><a href="/my-crawl4ai/08_async_logging_infrastructure_.html" class="nav-list-link">Chapter 8: Async Logging Infrastructure</a></li><li class="nav-list-item "><a href="/my-crawl4ai/09_api___docker_integration_.html" class="nav-list-link">Chapter 9: API & Docker Integration</a></li></ul></li><li class="nav-list-item active"><button class="nav-list-expander btn-reset" aria-label="toggle items in My Tutorial for Modular's Max category" aria-pressed="true"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button><a href="/modular_max/" class="nav-list-link">My Tutorial for Modular's Max</a><ul class="nav-list"><li class="nav-list-item "><a href="/modular_max/01_settings___settings__class__.html" class="nav-list-link">Chapter 1: Settings Class</a></li><li class="nav-list-item "><a href="/modular_max/02_serving_api_layer__fastapi_app___routers__.html" class="nav-list-link">Chapter 2: Serving API Layer</a></li><li class="nav-list-item active"><a href="/modular_max/03_llm_pipeline_orchestrator___tokengeneratorpipeline___.html" class="nav-list-link active">Chapter 3: LLM Pipeline Orchestrator</a></li><li class="nav-list-item "><a href="/modular_max/04_model_worker_.html" class="nav-list-link">Chapter 4: Model Worker</a></li><li class="nav-list-item "><a href="/modular_max/05_scheduler___tokengenerationscheduler____embeddingsscheduler___.html" class="nav-list-link">Chapter 5: Scheduler</a></li><li class="nav-list-item "><a href="/modular_max/06_kv_cache_management_.html" class="nav-list-link">Chapter 6: KV Cache Management</a></li><li class="nav-list-item "><a href="/modular_max/07_enginequeue_.html" class="nav-list-link">Chapter 7: EngineQueue</a></li><li class="nav-list-item "><a href="/modular_max/08_telemetry_and_metrics___metrics____metricclient___.html" class="nav-list-link">Chapter 8: Telemetry and Metrics</a></li></ul></li><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in My Tutorial for Mojo v1 category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button><a href="/mojo-v1/" class="nav-list-link">My Tutorial for Mojo v1</a><ul class="nav-list"><li class="nav-list-item "><a href="/mojo-v1/01_addressspace_.html" class="nav-list-link">Chapter 1: AddressSpace</a></li><li class="nav-list-item "><a href="/mojo-v1/02_unsafepointer_.html" class="nav-list-link">Chapter 2: UnsafePointer</a></li><li class="nav-list-item "><a href="/mojo-v1/03_indexlist_.html" class="nav-list-link">Chapter 3: IndexList</a></li><li class="nav-list-item "><a href="/mojo-v1/04_dimlist_.html" class="nav-list-link">Chapter 4: DimList</a></li><li class="nav-list-item "><a href="/mojo-v1/05_ndbuffer_.html" class="nav-list-link">Chapter 5: NDBuffer</a></li><li class="nav-list-item "><a href="/mojo-v1/06_n_d_to_1d_indexing_logic__strided_memory_access__.html" class="nav-list-link">Chapter 6: N-D to 1D Indexing Logic</a></li></ul></li><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in My Tutorial for Mojo v2 category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button><a href="/mojo-v2/" class="nav-list-link">My Tutorial for Mojo v2</a><ul class="nav-list"><li class="nav-list-item "><a href="/mojo-v2/01_unsafepointer__as_used_by_ndbuffer__.html" class="nav-list-link">Chapter 1: UnsafePointer</a></li><li class="nav-list-item "><a href="/mojo-v2/02_dimlist_and_dim_.html" class="nav-list-link">Chapter 2: DimList and Dim</a></li><li class="nav-list-item "><a href="/mojo-v2/03_ndbuffer_.html" class="nav-list-link">Chapter 3: NDBuffer</a></li><li class="nav-list-item "><a href="/mojo-v2/04_strides_and_offset_computation_.html" class="nav-list-link">Chapter 4: Strides and Offset Computation</a></li><li class="nav-list-item "><a href="/mojo-v2/05_simd_data_access_.html" class="nav-list-link">Chapter 5: SIMD Data Access</a></li></ul></li><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Comparisons category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button><a href="/comparisons/" class="nav-list-link">Comparisons</a><ul class="nav-list"><li class="nav-list-item "><a href="/comparisons/crawl4ai-versions.html" class="nav-list-link">The original version of Crawl4AI vs. the sanity check I did</a></li><li class="nav-list-item "><a href="/comparisons/mojo-versions.html" class="nav-list-link">The 1st version of the Mojo Tutorial vs. The 2nd version of the Mojo Tutorial</a></li></ul></li><li class="nav-list-item"><a href="/generating/" class="nav-list-link">Generating & Preparing for the Mojo Tutorials using Pocket Flow</a></li><li class="nav-list-item"><a href="/design.html" class="nav-list-link">System Design</a></li></ul> <div class="nav-category">Crawl4AI</div> <ul class="nav-list"></ul> <div class="nav-category">Modular Max</div> <ul class="nav-list"></ul> <div class="nav-category">Mojo (v1)</div> <ul class="nav-list"></ul> <div class="nav-category">Mojo (v2)</div> <ul class="nav-list"></ul> <div class="nav-category">Generating</div> <ul class="nav-list"></ul> <div class="nav-category">AI Design</div> <ul class="nav-list"></ul> </nav> <footer class="site-footer"> This site uses <a href="https://github.com/just-the-docs/just-the-docs">Just the Docs</a>, a documentation theme for Jekyll. </footer> </div> <div class="main" id="top"> <div id="main-header" class="main-header"> <div class="search" role="search"> <div class="search-input-wrap"> <input type="text" id="search-input" class="search-input" tabindex="0" placeholder="Search Two Tutorials for Mojo using Pocket Flow" aria-label="Search Two Tutorials for Mojo using Pocket Flow" autocomplete="off"> <label for="search-input" class="search-label"><svg viewBox="0 0 24 24" class="search-icon"><use xlink:href="#svg-search"></use></svg></label> </div> <div id="search-results" class="search-results"></div> </div> <nav aria-label="Auxiliary" class="aux-nav"> <ul class="aux-nav-list"> <li class="aux-nav-list-item"> <a href="https://github.com/MrBesterTester/pf-understand" class="site-button" > View on GitHub </a> </li> </ul> </nav> </div> <div id="main-content-wrap" class="main-content-wrap"> <nav aria-label="Breadcrumb" class="breadcrumb-nav"> <ol class="breadcrumb-nav-list"> <li class="breadcrumb-nav-list-item"><a href="/modular_max/">My Tutorial for Modular's Max</a></li> <li class="breadcrumb-nav-list-item"><span>Chapter 3: LLM Pipeline Orchestrator</span></li> </ol> </nav> <div id="main-content" class="main-content"> <main> <h1 id="chapter-3-llm-pipeline-orchestrator-tokengeneratorpipeline"> <a href="#chapter-3-llm-pipeline-orchestrator-tokengeneratorpipeline" class="anchor-heading" aria-labelledby="chapter-3-llm-pipeline-orchestrator-tokengeneratorpipeline"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Chapter 3: LLM Pipeline Orchestrator (<code class="language-plaintext highlighter-rouge">TokenGeneratorPipeline</code>) </h1> <p>In <a href="02_serving_api_layer__fastapi_app___routers__.md">Chapter 2: Serving API Layer (FastAPI App &amp; Routers)</a>, we saw how <code class="language-plaintext highlighter-rouge">modular</code> receives requests from the outside world, like a receptionist directing calls. But once a request like “tell me a story about a brave knight” arrives, who actually takes that request, gets it ready for the language model, and makes sure the story comes back?</p> <p>That’s where the <strong><code class="language-plaintext highlighter-rouge">TokenGeneratorPipeline</code></strong> comes in! It’s the general manager for a specific language model.</p> <h2 id="what-problem-does-the-tokengeneratorpipeline-solve"> <a href="#what-problem-does-the-tokengeneratorpipeline-solve" class="anchor-heading" aria-labelledby="what-problem-does-the-tokengeneratorpipeline-solve"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> What Problem Does the <code class="language-plaintext highlighter-rouge">TokenGeneratorPipeline</code> Solve? </h2> <p>Imagine you’ve asked <code class="language-plaintext highlighter-rouge">modular</code> to write that story. The API Layer has accepted your request. Now what?</p> <ul> <li>Your request (“tell me a story…”) is in human language. The AI model doesn’t understand English directly; it understands “tokens” (like special code words or numbers).</li> <li>The AI model itself is a powerful but specialized piece of machinery (the <a href="04_model_worker_.md">Model Worker</a>, which we’ll cover later). It needs to be told exactly what to do and how.</li> <li>Once the model starts generating the story, it might produce it word by word (or token by token). Someone needs to collect these pieces and send them back to you, either as they come or all at once.</li> </ul> <p>The <code class="language-plaintext highlighter-rouge">TokenGeneratorPipeline</code> acts like an <strong>assembly line manager</strong> for this whole process. It takes the raw material (your prompt), ensures it’s correctly prepared, sends it to the machinery (the model worker), and then makes sure the finished product (the generated story) is delivered efficiently.</p> <p>It handles:</p> <ol> <li><strong>Preparation</strong>: Using a “tokenizer” to translate your human-language prompt into model-understandable tokens.</li> <li><strong>Coordination</strong>: Sending this prepared request to the correct <a href="04_model_worker_.md">Model Worker</a> via an <a href="07_enginequeue_.md">EngineQueue</a> (think of it as a high-tech conveyor belt).</li> <li><strong>Delivery</strong>: Streaming the model’s answer back token by token, or as a complete response.</li> </ol> <h2 id="meet-the-tokengeneratorpipeline"> <a href="#meet-the-tokengeneratorpipeline" class="anchor-heading" aria-labelledby="meet-the-tokengeneratorpipeline"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Meet the <code class="language-plaintext highlighter-rouge">TokenGeneratorPipeline</code> </h2> <p>The <code class="language-plaintext highlighter-rouge">TokenGeneratorPipeline</code> is the central orchestrator for getting text from a Large Language Model (LLM). Let’s look at its key collaborators:</p> <ul> <li><strong>Tokenizer (<code class="language-plaintext highlighter-rouge">PipelineTokenizer</code>)</strong>: This is like a specialized translator. It converts human-readable text (your prompt) into a sequence of numbers (tokens) that the LLM can understand. It also does the reverse: converts tokens from the LLM back into human-readable text.</li> <li><strong>EngineQueue</strong>: This is a communication channel. The <code class="language-plaintext highlighter-rouge">TokenGeneratorPipeline</code> places the tokenized request into this queue, and the <a href="04_model_worker_.md">Model Worker</a> picks it up from there. It’s also used to send generated tokens back from the worker.</li> <li><strong>Model Worker</strong>: This is the component that actually runs the LLM to generate new tokens based on the input. We’ll explore this in <a href="04_model_worker_.md">Chapter 4: Model Worker</a>.</li> </ul> <h3 id="how-its-set-up"> <a href="#how-its-set-up" class="anchor-heading" aria-labelledby="how-its-set-up"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> How It’s Set Up </h3> <p>In <a href="02_serving_api_layer__fastapi_app___routers__.md">Chapter 2: Serving API Layer (FastAPI App &amp; Routers)</a>, we saw that the main FastAPI <code class="language-plaintext highlighter-rouge">app</code> has a “lifespan” function. This is where the <code class="language-plaintext highlighter-rouge">TokenGeneratorPipeline</code> is typically created and made available to the API endpoint handlers.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Simplified from: src/max/serve/api_server.py
</span>
<span class="c1"># (inside the `lifespan` async context manager)
# ...
</span><span class="n">pipeline</span><span class="p">:</span> <span class="n">TokenGeneratorPipeline</span> <span class="o">=</span> <span class="n">TokenGeneratorPipeline</span><span class="p">(</span>
    <span class="n">model_name</span><span class="o">=</span><span class="s">"my-cool-model"</span><span class="p">,</span> <span class="c1"># Name of the model being served
</span>    <span class="n">tokenizer</span><span class="o">=</span><span class="n">my_tokenizer</span><span class="p">,</span>      <span class="c1"># The translator we talked about
</span>    <span class="n">engine_queue</span><span class="o">=</span><span class="n">engine_queue</span><span class="p">,</span>   <span class="c1"># The conveyor belt to the Model Worker
</span><span class="p">)</span>
<span class="n">app</span><span class="p">.</span><span class="n">state</span><span class="p">.</span><span class="n">pipeline</span> <span class="o">=</span> <span class="n">pipeline</span> <span class="c1"># Make it accessible to API endpoints
# ...
</span></code></pre></div></div> <p>This means that when an API endpoint (like <code class="language-plaintext highlighter-rouge">/v1/chat/completions</code>) needs to generate text, it can access this <code class="language-plaintext highlighter-rouge">pipeline</code> object.</p> <h3 id="receiving-a-request-and-generating-text"> <a href="#receiving-a-request-and-generating-text" class="anchor-heading" aria-labelledby="receiving-a-request-and-generating-text"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Receiving a Request and Generating Text </h3> <p>When an API endpoint (like <code class="language-plaintext highlighter-rouge">openai_create_chat_completion</code> from <code class="language-plaintext highlighter-rouge">src/max/serve/router/openai_routes.py</code>) receives a user’s request, it prepares a <code class="language-plaintext highlighter-rouge">TokenGeneratorRequest</code> object. This object is like a work order for the pipeline.</p> <p>Let’s look at what’s in a <code class="language-plaintext highlighter-rouge">TokenGeneratorRequest</code>:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Simplified from: src/max/pipelines/core/interfaces/text_generation.py
</span><span class="o">@</span><span class="n">dataclass</span><span class="p">(</span><span class="n">frozen</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">TokenGeneratorRequest</span><span class="p">:</span>
    <span class="nb">id</span><span class="p">:</span> <span class="nb">str</span>                <span class="c1"># A unique ID for this specific request
</span>    <span class="n">model_name</span><span class="p">:</span> <span class="nb">str</span>        <span class="c1"># Which model to use
</span>    <span class="n">messages</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">list</span><span class="p">[</span><span class="n">TokenGeneratorRequestMessage</span><span class="p">]]</span> <span class="o">=</span> <span class="bp">None</span> <span class="c1"># For chat prompts
</span>    <span class="n">prompt</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Sequence</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="bp">None</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span> <span class="c1"># For simple prompts
</span>    <span class="n">max_new_tokens</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span> <span class="c1"># How many tokens to generate at most
</span>    <span class="c1"># ... other fields like timestamp, tools, etc.
</span></code></pre></div></div> <p>This <code class="language-plaintext highlighter-rouge">TokenGeneratorRequest</code> contains all the information the pipeline needs.</p> <p>The API endpoint then calls a method on the <code class="language-plaintext highlighter-rouge">TokenGeneratorPipeline</code>, usually <code class="language-plaintext highlighter-rouge">next_token()</code> for streaming responses or <code class="language-plaintext highlighter-rouge">all_tokens()</code> for a complete response.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Simplified example of how an API route might use the pipeline
# (Inspired by src/max/serve/router/openai_routes.py)
</span>
<span class="c1"># async def openai_create_chat_completion(request: Request):
</span>    <span class="c1"># ... (code to parse user's HTTP request into `my_chat_request_data`) ...
</span>
    <span class="c1"># pipeline is taken from request.app.state.pipeline
</span>    <span class="c1"># pipeline: TokenGeneratorPipeline = request.app.state.pipeline
</span>
    <span class="c1"># 1. Create the work order (TokenGeneratorRequest)
</span>    <span class="n">token_gen_request</span> <span class="o">=</span> <span class="n">TokenGeneratorRequest</span><span class="p">(</span>
        <span class="nb">id</span><span class="o">=</span><span class="s">"unique_req_abc123"</span><span class="p">,</span>
        <span class="n">model_name</span><span class="o">=</span><span class="n">pipeline</span><span class="p">.</span><span class="n">model_name</span><span class="p">,</span>
        <span class="n">messages</span><span class="o">=</span><span class="p">[{</span><span class="s">"role"</span><span class="p">:</span> <span class="s">"user"</span><span class="p">,</span> <span class="s">"content"</span><span class="p">:</span> <span class="s">"Tell me a short story."</span><span class="p">}],</span>
        <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">50</span>
    <span class="p">)</span>

    <span class="c1"># 2. Ask the pipeline to generate text (streaming token by token)
</span>    <span class="c1"># response_stream = pipeline.next_token(token_gen_request)
</span>    
    <span class="c1"># async for generated_output_chunk in response_stream:
</span>    <span class="c1">#     # generated_output_chunk is a TokenGeneratorOutput
</span>    <span class="c1">#     print(generated_output_chunk.decoded_token, end="")
</span>    <span class="c1">#     # In a real server, this would be sent back to the user over HTTP
</span>    <span class="c1"># Output (example, token by token):
</span>    <span class="c1"># "Once "
</span>    <span class="c1"># "upon "
</span>    <span class="c1"># "a "
</span>    <span class="c1"># "time"
</span>    <span class="c1"># ", "
</span>    <span class="c1"># "in "
</span>    <span class="c1"># "a "
</span>    <span class="c1"># # ... and so on
</span></code></pre></div></div> <p>The <code class="language-plaintext highlighter-rouge">pipeline.next_token()</code> method returns an “async generator.” This means it yields <code class="language-plaintext highlighter-rouge">TokenGeneratorOutput</code> objects one by one as the model produces them.</p> <p>Let’s see what a <code class="language-plaintext highlighter-rouge">TokenGeneratorOutput</code> looks like:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Simplified from: src/max/serve/pipelines/llm.py
</span><span class="o">@</span><span class="n">dataclass</span><span class="p">(</span><span class="n">frozen</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="k">class</span> <span class="nc">TokenGeneratorOutput</span><span class="p">:</span>
    <span class="n">decoded_token</span><span class="p">:</span> <span class="nb">str</span> <span class="c1"># The actual text part generated in this step
</span>    <span class="c1"># ... other fields like log probabilities, token counts, etc.
</span></code></pre></div></div> <p>Each <code class="language-plaintext highlighter-rouge">TokenGeneratorOutput</code> contains a piece of the generated text (<code class="language-plaintext highlighter-rouge">decoded_token</code>).</p> <h2 id="under-the-hood-the-journey-of-a-prompt"> <a href="#under-the-hood-the-journey-of-a-prompt" class="anchor-heading" aria-labelledby="under-the-hood-the-journey-of-a-prompt"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Under the Hood: The Journey of a Prompt </h2> <p>So, what happens inside the <code class="language-plaintext highlighter-rouge">TokenGeneratorPipeline</code> when its <code class="language-plaintext highlighter-rouge">next_token()</code> method is called?</p> <ol> <li><strong>Context Creation</strong>: The pipeline first uses its <code class="language-plaintext highlighter-rouge">tokenizer</code> to create a “context” from the <code class="language-plaintext highlighter-rouge">TokenGeneratorRequest</code>. This involves converting the prompt messages or text into the initial set of tokens the model needs. This step uses the <code class="language-plaintext highlighter-rouge">tokenizer.new_context()</code> method. <ul> <li><em>Analogy</em>: The assembly line manager gives the raw materials (your prompt) to a specialist (tokenizer) who prepares them (converts to tokens) for the main machine.</li> </ul> </li> <li><strong>Sending to <code class="language-plaintext highlighter-rouge">EngineQueue</code></strong>: The request (now in a model-understandable format, often part of the “context”) is put into the <code class="language-plaintext highlighter-rouge">EngineQueue</code>. This queue acts as a buffer and a communication channel to the <a href="04_model_worker_.md">Model Worker</a>. <ul> <li><em>Analogy</em>: The prepared materials are placed on a conveyor belt (<code class="language-plaintext highlighter-rouge">EngineQueue</code>) leading to the main processing machinery (<a href="04_model_worker_.md">Model Worker</a>).</li> </ul> </li> <li><strong>Model Processing (by Model Worker)</strong>: The <a href="04_model_worker_.md">Model Worker</a> (running in a separate process or thread) picks up the request from the <code class="language-plaintext highlighter-rouge">EngineQueue</code>. It runs the LLM to generate the next token(s). <ul> <li><em>Analogy</em>: The machinery processes the materials and produces a part of the finished product.</li> </ul> </li> <li><strong>Receiving from <code class="language-plaintext highlighter-rouge">EngineQueue</code></strong>: The generated token(s) are sent back by the <a href="04_model_worker_.md">Model Worker</a> through the <code class="language-plaintext highlighter-rouge">EngineQueue</code> to the <code class="language-plaintext highlighter-rouge">TokenGeneratorPipeline</code>. <ul> <li><em>Analogy</em>: The finished part comes back on another conveyor belt.</li> </ul> </li> <li><strong>Decoding</strong>: The <code class="language-plaintext highlighter-rouge">TokenGeneratorPipeline</code> receives these raw generated tokens. It uses its <code class="language-plaintext highlighter-rouge">tokenizer</code> again, this time to <code class="language-plaintext highlighter-rouge">decode()</code> the tokens back into human-readable text. <ul> <li><em>Analogy</em>: The manager takes the finished part and asks the translator (tokenizer) to convert it back into something the customer understands.</li> </ul> </li> <li> <p><strong>Yielding Output</strong>: The decoded text (and other info) is packaged into a <code class="language-plaintext highlighter-rouge">TokenGeneratorOutput</code> object and “yielded” back to the caller (the API endpoint). If it’s a streaming request, this happens for each new piece of text generated.</p> </li> <li><strong>Repetition</strong>: Steps 3-6 repeat until the model finishes generating (e.g., it reaches <code class="language-plaintext highlighter-rouge">max_new_tokens</code>, generates an “end-of-sequence” token, or a stop phrase is detected).</li> </ol> <p>Here’s a simplified diagram of this flow:</p><pre><code class="language-mermaid">sequenceDiagram
    participant APIEndpoint as API Endpoint Handler
    participant TGPipeline as TokenGeneratorPipeline
    participant Tokenizer as PipelineTokenizer
    participant EngQueue as EngineQueue
    participant MWorker as Model Worker

    APIEndpoint-&gt;&gt;TGPipeline: Call next_token(request)
    TGPipeline-&gt;&gt;Tokenizer: new_context(request) [Encode Prompt]
    Tokenizer--&gt;&gt;TGPipeline: Model-ready context
    TGPipeline-&gt;&gt;EngQueue: stream(request.id, context)
    EngQueue-&gt;&gt;MWorker: Forward request data
    loop Generation Loop
        MWorker-&gt;&gt;MWorker: Generate next token(s)
        MWorker-&gt;&gt;EngQueue: Send generated token(s) back
        EngQueue--&gt;&gt;TGPipeline: Receive generated token(s)
        TGPipeline-&gt;&gt;Tokenizer: decode(token(s))
        Tokenizer--&gt;&gt;TGPipeline: Decoded text chunk
        TGPipeline--&gt;&gt;APIEndpoint: Yield TokenGeneratorOutput (chunk)
    end
    APIEndpoint--&gt;&gt;User: Streams text chunks
</code></pre><h3 id="diving-into-the-code-srcmaxservepipelinesllmpy"> <a href="#diving-into-the-code-srcmaxservepipelinesllmpy" class="anchor-heading" aria-labelledby="diving-into-the-code-srcmaxservepipelinesllmpy"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Diving into the Code (<code class="language-plaintext highlighter-rouge">src/max/serve/pipelines/llm.py</code>) </h3> <p>Let’s look at a very simplified structure of the <code class="language-plaintext highlighter-rouge">TokenGeneratorPipeline</code> and its <code class="language-plaintext highlighter-rouge">next_token</code> method:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Simplified from: src/max/serve/pipelines/llm.py
</span>
<span class="k">class</span> <span class="nc">TokenGeneratorPipeline</span><span class="p">(</span><span class="n">Generic</span><span class="p">[</span><span class="n">TokenGeneratorContext</span><span class="p">]):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">model_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">tokenizer</span><span class="p">:</span> <span class="n">PipelineTokenizer</span><span class="p">,</span> <span class="c1"># Our "translator"
</span>        <span class="n">engine_queue</span><span class="p">:</span> <span class="n">EngineQueue</span><span class="p">,</span>   <span class="c1"># Our "conveyor belt"
</span>    <span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">model_name</span> <span class="o">=</span> <span class="n">model_name</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">tokenizer</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">engine_queue</span> <span class="o">=</span> <span class="n">engine_queue</span>
        <span class="c1"># ... other initializations ...
</span>
    <span class="k">async</span> <span class="k">def</span> <span class="nf">next_token</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">request</span><span class="p">:</span> <span class="n">TokenGeneratorRequest</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">AsyncGenerator</span><span class="p">[</span><span class="n">TokenGeneratorOutput</span><span class="p">,</span> <span class="bp">None</span><span class="p">]:</span>
        <span class="c1"># 1. Create context using the tokenizer
</span>        <span class="n">context</span> <span class="o">=</span> <span class="k">await</span> <span class="bp">self</span><span class="p">.</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">new_context</span><span class="p">(</span><span class="n">request</span><span class="p">)</span>

        <span class="c1"># 2. Stream responses from the engine_queue (which talks to Model Worker)
</span>        <span class="k">async</span> <span class="k">for</span> <span class="n">response_from_worker</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">engine_queue</span><span class="p">.</span><span class="n">stream</span><span class="p">(</span>
            <span class="n">request</span><span class="p">.</span><span class="nb">id</span><span class="p">,</span> <span class="n">context</span>
        <span class="p">):</span>
            <span class="c1"># `response_from_worker` contains raw tokens from the model
</span>
            <span class="c1"># 5. Decode the raw token from worker into text
</span>            <span class="n">decoded_text_chunk</span> <span class="o">=</span> <span class="k">await</span> <span class="bp">self</span><span class="p">.</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">decode</span><span class="p">(</span>
                <span class="n">context</span><span class="p">,</span>
                <span class="n">response_from_worker</span><span class="p">.</span><span class="n">next_token</span><span class="p">,</span> <span class="c1"># The raw token
</span>                <span class="c1"># ... other args like skip_special_tokens ...
</span>            <span class="p">)</span>

            <span class="c1"># 6. Yield the human-readable output
</span>            <span class="k">yield</span> <span class="n">TokenGeneratorOutput</span><span class="p">(</span><span class="n">decoded_token</span><span class="o">=</span><span class="n">decoded_text_chunk</span><span class="p">)</span>
        <span class="c1"># ... (error handling and cleanup would be here) ...
</span></code></pre></div></div> <p>In this snippet:</p> <ul> <li>The <code class="language-plaintext highlighter-rouge">__init__</code> method shows how the pipeline is set up with its crucial components: the <code class="language-plaintext highlighter-rouge">tokenizer</code> and the <code class="language-plaintext highlighter-rouge">engine_queue</code>.</li> <li>The <code class="language-plaintext highlighter-rouge">next_token</code> method: <ul> <li>Calls <code class="language-plaintext highlighter-rouge">self.tokenizer.new_context()</code> to prepare the initial input for the model.</li> <li>Uses <code class="language-plaintext highlighter-rouge">self.engine_queue.stream()</code> to send the request to the <a href="04_model_worker_.md">Model Worker</a> (via the queue) and get back a stream of raw responses.</li> <li>For each raw response from the worker, it calls <code class="language-plaintext highlighter-rouge">self.tokenizer.decode()</code> to convert the model’s output token back into text.</li> <li>It then <code class="language-plaintext highlighter-rouge">yield</code>s a <code class="language-plaintext highlighter-rouge">TokenGeneratorOutput</code> containing this piece of decoded text.</li> </ul> </li> </ul> <p>The <code class="language-plaintext highlighter-rouge">PipelineTokenizer</code> itself is an interface (defined in <code class="language-plaintext highlighter-rouge">src/max/pipelines/core/interfaces/text_generation.py</code>). A concrete implementation of this tokenizer would know the specific vocabulary and rules for the particular LLM being used.</p> <p>Key methods of <code class="language-plaintext highlighter-rouge">PipelineTokenizer</code>:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Simplified from: src/max/pipelines/core/interfaces/text_generation.py
</span>
<span class="k">class</span> <span class="nc">PipelineTokenizer</span><span class="p">(</span><span class="n">Protocol</span><span class="p">):</span>
    <span class="c1"># ... (properties like eos token) ...
</span>
    <span class="k">async</span> <span class="k">def</span> <span class="nf">new_context</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">request</span><span class="p">:</span> <span class="n">TokenGeneratorRequest</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">TokenGeneratorContext</span><span class="p">:</span>
        <span class="c1"># Prepares the initial state for the model from the user's request.
</span>        <span class="c1"># This often involves encoding the prompt messages.
</span>        <span class="c1"># ...
</span>        <span class="k">pass</span>

    <span class="k">async</span> <span class="k">def</span> <span class="nf">encode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">prompt</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="p">...)</span> <span class="o">-&gt;</span> <span class="n">TokenizerEncoded</span><span class="p">:</span>
        <span class="c1"># Converts a string prompt into a sequence of token IDs.
</span>        <span class="c1"># ...
</span>        <span class="k">pass</span>

    <span class="k">async</span> <span class="k">def</span> <span class="nf">decode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">context</span><span class="p">:</span> <span class="n">TokenGeneratorContext</span><span class="p">,</span> <span class="n">encoded_tokens</span><span class="p">,</span> <span class="p">...)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="c1"># Converts a sequence of token IDs back into a string.
</span>        <span class="c1"># ...
</span>        <span class="k">pass</span>
</code></pre></div></div> <p>The <code class="language-plaintext highlighter-rouge">TokenGeneratorPipeline</code> acts as the skilled manager ensuring that your request smoothly goes through all these steps, from human language to model tokens, through the model, back to tokens, and finally back to human language for you to read.</p> <h2 id="conclusion"> <a href="#conclusion" class="anchor-heading" aria-labelledby="conclusion"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Conclusion </h2> <p>The <code class="language-plaintext highlighter-rouge">TokenGeneratorPipeline</code> is the heart of request processing in <code class="language-plaintext highlighter-rouge">modular</code> for language models. It’s the “general manager” that:</p> <ul> <li>Takes a <code class="language-plaintext highlighter-rouge">TokenGeneratorRequest</code> (the work order).</li> <li>Uses a <code class="language-plaintext highlighter-rouge">PipelineTokenizer</code> (the translator) to convert human language to model-understandable tokens and back.</li> <li>Communicates with the <a href="04_model_worker_.md">Model Worker</a> (the machinery) through an <code class="language-plaintext highlighter-rouge">EngineQueue</code> (the conveyor belt).</li> <li>Delivers the generated text, often streaming it token by token as <code class="language-plaintext highlighter-rouge">TokenGeneratorOutput</code>.</li> </ul> <p>It efficiently orchestrates the entire journey of your prompt to get you the response you need from the LLM.</p> <p>Now that we understand how requests are managed and prepared, what about the component that <em>actually</em> runs the AI model? In the next chapter, we’ll delve into the <a href="04_model_worker_.md">Model Worker</a>.</p><hr /> <p>Generated by <a href="https://github.com/The-Pocket/Tutorial-Codebase-Knowledge">AI Codebase Knowledge Builder</a></p> </main> </div> </div> <div class="search-overlay"></div> </div> <script type="module"> import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.6.0/dist/mermaid.esm.min.mjs'; var config = {} ; mermaid.initialize(config); mermaid.run({ querySelector: '.language-mermaid', }); </script> </body> </html>
