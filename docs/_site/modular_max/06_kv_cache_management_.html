<h1 id="chapter-6-kv-cache-management">Chapter 6: KV Cache Management</h1>

<p>In <a href="05_scheduler___tokengenerationscheduler____embeddingsscheduler___.md">Chapter 5: Scheduler (<code class="language-plaintext highlighter-rouge">TokenGenerationScheduler</code>, <code class="language-plaintext highlighter-rouge">EmbeddingsScheduler</code>)</a>, we saw how the Scheduler intelligently batches requests to keep our AI model busy. But when an LLM generates text, especially long sequences, how does it “remember” what it said at the beginning of a sentence or paragraph to make sense of what comes next? If it had to re-read and re-calculate everything from scratch for every new word, it would be incredibly slow!</p>

<p>This is where <strong>KV Cache Management</strong> comes to the rescue. It’s like giving our LLM a super-fast, very smart scratchpad.</p>

<h2 id="what-problem-does-kv-cache-management-solve">What Problem Does KV Cache Management Solve?</h2>

<p>Imagine you’re asking an LLM to write a story: “Once upon a time, in a land far, far away, lived a brave…”</p>

<p>When the LLM generates “brave,” it needs to remember “Once upon a time, in a land far, far away, lived a” to make sure “brave” fits the context. This “memory” involves complex calculations called “attention” (specifically, Key and Value tensors, or KV for short).</p>

<p>If, for the <em>next</em> word (e.g., “knight”), the LLM had to re-calculate the attention for “Once upon a time… lived a brave” all over again, it would be a huge waste of time and energy.</p>

<p><strong>KV Cache Management</strong> solves this by:</p>
<ol>
  <li><strong>Storing</strong> these intermediate attention calculations (the Keys and Values) from previously processed tokens in a special memory area called the <strong>KV Cache</strong>.</li>
  <li><strong>Reusing</strong> these stored calculations when generating subsequent tokens.</li>
</ol>

<p>This dramatically speeds up text generation because the model only needs to compute attention for the newest token, not the entire sequence again and again. It’s like a high-speed “thought buffer” for the LLM.</p>

<p>The <code class="language-plaintext highlighter-rouge">KVCacheManager</code> is the component in <code class="language-plaintext highlighter-rouge">modular</code> responsible for handling this critical memory.</p>

<h2 id="key-concepts-the-llms-smart-scratchpad">Key Concepts: The LLM’s Smart Scratchpad</h2>

<p>Let’s break down how this “smart scratchpad” works, especially with advanced techniques like <strong>Paged Attention</strong>:</p>

<h3 id="1-the-kv-cache-whats-stored">1. The KV Cache: What’s Stored?</h3>

<p>When the LLM processes a sequence of tokens (like your prompt “Tell me a story”), for each token, it computes Key (K) and Value (V) tensors in its attention layers. These K and V tensors represent the “importance” and “content” of previous tokens.</p>
<ul>
  <li><strong>Keys (K)</strong>: Help the model decide which past tokens are relevant to the current token.</li>
  <li><strong>Values (V)</strong>: Provide the information from those relevant past tokens.</li>
</ul>

<p>The KV Cache stores these K and V tensors for all processed tokens in a sequence. So, when generating the 100th word, the K and V tensors for the first 99 words are already in the cache, ready to be used.</p>

<h3 id="2-paged-attention-virtual-memory-for-the-gpu">2. Paged Attention: Virtual Memory for the GPU</h3>

<p>Storing the KV Cache for many long sequences can take up a lot of GPU memory. If managed poorly, this can lead to “out of memory” errors or limit how many requests we can process at once.</p>

<p><strong>Paged Attention</strong> is a clever technique, similar to how virtual memory works in your computer’s operating system, but for the GPU’s KV Cache.</p>
<ul>
  <li><strong>Analogy</strong>: Imagine your LLM’s scratchpad isn’t one long scroll of paper, but a binder full of fixed-size pages.</li>
  <li><strong>Blocks/Pages</strong>: The KV Cache is divided into fixed-size chunks called “blocks” or “pages.” Each block can store the KV tensors for a certain number of tokens (e.g., 128 tokens).</li>
  <li><strong>Flexibility</strong>: Instead of needing one large contiguous memory block for each request’s entire KV history, the system can allocate these smaller, non-contiguous pages as needed.</li>
</ul>

<p>This provides several benefits:</p>
<ul>
  <li><strong>Reduced Fragmentation</strong>: It’s easier to find and allocate memory for new requests because we’re looking for small pages, not one huge chunk.</li>
  <li><strong>Higher Concurrency</strong>: More requests can share the GPU memory efficiently.</li>
  <li><strong>Prefix Caching</strong>: If multiple requests start with the same prompt (e.g., a system message), their initial KV Cache pages can be shared, saving memory and computation. This is like having pre-written common introductions in our page binder.</li>
</ul>

<h3 id="3-the-kvcachemanager-and-blockmanager">3. The <code class="language-plaintext highlighter-rouge">KVCacheManager</code> and <code class="language-plaintext highlighter-rouge">BlockManager</code></h3>

<ul>
  <li><strong><code class="language-plaintext highlighter-rouge">KVCacheManager</code></strong>: This is the main abstraction (an abstract base class defined in <code class="language-plaintext highlighter-rouge">src/max/nn/kv_cache/manager.py</code>). It defines the interface for managing the KV Cache. A key implementation is the <code class="language-plaintext highlighter-rouge">PagedKVCacheManager</code> (in <code class="language-plaintext highlighter-rouge">src/max/nn/kv_cache/paged_cache/paged_cache.py</code>), which uses Paged Attention.</li>
  <li><strong><code class="language-plaintext highlighter-rouge">BlockManager</code></strong>: Used by <code class="language-plaintext highlighter-rouge">PagedKVCacheManager</code>, this component (in <code class="language-plaintext highlighter-rouge">src/max/nn/kv_cache/paged_cache/block_manager.py</code>) is the low-level accountant for the pages. It keeps track of:
    <ul>
      <li>Which pages are free.</li>
      <li>Which pages are assigned to which requests.</li>
      <li>Handles allocation, deallocation, and potentially even copying pages (e.g., for prefix caching or swapping to CPU memory if GPU memory is full).</li>
    </ul>
  </li>
</ul>

<h3 id="4-communicating-with-the-model-kvcacheinputs">4. Communicating with the Model: <code class="language-plaintext highlighter-rouge">KVCacheInputs</code></h3>

<p>When the model needs to perform an attention calculation, the <code class="language-plaintext highlighter-rouge">KVCacheManager</code> provides it with <code class="language-plaintext highlighter-rouge">KVCacheInputs</code>. These are special data structures (like <code class="language-plaintext highlighter-rouge">RaggedKVCacheInputs</code> or <code class="language-plaintext highlighter-rouge">PaddedKVCacheInputs</code>) that tell the model:</p>
<ul>
  <li>Where the actual KV cache data is stored (e.g., a pointer to the GPU memory holding all blocks).</li>
  <li>A “lookup table” or similar mechanism that maps logical token positions in a request to the specific physical blocks in the cache.</li>
  <li>The current lengths of the cached sequences.</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Simplified dataclass definitions from src/max/nn/kv_cache/manager.py
</span>
<span class="o">@</span><span class="n">dataclass</span>
<span class="k">class</span> <span class="nc">KVCacheInputs</span><span class="p">:</span> <span class="c1"># Base class
</span>    <span class="k">pass</span>

<span class="o">@</span><span class="n">dataclass</span>
<span class="k">class</span> <span class="nc">RaggedKVCacheInputs</span><span class="p">(</span><span class="n">KVCacheInputs</span><span class="p">):</span>
    <span class="c1"># For Paged Attention, these tell the model how to find its "pages"
</span>    <span class="n">blocks</span><span class="p">:</span> <span class="n">Tensor</span>          <span class="c1"># The big tensor on GPU holding all cache blocks
</span>    <span class="n">cache_lengths</span><span class="p">:</span> <span class="n">Tensor</span>   <span class="c1"># How many tokens are already cached for each request
</span>    <span class="n">lookup_table</span><span class="p">:</span> <span class="n">Tensor</span>    <span class="c1"># Maps request tokens to physical block locations
</span>    <span class="n">max_lengths</span><span class="p">:</span> <span class="n">Tensor</span>     <span class="c1"># Helps manage generation limits
</span></code></pre></div></div>
<p>This <code class="language-plaintext highlighter-rouge">RaggedKVCacheInputs</code> essentially gives the model a map to navigate its “paged scratchpad.”</p>

<h2 id="how-the-scheduler-uses-the-kv-cache-manager">How the Scheduler Uses the KV Cache Manager</h2>

<p>The <a href="05_scheduler___tokengenerationscheduler____embeddingsscheduler___.md">Scheduler (<code class="language-plaintext highlighter-rouge">TokenGenerationScheduler</code>, <code class="language-plaintext highlighter-rouge">EmbeddingsScheduler</code>)</a> (Chapter 5), which lives inside the <a href="04_model_worker_.md">Model Worker</a> (Chapter 4), heavily relies on the <code class="language-plaintext highlighter-rouge">KVCacheManager</code> (specifically, an instance like <code class="language-plaintext highlighter-rouge">PagedKVCacheManager</code>) to manage memory for the requests it’s processing.</p>

<p>Here’s a simplified flow:</p>

<ol>
  <li><strong>New Request Arrives</strong>:
    <ul>
      <li>The Scheduler gets a new request. Before it can be processed, it needs space in the KV Cache.</li>
      <li>Scheduler: “Hey <code class="language-plaintext highlighter-rouge">KVCacheManager</code>, I have a new request <code class="language-plaintext highlighter-rouge">req_A</code>. Can you reserve a spot for it?”</li>
      <li><code class="language-plaintext highlighter-rouge">KVCacheManager.claim(1)</code>: The manager reserves a logical “slot” for this request. Let’s say it gets slot ID <code class="language-plaintext highlighter-rouge">0</code>.</li>
      <li>Scheduler (to itself): “Okay, <code class="language-plaintext highlighter-rouge">req_A</code> is now <code class="language-plaintext highlighter-rouge">seq_id=0</code> in the cache.”</li>
    </ul>
  </li>
  <li><strong>Preparing the First Batch (Context Encoding)</strong>:
    <ul>
      <li>The Scheduler wants to process the initial prompt for <code class="language-plaintext highlighter-rouge">req_A</code>.</li>
      <li>Scheduler: “Hey <code class="language-plaintext highlighter-rouge">KVCacheManager</code> (<code class="language-plaintext highlighter-rouge">paged_manager</code> in <code class="language-plaintext highlighter-rouge">TokenGenerationScheduler</code>), prepare the cache for <code class="language-plaintext highlighter-rouge">req_A</code> (now <code class="language-plaintext highlighter-rouge">ctx_A</code> which knows its <code class="language-plaintext highlighter-rouge">seq_id=0</code>) and tell me if it’s possible.”</li>
      <li><code class="language-plaintext highlighter-rouge">PagedKVCacheManager.prefetch(ctx_A)</code>:
        <ul>
          <li>The internal <code class="language-plaintext highlighter-rouge">BlockManager</code> looks for free pages on the GPU.</li>
          <li>It might reuse pages if <code class="language-plaintext highlighter-rouge">req_A</code>’s prompt has a common prefix already cached (prefix caching).</li>
          <li>It allocates the necessary pages for <code class="language-plaintext highlighter-rouge">req_A</code>’s prompt.</li>
          <li>If successful, it returns <code class="language-plaintext highlighter-rouge">True</code>. If not (e.g., no GPU memory), it returns <code class="language-plaintext highlighter-rouge">False</code>, and the Scheduler might have to wait or evict something.</li>
        </ul>
      </li>
      <li>This <code class="language-plaintext highlighter-rouge">prefetch</code> step ensures that when <code class="language-plaintext highlighter-rouge">fetch</code> is called later, the memory is ready.</li>
    </ul>
  </li>
  <li><strong>Running the Model (Getting Cache Inputs)</strong>:
    <ul>
      <li>Scheduler: “Okay, <code class="language-plaintext highlighter-rouge">KVCacheManager</code>, give me the actual cache input tensors for the current batch (which includes <code class="language-plaintext highlighter-rouge">req_A</code>).”</li>
      <li><code class="language-plaintext highlighter-rouge">PagedKVCacheManager.fetch([ctx_A, ...])</code>:
        <ul>
          <li>This constructs <code class="language-plaintext highlighter-rouge">RaggedKVCacheInputs</code> (or similar). It populates the <code class="language-plaintext highlighter-rouge">lookup_table</code> that tells the model which physical pages correspond to <code class="language-plaintext highlighter-rouge">req_A</code>’s tokens.</li>
          <li>These inputs are passed to the AI model. The model uses them to read from and write to the correct pages in the KV Cache during attention.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>After Model Step (Committing to Cache)</strong>:
    <ul>
      <li>The model generates a new token for <code class="language-plaintext highlighter-rouge">req_A</code>.</li>
      <li>Scheduler: “Hey <code class="language-plaintext highlighter-rouge">KVCacheManager</code>, <code class="language-plaintext highlighter-rouge">req_A</code> has a new token. Update its state in the cache.”</li>
      <li><code class="language-plaintext highlighter-rouge">PagedKVCacheManager.step([ctx_A, ...])</code>:
        <ul>
          <li>The internal <code class="language-plaintext highlighter-rouge">BlockManager</code> “commits” the newly generated token’s KV data. This might involve marking a page as fully used and potentially making it available for future prefix caching.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>Request Finishes</strong>:
    <ul>
      <li><code class="language-plaintext highlighter-rouge">req_A</code> has finished generating all its tokens.</li>
      <li>Scheduler: “<code class="language-plaintext highlighter-rouge">KVCacheManager</code>, <code class="language-plaintext highlighter-rouge">req_A</code> (identified by <code class="language-plaintext highlighter-rouge">seq_id=0</code>) is done. You can free its memory.”</li>
      <li><code class="language-plaintext highlighter-rouge">PagedKVCacheManager.release(seq_id=0)</code>: The <code class="language-plaintext highlighter-rouge">BlockManager</code> returns all pages used by <code class="language-plaintext highlighter-rouge">req_A</code> to the free pool.</li>
    </ul>
  </li>
</ol>

<p>This interaction ensures that GPU memory for the KV Cache is used efficiently, allowing many requests to be processed.</p>

<h2 id="under-the-hood-a-closer-look-at-pagedkvcachemanager">Under the Hood: A Closer Look at <code class="language-plaintext highlighter-rouge">PagedKVCacheManager</code></h2>

<p>Let’s peek at the internal workings.</p>

<h3 id="high-level-flow-with-paged-attention">High-Level Flow with Paged Attention:</h3>

<pre><code class="language-mermaid">sequenceDiagram
    participant Sched as Scheduler
    participant PKVCM as PagedKVCacheManager
    participant BM as BlockManager (inside PKVCM)
    participant BP as BlockPool (inside BM)
    participant Model as AI Model

    Note over Sched: New request `req_A` arrives
    Sched-&gt;&gt;PKVCM: claim(1) -&gt; returns slot_id (e.g., 0)
    Note over Sched: Scheduler associates `req_A` with slot_id 0

    Note over Sched: Prepare for prompt processing
    Sched-&gt;&gt;PKVCM: prefetch(req_A_context)
    PKVCM-&gt;&gt;BM: prefetch(req_A_context)
    BM-&gt;&gt;BM: Check prefix cache (reuse blocks?)
    BM-&gt;&gt;BP: Allocate new blocks for req_A's prompt
    BP--&gt;&gt;BM: Allocated block IDs
    BM--&gt;&gt;PKVCM: Success/Failure
    PKVCM--&gt;&gt;Sched: Prefetch successful

    Note over Sched: Time to run model
    Sched-&gt;&gt;PKVCM: fetch(batch_with_req_A)
    PKVCM-&gt;&gt;PKVCM: Create lookup_table for batch
    PKVCM--&gt;&gt;Sched: KVCacheInputs (with lookup_table)
    Sched-&gt;&gt;Model: Execute with KVCacheInputs
    Model-&gt;&gt;Model: Reads/Writes KV Cache using lookup_table
    Model--&gt;&gt;Sched: Generated tokens

    Note over Sched: Commit new tokens
    Sched-&gt;&gt;PKVCM: step(batch_with_req_A)
    PKVCM-&gt;&gt;BM: step(req_A_context)
    BM-&gt;&gt;BM: Commit blocks, update prefix cache info

    Note over Sched: Request `req_A` completes
    Sched-&gt;&gt;PKVCM: release(slot_id=0)
    PKVCM-&gt;&gt;BM: release(slot_id=0)
    BM-&gt;&gt;BP: Free blocks used by req_A
</code></pre>

<h3 id="code-snippets">Code Snippets:</h3>

<p><strong>1. <code class="language-plaintext highlighter-rouge">KVCacheManager</code> Abstract Base Class (<code class="language-plaintext highlighter-rouge">src/max/nn/kv_cache/manager.py</code>)</strong>
This class defines the contract that all KV cache managers must follow.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Simplified from: src/max/nn/kv_cache/manager.py
</span><span class="k">class</span> <span class="nc">KVCacheManager</span><span class="p">(</span><span class="n">ABC</span><span class="p">,</span> <span class="n">Generic</span><span class="p">[</span><span class="n">T</span><span class="p">]):</span> <span class="c1"># T is KVCacheAwareContext
</span>    <span class="c1"># ... (constructor) ...
</span>
    <span class="o">@</span><span class="n">abstractmethod</span>
    <span class="k">def</span> <span class="nf">fetch</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">T</span><span class="p">],</span> <span class="n">num_steps</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="n">KVCacheInputs</span><span class="p">]:</span>
        <span class="c1"># Returns KVCacheInputs for the model to use
</span>        <span class="p">...</span>

    <span class="k">def</span> <span class="nf">claim</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>
        <span class="c1"># Claims 'n' logical slots for new sequences, returns their IDs
</span>        <span class="n">seq_ids</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
            <span class="n">seq_id</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">available</span><span class="p">.</span><span class="n">pop</span><span class="p">()</span> <span class="c1"># 'available' is a set of free slot IDs
</span>            <span class="bp">self</span><span class="p">.</span><span class="n">active</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">seq_id</span><span class="p">)</span>
            <span class="n">seq_ids</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">seq_id</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">seq_ids</span>

    <span class="k">def</span> <span class="nf">release</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">seq_id</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="c1"># Releases a slot ID, making it available again
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">active</span><span class="p">.</span><span class="n">remove</span><span class="p">(</span><span class="n">seq_id</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">available</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">seq_id</span><span class="p">)</span>
    
    <span class="o">@</span><span class="n">abstractmethod</span>
    <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">T</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="c1"># Commits new tokens into the cache after a model step
</span>        <span class="p">...</span>
</code></pre></div></div>
<p>This shows the key methods <code class="language-plaintext highlighter-rouge">fetch</code>, <code class="language-plaintext highlighter-rouge">claim</code>, and <code class="language-plaintext highlighter-rouge">release</code> that the Scheduler interacts with.</p>

<p><strong>2. <code class="language-plaintext highlighter-rouge">PagedKVCacheManager</code> Initialization (<code class="language-plaintext highlighter-rouge">src/max/nn/kv_cache/paged_cache/paged_cache.py</code>)</strong>
This is where the paged attention system is set up.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Simplified from: src/max/nn/kv_cache/paged_cache/paged_cache.py
</span><span class="k">class</span> <span class="nc">PagedKVCacheManager</span><span class="p">(</span><span class="n">KVCacheManager</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">:</span> <span class="n">KVCacheParams</span><span class="p">,</span> <span class="p">...,</span> <span class="n">page_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">128</span><span class="p">,</span> <span class="p">...):</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">(...)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">page_size</span> <span class="o">=</span> <span class="n">page_size</span>
        <span class="c1"># Calculate total_num_pages based on available GPU memory and page_size
</span>        <span class="c1"># ...
</span>        
        <span class="c1"># Allocate the main GPU tensor to hold all cache blocks (pages)
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">device_tensors</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span> <span class="c1"># One per GPU device
</span>        <span class="k">for</span> <span class="n">device</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">devices</span><span class="p">:</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">device_tensors</span><span class="p">.</span><span class="n">append</span><span class="p">(</span>
                <span class="n">Tensor</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">block_shape</span><span class="p">(),</span> <span class="p">...)</span> <span class="c1"># shape like [num_pages, 2, layers, page_size, heads, dim]
</span>            <span class="p">)</span>

        <span class="c1"># Initialize the BlockManager to manage these pages
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">block_manager</span> <span class="o">=</span> <span class="n">BlockManager</span><span class="p">(</span>
            <span class="n">total_num_blocks</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">total_num_pages</span><span class="p">,</span>
            <span class="n">block_size</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">page_size</span><span class="p">,</span>
            <span class="c1"># ... other args like block_copy_engine for prefix caching ...
</span>        <span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">prefetched_seq_ids</span><span class="p">:</span> <span class="nb">set</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
</code></pre></div></div>
<p>The <code class="language-plaintext highlighter-rouge">PagedKVCacheManager</code> allocates a large chunk of memory on the GPU (<code class="language-plaintext highlighter-rouge">self.device_tensors</code>) to hold all the blocks/pages. It then creates a <code class="language-plaintext highlighter-rouge">BlockManager</code> to manage these pages.</p>

<p><strong>3. <code class="language-plaintext highlighter-rouge">PagedKVCacheManager.prefetch</code> and <code class="language-plaintext highlighter-rouge">fetch</code></strong>
The <code class="language-plaintext highlighter-rouge">prefetch</code> method ensures blocks are available <em>before</em> <code class="language-plaintext highlighter-rouge">fetch</code> is called.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Simplified from: src/max/nn/kv_cache/paged_cache/paged_cache.py
</span>    <span class="o">@</span><span class="n">traced</span> <span class="c1"># For performance profiling
</span>    <span class="k">def</span> <span class="nf">prefetch</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">:</span> <span class="n">KVCacheAwareContext</span><span class="p">,</span> <span class="n">num_steps</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
        <span class="n">seq_id</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="n">cache_seq_id</span>
        <span class="c1"># Ask BlockManager to reuse/allocate blocks for this request
</span>        <span class="n">scheduled</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">block_manager</span><span class="p">.</span><span class="n">prefetch</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">num_steps</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">scheduled</span><span class="p">:</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">prefetched_seq_ids</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">seq_id</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">scheduled</span>

    <span class="o">@</span><span class="n">traced</span>
    <span class="k">def</span> <span class="nf">fetch</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">KVCacheAwareContext</span><span class="p">],</span> <span class="n">num_steps</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="n">RaggedKVCacheInputs</span><span class="p">]:</span>
        <span class="c1"># ... (Ensure prefetch was called for items in batch or call it now) ...
</span>        
        <span class="c1"># For each request in the batch:
</span>        <span class="c1">#   Get its allocated block IDs from self.block_manager.get_req_blocks(seq_id)
</span>        <span class="c1">#   Construct a lookup_table_np (numpy array) mapping [batch_idx, logical_page_idx] to physical_block_id
</span>        <span class="c1">#   Construct cache_lengths_np (how many tokens already cached)
</span>        <span class="c1"># ...
</span>        
        <span class="c1"># Convert numpy arrays to Tensors
</span>        <span class="c1"># lut_table_host = Tensor.from_numpy(lookup_table_np)
</span>        <span class="c1"># cache_lengths_host = Tensor.from_numpy(cache_lengths_np)
</span>        
        <span class="c1"># Create RaggedKVCacheInputs for each device
</span>        <span class="n">ret_list</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">device</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">devices</span><span class="p">):</span>
            <span class="n">ret_list</span><span class="p">.</span><span class="n">append</span><span class="p">(</span>
                <span class="n">RaggedKVCacheInputs</span><span class="p">(</span>
                    <span class="n">blocks</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">device_tensors</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="c1"># The big GPU tensor of blocks
</span>                    <span class="n">cache_lengths</span><span class="o">=</span><span class="n">cache_lengths_host</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">),</span>
                    <span class="n">lookup_table</span><span class="o">=</span><span class="n">lut_table_host</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">),</span>
                    <span class="n">max_lengths</span><span class="o">=</span><span class="n">max_lengths_host</span><span class="p">,</span> <span class="c1"># For generation limits
</span>                <span class="p">)</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="n">ret_list</span>
</code></pre></div></div>
<p><code class="language-plaintext highlighter-rouge">prefetch</code> relies on the <code class="language-plaintext highlighter-rouge">BlockManager</code> to do the heavy lifting of block allocation. <code class="language-plaintext highlighter-rouge">fetch</code> then assembles the <code class="language-plaintext highlighter-rouge">RaggedKVCacheInputs</code> that the model will use, including the crucial <code class="language-plaintext highlighter-rouge">lookup_table</code>.</p>

<p><strong>4. <code class="language-plaintext highlighter-rouge">BlockManager</code> in Action (<code class="language-plaintext highlighter-rouge">src/max/nn/kv_cache/paged_cache/block_manager.py</code>)</strong>
The <code class="language-plaintext highlighter-rouge">BlockManager</code> handles the actual page allocations and prefix caching.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Simplified from: src/max/nn/kv_cache/paged_cache/block_manager.py
</span><span class="k">class</span> <span class="nc">BlockManager</span><span class="p">(</span><span class="n">Generic</span><span class="p">[</span><span class="n">T</span><span class="p">]):</span> <span class="c1"># T is KVCacheAwareContext
</span>    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="p">...,</span> <span class="n">total_num_blocks</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">block_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="p">...):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">block_size</span> <span class="o">=</span> <span class="n">block_size</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">device_block_pool</span> <span class="o">=</span> <span class="n">BlockPool</span><span class="p">(</span><span class="n">total_num_blocks</span><span class="p">,</span> <span class="p">...)</span>
        <span class="c1"># self.host_block_pool (if swapping to CPU is enabled)
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">req_to_blocks</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">list</span><span class="p">[</span><span class="n">KVCacheBlock</span><span class="p">]]</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">list</span><span class="p">)</span> <span class="c1"># Tracks blocks per request
</span>
    <span class="k">def</span> <span class="nf">prefetch</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ctx</span><span class="p">:</span> <span class="n">T</span><span class="p">,</span> <span class="n">num_steps</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">reuse_blocks_from_prefix_cache</span><span class="p">(</span><span class="n">ctx</span><span class="p">)</span> <span class="c1"># Try to use existing cached blocks
</span>        <span class="k">try</span><span class="p">:</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">allocate_new_blocks</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">num_steps</span><span class="p">)</span> <span class="c1"># Allocate remaining needed blocks
</span>        <span class="k">except</span> <span class="nb">RuntimeError</span><span class="p">:</span> <span class="c1"># Not enough free blocks
</span>            <span class="k">return</span> <span class="bp">False</span>
        <span class="k">return</span> <span class="bp">True</span>

    <span class="k">def</span> <span class="nf">allocate_new_blocks</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ctx</span><span class="p">:</span> <span class="n">T</span><span class="p">,</span> <span class="n">num_steps</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">):</span>
        <span class="c1"># Calculate num_new_blocks needed for ctx.current_length + num_steps
</span>        <span class="c1"># ...
</span>        <span class="k">if</span> <span class="n">num_new_blocks</span> <span class="o">&gt;</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">device_block_pool</span><span class="p">.</span><span class="n">free_block_queue</span><span class="p">):</span>
            <span class="k">raise</span> <span class="nb">RuntimeError</span><span class="p">(</span><span class="s">"Not enough free blocks"</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_new_blocks</span><span class="p">):</span>
            <span class="c1"># Get a KVCacheBlock object from the pool
</span>            <span class="n">new_block</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">device_block_pool</span><span class="p">.</span><span class="n">alloc_block</span><span class="p">()</span> 
            <span class="bp">self</span><span class="p">.</span><span class="n">req_to_blocks</span><span class="p">[</span><span class="n">ctx</span><span class="p">.</span><span class="n">cache_seq_id</span><span class="p">].</span><span class="n">append</span><span class="p">(</span><span class="n">new_block</span><span class="p">)</span>
            
    <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ctx</span><span class="p">:</span> <span class="n">T</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span> <span class="c1"># Called after model generates tokens
</span>        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">enable_prefix_caching</span><span class="p">:</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">commit_to_prefix_cache</span><span class="p">(</span><span class="n">ctx</span><span class="p">)</span> <span class="c1"># Marks blocks as "committed"
</span>
    <span class="k">def</span> <span class="nf">release</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">seq_id</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">block</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">req_to_blocks</span><span class="p">[</span><span class="n">seq_id</span><span class="p">]:</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">device_block_pool</span><span class="p">.</span><span class="n">free_block</span><span class="p">(</span><span class="n">block</span><span class="p">)</span> <span class="c1"># Return block to the pool
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">req_to_blocks</span><span class="p">[</span><span class="n">seq_id</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
</code></pre></div></div>
<p>The <code class="language-plaintext highlighter-rouge">BlockManager</code> uses a <code class="language-plaintext highlighter-rouge">BlockPool</code> (which manages lists of free and used blocks) to perform its duties. <code class="language-plaintext highlighter-rouge">reuse_blocks_from_prefix_cache</code> and <code class="language-plaintext highlighter-rouge">commit_to_prefix_cache</code> are key for the prefix caching optimization.</p>

<h3 id="what-about-swapping-and-the-kvcacheagent">What About Swapping and the <code class="language-plaintext highlighter-rouge">KVCacheAgent</code>?</h3>

<ul>
  <li><strong>Swapping to Host Memory</strong>: If GPU memory for the KV Cache is extremely full, the <code class="language-plaintext highlighter-rouge">BlockManager</code> can be configured (via <code class="language-plaintext highlighter-rouge">total_num_host_blocks</code> and a <code class="language-plaintext highlighter-rouge">host_block_pool</code>) to copy some less-recently-used blocks from GPU to regular CPU RAM. If those blocks are needed again, they’re copied back. This is like your computer using its hard drive as extra RAM, but slower. (See <code class="language-plaintext highlighter-rouge">maybe_offload_gpu_block_to_host</code> in <code class="language-plaintext highlighter-rouge">BlockManager</code>).</li>
  <li><strong><code class="language-plaintext highlighter-rouge">KVCacheAgent</code></strong>: <code class="language-plaintext highlighter-rouge">modular</code> has an experimental feature called the <code class="language-plaintext highlighter-rouge">KVCacheAgent</code>. If enabled (via the <code class="language-plaintext highlighter-rouge">experimental_enable_kvcache_agent</code> setting in <code class="language-plaintext highlighter-rouge">src/max/entrypoints/cli/serve.py</code>), it starts a separate gRPC server (<code class="language-plaintext highlighter-rouge">src/max/serve/kvcache_agent/kvcache_agent.py</code> started by <code class="language-plaintext highlighter-rouge">src/max/serve/pipelines/kvcache_worker.py</code>).
    <ul>
      <li><strong>Purpose</strong>: This agent can expose the state of the KV Cache (e.g., which blocks are on GPU, which are on CPU, how full the cache is) to external monitoring systems or potentially even to other processes that might want to influence cache management.</li>
      <li><strong>How it works</strong>: The <code class="language-plaintext highlighter-rouge">BlockManager</code> (or other parts of the cache system) would send messages (like <code class="language-plaintext highlighter-rouge">KVCacheChangeMessage</code>) to a queue. The <code class="language-plaintext highlighter-rouge">KVCacheAgentServer</code> reads from this queue and updates its internal state, which subscribers can then query via gRPC. This is an advanced feature for observability and control.</li>
    </ul>
  </li>
</ul>

<h2 id="conclusion">Conclusion</h2>

<p>KV Cache Management is a cornerstone of efficient LLM inference. By storing and reusing intermediate attention computations (Keys and Values), it drastically reduces redundant calculations. Techniques like <strong>Paged Attention</strong>, managed by components like <code class="language-plaintext highlighter-rouge">PagedKVCacheManager</code> and <code class="language-plaintext highlighter-rouge">BlockManager</code>, further optimize this by:</p>
<ul>
  <li>Treating GPU memory like a set of manageable pages.</li>
  <li>Reducing memory fragmentation.</li>
  <li>Enabling higher concurrency and features like prefix caching.</li>
  <li>Potentially swapping less-used cache data to host memory.</li>
</ul>

<p>This “smart scratchpad” allows the LLM to “remember” its previous thought process quickly and efficiently, leading to faster generation of coherent text. The <a href="05_scheduler___tokengenerationscheduler____embeddingsscheduler___.md">Scheduler (<code class="language-plaintext highlighter-rouge">TokenGenerationScheduler</code>, <code class="language-plaintext highlighter-rouge">EmbeddingsScheduler</code>)</a> works hand-in-hand with the KV Cache Manager to orchestrate this memory usage.</p>

<p>But how do all these components—the Pipeline Orchestrator, the Model Worker, the Scheduler, and the KV Cache Manager—talk to each other, especially when they might be in different processes? That’s where our next topic, the <a href="07_enginequeue_.md">EngineQueue</a>, comes in!</p>

<hr />

<p>Generated by <a href="https://github.com/The-Pocket/Tutorial-Codebase-Knowledge">AI Codebase Knowledge Builder</a></p>
