<!DOCTYPE html> <html lang="en-US"> <head> <meta charset="UTF-8"> <meta http-equiv="X-UA-Compatible" content="IE=Edge"> <link rel="stylesheet" href="/assets/css/just-the-docs-default.css"> <script src="/assets/js/vendor/lunr.min.js"></script> <script src="/assets/js/just-the-docs.js"></script> <meta name="viewport" content="width=device-width, initial-scale=1"> <!-- Begin Jekyll SEO tag v2.8.0 --> <title>BaseDispatcher | Two Tutorials for Mojo using Pocket Flow</title> <meta name="generator" content="Jekyll v4.3.4" /> <meta property="og:title" content="BaseDispatcher" /> <meta name="author" content="Sam Kirk" /> <meta property="og:locale" content="en_US" /> <meta name="description" content="Documentation generated using AI to explain codebases" /> <meta property="og:description" content="Documentation generated using AI to explain codebases" /> <link rel="canonical" href="http://localhost:4000/Crawl4AI/10_basedispatcher.html" /> <meta property="og:url" content="http://localhost:4000/Crawl4AI/10_basedispatcher.html" /> <meta property="og:site_name" content="Two Tutorials for Mojo using Pocket Flow" /> <meta property="og:type" content="website" /> <meta name="twitter:card" content="summary" /> <meta property="twitter:title" content="BaseDispatcher" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"WebPage","author":{"@type":"Person","name":"Sam Kirk","url":"https://www.columbia.edu/~zh2408/"},"description":"Documentation generated using AI to explain codebases","headline":"BaseDispatcher","url":"http://localhost:4000/Crawl4AI/10_basedispatcher.html"}</script> <!-- End Jekyll SEO tag --> <!-- Add Mermaid support --> <script src="https://cdn.jsdelivr.net/npm/mermaid@11.6.0/dist/mermaid.min.js"></script> <script> document.addEventListener("DOMContentLoaded", function() { mermaid.initialize({ startOnLoad: true, theme: "default" }); // Process code blocks document.querySelectorAll('pre code.language-mermaid').forEach(function(block) { // Create a div with class 'mermaid' var mermaidDiv = document.createElement('div'); mermaidDiv.className = 'mermaid'; mermaidDiv.innerHTML = block.textContent; // Replace the parent pre with the mermaid div block.parentNode.parentNode.replaceChild(mermaidDiv, block.parentNode); console.log("Processed Mermaid block:", mermaidDiv.innerHTML.substring(0, 50) + "..."); }); console.log("Mermaid initialization complete. Version:", mermaid.version()); }); </script> </head> <body> <a class="skip-to-main" href="#main-content">Skip to main content</a> <svg xmlns="http://www.w3.org/2000/svg" class="d-none"> <symbol id="svg-link" viewBox="0 0 24 24"> <title>Link</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-link"> <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path> </svg> </symbol> <symbol id="svg-menu" viewBox="0 0 24 24"> <title>Menu</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu"> <line x1="3" y1="12" x2="21" y2="12"></line><line x1="3" y1="6" x2="21" y2="6"></line><line x1="3" y1="18" x2="21" y2="18"></line> </svg> </symbol> <symbol id="svg-arrow-right" viewBox="0 0 24 24"> <title>Expand</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-chevron-right"> <polyline points="9 18 15 12 9 6"></polyline> </svg> </symbol> <!-- Feather. MIT License: https://github.com/feathericons/feather/blob/master/LICENSE --> <symbol id="svg-external-link" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-external-link"> <title id="svg-external-link-title">(external link)</title> <path d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"></path><polyline points="15 3 21 3 21 9"></polyline><line x1="10" y1="14" x2="21" y2="3"></line> </symbol> <symbol id="svg-doc" viewBox="0 0 24 24"> <title>Document</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file"> <path d="M13 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V9z"></path><polyline points="13 2 13 9 20 9"></polyline> </svg> </symbol> <symbol id="svg-search" viewBox="0 0 24 24"> <title>Search</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-search"> <circle cx="11" cy="11" r="8"></circle><line x1="21" y1="21" x2="16.65" y2="16.65"></line> </svg> </symbol> <!-- Bootstrap Icons. MIT License: https://github.com/twbs/icons/blob/main/LICENSE.md --> <symbol id="svg-copy" viewBox="0 0 16 16"> <title>Copy</title> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard" viewBox="0 0 16 16"> <path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1h1a1 1 0 0 1 1 1V14a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V3.5a1 1 0 0 1 1-1h1v-1z"/> <path d="M9.5 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3zm-3-1A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3z"/> </svg> </symbol> <symbol id="svg-copied" viewBox="0 0 16 16"> <title>Copied</title> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard-check-fill" viewBox="0 0 16 16"> <path d="M6.5 0A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3Zm3 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3Z"/> <path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1A2.5 2.5 0 0 1 9.5 5h-3A2.5 2.5 0 0 1 4 2.5v-1Zm6.854 7.354-3 3a.5.5 0 0 1-.708 0l-1.5-1.5a.5.5 0 0 1 .708-.708L7.5 10.793l2.646-2.647a.5.5 0 0 1 .708.708Z"/> </svg> </symbol> </svg> <div class="side-bar"> <div class="site-header" role="banner"> <a href="/" class="site-title lh-tight"> Two Tutorials for Mojo using Pocket Flow </a> <button id="menu-button" class="site-button btn-reset" aria-label="Toggle menu" aria-pressed="false"> <svg viewBox="0 0 24 24" class="icon" aria-hidden="true"><use xlink:href="#svg-menu"></use></svg> </a> </div> <nav aria-label="Main" id="site-nav" class="site-nav"> <ul class="nav-list"><li class="nav-list-item"><a href="/" class="nav-list-link">Home</a></li><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in My Tutorial for Crawl4ai category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button><a href="/my-crawl4AI/" class="nav-list-link">My Tutorial for Crawl4ai</a><ul class="nav-list"></ul></li><li class="nav-list-item"><a href="/design.html" class="nav-list-link">System Design</a></li><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in My Tutorial for Modular's Max category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button><a href="/modular_max/" class="nav-list-link">My Tutorial for Modular's Max</a><ul class="nav-list"><li class="nav-list-item "><a href="/modular_max/01_settings___settings__class__.html" class="nav-list-link">Chapter 1: Settings Class</a></li></ul></li><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in My Tutorial for Mojo v1 category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button><a href="/mojo-v1/" class="nav-list-link">My Tutorial for Mojo v1</a><ul class="nav-list"><li class="nav-list-item "><a href="/mojo-v1/01_addressspace_.html" class="nav-list-link">Chapter 1: AddressSpace</a></li></ul></li><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in My Tutorial for Mojo v2 category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button><a href="/mojo-v2/" class="nav-list-link">My Tutorial for Mojo v2</a><ul class="nav-list"><li class="nav-list-item "><a href="/mojo-v2/01_unsafepointer__as_used_by_ndbuffer__.html" class="nav-list-link">Chapter 1: UnsafePointer</a></li></ul></li><li class="nav-list-item active"><button class="nav-list-expander btn-reset" aria-label="toggle items in Crawl4AI category" aria-pressed="true"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button><a href="/Crawl4AI/" class="nav-list-link">Crawl4AI</a><ul class="nav-list"><li class="nav-list-item "><a href="/Crawl4AI/01_asynccrawlerstrategy.html" class="nav-list-link">AsyncCrawlerStrategy</a></li><li class="nav-list-item "><a href="/Crawl4AI/02_asyncwebcrawler.html" class="nav-list-link">AsyncWebCrawler</a></li><li class="nav-list-item "><a href="/Crawl4AI/03_crawlerrunconfig.html" class="nav-list-link">CrawlerRunConfig</a></li><li class="nav-list-item "><a href="/Crawl4AI/04_contentscrapingstrategy.html" class="nav-list-link">ContentScrapingStrategy</a></li><li class="nav-list-item "><a href="/Crawl4AI/05_relevantcontentfilter.html" class="nav-list-link">RelevantContentFilter</a></li><li class="nav-list-item "><a href="/Crawl4AI/06_extractionstrategy.html" class="nav-list-link">ExtractionStrategy</a></li><li class="nav-list-item "><a href="/Crawl4AI/07_crawlresult.html" class="nav-list-link">CrawlResult</a></li><li class="nav-list-item "><a href="/Crawl4AI/08_deepcrawlstrategy.html" class="nav-list-link">DeepCrawlStrategy</a></li><li class="nav-list-item "><a href="/Crawl4AI/09_cachecontext___cachemode.html" class="nav-list-link">CacheContext & CacheMode</a></li><li class="nav-list-item active"><a href="/Crawl4AI/10_basedispatcher.html" class="nav-list-link active">BaseDispatcher</a></li></ul></li></ul> <div class="nav-category">Crawl4AI</div> <ul class="nav-list"></ul> <div class="nav-category">Modular Max</div> <ul class="nav-list"></ul> <div class="nav-category">Mojo (v1)</div> <ul class="nav-list"></ul> <div class="nav-category">Mojo (v2)</div> <ul class="nav-list"></ul> </nav> <footer class="site-footer"> This site uses <a href="https://github.com/just-the-docs/just-the-docs">Just the Docs</a>, a documentation theme for Jekyll. </footer> </div> <div class="main" id="top"> <div id="main-header" class="main-header"> <div class="search" role="search"> <div class="search-input-wrap"> <input type="text" id="search-input" class="search-input" tabindex="0" placeholder="Search Two Tutorials for Mojo using Pocket Flow" aria-label="Search Two Tutorials for Mojo using Pocket Flow" autocomplete="off"> <label for="search-input" class="search-label"><svg viewBox="0 0 24 24" class="search-icon"><use xlink:href="#svg-search"></use></svg></label> </div> <div id="search-results" class="search-results"></div> </div> <nav aria-label="Auxiliary" class="aux-nav"> <ul class="aux-nav-list"> <li class="aux-nav-list-item"> <a href="https://github.com/MrBesterTester/pf-understand" class="site-button" > View on GitHub </a> </li> </ul> </nav> </div> <div id="main-content-wrap" class="main-content-wrap"> <nav aria-label="Breadcrumb" class="breadcrumb-nav"> <ol class="breadcrumb-nav-list"> <li class="breadcrumb-nav-list-item"><a href="/Crawl4AI/">Crawl4AI</a></li> <li class="breadcrumb-nav-list-item"><span>BaseDispatcher</span></li> </ol> </nav> <div id="main-content" class="main-content"> <main> <h1 id="chapter-10-orchestrating-the-crawl---basedispatcher"> <a href="#chapter-10-orchestrating-the-crawl---basedispatcher" class="anchor-heading" aria-labelledby="chapter-10-orchestrating-the-crawl---basedispatcher"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Chapter 10: Orchestrating the Crawl - BaseDispatcher </h1> <p>In <a href="09_cachecontext___cachemode.md">Chapter 9: Smart Fetching with Caching - CacheContext / CacheMode</a>, we learned how Crawl4AI uses caching to cleverly avoid re-fetching the same webpage multiple times, which is especially helpful when crawling many URLs. We’ve also seen how methods like <code class="language-plaintext highlighter-rouge">arun_many()</code> (<a href="02_asyncwebcrawler.md">Chapter 2: Meet the General Manager - AsyncWebCrawler</a>) or strategies like <a href="08_deepcrawlstrategy.md">DeepCrawlStrategy</a> can lead to potentially hundreds or thousands of individual URLs needing to be crawled.</p> <p>This raises a question: if we have 1000 URLs to crawl, does Crawl4AI try to crawl all 1000 simultaneously? That would likely overwhelm your computer’s resources (like memory and CPU) and could also flood the target website with too many requests, potentially getting you blocked! How does Crawl4AI manage running many crawls efficiently and responsibly?</p> <h2 id="what-problem-does-basedispatcher-solve"> <a href="#what-problem-does-basedispatcher-solve" class="anchor-heading" aria-labelledby="what-problem-does-basedispatcher-solve"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> What Problem Does <code class="language-plaintext highlighter-rouge">BaseDispatcher</code> Solve? </h2> <p>Imagine you’re managing a fleet of delivery drones (<code class="language-plaintext highlighter-rouge">AsyncWebCrawler</code> tasks) that need to pick up packages from many different addresses (URLs). If you launch all 1000 drones at the exact same moment:</p> <ul> <li>Your control station (your computer) might crash due to the processing load.</li> <li>The central warehouse (the target website) might get overwhelmed by simultaneous arrivals.</li> <li>Some drones might collide or interfere with each other.</li> </ul> <p>You need a <strong>Traffic Controller</strong> or a <strong>Dispatch Center</strong> to manage the fleet. This controller decides:</p> <ol> <li>How many drones can be active in the air at any one time.</li> <li>When to launch the next drone, maybe based on available airspace (system resources) or just a simple count limit.</li> <li>How to handle potential delays or issues (like rate limiting from a specific website).</li> </ol> <p>In Crawl4AI, the <code class="language-plaintext highlighter-rouge">BaseDispatcher</code> acts as this <strong>Traffic Controller</strong> or <strong>Task Scheduler</strong> for concurrent crawling operations, primarily when using <code class="language-plaintext highlighter-rouge">arun_many()</code>. It manages <em>how</em> multiple crawl tasks are executed concurrently, ensuring the process is efficient without overwhelming your system or the target websites.</p> <h2 id="what-is-basedispatcher"> <a href="#what-is-basedispatcher" class="anchor-heading" aria-labelledby="what-is-basedispatcher"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> What is <code class="language-plaintext highlighter-rouge">BaseDispatcher</code>? </h2> <p><code class="language-plaintext highlighter-rouge">BaseDispatcher</code> is an abstract concept (a blueprint or job description) in Crawl4AI. It defines <em>that</em> we need a system for managing the execution of multiple, concurrent crawling tasks. It specifies the <em>interface</em> for how the main <code class="language-plaintext highlighter-rouge">AsyncWebCrawler</code> interacts with such a system, but the specific <em>logic</em> for managing concurrency can vary.</p> <p>Think of it as the control panel for our drone fleet – the panel exists, but the specific rules programmed into it determine how drones are dispatched.</p> <h2 id="the-different-controllers-ways-to-dispatch-tasks"> <a href="#the-different-controllers-ways-to-dispatch-tasks" class="anchor-heading" aria-labelledby="the-different-controllers-ways-to-dispatch-tasks"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> The Different Controllers: Ways to Dispatch Tasks </h2> <p>Crawl4AI provides concrete implementations (the actual traffic control systems) based on the <code class="language-plaintext highlighter-rouge">BaseDispatcher</code> blueprint:</p> <ol> <li><strong><code class="language-plaintext highlighter-rouge">SemaphoreDispatcher</code> (The Simple Counter):</strong> <ul> <li><strong>Analogy:</strong> A parking garage with a fixed number of spots (e.g., 10). A gate (<code class="language-plaintext highlighter-rouge">asyncio.Semaphore</code>) only lets a new car in if one of the 10 spots is free.</li> <li><strong>How it works:</strong> You tell it the maximum number of crawls that can run <em>at the same time</em> (e.g., <code class="language-plaintext highlighter-rouge">semaphore_count=10</code>). It uses a simple counter (a semaphore) to ensure that no more than this number of crawls are active simultaneously. When one crawl finishes, it allows another one from the queue to start.</li> <li><strong>Good for:</strong> Simple, direct control over concurrency when you know a specific limit works well for your system and the target sites.</li> </ul> </li> <li><strong><code class="language-plaintext highlighter-rouge">MemoryAdaptiveDispatcher</code> (The Resource-Aware Controller - Default):</strong> <ul> <li><strong>Analogy:</strong> A smart parking garage attendant who checks not just the number of cars, but also the <em>total space</em> they occupy (system memory). They might stop letting cars in if the garage is nearing its memory capacity, even if some numbered spots are technically free.</li> <li><strong>How it works:</strong> This dispatcher monitors your system’s available memory. It tries to run multiple crawls concurrently (up to a configurable maximum like <code class="language-plaintext highlighter-rouge">max_session_permit</code>), but it will pause launching new crawls if the system memory usage exceeds a certain threshold (e.g., <code class="language-plaintext highlighter-rouge">memory_threshold_percent=90.0</code>). It adapts the concurrency level based on available resources.</li> <li><strong>Good for:</strong> Automatically adjusting concurrency to prevent out-of-memory errors, especially when crawl tasks vary significantly in resource usage. <strong>This is the default dispatcher used by <code class="language-plaintext highlighter-rouge">arun_many</code> if you don’t specify one.</strong></li> </ul> </li> </ol> <p>These dispatchers can also optionally work with a <code class="language-plaintext highlighter-rouge">RateLimiter</code> component, which adds politeness rules for specific websites (e.g., slowing down requests to a domain if it returns “429 Too Many Requests”).</p> <h2 id="how-arun_many-uses-the-dispatcher"> <a href="#how-arun_many-uses-the-dispatcher" class="anchor-heading" aria-labelledby="how-arun_many-uses-the-dispatcher"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> How <code class="language-plaintext highlighter-rouge">arun_many</code> Uses the Dispatcher </h2> <p>When you call <code class="language-plaintext highlighter-rouge">crawler.arun_many(urls=...)</code>, here’s the basic flow involving the dispatcher:</p> <ol> <li><strong>Get URLs:</strong> <code class="language-plaintext highlighter-rouge">arun_many</code> receives the list of URLs you want to crawl.</li> <li><strong>Select Dispatcher:</strong> It checks if you provided a specific <code class="language-plaintext highlighter-rouge">dispatcher</code> instance. If not, it creates an instance of the default <code class="language-plaintext highlighter-rouge">MemoryAdaptiveDispatcher</code>.</li> <li><strong>Delegate Execution:</strong> It hands over the list of URLs and the <code class="language-plaintext highlighter-rouge">CrawlerRunConfig</code> to the chosen dispatcher’s <code class="language-plaintext highlighter-rouge">run_urls</code> (or <code class="language-plaintext highlighter-rouge">run_urls_stream</code>) method.</li> <li><strong>Manage Tasks:</strong> The dispatcher takes charge: <ul> <li>It iterates through the URLs.</li> <li>For each URL, it decides <em>when</em> to start the actual crawl based on its rules (semaphore count, memory usage, rate limits).</li> <li>When ready, it typically calls the single-page <code class="language-plaintext highlighter-rouge">crawler.arun(url, config)</code> method internally for that specific URL, wrapped within its concurrency control mechanism.</li> <li>It manages the running tasks (e.g., using <code class="language-plaintext highlighter-rouge">asyncio.create_task</code> and <code class="language-plaintext highlighter-rouge">asyncio.wait</code>).</li> </ul> </li> <li><strong>Collect Results:</strong> As individual <code class="language-plaintext highlighter-rouge">arun</code> calls complete, the dispatcher collects their <code class="language-plaintext highlighter-rouge">CrawlResult</code> objects.</li> <li><strong>Return:</strong> Once all URLs are processed, the dispatcher returns the list of results (or yields them if streaming).</li> </ol><pre><code class="language-mermaid">sequenceDiagram
    participant User
    participant AWC as AsyncWebCrawler
    participant Dispatcher as BaseDispatcher (e.g., MemoryAdaptive)
    participant TaskPool as Concurrency Manager

    User-&gt;&gt;AWC: arun_many(urls, config, dispatcher?)
    AWC-&gt;&gt;Dispatcher: run_urls(crawler=AWC, urls, config)
    Dispatcher-&gt;&gt;TaskPool: Initialize (e.g., set max concurrency)
    loop For each URL in urls
        Dispatcher-&gt;&gt;TaskPool: Can I start a new task? (Checks limits)
        alt Yes
            TaskPool--&gt;&gt;Dispatcher: OK
            Note over Dispatcher: Create task: call AWC.arun(url, config) internally
            Dispatcher-&gt;&gt;TaskPool: Add new task
        else No
            TaskPool--&gt;&gt;Dispatcher: Wait
            Note over Dispatcher: Waits for a running task to finish
        end
    end
    Note over Dispatcher: Manages running tasks, collects results
    Dispatcher--&gt;&gt;AWC: List of CrawlResults
    AWC--&gt;&gt;User: List of CrawlResults
</code></pre><h2 id="using-the-dispatcher-often-implicitly"> <a href="#using-the-dispatcher-often-implicitly" class="anchor-heading" aria-labelledby="using-the-dispatcher-often-implicitly"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Using the Dispatcher (Often Implicitly!) </h2> <p>Most of the time, you don’t need to think about the dispatcher explicitly. When you use <code class="language-plaintext highlighter-rouge">arun_many</code>, the default <code class="language-plaintext highlighter-rouge">MemoryAdaptiveDispatcher</code> handles things automatically.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># chapter10_example_1.py
</span><span class="kn">import</span> <span class="nn">asyncio</span>
<span class="kn">from</span> <span class="nn">crawl4ai</span> <span class="kn">import</span> <span class="n">AsyncWebCrawler</span><span class="p">,</span> <span class="n">CrawlerRunConfig</span>

<span class="k">async</span> <span class="k">def</span> <span class="nf">main</span><span class="p">():</span>
    <span class="n">urls_to_crawl</span> <span class="o">=</span> <span class="p">[</span>
        <span class="s">"https://httpbin.org/html"</span><span class="p">,</span>
        <span class="s">"https://httpbin.org/links/5/0"</span><span class="p">,</span> <span class="c1"># Page with 5 links
</span>        <span class="s">"https://httpbin.org/robots.txt"</span><span class="p">,</span>
        <span class="s">"https://httpbin.org/status/200"</span><span class="p">,</span>
    <span class="p">]</span>

    <span class="c1"># We DON'T specify a dispatcher here.
</span>    <span class="c1"># arun_many will use the default MemoryAdaptiveDispatcher.
</span>    <span class="k">async</span> <span class="k">with</span> <span class="n">AsyncWebCrawler</span><span class="p">()</span> <span class="k">as</span> <span class="n">crawler</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Crawling </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">urls_to_crawl</span><span class="p">)</span><span class="si">}</span><span class="s"> URLs using the default dispatcher..."</span><span class="p">)</span>
        <span class="n">config</span> <span class="o">=</span> <span class="n">CrawlerRunConfig</span><span class="p">(</span><span class="n">stream</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span> <span class="c1"># Get results as a list at the end
</span>
        <span class="c1"># The MemoryAdaptiveDispatcher manages concurrency behind the scenes.
</span>        <span class="n">results</span> <span class="o">=</span> <span class="k">await</span> <span class="n">crawler</span><span class="p">.</span><span class="n">arun_many</span><span class="p">(</span><span class="n">urls</span><span class="o">=</span><span class="n">urls_to_crawl</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">)</span>

        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"</span><span class="se">\n</span><span class="s">Finished! Got </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">results</span><span class="p">)</span><span class="si">}</span><span class="s"> results."</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">result</span> <span class="ow">in</span> <span class="n">results</span><span class="p">:</span>
            <span class="n">status</span> <span class="o">=</span> <span class="s">"✅"</span> <span class="k">if</span> <span class="n">result</span><span class="p">.</span><span class="n">success</span> <span class="k">else</span> <span class="s">"❌"</span>
            <span class="n">url_short</span> <span class="o">=</span> <span class="n">result</span><span class="p">.</span><span class="n">url</span><span class="p">.</span><span class="n">split</span><span class="p">(</span><span class="s">'/'</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
            <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"  </span><span class="si">{</span><span class="n">status</span><span class="si">}</span><span class="s"> </span><span class="si">{</span><span class="n">url_short</span><span class="si">:</span><span class="o">&lt;</span><span class="mi">15</span><span class="si">}</span><span class="s"> | Title: </span><span class="si">{</span><span class="n">result</span><span class="p">.</span><span class="n">metadata</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">'title'</span><span class="p">,</span> <span class="s">'N/A'</span><span class="p">)</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>

<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="s">"__main__"</span><span class="p">:</span>
    <span class="n">asyncio</span><span class="p">.</span><span class="n">run</span><span class="p">(</span><span class="n">main</span><span class="p">())</span>
</code></pre></div></div> <p><strong>Explanation:</strong></p> <ul> <li>We call <code class="language-plaintext highlighter-rouge">crawler.arun_many</code> without passing a <code class="language-plaintext highlighter-rouge">dispatcher</code> argument.</li> <li>Crawl4AI automatically creates and uses a <code class="language-plaintext highlighter-rouge">MemoryAdaptiveDispatcher</code>.</li> <li>This dispatcher runs the crawls concurrently, adapting to your system’s memory, and returns all the results once completed (because <code class="language-plaintext highlighter-rouge">stream=False</code>). You benefit from concurrency without explicit setup.</li> </ul> <h2 id="explicitly-choosing-a-dispatcher"> <a href="#explicitly-choosing-a-dispatcher" class="anchor-heading" aria-labelledby="explicitly-choosing-a-dispatcher"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Explicitly Choosing a Dispatcher </h2> <p>What if you want simpler, fixed concurrency? You can explicitly create and pass a <code class="language-plaintext highlighter-rouge">SemaphoreDispatcher</code>.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># chapter10_example_2.py
</span><span class="kn">import</span> <span class="nn">asyncio</span>
<span class="kn">from</span> <span class="nn">crawl4ai</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">AsyncWebCrawler</span><span class="p">,</span>
    <span class="n">CrawlerRunConfig</span><span class="p">,</span>
    <span class="n">SemaphoreDispatcher</span> <span class="c1"># 1. Import the specific dispatcher
</span><span class="p">)</span>

<span class="k">async</span> <span class="k">def</span> <span class="nf">main</span><span class="p">():</span>
    <span class="n">urls_to_crawl</span> <span class="o">=</span> <span class="p">[</span>
        <span class="s">"https://httpbin.org/delay/1"</span><span class="p">,</span> <span class="c1"># Takes 1 second
</span>        <span class="s">"https://httpbin.org/delay/1"</span><span class="p">,</span>
        <span class="s">"https://httpbin.org/delay/1"</span><span class="p">,</span>
        <span class="s">"https://httpbin.org/delay/1"</span><span class="p">,</span>
        <span class="s">"https://httpbin.org/delay/1"</span><span class="p">,</span>
    <span class="p">]</span>

    <span class="c1"># 2. Create an instance of the SemaphoreDispatcher
</span>    <span class="c1">#    Allow only 2 crawls to run at the same time.
</span>    <span class="n">semaphore_controller</span> <span class="o">=</span> <span class="n">SemaphoreDispatcher</span><span class="p">(</span><span class="n">semaphore_count</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Using SemaphoreDispatcher with limit: </span><span class="si">{</span><span class="n">semaphore_controller</span><span class="p">.</span><span class="n">semaphore_count</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>

    <span class="k">async</span> <span class="k">with</span> <span class="n">AsyncWebCrawler</span><span class="p">()</span> <span class="k">as</span> <span class="n">crawler</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Crawling </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">urls_to_crawl</span><span class="p">)</span><span class="si">}</span><span class="s"> URLs with explicit dispatcher..."</span><span class="p">)</span>
        <span class="n">config</span> <span class="o">=</span> <span class="n">CrawlerRunConfig</span><span class="p">(</span><span class="n">stream</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

        <span class="c1"># 3. Pass the dispatcher instance to arun_many
</span>        <span class="n">results</span> <span class="o">=</span> <span class="k">await</span> <span class="n">crawler</span><span class="p">.</span><span class="n">arun_many</span><span class="p">(</span>
            <span class="n">urls</span><span class="o">=</span><span class="n">urls_to_crawl</span><span class="p">,</span>
            <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">,</span>
            <span class="n">dispatcher</span><span class="o">=</span><span class="n">semaphore_controller</span> <span class="c1"># Pass our controller
</span>        <span class="p">)</span>

        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"</span><span class="se">\n</span><span class="s">Finished! Got </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">results</span><span class="p">)</span><span class="si">}</span><span class="s"> results."</span><span class="p">)</span>
        <span class="c1"># This crawl likely took around 3 seconds (5 tasks, 1s each, 2 concurrent = ceil(5/2)*1s)
</span>        <span class="k">for</span> <span class="n">result</span> <span class="ow">in</span> <span class="n">results</span><span class="p">:</span>
            <span class="n">status</span> <span class="o">=</span> <span class="s">"✅"</span> <span class="k">if</span> <span class="n">result</span><span class="p">.</span><span class="n">success</span> <span class="k">else</span> <span class="s">"❌"</span>
            <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"  </span><span class="si">{</span><span class="n">status</span><span class="si">}</span><span class="s"> </span><span class="si">{</span><span class="n">result</span><span class="p">.</span><span class="n">url</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>

<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="s">"__main__"</span><span class="p">:</span>
    <span class="n">asyncio</span><span class="p">.</span><span class="n">run</span><span class="p">(</span><span class="n">main</span><span class="p">())</span>
</code></pre></div></div> <p><strong>Explanation:</strong></p> <ol> <li><strong>Import:</strong> We import <code class="language-plaintext highlighter-rouge">SemaphoreDispatcher</code>.</li> <li><strong>Instantiate:</strong> We create <code class="language-plaintext highlighter-rouge">SemaphoreDispatcher(semaphore_count=2)</code>, limiting concurrency to 2 simultaneous crawls.</li> <li><strong>Pass Dispatcher:</strong> We pass our <code class="language-plaintext highlighter-rouge">semaphore_controller</code> instance directly to the <code class="language-plaintext highlighter-rouge">dispatcher</code> parameter of <code class="language-plaintext highlighter-rouge">arun_many</code>.</li> <li><strong>Execution:</strong> Now, <code class="language-plaintext highlighter-rouge">arun_many</code> uses our <code class="language-plaintext highlighter-rouge">SemaphoreDispatcher</code>. It will start the first two crawls. As one finishes, it will start the next one from the list, always ensuring no more than two are running concurrently.</li> </ol> <h2 id="a-glimpse-under-the-hood"> <a href="#a-glimpse-under-the-hood" class="anchor-heading" aria-labelledby="a-glimpse-under-the-hood"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> A Glimpse Under the Hood </h2> <p>Where are these dispatchers defined? In <code class="language-plaintext highlighter-rouge">crawl4ai/async_dispatcher.py</code>.</p> <p><strong>The Blueprint (<code class="language-plaintext highlighter-rouge">BaseDispatcher</code>):</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Simplified from crawl4ai/async_dispatcher.py
</span><span class="kn">from</span> <span class="nn">abc</span> <span class="kn">import</span> <span class="n">ABC</span><span class="p">,</span> <span class="n">abstractmethod</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">List</span><span class="p">,</span> <span class="n">Optional</span>
<span class="c1"># ... other imports like CrawlerRunConfig, CrawlerTaskResult, AsyncWebCrawler ...
</span>
<span class="k">class</span> <span class="nc">BaseDispatcher</span><span class="p">(</span><span class="n">ABC</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">rate_limiter</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">RateLimiter</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
        <span class="n">monitor</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">CrawlerMonitor</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">crawler</span> <span class="o">=</span> <span class="bp">None</span> <span class="c1"># Will be set by arun_many
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">rate_limiter</span> <span class="o">=</span> <span class="n">rate_limiter</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">monitor</span> <span class="o">=</span> <span class="n">monitor</span>
        <span class="c1"># ... other common state ...
</span>
    <span class="o">@</span><span class="n">abstractmethod</span>
    <span class="k">async</span> <span class="k">def</span> <span class="nf">crawl_url</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">url</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">config</span><span class="p">:</span> <span class="n">CrawlerRunConfig</span><span class="p">,</span>
        <span class="n">task_id</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="c1"># ... maybe other internal params ...
</span>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">CrawlerTaskResult</span><span class="p">:</span>
        <span class="s">"""Crawls a single URL, potentially handling concurrency primitives."""</span>
        <span class="c1"># This is often the core worker method called by run_urls
</span>        <span class="k">pass</span>

    <span class="o">@</span><span class="n">abstractmethod</span>
    <span class="k">async</span> <span class="k">def</span> <span class="nf">run_urls</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">urls</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
        <span class="n">crawler</span><span class="p">:</span> <span class="s">"AsyncWebCrawler"</span><span class="p">,</span>
        <span class="n">config</span><span class="p">:</span> <span class="n">CrawlerRunConfig</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">CrawlerTaskResult</span><span class="p">]:</span>
        <span class="s">"""Manages the concurrent execution of crawl_url for multiple URLs."""</span>
        <span class="c1"># This is the main entry point called by arun_many
</span>        <span class="k">pass</span>

    <span class="k">async</span> <span class="k">def</span> <span class="nf">run_urls_stream</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">urls</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
        <span class="n">crawler</span><span class="p">:</span> <span class="s">"AsyncWebCrawler"</span><span class="p">,</span>
        <span class="n">config</span><span class="p">:</span> <span class="n">CrawlerRunConfig</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">AsyncGenerator</span><span class="p">[</span><span class="n">CrawlerTaskResult</span><span class="p">,</span> <span class="bp">None</span><span class="p">]:</span>
         <span class="s">""" Streaming version of run_urls (might be implemented in base or subclasses) """</span>
         <span class="c1"># Example default implementation (subclasses might override)
</span>         <span class="n">results</span> <span class="o">=</span> <span class="k">await</span> <span class="bp">self</span><span class="p">.</span><span class="n">run_urls</span><span class="p">(</span><span class="n">urls</span><span class="p">,</span> <span class="n">crawler</span><span class="p">,</span> <span class="n">config</span><span class="p">)</span>
         <span class="k">for</span> <span class="n">res</span> <span class="ow">in</span> <span class="n">results</span><span class="p">:</span> <span class="k">yield</span> <span class="n">res</span> <span class="c1"># Naive stream, real one is more complex
</span>
    <span class="c1"># ... other potential helper methods ...
</span></code></pre></div></div> <p><strong>Example Implementation (<code class="language-plaintext highlighter-rouge">SemaphoreDispatcher</code>):</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Simplified from crawl4ai/async_dispatcher.py
</span><span class="kn">import</span> <span class="nn">asyncio</span>
<span class="kn">import</span> <span class="nn">uuid</span>
<span class="kn">import</span> <span class="nn">psutil</span> <span class="c1"># For memory tracking in crawl_url
</span><span class="kn">import</span> <span class="nn">time</span>   <span class="c1"># For timing in crawl_url
# ... other imports ...
</span>
<span class="k">class</span> <span class="nc">SemaphoreDispatcher</span><span class="p">(</span><span class="n">BaseDispatcher</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">semaphore_count</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span>
        <span class="c1"># ... other params like rate_limiter, monitor ...
</span>    <span class="p">):</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">(...)</span> <span class="c1"># Pass rate_limiter, monitor to base
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">semaphore_count</span> <span class="o">=</span> <span class="n">semaphore_count</span>

    <span class="k">async</span> <span class="k">def</span> <span class="nf">crawl_url</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">url</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">config</span><span class="p">:</span> <span class="n">CrawlerRunConfig</span><span class="p">,</span>
        <span class="n">task_id</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">semaphore</span><span class="p">:</span> <span class="n">asyncio</span><span class="p">.</span><span class="n">Semaphore</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span> <span class="c1"># Takes the semaphore
</span>    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">CrawlerTaskResult</span><span class="p">:</span>
        <span class="c1"># ... (Code to track start time, memory usage - similar to MemoryAdaptiveDispatcher's version)
</span>        <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="n">time</span><span class="p">()</span>
        <span class="n">error_message</span> <span class="o">=</span> <span class="s">""</span>
        <span class="n">memory_usage</span> <span class="o">=</span> <span class="n">peak_memory</span> <span class="o">=</span> <span class="mf">0.0</span>
        <span class="n">result</span> <span class="o">=</span> <span class="bp">None</span>

        <span class="k">try</span><span class="p">:</span>
            <span class="c1"># Update monitor state if used
</span>            <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">monitor</span><span class="p">:</span> <span class="bp">self</span><span class="p">.</span><span class="n">monitor</span><span class="p">.</span><span class="n">update_task</span><span class="p">(</span><span class="n">task_id</span><span class="p">,</span> <span class="n">status</span><span class="o">=</span><span class="n">CrawlStatus</span><span class="p">.</span><span class="n">IN_PROGRESS</span><span class="p">)</span>

            <span class="c1"># Wait for rate limiter if used
</span>            <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">rate_limiter</span><span class="p">:</span> <span class="k">await</span> <span class="bp">self</span><span class="p">.</span><span class="n">rate_limiter</span><span class="p">.</span><span class="n">wait_if_needed</span><span class="p">(</span><span class="n">url</span><span class="p">)</span>

            <span class="c1"># --- Core Semaphore Logic ---
</span>            <span class="k">async</span> <span class="k">with</span> <span class="n">semaphore</span><span class="p">:</span> <span class="c1"># Acquire a spot from the semaphore
</span>                <span class="c1"># Now that we have a spot, run the actual crawl
</span>                <span class="n">process</span> <span class="o">=</span> <span class="n">psutil</span><span class="p">.</span><span class="n">Process</span><span class="p">()</span>
                <span class="n">start_memory</span> <span class="o">=</span> <span class="n">process</span><span class="p">.</span><span class="n">memory_info</span><span class="p">().</span><span class="n">rss</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1024</span> <span class="o">*</span> <span class="mi">1024</span><span class="p">)</span>

                <span class="c1"># Call the single-page crawl method of the main crawler
</span>                <span class="n">result</span> <span class="o">=</span> <span class="k">await</span> <span class="bp">self</span><span class="p">.</span><span class="n">crawler</span><span class="p">.</span><span class="n">arun</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">,</span> <span class="n">session_id</span><span class="o">=</span><span class="n">task_id</span><span class="p">)</span>

                <span class="n">end_memory</span> <span class="o">=</span> <span class="n">process</span><span class="p">.</span><span class="n">memory_info</span><span class="p">().</span><span class="n">rss</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1024</span> <span class="o">*</span> <span class="mi">1024</span><span class="p">)</span>
                <span class="n">memory_usage</span> <span class="o">=</span> <span class="n">peak_memory</span> <span class="o">=</span> <span class="n">end_memory</span> <span class="o">-</span> <span class="n">start_memory</span>
            <span class="c1"># --- Semaphore spot is released automatically on exiting 'async with' ---
</span>
            <span class="c1"># Update rate limiter based on result status if used
</span>            <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">rate_limiter</span> <span class="ow">and</span> <span class="n">result</span><span class="p">.</span><span class="n">status_code</span><span class="p">:</span>
                 <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="p">.</span><span class="n">rate_limiter</span><span class="p">.</span><span class="n">update_delay</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">result</span><span class="p">.</span><span class="n">status_code</span><span class="p">):</span>
                    <span class="c1"># Handle retry limit exceeded
</span>                    <span class="n">error_message</span> <span class="o">=</span> <span class="s">"Rate limit retry count exceeded"</span>
                    <span class="c1"># ... update monitor, prepare error result ...
</span>
            <span class="c1"># Update monitor status (success/fail)
</span>            <span class="k">if</span> <span class="n">result</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">result</span><span class="p">.</span><span class="n">success</span><span class="p">:</span> <span class="n">error_message</span> <span class="o">=</span> <span class="n">result</span><span class="p">.</span><span class="n">error_message</span>
            <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">monitor</span><span class="p">:</span> <span class="bp">self</span><span class="p">.</span><span class="n">monitor</span><span class="p">.</span><span class="n">update_task</span><span class="p">(</span><span class="n">task_id</span><span class="p">,</span> <span class="n">status</span><span class="o">=</span><span class="n">CrawlStatus</span><span class="p">.</span><span class="n">COMPLETED</span> <span class="k">if</span> <span class="n">result</span><span class="p">.</span><span class="n">success</span> <span class="k">else</span> <span class="n">CrawlStatus</span><span class="p">.</span><span class="n">FAILED</span><span class="p">)</span>

        <span class="k">except</span> <span class="nb">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="c1"># Handle unexpected errors during the crawl
</span>            <span class="n">error_message</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">monitor</span><span class="p">:</span> <span class="bp">self</span><span class="p">.</span><span class="n">monitor</span><span class="p">.</span><span class="n">update_task</span><span class="p">(</span><span class="n">task_id</span><span class="p">,</span> <span class="n">status</span><span class="o">=</span><span class="n">CrawlStatus</span><span class="p">.</span><span class="n">FAILED</span><span class="p">)</span>
            <span class="c1"># Create a failed CrawlResult if needed
</span>            <span class="k">if</span> <span class="ow">not</span> <span class="n">result</span><span class="p">:</span> <span class="n">result</span> <span class="o">=</span> <span class="n">CrawlResult</span><span class="p">(</span><span class="n">url</span><span class="o">=</span><span class="n">url</span><span class="p">,</span> <span class="n">html</span><span class="o">=</span><span class="s">""</span><span class="p">,</span> <span class="n">success</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">error_message</span><span class="o">=</span><span class="n">error_message</span><span class="p">)</span>

        <span class="k">finally</span><span class="p">:</span>
            <span class="c1"># Final monitor update with timing, memory etc.
</span>             <span class="n">end_time</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="n">time</span><span class="p">()</span>
             <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">monitor</span><span class="p">:</span> <span class="bp">self</span><span class="p">.</span><span class="n">monitor</span><span class="p">.</span><span class="n">update_task</span><span class="p">(...)</span>

        <span class="c1"># Package everything into CrawlerTaskResult
</span>        <span class="k">return</span> <span class="n">CrawlerTaskResult</span><span class="p">(...)</span>


    <span class="k">async</span> <span class="k">def</span> <span class="nf">run_urls</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">crawler</span><span class="p">:</span> <span class="s">"AsyncWebCrawler"</span><span class="p">,</span>
        <span class="n">urls</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
        <span class="n">config</span><span class="p">:</span> <span class="n">CrawlerRunConfig</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">CrawlerTaskResult</span><span class="p">]:</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">crawler</span> <span class="o">=</span> <span class="n">crawler</span> <span class="c1"># Store the crawler instance
</span>        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">monitor</span><span class="p">:</span> <span class="bp">self</span><span class="p">.</span><span class="n">monitor</span><span class="p">.</span><span class="n">start</span><span class="p">()</span>

        <span class="k">try</span><span class="p">:</span>
            <span class="c1"># Create the semaphore with the specified count
</span>            <span class="n">semaphore</span> <span class="o">=</span> <span class="n">asyncio</span><span class="p">.</span><span class="n">Semaphore</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">semaphore_count</span><span class="p">)</span>
            <span class="n">tasks</span> <span class="o">=</span> <span class="p">[]</span>

            <span class="c1"># Create a crawl task for each URL, passing the semaphore
</span>            <span class="k">for</span> <span class="n">url</span> <span class="ow">in</span> <span class="n">urls</span><span class="p">:</span>
                <span class="n">task_id</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">uuid</span><span class="p">.</span><span class="n">uuid4</span><span class="p">())</span>
                <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">monitor</span><span class="p">:</span> <span class="bp">self</span><span class="p">.</span><span class="n">monitor</span><span class="p">.</span><span class="n">add_task</span><span class="p">(</span><span class="n">task_id</span><span class="p">,</span> <span class="n">url</span><span class="p">)</span>
                <span class="c1"># Create an asyncio task to run crawl_url
</span>                <span class="n">task</span> <span class="o">=</span> <span class="n">asyncio</span><span class="p">.</span><span class="n">create_task</span><span class="p">(</span>
                    <span class="bp">self</span><span class="p">.</span><span class="n">crawl_url</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">config</span><span class="p">,</span> <span class="n">task_id</span><span class="p">,</span> <span class="n">semaphore</span><span class="o">=</span><span class="n">semaphore</span><span class="p">)</span>
                <span class="p">)</span>
                <span class="n">tasks</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">task</span><span class="p">)</span>

            <span class="c1"># Wait for all created tasks to complete
</span>            <span class="c1"># asyncio.gather runs them concurrently, respecting the semaphore limit
</span>            <span class="n">results</span> <span class="o">=</span> <span class="k">await</span> <span class="n">asyncio</span><span class="p">.</span><span class="n">gather</span><span class="p">(</span><span class="o">*</span><span class="n">tasks</span><span class="p">,</span> <span class="n">return_exceptions</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

            <span class="c1"># Process results (handle potential exceptions returned by gather)
</span>            <span class="n">final_results</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">res</span> <span class="ow">in</span> <span class="n">results</span><span class="p">:</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">res</span><span class="p">,</span> <span class="nb">Exception</span><span class="p">):</span>
                    <span class="c1"># Handle case where gather caught an exception from a task
</span>                    <span class="c1"># You might create a failed CrawlerTaskResult here
</span>                    <span class="k">pass</span>
                <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">res</span><span class="p">,</span> <span class="n">CrawlerTaskResult</span><span class="p">):</span>
                    <span class="n">final_results</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">res</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">final_results</span>
        <span class="k">finally</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">monitor</span><span class="p">:</span> <span class="bp">self</span><span class="p">.</span><span class="n">monitor</span><span class="p">.</span><span class="n">stop</span><span class="p">()</span>

    <span class="c1"># run_urls_stream would have similar logic but use asyncio.as_completed
</span>    <span class="c1"># or manage tasks manually to yield results as they finish.
</span></code></pre></div></div> <p>The key takeaway is that the <code class="language-plaintext highlighter-rouge">Dispatcher</code> orchestrates calls to the single-page <code class="language-plaintext highlighter-rouge">crawler.arun</code> method, wrapping them with concurrency controls (like the <code class="language-plaintext highlighter-rouge">async with semaphore:</code> block) before running them using <code class="language-plaintext highlighter-rouge">asyncio</code>’s concurrency tools (<code class="language-plaintext highlighter-rouge">asyncio.create_task</code>, <code class="language-plaintext highlighter-rouge">asyncio.gather</code>, etc.).</p> <h2 id="conclusion"> <a href="#conclusion" class="anchor-heading" aria-labelledby="conclusion"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Conclusion </h2> <p>You’ve learned about <code class="language-plaintext highlighter-rouge">BaseDispatcher</code>, the crucial “Traffic Controller” that manages concurrent crawls in Crawl4AI, especially for <code class="language-plaintext highlighter-rouge">arun_many</code>.</p> <ul> <li>It solves the problem of efficiently running many crawls without overloading systems or websites.</li> <li>It acts as a <strong>blueprint</strong> for managing concurrency.</li> <li>Key implementations: <ul> <li><strong><code class="language-plaintext highlighter-rouge">SemaphoreDispatcher</code></strong>: Uses a simple count limit.</li> <li><strong><code class="language-plaintext highlighter-rouge">MemoryAdaptiveDispatcher</code></strong>: Adjusts concurrency based on system memory (the default for <code class="language-plaintext highlighter-rouge">arun_many</code>).</li> </ul> </li> <li>The dispatcher is used <strong>automatically</strong> by <code class="language-plaintext highlighter-rouge">arun_many</code>, but you can provide a specific instance if needed.</li> <li>It orchestrates the execution of individual crawl tasks, respecting defined limits.</li> </ul> <p>Understanding the dispatcher helps appreciate how Crawl4AI handles large-scale crawling tasks responsibly and efficiently.</p> <p>This concludes our tour of the core concepts in Crawl4AI! We’ve covered how pages are fetched, how the process is managed, how content is cleaned, filtered, and extracted, how deep crawls are performed, how caching optimizes fetches, and finally, how concurrency is managed. You now have a solid foundation to start building powerful web data extraction and processing applications with Crawl4AI. Happy crawling!</p><hr /> <p>Generated by <a href="https://github.com/The-Pocket/Tutorial-Codebase-Knowledge">AI Codebase Knowledge Builder</a></p> </main> <hr> <footer> <p class="text-small text-grey-dk-100 mb-0">Copyright &copy; 2023 Sam Kirk</p> </footer> </div> </div> <div class="search-overlay"></div> </div> <script type="module"> import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.6.0/dist/mermaid.esm.min.mjs'; var config = {} ; mermaid.initialize(config); mermaid.run({ querySelector: '.language-mermaid', }); </script> </body> </html>
