<!DOCTYPE html> <html lang="en-US"> <head> <meta charset="UTF-8"> <meta http-equiv="X-UA-Compatible" content="IE=Edge"> <link rel="stylesheet" href="/assets/css/just-the-docs-default.css"> <script src="/assets/js/vendor/lunr.min.js"></script> <script src="/assets/js/just-the-docs.js"></script> <meta name="viewport" content="width=device-width, initial-scale=1"> <!-- Begin Jekyll SEO tag v2.8.0 --> <title>AsyncWebCrawler | Two Tutorials for Mojo using Pocket Flow</title> <meta name="generator" content="Jekyll v4.3.4" /> <meta property="og:title" content="AsyncWebCrawler" /> <meta name="author" content="Sam Kirk" /> <meta property="og:locale" content="en_US" /> <meta name="description" content="Documentation generated using AI to explain codebases" /> <meta property="og:description" content="Documentation generated using AI to explain codebases" /> <link rel="canonical" href="http://localhost:4000/Crawl4AI/02_asyncwebcrawler.html" /> <meta property="og:url" content="http://localhost:4000/Crawl4AI/02_asyncwebcrawler.html" /> <meta property="og:site_name" content="Two Tutorials for Mojo using Pocket Flow" /> <meta property="og:type" content="website" /> <meta name="twitter:card" content="summary" /> <meta property="twitter:title" content="AsyncWebCrawler" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"WebPage","author":{"@type":"Person","name":"Sam Kirk","url":"https://www.columbia.edu/~zh2408/"},"description":"Documentation generated using AI to explain codebases","headline":"AsyncWebCrawler","url":"http://localhost:4000/Crawl4AI/02_asyncwebcrawler.html"}</script> <!-- End Jekyll SEO tag --> <!-- Add Mermaid support --> <script src="https://cdn.jsdelivr.net/npm/mermaid@11.6.0/dist/mermaid.min.js"></script> <script> document.addEventListener("DOMContentLoaded", function() { mermaid.initialize({ startOnLoad: true, theme: "default" }); // Process code blocks document.querySelectorAll('pre code.language-mermaid').forEach(function(block) { // Create a div with class 'mermaid' var mermaidDiv = document.createElement('div'); mermaidDiv.className = 'mermaid'; mermaidDiv.innerHTML = block.textContent; // Replace the parent pre with the mermaid div block.parentNode.parentNode.replaceChild(mermaidDiv, block.parentNode); console.log("Processed Mermaid block:", mermaidDiv.innerHTML.substring(0, 50) + "..."); }); console.log("Mermaid initialization complete. Version:", mermaid.version()); }); </script> </head> <body> <a class="skip-to-main" href="#main-content">Skip to main content</a> <svg xmlns="http://www.w3.org/2000/svg" class="d-none"> <symbol id="svg-link" viewBox="0 0 24 24"> <title>Link</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-link"> <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path> </svg> </symbol> <symbol id="svg-menu" viewBox="0 0 24 24"> <title>Menu</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu"> <line x1="3" y1="12" x2="21" y2="12"></line><line x1="3" y1="6" x2="21" y2="6"></line><line x1="3" y1="18" x2="21" y2="18"></line> </svg> </symbol> <symbol id="svg-arrow-right" viewBox="0 0 24 24"> <title>Expand</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-chevron-right"> <polyline points="9 18 15 12 9 6"></polyline> </svg> </symbol> <!-- Feather. MIT License: https://github.com/feathericons/feather/blob/master/LICENSE --> <symbol id="svg-external-link" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-external-link"> <title id="svg-external-link-title">(external link)</title> <path d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"></path><polyline points="15 3 21 3 21 9"></polyline><line x1="10" y1="14" x2="21" y2="3"></line> </symbol> <symbol id="svg-doc" viewBox="0 0 24 24"> <title>Document</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file"> <path d="M13 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V9z"></path><polyline points="13 2 13 9 20 9"></polyline> </svg> </symbol> <symbol id="svg-search" viewBox="0 0 24 24"> <title>Search</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-search"> <circle cx="11" cy="11" r="8"></circle><line x1="21" y1="21" x2="16.65" y2="16.65"></line> </svg> </symbol> <!-- Bootstrap Icons. MIT License: https://github.com/twbs/icons/blob/main/LICENSE.md --> <symbol id="svg-copy" viewBox="0 0 16 16"> <title>Copy</title> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard" viewBox="0 0 16 16"> <path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1h1a1 1 0 0 1 1 1V14a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V3.5a1 1 0 0 1 1-1h1v-1z"/> <path d="M9.5 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3zm-3-1A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3z"/> </svg> </symbol> <symbol id="svg-copied" viewBox="0 0 16 16"> <title>Copied</title> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard-check-fill" viewBox="0 0 16 16"> <path d="M6.5 0A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3Zm3 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3Z"/> <path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1A2.5 2.5 0 0 1 9.5 5h-3A2.5 2.5 0 0 1 4 2.5v-1Zm6.854 7.354-3 3a.5.5 0 0 1-.708 0l-1.5-1.5a.5.5 0 0 1 .708-.708L7.5 10.793l2.646-2.647a.5.5 0 0 1 .708.708Z"/> </svg> </symbol> </svg> <div class="side-bar"> <div class="site-header" role="banner"> <a href="/" class="site-title lh-tight"> Two Tutorials for Mojo using Pocket Flow </a> <button id="menu-button" class="site-button btn-reset" aria-label="Toggle menu" aria-pressed="false"> <svg viewBox="0 0 24 24" class="icon" aria-hidden="true"><use xlink:href="#svg-menu"></use></svg> </a> </div> <nav aria-label="Main" id="site-nav" class="site-nav"> <ul class="nav-list"><li class="nav-list-item"><a href="/" class="nav-list-link">Home</a></li><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in My Tutorial for Crawl4ai category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button><a href="/my-crawl4AI/" class="nav-list-link">My Tutorial for Crawl4ai</a><ul class="nav-list"><li class="nav-list-item "><a href="/my-crawl4ai/01_configuration_system_.html" class="nav-list-link">Chapter 1: Configuration System</a></li><li class="nav-list-item "><a href="/my-crawl4ai/02_asyncwebcrawler_.html" class="nav-list-link">Chapter 2: AsyncWebCrawler</a></li><li class="nav-list-item "><a href="/my-crawl4ai/03_content_extraction_pipeline_.html" class="nav-list-link">Chapter 3: Content Extraction Pipeline</a></li><li class="nav-list-item "><a href="/my-crawl4ai/04_url_filtering___scoring_.html" class="nav-list-link">Chapter 4: URL Filtering & Scoring</a></li><li class="nav-list-item "><a href="/my-crawl4ai/05_deep_crawling_system_.html" class="nav-list-link">Chapter 5: Deep Crawling System</a></li><li class="nav-list-item "><a href="/my-crawl4ai/06_caching_system_.html" class="nav-list-link">Chapter 6: Caching System</a></li><li class="nav-list-item "><a href="/my-crawl4ai/07_dispatcher_framework_.html" class="nav-list-link">Chapter 7: Dispatcher Framework</a></li><li class="nav-list-item "><a href="/my-crawl4ai/08_async_logging_infrastructure_.html" class="nav-list-link">Chapter 8: Async Logging Infrastructure</a></li><li class="nav-list-item "><a href="/my-crawl4ai/09_api___docker_integration_.html" class="nav-list-link">Chapter 9: API & Docker Integration</a></li></ul></li><li class="nav-list-item"><a href="/design.html" class="nav-list-link">System Design</a></li><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in My Tutorial for Modular's Max category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button><a href="/modular_max/" class="nav-list-link">My Tutorial for Modular's Max</a><ul class="nav-list"><li class="nav-list-item "><a href="/modular_max/01_settings___settings__class__.html" class="nav-list-link">Chapter 1: Settings Class</a></li><li class="nav-list-item "><a href="/modular_max/02_serving_api_layer__fastapi_app___routers__.html" class="nav-list-link">Chapter 2: Serving API Layer</a></li><li class="nav-list-item "><a href="/modular_max/03_llm_pipeline_orchestrator___tokengeneratorpipeline___.html" class="nav-list-link">Chapter 3: LLM Pipeline Orchestrator</a></li><li class="nav-list-item "><a href="/modular_max/04_model_worker_.html" class="nav-list-link">Chapter 4: Model Worker</a></li><li class="nav-list-item "><a href="/modular_max/05_scheduler___tokengenerationscheduler____embeddingsscheduler___.html" class="nav-list-link">Chapter 5: Scheduler</a></li><li class="nav-list-item "><a href="/modular_max/06_kv_cache_management_.html" class="nav-list-link">Chapter 6: KV Cache Management</a></li><li class="nav-list-item "><a href="/modular_max/07_enginequeue_.html" class="nav-list-link">Chapter 7: EngineQueue</a></li><li class="nav-list-item "><a href="/modular_max/08_telemetry_and_metrics___metrics____metricclient___.html" class="nav-list-link">Chapter 8: Telemetry and Metrics</a></li></ul></li><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in My Tutorial for Mojo v1 category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button><a href="/mojo-v1/" class="nav-list-link">My Tutorial for Mojo v1</a><ul class="nav-list"><li class="nav-list-item "><a href="/mojo-v1/01_addressspace_.html" class="nav-list-link">Chapter 1: AddressSpace</a></li><li class="nav-list-item "><a href="/mojo-v1/02_unsafepointer_.html" class="nav-list-link">Chapter 2: UnsafePointer</a></li><li class="nav-list-item "><a href="/mojo-v1/03_indexlist_.html" class="nav-list-link">Chapter 3: IndexList</a></li><li class="nav-list-item "><a href="/mojo-v1/04_dimlist_.html" class="nav-list-link">Chapter 4: DimList</a></li><li class="nav-list-item "><a href="/mojo-v1/05_ndbuffer_.html" class="nav-list-link">Chapter 5: NDBuffer</a></li><li class="nav-list-item "><a href="/mojo-v1/06_n_d_to_1d_indexing_logic__strided_memory_access__.html" class="nav-list-link">Chapter 6: N-D to 1D Indexing Logic</a></li></ul></li><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in My Tutorial for Mojo v2 category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button><a href="/mojo-v2/" class="nav-list-link">My Tutorial for Mojo v2</a><ul class="nav-list"><li class="nav-list-item "><a href="/mojo-v2/01_unsafepointer__as_used_by_ndbuffer__.html" class="nav-list-link">Chapter 1: UnsafePointer</a></li><li class="nav-list-item "><a href="/mojo-v2/02_dimlist_and_dim_.html" class="nav-list-link">Chapter 2: DimList and Dim</a></li><li class="nav-list-item "><a href="/mojo-v2/03_ndbuffer_.html" class="nav-list-link">Chapter 3: NDBuffer</a></li><li class="nav-list-item "><a href="/mojo-v2/04_strides_and_offset_computation_.html" class="nav-list-link">Chapter 4: Strides and Offset Computation</a></li><li class="nav-list-item "><a href="/mojo-v2/05_simd_data_access_.html" class="nav-list-link">Chapter 5: SIMD Data Access</a></li></ul></li><li class="nav-list-item active"><button class="nav-list-expander btn-reset" aria-label="toggle items in Crawl4AI category" aria-pressed="true"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button><a href="/Crawl4AI/" class="nav-list-link">Crawl4AI</a><ul class="nav-list"><li class="nav-list-item "><a href="/Crawl4AI/01_asynccrawlerstrategy.html" class="nav-list-link">AsyncCrawlerStrategy</a></li><li class="nav-list-item "><a href="/Crawl4AI/01_configuration_system_.html" class="nav-list-link">Chapter 1: Configuration System</a></li><li class="nav-list-item active"><a href="/Crawl4AI/02_asyncwebcrawler.html" class="nav-list-link active">AsyncWebCrawler</a></li><li class="nav-list-item "><a href="/Crawl4AI/02_asyncwebcrawler_.html" class="nav-list-link">Chapter 2: AsyncWebCrawler</a></li><li class="nav-list-item "><a href="/Crawl4AI/03_content_extraction_pipeline_.html" class="nav-list-link">Chapter 3: Content Extraction Pipeline</a></li><li class="nav-list-item "><a href="/Crawl4AI/03_crawlerrunconfig.html" class="nav-list-link">CrawlerRunConfig</a></li><li class="nav-list-item "><a href="/Crawl4AI/04_contentscrapingstrategy.html" class="nav-list-link">ContentScrapingStrategy</a></li><li class="nav-list-item "><a href="/Crawl4AI/04_url_filtering___scoring_.html" class="nav-list-link">Chapter 4: URL Filtering & Scoring</a></li><li class="nav-list-item "><a href="/Crawl4AI/05_deep_crawling_system_.html" class="nav-list-link">Chapter 5: Deep Crawling System</a></li><li class="nav-list-item "><a href="/Crawl4AI/05_relevantcontentfilter.html" class="nav-list-link">RelevantContentFilter</a></li><li class="nav-list-item "><a href="/Crawl4AI/06_caching_system_.html" class="nav-list-link">Chapter 6: Caching System</a></li><li class="nav-list-item "><a href="/Crawl4AI/06_extractionstrategy.html" class="nav-list-link">ExtractionStrategy</a></li><li class="nav-list-item "><a href="/Crawl4AI/07_crawlresult.html" class="nav-list-link">CrawlResult</a></li><li class="nav-list-item "><a href="/Crawl4AI/07_dispatcher_framework_.html" class="nav-list-link">Chapter 7: Dispatcher Framework</a></li><li class="nav-list-item "><a href="/Crawl4AI/08_deepcrawlstrategy.html" class="nav-list-link">DeepCrawlStrategy</a></li><li class="nav-list-item "><a href="/Crawl4AI/08_async_logging_infrastructure_.html" class="nav-list-link">Chapter 8: Async Logging Infrastructure</a></li><li class="nav-list-item "><a href="/Crawl4AI/09_api___docker_integration_.html" class="nav-list-link">Chapter 9: API & Docker Integration</a></li><li class="nav-list-item "><a href="/Crawl4AI/09_cachecontext___cachemode.html" class="nav-list-link">CacheContext & CacheMode</a></li><li class="nav-list-item "><a href="/Crawl4AI/10_basedispatcher.html" class="nav-list-link">BaseDispatcher</a></li></ul></li></ul> <div class="nav-category">Crawl4AI</div> <ul class="nav-list"></ul> <div class="nav-category">Modular Max</div> <ul class="nav-list"></ul> <div class="nav-category">Mojo (v1)</div> <ul class="nav-list"></ul> <div class="nav-category">Mojo (v2)</div> <ul class="nav-list"></ul> </nav> <footer class="site-footer"> This site uses <a href="https://github.com/just-the-docs/just-the-docs">Just the Docs</a>, a documentation theme for Jekyll. </footer> </div> <div class="main" id="top"> <div id="main-header" class="main-header"> <div class="search" role="search"> <div class="search-input-wrap"> <input type="text" id="search-input" class="search-input" tabindex="0" placeholder="Search Two Tutorials for Mojo using Pocket Flow" aria-label="Search Two Tutorials for Mojo using Pocket Flow" autocomplete="off"> <label for="search-input" class="search-label"><svg viewBox="0 0 24 24" class="search-icon"><use xlink:href="#svg-search"></use></svg></label> </div> <div id="search-results" class="search-results"></div> </div> <nav aria-label="Auxiliary" class="aux-nav"> <ul class="aux-nav-list"> <li class="aux-nav-list-item"> <a href="https://github.com/MrBesterTester/pf-understand" class="site-button" > View on GitHub </a> </li> </ul> </nav> </div> <div id="main-content-wrap" class="main-content-wrap"> <nav aria-label="Breadcrumb" class="breadcrumb-nav"> <ol class="breadcrumb-nav-list"> <li class="breadcrumb-nav-list-item"><a href="/Crawl4AI/">Crawl4AI</a></li> <li class="breadcrumb-nav-list-item"><span>AsyncWebCrawler</span></li> </ol> </nav> <div id="main-content" class="main-content"> <main> <h1 id="chapter-2-meet-the-general-manager---asyncwebcrawler"> <a href="#chapter-2-meet-the-general-manager---asyncwebcrawler" class="anchor-heading" aria-labelledby="chapter-2-meet-the-general-manager---asyncwebcrawler"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Chapter 2: Meet the General Manager - AsyncWebCrawler </h1> <p>In <a href="01_asynccrawlerstrategy.md">Chapter 1: How We Fetch Webpages - AsyncCrawlerStrategy</a>, we learned about the different ways Crawl4AI can fetch the raw content of a webpage, like choosing between a fast drone (<code class="language-plaintext highlighter-rouge">AsyncHTTPCrawlerStrategy</code>) or a versatile delivery truck (<code class="language-plaintext highlighter-rouge">AsyncPlaywrightCrawlerStrategy</code>).</p> <p>But who decides <em>which</em> delivery vehicle to use? Who tells it <em>which</em> address (URL) to go to? And who takes the delivered package (the raw HTML) and turns it into something useful?</p> <p>That’s where the <code class="language-plaintext highlighter-rouge">AsyncWebCrawler</code> comes in. Think of it as the <strong>General Manager</strong> of the entire crawling operation.</p> <h2 id="what-problem-does-asyncwebcrawler-solve"> <a href="#what-problem-does-asyncwebcrawler-solve" class="anchor-heading" aria-labelledby="what-problem-does-asyncwebcrawler-solve"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> What Problem Does <code class="language-plaintext highlighter-rouge">AsyncWebCrawler</code> Solve? </h2> <p>Imagine you want to get information from a website. You need to:</p> <ol> <li>Decide <em>how</em> to fetch the page (like choosing the drone or truck from Chapter 1).</li> <li>Actually <em>fetch</em> the page content.</li> <li>Maybe <em>clean up</em> the messy HTML.</li> <li>Perhaps <em>extract</em> specific pieces of information (like product prices or article titles).</li> <li>Maybe <em>save</em> the results so you don’t have to fetch them again immediately (caching).</li> <li>Finally, give you the <em>final, processed result</em>.</li> </ol> <p>Doing all these steps manually for every URL would be tedious and complex. <code class="language-plaintext highlighter-rouge">AsyncWebCrawler</code> acts as the central coordinator, managing all these steps for you. You just tell it what URL to crawl and maybe some preferences, and it handles the rest.</p> <h2 id="what-is-asyncwebcrawler"> <a href="#what-is-asyncwebcrawler" class="anchor-heading" aria-labelledby="what-is-asyncwebcrawler"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> What is <code class="language-plaintext highlighter-rouge">AsyncWebCrawler</code>? </h2> <p><code class="language-plaintext highlighter-rouge">AsyncWebCrawler</code> is the main class you’ll interact with when using Crawl4AI. It’s the primary entry point for starting any crawling task.</p> <p><strong>Key Responsibilities:</strong></p> <ul> <li><strong>Initialization:</strong> Sets up the necessary components, like the browser (if needed).</li> <li><strong>Coordination:</strong> Takes your request (a URL and configuration) and orchestrates the different parts: <ul> <li>Delegates fetching to an <a href="01_asynccrawlerstrategy.md">AsyncCrawlerStrategy</a>.</li> <li>Manages caching using <a href="09_cachecontext___cachemode.md">CacheContext / CacheMode</a>.</li> <li>Uses a <a href="04_contentscrapingstrategy.md">ContentScrapingStrategy</a> to clean and parse HTML.</li> <li>Applies a <a href="05_relevantcontentfilter.md">RelevantContentFilter</a> if configured.</li> <li>Uses an <a href="06_extractionstrategy.md">ExtractionStrategy</a> to pull out specific data if needed.</li> </ul> </li> <li><strong>Result Packaging:</strong> Bundles everything up into a neat <a href="07_crawlresult.md">CrawlResult</a> object.</li> <li><strong>Resource Management:</strong> Handles starting and stopping resources (like browsers) cleanly.</li> </ul> <p>It’s the “conductor” making sure all the different instruments play together harmoniously.</p> <h2 id="your-first-crawl-using-arun"> <a href="#your-first-crawl-using-arun" class="anchor-heading" aria-labelledby="your-first-crawl-using-arun"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Your First Crawl: Using <code class="language-plaintext highlighter-rouge">arun</code> </h2> <p>Let’s see the <code class="language-plaintext highlighter-rouge">AsyncWebCrawler</code> in action. The most common way to use it is with an <code class="language-plaintext highlighter-rouge">async with</code> block, which automatically handles setup and cleanup. The main method to crawl a single URL is <code class="language-plaintext highlighter-rouge">arun</code>.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># chapter2_example_1.py
</span><span class="kn">import</span> <span class="nn">asyncio</span>
<span class="kn">from</span> <span class="nn">crawl4ai</span> <span class="kn">import</span> <span class="n">AsyncWebCrawler</span> <span class="c1"># Import the General Manager
</span>
<span class="k">async</span> <span class="k">def</span> <span class="nf">main</span><span class="p">():</span>
    <span class="c1"># Create the General Manager instance using 'async with'
</span>    <span class="c1"># This handles setup (like starting a browser if needed)
</span>    <span class="c1"># and cleanup (closing the browser).
</span>    <span class="k">async</span> <span class="k">with</span> <span class="n">AsyncWebCrawler</span><span class="p">()</span> <span class="k">as</span> <span class="n">crawler</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="s">"Crawler is ready!"</span><span class="p">)</span>

        <span class="c1"># Tell the manager to crawl a specific URL
</span>        <span class="n">url_to_crawl</span> <span class="o">=</span> <span class="s">"https://httpbin.org/html"</span> <span class="c1"># A simple example page
</span>        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Asking the crawler to fetch: </span><span class="si">{</span><span class="n">url_to_crawl</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>

        <span class="n">result</span> <span class="o">=</span> <span class="k">await</span> <span class="n">crawler</span><span class="p">.</span><span class="n">arun</span><span class="p">(</span><span class="n">url</span><span class="o">=</span><span class="n">url_to_crawl</span><span class="p">)</span>

        <span class="c1"># Check if the crawl was successful
</span>        <span class="k">if</span> <span class="n">result</span><span class="p">.</span><span class="n">success</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="s">"</span><span class="se">\n</span><span class="s">Success! Crawler got the content."</span><span class="p">)</span>
            <span class="c1"># The result object contains the processed data
</span>            <span class="c1"># We'll learn more about CrawlResult in Chapter 7
</span>            <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Page Title: </span><span class="si">{</span><span class="n">result</span><span class="p">.</span><span class="n">metadata</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">'title'</span><span class="p">,</span> <span class="s">'N/A'</span><span class="p">)</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
            <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"First 100 chars of Markdown: </span><span class="si">{</span><span class="n">result</span><span class="p">.</span><span class="n">markdown</span><span class="p">.</span><span class="n">raw_markdown</span><span class="p">[</span><span class="si">:</span><span class="mi">100</span><span class="p">]</span><span class="si">}</span><span class="s">..."</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"</span><span class="se">\n</span><span class="s">Failed to crawl: </span><span class="si">{</span><span class="n">result</span><span class="p">.</span><span class="n">error_message</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>

<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="s">"__main__"</span><span class="p">:</span>
    <span class="n">asyncio</span><span class="p">.</span><span class="n">run</span><span class="p">(</span><span class="n">main</span><span class="p">())</span>
</code></pre></div></div> <p><strong>Explanation:</strong></p> <ol> <li><strong><code class="language-plaintext highlighter-rouge">import AsyncWebCrawler</code></strong>: We import the main class.</li> <li><strong><code class="language-plaintext highlighter-rouge">async def main():</code></strong>: Crawl4AI uses Python’s <code class="language-plaintext highlighter-rouge">asyncio</code> for efficiency, so our code needs to be in an <code class="language-plaintext highlighter-rouge">async</code> function.</li> <li><strong><code class="language-plaintext highlighter-rouge">async with AsyncWebCrawler() as crawler:</code></strong>: This is the standard way to create and manage the crawler. The <code class="language-plaintext highlighter-rouge">async with</code> statement ensures that resources (like the underlying browser used by the default <code class="language-plaintext highlighter-rouge">AsyncPlaywrightCrawlerStrategy</code>) are properly started and stopped, even if errors occur.</li> <li><strong><code class="language-plaintext highlighter-rouge">crawler.arun(url=url_to_crawl)</code></strong>: This is the core command. We tell our <code class="language-plaintext highlighter-rouge">crawler</code> instance (the General Manager) to run (<code class="language-plaintext highlighter-rouge">arun</code>) the crawling process for the specified <code class="language-plaintext highlighter-rouge">url</code>. <code class="language-plaintext highlighter-rouge">await</code> is used because fetching webpages takes time, and <code class="language-plaintext highlighter-rouge">asyncio</code> allows other tasks to run while waiting.</li> <li><strong><code class="language-plaintext highlighter-rouge">result</code></strong>: The <code class="language-plaintext highlighter-rouge">arun</code> method returns a <code class="language-plaintext highlighter-rouge">CrawlResult</code> object. This object contains all the information gathered during the crawl (HTML, cleaned text, metadata, etc.). We’ll explore this object in detail in <a href="07_crawlresult.md">Chapter 7: Understanding the Results - CrawlResult</a>.</li> <li><strong><code class="language-plaintext highlighter-rouge">result.success</code></strong>: We check this boolean flag to see if the crawl completed without critical errors.</li> <li><strong>Accessing Data:</strong> If successful, we can access processed information like the page title (<code class="language-plaintext highlighter-rouge">result.metadata['title']</code>) or the content formatted as Markdown (<code class="language-plaintext highlighter-rouge">result.markdown.raw_markdown</code>).</li> </ol> <h2 id="configuring-the-crawl"> <a href="#configuring-the-crawl" class="anchor-heading" aria-labelledby="configuring-the-crawl"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Configuring the Crawl </h2> <p>Sometimes, the default behavior isn’t quite what you need. Maybe you want to use the faster “drone” strategy from Chapter 1, or perhaps you want to ensure you <em>always</em> fetch a fresh copy of the page, ignoring any saved cache.</p> <p>You can customize the behavior of a specific <code class="language-plaintext highlighter-rouge">arun</code> call by passing a <code class="language-plaintext highlighter-rouge">CrawlerRunConfig</code> object. Think of this as giving specific instructions to the General Manager for <em>this particular job</em>.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># chapter2_example_2.py
</span><span class="kn">import</span> <span class="nn">asyncio</span>
<span class="kn">from</span> <span class="nn">crawl4ai</span> <span class="kn">import</span> <span class="n">AsyncWebCrawler</span>
<span class="kn">from</span> <span class="nn">crawl4ai</span> <span class="kn">import</span> <span class="n">CrawlerRunConfig</span> <span class="c1"># Import configuration class
</span><span class="kn">from</span> <span class="nn">crawl4ai</span> <span class="kn">import</span> <span class="n">CacheMode</span> <span class="c1"># Import cache options
</span>
<span class="k">async</span> <span class="k">def</span> <span class="nf">main</span><span class="p">():</span>
    <span class="k">async</span> <span class="k">with</span> <span class="n">AsyncWebCrawler</span><span class="p">()</span> <span class="k">as</span> <span class="n">crawler</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="s">"Crawler is ready!"</span><span class="p">)</span>
        <span class="n">url_to_crawl</span> <span class="o">=</span> <span class="s">"https://httpbin.org/html"</span>

        <span class="c1"># Create a specific configuration for this run
</span>        <span class="c1"># Tell the crawler to BYPASS the cache (fetch fresh)
</span>        <span class="n">run_config</span> <span class="o">=</span> <span class="n">CrawlerRunConfig</span><span class="p">(</span>
            <span class="n">cache_mode</span><span class="o">=</span><span class="n">CacheMode</span><span class="p">.</span><span class="n">BYPASS</span>
        <span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="s">"Configuration: Bypass cache for this run."</span><span class="p">)</span>

        <span class="c1"># Pass the config object to the arun method
</span>        <span class="n">result</span> <span class="o">=</span> <span class="k">await</span> <span class="n">crawler</span><span class="p">.</span><span class="n">arun</span><span class="p">(</span>
            <span class="n">url</span><span class="o">=</span><span class="n">url_to_crawl</span><span class="p">,</span>
            <span class="n">config</span><span class="o">=</span><span class="n">run_config</span> <span class="c1"># Pass the specific instructions
</span>        <span class="p">)</span>

        <span class="k">if</span> <span class="n">result</span><span class="p">.</span><span class="n">success</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="s">"</span><span class="se">\n</span><span class="s">Success! Crawler got fresh content (cache bypassed)."</span><span class="p">)</span>
            <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Page Title: </span><span class="si">{</span><span class="n">result</span><span class="p">.</span><span class="n">metadata</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">'title'</span><span class="p">,</span> <span class="s">'N/A'</span><span class="p">)</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"</span><span class="se">\n</span><span class="s">Failed to crawl: </span><span class="si">{</span><span class="n">result</span><span class="p">.</span><span class="n">error_message</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>

<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="s">"__main__"</span><span class="p">:</span>
    <span class="n">asyncio</span><span class="p">.</span><span class="n">run</span><span class="p">(</span><span class="n">main</span><span class="p">())</span>
</code></pre></div></div> <p><strong>Explanation:</strong></p> <ol> <li><strong><code class="language-plaintext highlighter-rouge">from crawl4ai import CrawlerRunConfig, CacheMode</code></strong>: We import the necessary classes for configuration.</li> <li><strong><code class="language-plaintext highlighter-rouge">run_config = CrawlerRunConfig(...)</code></strong>: We create an instance of <code class="language-plaintext highlighter-rouge">CrawlerRunConfig</code>. This object holds various settings for a specific crawl job.</li> <li><strong><code class="language-plaintext highlighter-rouge">cache_mode=CacheMode.BYPASS</code></strong>: We set the <code class="language-plaintext highlighter-rouge">cache_mode</code>. <code class="language-plaintext highlighter-rouge">CacheMode.BYPASS</code> tells the crawler to ignore any previously saved results for this URL and fetch it directly from the web server. We’ll learn all about caching options in <a href="09_cachecontext___cachemode.md">Chapter 9: Smart Fetching with Caching - CacheContext / CacheMode</a>.</li> <li><strong><code class="language-plaintext highlighter-rouge">crawler.arun(..., config=run_config)</code></strong>: We pass our custom <code class="language-plaintext highlighter-rouge">run_config</code> object to the <code class="language-plaintext highlighter-rouge">arun</code> method using the <code class="language-plaintext highlighter-rouge">config</code> parameter.</li> </ol> <p>The <code class="language-plaintext highlighter-rouge">CrawlerRunConfig</code> is very powerful and lets you control many aspects of the crawl, including which scraping or extraction methods to use. We’ll dive deep into it in the next chapter: <a href="03_crawlerrunconfig.md">Chapter 3: Giving Instructions - CrawlerRunConfig</a>.</p> <h2 id="what-happens-when-you-call-arun-the-flow"> <a href="#what-happens-when-you-call-arun-the-flow" class="anchor-heading" aria-labelledby="what-happens-when-you-call-arun-the-flow"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> What Happens When You Call <code class="language-plaintext highlighter-rouge">arun</code>? (The Flow) </h2> <p>When you call <code class="language-plaintext highlighter-rouge">crawler.arun(url="...")</code>, the <code class="language-plaintext highlighter-rouge">AsyncWebCrawler</code> (our General Manager) springs into action and coordinates several steps behind the scenes:</p><pre><code class="language-mermaid">sequenceDiagram
    participant U as User
    participant AWC as AsyncWebCrawler (Manager)
    participant CC as Cache Check
    participant CS as AsyncCrawlerStrategy (Fetcher)
    participant SP as Scraping/Processing
    participant CR as CrawlResult (Final Report)

    U-&gt;&gt;AWC: arun("https://example.com", config)
    AWC-&gt;&gt;CC: Need content for "https://example.com"? (Respect CacheMode in config)
    alt Cache Hit &amp; Cache Mode allows reading
        CC--&gt;&gt;AWC: Yes, here's the cached result.
        AWC--&gt;&gt;CR: Package cached result.
        AWC--&gt;&gt;U: Here is the CrawlResult
    else Cache Miss or Cache Mode prevents reading
        CC--&gt;&gt;AWC: No cached result / Cannot read cache.
        AWC-&gt;&gt;CS: Please fetch "https://example.com" (using configured strategy)
        CS--&gt;&gt;AWC: Here's the raw response (HTML, etc.)
        AWC-&gt;&gt;SP: Process this raw content (Scrape, Filter, Extract based on config)
        SP--&gt;&gt;AWC: Here's the processed data (Markdown, Metadata, etc.)
        AWC-&gt;&gt;CC: Cache this result? (Respect CacheMode in config)
        CC--&gt;&gt;AWC: OK, cached.
        AWC--&gt;&gt;CR: Package new result.
        AWC--&gt;&gt;U: Here is the CrawlResult
    end

</code></pre><p><strong>Simplified Steps:</strong></p> <ol> <li><strong>Receive Request:</strong> The <code class="language-plaintext highlighter-rouge">AsyncWebCrawler</code> gets the URL and configuration from your <code class="language-plaintext highlighter-rouge">arun</code> call.</li> <li><strong>Check Cache:</strong> It checks if a valid result for this URL is already saved (cached) and if the <code class="language-plaintext highlighter-rouge">CacheMode</code> allows using it. (See <a href="09_cachecontext___cachemode.md">Chapter 9</a>).</li> <li><strong>Fetch (if needed):</strong> If no valid cached result exists or caching is bypassed, it asks the configured <a href="01_asynccrawlerstrategy.md">AsyncCrawlerStrategy</a> (e.g., Playwright or HTTP) to fetch the raw page content.</li> <li><strong>Process Content:</strong> It takes the raw HTML and passes it through various processing steps based on the configuration: <ul> <li><strong>Scraping:</strong> Cleaning up HTML, extracting basic structure using a <a href="04_contentscrapingstrategy.md">ContentScrapingStrategy</a>.</li> <li><strong>Filtering:</strong> Optionally filtering content for relevance using a <a href="05_relevantcontentfilter.md">RelevantContentFilter</a>.</li> <li><strong>Extraction:</strong> Optionally extracting specific structured data using an <a href="06_extractionstrategy.md">ExtractionStrategy</a>.</li> </ul> </li> <li><strong>Cache Result (if needed):</strong> If caching is enabled for writing, it saves the final processed result.</li> <li><strong>Return Result:</strong> It bundles everything into a <a href="07_crawlresult.md">CrawlResult</a> object and returns it to you.</li> </ol> <h2 id="crawling-many-pages-arun_many"> <a href="#crawling-many-pages-arun_many" class="anchor-heading" aria-labelledby="crawling-many-pages-arun_many"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Crawling Many Pages: <code class="language-plaintext highlighter-rouge">arun_many</code> </h2> <p>What if you have a whole list of URLs to crawl? Calling <code class="language-plaintext highlighter-rouge">arun</code> in a loop works, but it might not be the most efficient way. <code class="language-plaintext highlighter-rouge">AsyncWebCrawler</code> provides the <code class="language-plaintext highlighter-rouge">arun_many</code> method designed for this.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># chapter2_example_3.py
</span><span class="kn">import</span> <span class="nn">asyncio</span>
<span class="kn">from</span> <span class="nn">crawl4ai</span> <span class="kn">import</span> <span class="n">AsyncWebCrawler</span><span class="p">,</span> <span class="n">CrawlerRunConfig</span><span class="p">,</span> <span class="n">CacheMode</span>

<span class="k">async</span> <span class="k">def</span> <span class="nf">main</span><span class="p">():</span>
    <span class="k">async</span> <span class="k">with</span> <span class="n">AsyncWebCrawler</span><span class="p">()</span> <span class="k">as</span> <span class="n">crawler</span><span class="p">:</span>
        <span class="n">urls_to_crawl</span> <span class="o">=</span> <span class="p">[</span>
            <span class="s">"https://httpbin.org/html"</span><span class="p">,</span>
            <span class="s">"https://httpbin.org/links/10/0"</span><span class="p">,</span>
            <span class="s">"https://httpbin.org/robots.txt"</span>
        <span class="p">]</span>
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Asking crawler to fetch </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">urls_to_crawl</span><span class="p">)</span><span class="si">}</span><span class="s"> URLs."</span><span class="p">)</span>

        <span class="c1"># Use arun_many for multiple URLs
</span>        <span class="c1"># We can still pass a config that applies to all URLs in the batch
</span>        <span class="n">config</span> <span class="o">=</span> <span class="n">CrawlerRunConfig</span><span class="p">(</span><span class="n">cache_mode</span><span class="o">=</span><span class="n">CacheMode</span><span class="p">.</span><span class="n">BYPASS</span><span class="p">)</span>
        <span class="n">results</span> <span class="o">=</span> <span class="k">await</span> <span class="n">crawler</span><span class="p">.</span><span class="n">arun_many</span><span class="p">(</span><span class="n">urls</span><span class="o">=</span><span class="n">urls_to_crawl</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">)</span>

        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"</span><span class="se">\n</span><span class="s">Finished crawling! Got </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">results</span><span class="p">)</span><span class="si">}</span><span class="s"> results."</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">result</span> <span class="ow">in</span> <span class="n">results</span><span class="p">:</span>
            <span class="n">status</span> <span class="o">=</span> <span class="s">"Success"</span> <span class="k">if</span> <span class="n">result</span><span class="p">.</span><span class="n">success</span> <span class="k">else</span> <span class="s">"Failed"</span>
            <span class="n">url_short</span> <span class="o">=</span> <span class="n">result</span><span class="p">.</span><span class="n">url</span><span class="p">.</span><span class="n">split</span><span class="p">(</span><span class="s">'/'</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="c1"># Get last part of URL
</span>            <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"- URL: </span><span class="si">{</span><span class="n">url_short</span><span class="si">:</span><span class="o">&lt;</span><span class="mi">10</span><span class="si">}</span><span class="s"> | Status: </span><span class="si">{</span><span class="n">status</span><span class="si">:</span><span class="o">&lt;</span><span class="mi">7</span><span class="si">}</span><span class="s"> | Title: </span><span class="si">{</span><span class="n">result</span><span class="p">.</span><span class="n">metadata</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">'title'</span><span class="p">,</span> <span class="s">'N/A'</span><span class="p">)</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>

<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="s">"__main__"</span><span class="p">:</span>
    <span class="n">asyncio</span><span class="p">.</span><span class="n">run</span><span class="p">(</span><span class="n">main</span><span class="p">())</span>
</code></pre></div></div> <p><strong>Explanation:</strong></p> <ol> <li><strong><code class="language-plaintext highlighter-rouge">urls_to_crawl = [...]</code></strong>: We define a list of URLs.</li> <li><strong><code class="language-plaintext highlighter-rouge">await crawler.arun_many(urls=urls_to_crawl, config=config)</code></strong>: We call <code class="language-plaintext highlighter-rouge">arun_many</code>, passing the list of URLs. It handles crawling them concurrently (like dispatching multiple delivery trucks or drones efficiently).</li> <li><strong><code class="language-plaintext highlighter-rouge">results</code></strong>: <code class="language-plaintext highlighter-rouge">arun_many</code> returns a list where each item is a <code class="language-plaintext highlighter-rouge">CrawlResult</code> object corresponding to one of the input URLs.</li> </ol> <p><code class="language-plaintext highlighter-rouge">arun_many</code> is much more efficient for batch processing as it leverages <code class="language-plaintext highlighter-rouge">asyncio</code> to handle multiple fetches and processing tasks concurrently. It uses a <a href="10_basedispatcher.md">BaseDispatcher</a> internally to manage this concurrency.</p> <h2 id="under-the-hood-a-peek-at-the-code"> <a href="#under-the-hood-a-peek-at-the-code" class="anchor-heading" aria-labelledby="under-the-hood-a-peek-at-the-code"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Under the Hood (A Peek at the Code) </h2> <p>You don’t need to know the internal details to use <code class="language-plaintext highlighter-rouge">AsyncWebCrawler</code>, but seeing the structure can help. Inside the <code class="language-plaintext highlighter-rouge">crawl4ai</code> library, the file <code class="language-plaintext highlighter-rouge">async_webcrawler.py</code> defines this class.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Simplified from async_webcrawler.py
</span>
<span class="c1"># ... imports ...
</span><span class="kn">from</span> <span class="nn">.async_crawler_strategy</span> <span class="kn">import</span> <span class="n">AsyncCrawlerStrategy</span><span class="p">,</span> <span class="n">AsyncPlaywrightCrawlerStrategy</span>
<span class="kn">from</span> <span class="nn">.async_configs</span> <span class="kn">import</span> <span class="n">BrowserConfig</span><span class="p">,</span> <span class="n">CrawlerRunConfig</span>
<span class="kn">from</span> <span class="nn">.models</span> <span class="kn">import</span> <span class="n">CrawlResult</span>
<span class="kn">from</span> <span class="nn">.cache_context</span> <span class="kn">import</span> <span class="n">CacheContext</span><span class="p">,</span> <span class="n">CacheMode</span>
<span class="c1"># ... other strategy imports ...
</span>
<span class="k">class</span> <span class="nc">AsyncWebCrawler</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">crawler_strategy</span><span class="p">:</span> <span class="n">AsyncCrawlerStrategy</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span> <span class="c1"># You can provide a strategy...
</span>        <span class="n">config</span><span class="p">:</span> <span class="n">BrowserConfig</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span> <span class="c1"># Configuration for the browser
</span>        <span class="c1"># ... other parameters like logger, base_directory ...
</span>    <span class="p">):</span>
        <span class="c1"># If no strategy is given, it defaults to Playwright (the 'truck')
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">crawler_strategy</span> <span class="o">=</span> <span class="n">crawler_strategy</span> <span class="ow">or</span> <span class="n">AsyncPlaywrightCrawlerStrategy</span><span class="p">(...)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">browser_config</span> <span class="o">=</span> <span class="n">config</span> <span class="ow">or</span> <span class="n">BrowserConfig</span><span class="p">()</span>
        <span class="c1"># ... setup logger, directories, etc. ...
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">ready</span> <span class="o">=</span> <span class="bp">False</span> <span class="c1"># Flag to track if setup is complete
</span>
    <span class="k">async</span> <span class="k">def</span> <span class="nf">__aenter__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># This is called when you use 'async with'. It starts the strategy.
</span>        <span class="k">await</span> <span class="bp">self</span><span class="p">.</span><span class="n">crawler_strategy</span><span class="p">.</span><span class="n">__aenter__</span><span class="p">()</span>
        <span class="k">await</span> <span class="bp">self</span><span class="p">.</span><span class="n">awarmup</span><span class="p">()</span> <span class="c1"># Perform internal setup
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">ready</span> <span class="o">=</span> <span class="bp">True</span>
        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">async</span> <span class="k">def</span> <span class="nf">__aexit__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">exc_type</span><span class="p">,</span> <span class="n">exc_val</span><span class="p">,</span> <span class="n">exc_tb</span><span class="p">):</span>
        <span class="c1"># This is called when exiting 'async with'. It cleans up.
</span>        <span class="k">await</span> <span class="bp">self</span><span class="p">.</span><span class="n">crawler_strategy</span><span class="p">.</span><span class="n">__aexit__</span><span class="p">(</span><span class="n">exc_type</span><span class="p">,</span> <span class="n">exc_val</span><span class="p">,</span> <span class="n">exc_tb</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">ready</span> <span class="o">=</span> <span class="bp">False</span>

    <span class="k">async</span> <span class="k">def</span> <span class="nf">arun</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">url</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">CrawlerRunConfig</span> <span class="o">=</span> <span class="bp">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">CrawlResult</span><span class="p">:</span>
        <span class="c1"># 1. Ensure config exists, set defaults (like CacheMode.ENABLED)
</span>        <span class="n">crawler_config</span> <span class="o">=</span> <span class="n">config</span> <span class="ow">or</span> <span class="n">CrawlerRunConfig</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">crawler_config</span><span class="p">.</span><span class="n">cache_mode</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">crawler_config</span><span class="p">.</span><span class="n">cache_mode</span> <span class="o">=</span> <span class="n">CacheMode</span><span class="p">.</span><span class="n">ENABLED</span>

        <span class="c1"># 2. Create CacheContext to manage caching logic
</span>        <span class="n">cache_context</span> <span class="o">=</span> <span class="n">CacheContext</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">crawler_config</span><span class="p">.</span><span class="n">cache_mode</span><span class="p">)</span>

        <span class="c1"># 3. Try reading from cache if allowed
</span>        <span class="n">cached_result</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="k">if</span> <span class="n">cache_context</span><span class="p">.</span><span class="n">should_read</span><span class="p">():</span>
            <span class="n">cached_result</span> <span class="o">=</span> <span class="k">await</span> <span class="n">async_db_manager</span><span class="p">.</span><span class="n">aget_cached_url</span><span class="p">(</span><span class="n">url</span><span class="p">)</span>

        <span class="c1"># 4. If cache hit and valid, return cached result
</span>        <span class="k">if</span> <span class="n">cached_result</span> <span class="ow">and</span> <span class="bp">self</span><span class="p">.</span><span class="n">_is_cache_valid</span><span class="p">(</span><span class="n">cached_result</span><span class="p">,</span> <span class="n">crawler_config</span><span class="p">):</span>
             <span class="c1"># ... log cache hit ...
</span>             <span class="k">return</span> <span class="n">cached_result</span>

        <span class="c1"># 5. If no cache hit or cache invalid/bypassed: Fetch fresh content
</span>        <span class="c1">#    Delegate to the configured AsyncCrawlerStrategy
</span>        <span class="n">async_response</span> <span class="o">=</span> <span class="k">await</span> <span class="bp">self</span><span class="p">.</span><span class="n">crawler_strategy</span><span class="p">.</span><span class="n">crawl</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">crawler_config</span><span class="p">)</span>

        <span class="c1"># 6. Process the HTML (scrape, filter, extract)
</span>        <span class="c1">#    This involves calling other strategies based on config
</span>        <span class="n">crawl_result</span> <span class="o">=</span> <span class="k">await</span> <span class="bp">self</span><span class="p">.</span><span class="n">aprocess_html</span><span class="p">(</span>
            <span class="n">url</span><span class="o">=</span><span class="n">url</span><span class="p">,</span>
            <span class="n">html</span><span class="o">=</span><span class="n">async_response</span><span class="p">.</span><span class="n">html</span><span class="p">,</span>
            <span class="n">config</span><span class="o">=</span><span class="n">crawler_config</span><span class="p">,</span>
            <span class="c1"># ... other details from async_response ...
</span>        <span class="p">)</span>

        <span class="c1"># 7. Write to cache if allowed
</span>        <span class="k">if</span> <span class="n">cache_context</span><span class="p">.</span><span class="n">should_write</span><span class="p">():</span>
            <span class="k">await</span> <span class="n">async_db_manager</span><span class="p">.</span><span class="n">acache_url</span><span class="p">(</span><span class="n">crawl_result</span><span class="p">)</span>

        <span class="c1"># 8. Return the final CrawlResult
</span>        <span class="k">return</span> <span class="n">crawl_result</span>

    <span class="k">async</span> <span class="k">def</span> <span class="nf">aprocess_html</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">url</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">html</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">CrawlerRunConfig</span><span class="p">,</span> <span class="p">...)</span> <span class="o">-&gt;</span> <span class="n">CrawlResult</span><span class="p">:</span>
        <span class="c1"># This internal method handles:
</span>        <span class="c1"># - Getting the configured ContentScrapingStrategy
</span>        <span class="c1"># - Calling its 'scrap' method
</span>        <span class="c1"># - Getting the configured MarkdownGenerationStrategy
</span>        <span class="c1"># - Calling its 'generate_markdown' method
</span>        <span class="c1"># - Getting the configured ExtractionStrategy (if any)
</span>        <span class="c1"># - Calling its 'run' method
</span>        <span class="c1"># - Packaging everything into a CrawlResult
</span>        <span class="c1"># ... implementation details ...
</span>        <span class="k">pass</span> <span class="c1"># Simplified
</span>
    <span class="k">async</span> <span class="k">def</span> <span class="nf">arun_many</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">urls</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="n">config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">CrawlerRunConfig</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span> <span class="p">...)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">CrawlResult</span><span class="p">]:</span>
        <span class="c1"># Uses a Dispatcher (like MemoryAdaptiveDispatcher)
</span>        <span class="c1"># to run self.arun for each URL concurrently.
</span>        <span class="c1"># ... implementation details using a dispatcher ...
</span>        <span class="k">pass</span> <span class="c1"># Simplified
</span>
    <span class="c1"># ... other methods like awarmup, close, caching helpers ...
</span></code></pre></div></div> <p>The key takeaway is that <code class="language-plaintext highlighter-rouge">AsyncWebCrawler</code> doesn’t do the fetching or detailed processing <em>itself</em>. It acts as the central hub, coordinating calls to the various specialized <code class="language-plaintext highlighter-rouge">Strategy</code> classes based on the provided configuration.</p> <h2 id="conclusion"> <a href="#conclusion" class="anchor-heading" aria-labelledby="conclusion"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Conclusion </h2> <p>You’ve met the General Manager: <code class="language-plaintext highlighter-rouge">AsyncWebCrawler</code>!</p> <ul> <li>It’s the <strong>main entry point</strong> for using Crawl4AI.</li> <li>It <strong>coordinates</strong> all the steps: fetching, caching, scraping, extracting.</li> <li>You primarily interact with it using <code class="language-plaintext highlighter-rouge">async with</code> and the <code class="language-plaintext highlighter-rouge">arun()</code> (single URL) or <code class="language-plaintext highlighter-rouge">arun_many()</code> (multiple URLs) methods.</li> <li>It takes a URL and an optional <code class="language-plaintext highlighter-rouge">CrawlerRunConfig</code> object to customize the crawl.</li> <li>It returns a comprehensive <code class="language-plaintext highlighter-rouge">CrawlResult</code> object.</li> </ul> <p>Now that you understand the central role of <code class="language-plaintext highlighter-rouge">AsyncWebCrawler</code>, let’s explore how to give it detailed instructions for each crawling job.</p> <p><strong>Next:</strong> Let’s dive into the specifics of configuration with <a href="03_crawlerrunconfig.md">Chapter 3: Giving Instructions - CrawlerRunConfig</a>.</p><hr /> <p>Generated by <a href="https://github.com/The-Pocket/Tutorial-Codebase-Knowledge">AI Codebase Knowledge Builder</a></p> </main> <hr> <footer> <p class="text-small text-grey-dk-100 mb-0">Copyright &copy; 2023 Sam Kirk</p> </footer> </div> </div> <div class="search-overlay"></div> </div> <script type="module"> import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.6.0/dist/mermaid.esm.min.mjs'; var config = {} ; mermaid.initialize(config); mermaid.run({ querySelector: '.language-mermaid', }); </script> </body> </html>
