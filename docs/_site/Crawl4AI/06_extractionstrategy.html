<!DOCTYPE html> <html lang="en-US"> <head> <meta charset="UTF-8"> <meta http-equiv="X-UA-Compatible" content="IE=Edge"> <link rel="stylesheet" href="/assets/css/just-the-docs-default.css"> <script src="/assets/js/vendor/lunr.min.js"></script> <script src="/assets/js/just-the-docs.js"></script> <meta name="viewport" content="width=device-width, initial-scale=1"> <!-- Begin Jekyll SEO tag v2.8.0 --> <title>ExtractionStrategy | Two Tutorials for Mojo using Pocket Flow</title> <meta name="generator" content="Jekyll v4.3.4" /> <meta property="og:title" content="ExtractionStrategy" /> <meta name="author" content="Sam Kirk" /> <meta property="og:locale" content="en_US" /> <meta name="description" content="Documentation generated using AI to explain codebases" /> <meta property="og:description" content="Documentation generated using AI to explain codebases" /> <link rel="canonical" href="http://localhost:4000/Crawl4AI/06_extractionstrategy.html" /> <meta property="og:url" content="http://localhost:4000/Crawl4AI/06_extractionstrategy.html" /> <meta property="og:site_name" content="Two Tutorials for Mojo using Pocket Flow" /> <meta property="og:type" content="website" /> <meta name="twitter:card" content="summary" /> <meta property="twitter:title" content="ExtractionStrategy" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"WebPage","author":{"@type":"Person","name":"Sam Kirk","url":"https://www.columbia.edu/~zh2408/"},"description":"Documentation generated using AI to explain codebases","headline":"ExtractionStrategy","url":"http://localhost:4000/Crawl4AI/06_extractionstrategy.html"}</script> <!-- End Jekyll SEO tag --> <!-- Add Mermaid support --> <script src="https://cdn.jsdelivr.net/npm/mermaid@11.6.0/dist/mermaid.min.js"></script> <script> document.addEventListener("DOMContentLoaded", function() { mermaid.initialize({ startOnLoad: true, theme: "default" }); // Process code blocks document.querySelectorAll('pre code.language-mermaid').forEach(function(block) { // Create a div with class 'mermaid' var mermaidDiv = document.createElement('div'); mermaidDiv.className = 'mermaid'; mermaidDiv.innerHTML = block.textContent; // Replace the parent pre with the mermaid div block.parentNode.parentNode.replaceChild(mermaidDiv, block.parentNode); console.log("Processed Mermaid block:", mermaidDiv.innerHTML.substring(0, 50) + "..."); }); console.log("Mermaid initialization complete. Version:", mermaid.version()); }); </script> </head> <body> <a class="skip-to-main" href="#main-content">Skip to main content</a> <svg xmlns="http://www.w3.org/2000/svg" class="d-none"> <symbol id="svg-link" viewBox="0 0 24 24"> <title>Link</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-link"> <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path> </svg> </symbol> <symbol id="svg-menu" viewBox="0 0 24 24"> <title>Menu</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu"> <line x1="3" y1="12" x2="21" y2="12"></line><line x1="3" y1="6" x2="21" y2="6"></line><line x1="3" y1="18" x2="21" y2="18"></line> </svg> </symbol> <symbol id="svg-arrow-right" viewBox="0 0 24 24"> <title>Expand</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-chevron-right"> <polyline points="9 18 15 12 9 6"></polyline> </svg> </symbol> <!-- Feather. MIT License: https://github.com/feathericons/feather/blob/master/LICENSE --> <symbol id="svg-external-link" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-external-link"> <title id="svg-external-link-title">(external link)</title> <path d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"></path><polyline points="15 3 21 3 21 9"></polyline><line x1="10" y1="14" x2="21" y2="3"></line> </symbol> <symbol id="svg-doc" viewBox="0 0 24 24"> <title>Document</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file"> <path d="M13 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V9z"></path><polyline points="13 2 13 9 20 9"></polyline> </svg> </symbol> <symbol id="svg-search" viewBox="0 0 24 24"> <title>Search</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-search"> <circle cx="11" cy="11" r="8"></circle><line x1="21" y1="21" x2="16.65" y2="16.65"></line> </svg> </symbol> <!-- Bootstrap Icons. MIT License: https://github.com/twbs/icons/blob/main/LICENSE.md --> <symbol id="svg-copy" viewBox="0 0 16 16"> <title>Copy</title> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard" viewBox="0 0 16 16"> <path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1h1a1 1 0 0 1 1 1V14a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V3.5a1 1 0 0 1 1-1h1v-1z"/> <path d="M9.5 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3zm-3-1A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3z"/> </svg> </symbol> <symbol id="svg-copied" viewBox="0 0 16 16"> <title>Copied</title> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard-check-fill" viewBox="0 0 16 16"> <path d="M6.5 0A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3Zm3 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3Z"/> <path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1A2.5 2.5 0 0 1 9.5 5h-3A2.5 2.5 0 0 1 4 2.5v-1Zm6.854 7.354-3 3a.5.5 0 0 1-.708 0l-1.5-1.5a.5.5 0 0 1 .708-.708L7.5 10.793l2.646-2.647a.5.5 0 0 1 .708.708Z"/> </svg> </symbol> </svg> <div class="side-bar"> <div class="site-header" role="banner"> <a href="/" class="site-title lh-tight"> Two Tutorials for Mojo using Pocket Flow </a> <button id="menu-button" class="site-button btn-reset" aria-label="Toggle menu" aria-pressed="false"> <svg viewBox="0 0 24 24" class="icon" aria-hidden="true"><use xlink:href="#svg-menu"></use></svg> </a> </div> <nav aria-label="Main" id="site-nav" class="site-nav"> <ul class="nav-list"><li class="nav-list-item"><a href="/" class="nav-list-link">Home</a></li><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in My Tutorial for Crawl4ai category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button><a href="/my-crawl4AI/" class="nav-list-link">My Tutorial for Crawl4ai</a><ul class="nav-list"><li class="nav-list-item "><a href="/my-crawl4ai/01_configuration_system_.html" class="nav-list-link">Chapter 1: Configuration System</a></li><li class="nav-list-item "><a href="/my-crawl4ai/02_asyncwebcrawler_.html" class="nav-list-link">Chapter 2: AsyncWebCrawler</a></li><li class="nav-list-item "><a href="/my-crawl4ai/03_content_extraction_pipeline_.html" class="nav-list-link">Chapter 3: Content Extraction Pipeline</a></li><li class="nav-list-item "><a href="/my-crawl4ai/04_url_filtering___scoring_.html" class="nav-list-link">Chapter 4: URL Filtering & Scoring</a></li><li class="nav-list-item "><a href="/my-crawl4ai/05_deep_crawling_system_.html" class="nav-list-link">Chapter 5: Deep Crawling System</a></li><li class="nav-list-item "><a href="/my-crawl4ai/06_caching_system_.html" class="nav-list-link">Chapter 6: Caching System</a></li><li class="nav-list-item "><a href="/my-crawl4ai/07_dispatcher_framework_.html" class="nav-list-link">Chapter 7: Dispatcher Framework</a></li><li class="nav-list-item "><a href="/my-crawl4ai/08_async_logging_infrastructure_.html" class="nav-list-link">Chapter 8: Async Logging Infrastructure</a></li><li class="nav-list-item "><a href="/my-crawl4ai/09_api___docker_integration_.html" class="nav-list-link">Chapter 9: API & Docker Integration</a></li></ul></li><li class="nav-list-item"><a href="/design.html" class="nav-list-link">System Design</a></li><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in My Tutorial for Modular's Max category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button><a href="/modular_max/" class="nav-list-link">My Tutorial for Modular's Max</a><ul class="nav-list"><li class="nav-list-item "><a href="/modular_max/01_settings___settings__class__.html" class="nav-list-link">Chapter 1: Settings Class</a></li><li class="nav-list-item "><a href="/modular_max/02_serving_api_layer__fastapi_app___routers__.html" class="nav-list-link">Chapter 2: Serving API Layer</a></li><li class="nav-list-item "><a href="/modular_max/03_llm_pipeline_orchestrator___tokengeneratorpipeline___.html" class="nav-list-link">Chapter 3: LLM Pipeline Orchestrator</a></li><li class="nav-list-item "><a href="/modular_max/04_model_worker_.html" class="nav-list-link">Chapter 4: Model Worker</a></li><li class="nav-list-item "><a href="/modular_max/05_scheduler___tokengenerationscheduler____embeddingsscheduler___.html" class="nav-list-link">Chapter 5: Scheduler</a></li><li class="nav-list-item "><a href="/modular_max/06_kv_cache_management_.html" class="nav-list-link">Chapter 6: KV Cache Management</a></li><li class="nav-list-item "><a href="/modular_max/07_enginequeue_.html" class="nav-list-link">Chapter 7: EngineQueue</a></li><li class="nav-list-item "><a href="/modular_max/08_telemetry_and_metrics___metrics____metricclient___.html" class="nav-list-link">Chapter 8: Telemetry and Metrics</a></li></ul></li><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in My Tutorial for Mojo v1 category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button><a href="/mojo-v1/" class="nav-list-link">My Tutorial for Mojo v1</a><ul class="nav-list"><li class="nav-list-item "><a href="/mojo-v1/01_addressspace_.html" class="nav-list-link">Chapter 1: AddressSpace</a></li><li class="nav-list-item "><a href="/mojo-v1/02_unsafepointer_.html" class="nav-list-link">Chapter 2: UnsafePointer</a></li><li class="nav-list-item "><a href="/mojo-v1/03_indexlist_.html" class="nav-list-link">Chapter 3: IndexList</a></li><li class="nav-list-item "><a href="/mojo-v1/04_dimlist_.html" class="nav-list-link">Chapter 4: DimList</a></li><li class="nav-list-item "><a href="/mojo-v1/05_ndbuffer_.html" class="nav-list-link">Chapter 5: NDBuffer</a></li><li class="nav-list-item "><a href="/mojo-v1/06_n_d_to_1d_indexing_logic__strided_memory_access__.html" class="nav-list-link">Chapter 6: N-D to 1D Indexing Logic</a></li></ul></li><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in My Tutorial for Mojo v2 category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button><a href="/mojo-v2/" class="nav-list-link">My Tutorial for Mojo v2</a><ul class="nav-list"><li class="nav-list-item "><a href="/mojo-v2/01_unsafepointer__as_used_by_ndbuffer__.html" class="nav-list-link">Chapter 1: UnsafePointer</a></li><li class="nav-list-item "><a href="/mojo-v2/02_dimlist_and_dim_.html" class="nav-list-link">Chapter 2: DimList and Dim</a></li><li class="nav-list-item "><a href="/mojo-v2/03_ndbuffer_.html" class="nav-list-link">Chapter 3: NDBuffer</a></li><li class="nav-list-item "><a href="/mojo-v2/04_strides_and_offset_computation_.html" class="nav-list-link">Chapter 4: Strides and Offset Computation</a></li><li class="nav-list-item "><a href="/mojo-v2/05_simd_data_access_.html" class="nav-list-link">Chapter 5: SIMD Data Access</a></li></ul></li><li class="nav-list-item active"><button class="nav-list-expander btn-reset" aria-label="toggle items in Crawl4AI category" aria-pressed="true"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button><a href="/Crawl4AI/" class="nav-list-link">Crawl4AI</a><ul class="nav-list"><li class="nav-list-item "><a href="/Crawl4AI/01_asynccrawlerstrategy.html" class="nav-list-link">AsyncCrawlerStrategy</a></li><li class="nav-list-item "><a href="/Crawl4AI/01_configuration_system_.html" class="nav-list-link">Chapter 1: Configuration System</a></li><li class="nav-list-item "><a href="/Crawl4AI/02_asyncwebcrawler.html" class="nav-list-link">AsyncWebCrawler</a></li><li class="nav-list-item "><a href="/Crawl4AI/02_asyncwebcrawler_.html" class="nav-list-link">Chapter 2: AsyncWebCrawler</a></li><li class="nav-list-item "><a href="/Crawl4AI/03_content_extraction_pipeline_.html" class="nav-list-link">Chapter 3: Content Extraction Pipeline</a></li><li class="nav-list-item "><a href="/Crawl4AI/03_crawlerrunconfig.html" class="nav-list-link">CrawlerRunConfig</a></li><li class="nav-list-item "><a href="/Crawl4AI/04_contentscrapingstrategy.html" class="nav-list-link">ContentScrapingStrategy</a></li><li class="nav-list-item "><a href="/Crawl4AI/04_url_filtering___scoring_.html" class="nav-list-link">Chapter 4: URL Filtering & Scoring</a></li><li class="nav-list-item "><a href="/Crawl4AI/05_deep_crawling_system_.html" class="nav-list-link">Chapter 5: Deep Crawling System</a></li><li class="nav-list-item "><a href="/Crawl4AI/05_relevantcontentfilter.html" class="nav-list-link">RelevantContentFilter</a></li><li class="nav-list-item "><a href="/Crawl4AI/06_caching_system_.html" class="nav-list-link">Chapter 6: Caching System</a></li><li class="nav-list-item active"><a href="/Crawl4AI/06_extractionstrategy.html" class="nav-list-link active">ExtractionStrategy</a></li><li class="nav-list-item "><a href="/Crawl4AI/07_crawlresult.html" class="nav-list-link">CrawlResult</a></li><li class="nav-list-item "><a href="/Crawl4AI/07_dispatcher_framework_.html" class="nav-list-link">Chapter 7: Dispatcher Framework</a></li><li class="nav-list-item "><a href="/Crawl4AI/08_deepcrawlstrategy.html" class="nav-list-link">DeepCrawlStrategy</a></li><li class="nav-list-item "><a href="/Crawl4AI/08_async_logging_infrastructure_.html" class="nav-list-link">Chapter 8: Async Logging Infrastructure</a></li><li class="nav-list-item "><a href="/Crawl4AI/09_api___docker_integration_.html" class="nav-list-link">Chapter 9: API & Docker Integration</a></li><li class="nav-list-item "><a href="/Crawl4AI/09_cachecontext___cachemode.html" class="nav-list-link">CacheContext & CacheMode</a></li><li class="nav-list-item "><a href="/Crawl4AI/10_basedispatcher.html" class="nav-list-link">BaseDispatcher</a></li></ul></li></ul> <div class="nav-category">Crawl4AI</div> <ul class="nav-list"></ul> <div class="nav-category">Modular Max</div> <ul class="nav-list"></ul> <div class="nav-category">Mojo (v1)</div> <ul class="nav-list"></ul> <div class="nav-category">Mojo (v2)</div> <ul class="nav-list"></ul> </nav> <footer class="site-footer"> This site uses <a href="https://github.com/just-the-docs/just-the-docs">Just the Docs</a>, a documentation theme for Jekyll. </footer> </div> <div class="main" id="top"> <div id="main-header" class="main-header"> <div class="search" role="search"> <div class="search-input-wrap"> <input type="text" id="search-input" class="search-input" tabindex="0" placeholder="Search Two Tutorials for Mojo using Pocket Flow" aria-label="Search Two Tutorials for Mojo using Pocket Flow" autocomplete="off"> <label for="search-input" class="search-label"><svg viewBox="0 0 24 24" class="search-icon"><use xlink:href="#svg-search"></use></svg></label> </div> <div id="search-results" class="search-results"></div> </div> <nav aria-label="Auxiliary" class="aux-nav"> <ul class="aux-nav-list"> <li class="aux-nav-list-item"> <a href="https://github.com/MrBesterTester/pf-understand" class="site-button" > View on GitHub </a> </li> </ul> </nav> </div> <div id="main-content-wrap" class="main-content-wrap"> <nav aria-label="Breadcrumb" class="breadcrumb-nav"> <ol class="breadcrumb-nav-list"> <li class="breadcrumb-nav-list-item"><a href="/Crawl4AI/">Crawl4AI</a></li> <li class="breadcrumb-nav-list-item"><span>ExtractionStrategy</span></li> </ol> </nav> <div id="main-content" class="main-content"> <main> <h1 id="chapter-6-getting-specific-data---extractionstrategy"> <a href="#chapter-6-getting-specific-data---extractionstrategy" class="anchor-heading" aria-labelledby="chapter-6-getting-specific-data---extractionstrategy"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Chapter 6: Getting Specific Data - ExtractionStrategy </h1> <p>In the previous chapter, <a href="05_relevantcontentfilter.md">Chapter 5: Focusing on What Matters - RelevantContentFilter</a>, we learned how to sift through the cleaned webpage content to keep only the parts relevant to our query or goal, producing a focused <code class="language-plaintext highlighter-rouge">fit_markdown</code>. This is great for tasks like summarization or getting the main gist of an article.</p> <p>But sometimes, we need more than just relevant text. Imagine you’re analyzing an e-commerce website listing products. You don’t just want the <em>description</em>; you need the exact <strong>product name</strong>, the specific <strong>price</strong>, the <strong>customer rating</strong>, and maybe the <strong>SKU number</strong>, all neatly organized. How do we tell Crawl4AI to find these <em>specific</em> pieces of information and return them in a structured format, like a JSON object?</p> <h2 id="what-problem-does-extractionstrategy-solve"> <a href="#what-problem-does-extractionstrategy-solve" class="anchor-heading" aria-labelledby="what-problem-does-extractionstrategy-solve"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> What Problem Does <code class="language-plaintext highlighter-rouge">ExtractionStrategy</code> Solve? </h2> <p>Think of the content we’ve processed so far (like the cleaned HTML or the generated Markdown) as a detailed report delivered by a researcher. <code class="language-plaintext highlighter-rouge">RelevantContentFilter</code> helped trim the report down to the most relevant pages.</p> <p>Now, we need to give specific instructions to an <strong>Analyst</strong> to go through that focused report and pull out precise data points. We don’t just want the report; we want a filled-in spreadsheet with columns for “Product Name,” “Price,” and “Rating.”</p> <p><code class="language-plaintext highlighter-rouge">ExtractionStrategy</code> is the set of instructions we give to this Analyst. It defines <em>how</em> to locate and extract specific, structured information (like fields in a database or keys in a JSON object) from the content.</p> <h2 id="what-is-extractionstrategy"> <a href="#what-is-extractionstrategy" class="anchor-heading" aria-labelledby="what-is-extractionstrategy"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> What is <code class="language-plaintext highlighter-rouge">ExtractionStrategy</code>? </h2> <p><code class="language-plaintext highlighter-rouge">ExtractionStrategy</code> is a core concept (a blueprint) in Crawl4AI that represents the <strong>method used to extract structured data</strong> from the processed content (which could be HTML or Markdown). It specifies <em>that</em> we need a way to find specific fields, but the actual <em>technique</em> used to find them can vary.</p> <p>This allows us to choose the best “Analyst” for the job, depending on the complexity of the website and the data we need.</p> <h2 id="the-different-analysts-ways-to-extract-data"> <a href="#the-different-analysts-ways-to-extract-data" class="anchor-heading" aria-labelledby="the-different-analysts-ways-to-extract-data"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> The Different Analysts: Ways to Extract Data </h2> <p>Crawl4AI offers several concrete implementations (the different Analysts) for extracting structured data:</p> <ol> <li><strong>The Precise Locator (<code class="language-plaintext highlighter-rouge">JsonCssExtractionStrategy</code> &amp; <code class="language-plaintext highlighter-rouge">JsonXPathExtractionStrategy</code>)</strong> <ul> <li><strong>Analogy:</strong> An analyst who uses very precise map coordinates (CSS Selectors or XPath expressions) to find information on a page. They need to be told exactly where to look. “The price is always in the HTML element with the ID <code class="language-plaintext highlighter-rouge">#product-price</code>.”</li> <li><strong>How it works:</strong> You define a <strong>schema</strong> (a Python dictionary) that maps the names of the fields you want (e.g., “product_name”, “price”) to the specific CSS selector (<code class="language-plaintext highlighter-rouge">JsonCssExtractionStrategy</code>) or XPath expression (<code class="language-plaintext highlighter-rouge">JsonXPathExtractionStrategy</code>) that locates that information within the HTML structure.</li> <li><strong>Pros:</strong> Very fast and reliable if the website structure is consistent and predictable. Doesn’t require external AI services.</li> <li><strong>Cons:</strong> Can break easily if the website changes its layout (selectors become invalid). Requires you to inspect the HTML and figure out the correct selectors.</li> <li><strong>Input:</strong> Typically works directly on the raw or cleaned HTML.</li> </ul> </li> <li><strong>The Smart Interpreter (<code class="language-plaintext highlighter-rouge">LLMExtractionStrategy</code>)</strong> <ul> <li><strong>Analogy:</strong> A highly intelligent analyst who can <em>read and understand</em> the content. You give them a list of fields you need (a schema) or even just natural language instructions (“Find the product name, its price, and a short description”). They read the content (usually Markdown) and use their understanding of language and context to figure out the values, even if the layout isn’t perfectly consistent.</li> <li><strong>How it works:</strong> You provide a desired output schema (e.g., a Pydantic model or a dictionary structure) or a natural language instruction. The strategy sends the content (often the generated Markdown, possibly split into chunks) along with your schema/instruction to a configured Large Language Model (LLM) like GPT or Llama. The LLM reads the text and generates the structured data (usually JSON) according to your request.</li> <li><strong>Pros:</strong> Much more resilient to website layout changes. Can understand context and handle variations. Can extract data based on meaning, not just location.</li> <li><strong>Cons:</strong> Requires setting up access to an LLM (API keys, potentially costs). Can be significantly slower than selector-based methods. The quality of extraction depends on the LLM’s capabilities and the clarity of your instructions/schema.</li> <li><strong>Input:</strong> Often works best on the cleaned Markdown representation of the content, but can sometimes use HTML.</li> </ul> </li> </ol> <h2 id="how-to-use-an-extractionstrategy"> <a href="#how-to-use-an-extractionstrategy" class="anchor-heading" aria-labelledby="how-to-use-an-extractionstrategy"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> How to Use an <code class="language-plaintext highlighter-rouge">ExtractionStrategy</code> </h2> <p>You tell the <code class="language-plaintext highlighter-rouge">AsyncWebCrawler</code> which extraction strategy to use (if any) by setting the <code class="language-plaintext highlighter-rouge">extraction_strategy</code> parameter within the <a href="03_crawlerrunconfig.md">CrawlerRunConfig</a> object you pass to <code class="language-plaintext highlighter-rouge">arun</code> or <code class="language-plaintext highlighter-rouge">arun_many</code>.</p> <h3 id="example-1-extracting-data-with-jsoncssextractionstrategy"> <a href="#example-1-extracting-data-with-jsoncssextractionstrategy" class="anchor-heading" aria-labelledby="example-1-extracting-data-with-jsoncssextractionstrategy"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Example 1: Extracting Data with <code class="language-plaintext highlighter-rouge">JsonCssExtractionStrategy</code> </h3> <p>Let’s imagine we want to extract the title (from the <code class="language-plaintext highlighter-rouge">&lt;h1&gt;</code> tag) and the main heading (from the <code class="language-plaintext highlighter-rouge">&lt;h1&gt;</code> tag) of the simple <code class="language-plaintext highlighter-rouge">httpbin.org/html</code> page.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># chapter6_example_1.py
</span><span class="kn">import</span> <span class="nn">asyncio</span>
<span class="kn">import</span> <span class="nn">json</span>
<span class="kn">from</span> <span class="nn">crawl4ai</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">AsyncWebCrawler</span><span class="p">,</span>
    <span class="n">CrawlerRunConfig</span><span class="p">,</span>
    <span class="n">JsonCssExtractionStrategy</span> <span class="c1"># Import the CSS strategy
</span><span class="p">)</span>

<span class="k">async</span> <span class="k">def</span> <span class="nf">main</span><span class="p">():</span>
    <span class="c1"># 1. Define the extraction schema (Field Name -&gt; CSS Selector)
</span>    <span class="n">extraction_schema</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s">"baseSelector"</span><span class="p">:</span> <span class="s">"body"</span><span class="p">,</span> <span class="c1"># Operate within the body tag
</span>        <span class="s">"fields"</span><span class="p">:</span> <span class="p">[</span>
            <span class="p">{</span><span class="s">"name"</span><span class="p">:</span> <span class="s">"page_title"</span><span class="p">,</span> <span class="s">"selector"</span><span class="p">:</span> <span class="s">"title"</span><span class="p">,</span> <span class="s">"type"</span><span class="p">:</span> <span class="s">"text"</span><span class="p">},</span>
            <span class="p">{</span><span class="s">"name"</span><span class="p">:</span> <span class="s">"main_heading"</span><span class="p">,</span> <span class="s">"selector"</span><span class="p">:</span> <span class="s">"h1"</span><span class="p">,</span> <span class="s">"type"</span><span class="p">:</span> <span class="s">"text"</span><span class="p">}</span>
        <span class="p">]</span>
    <span class="p">}</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"Extraction Schema defined using CSS selectors."</span><span class="p">)</span>

    <span class="c1"># 2. Create an instance of the strategy with the schema
</span>    <span class="n">css_extractor</span> <span class="o">=</span> <span class="n">JsonCssExtractionStrategy</span><span class="p">(</span><span class="n">schema</span><span class="o">=</span><span class="n">extraction_schema</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Using strategy: </span><span class="si">{</span><span class="n">css_extractor</span><span class="p">.</span><span class="n">__class__</span><span class="p">.</span><span class="n">__name__</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>

    <span class="c1"># 3. Create CrawlerRunConfig and set the extraction_strategy
</span>    <span class="n">run_config</span> <span class="o">=</span> <span class="n">CrawlerRunConfig</span><span class="p">(</span>
        <span class="n">extraction_strategy</span><span class="o">=</span><span class="n">css_extractor</span>
    <span class="p">)</span>

    <span class="c1"># 4. Run the crawl
</span>    <span class="k">async</span> <span class="k">with</span> <span class="n">AsyncWebCrawler</span><span class="p">()</span> <span class="k">as</span> <span class="n">crawler</span><span class="p">:</span>
        <span class="n">url_to_crawl</span> <span class="o">=</span> <span class="s">"https://httpbin.org/html"</span>
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"</span><span class="se">\n</span><span class="s">Crawling </span><span class="si">{</span><span class="n">url_to_crawl</span><span class="si">}</span><span class="s"> to extract structured data..."</span><span class="p">)</span>

        <span class="n">result</span> <span class="o">=</span> <span class="k">await</span> <span class="n">crawler</span><span class="p">.</span><span class="n">arun</span><span class="p">(</span><span class="n">url</span><span class="o">=</span><span class="n">url_to_crawl</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">run_config</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">result</span><span class="p">.</span><span class="n">success</span> <span class="ow">and</span> <span class="n">result</span><span class="p">.</span><span class="n">extracted_content</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="s">"</span><span class="se">\n</span><span class="s">Extraction successful!"</span><span class="p">)</span>
            <span class="c1"># The extracted data is stored as a JSON string in result.extracted_content
</span>            <span class="c1"># Parse the JSON string to work with the data as a Python object
</span>            <span class="n">extracted_data</span> <span class="o">=</span> <span class="n">json</span><span class="p">.</span><span class="n">loads</span><span class="p">(</span><span class="n">result</span><span class="p">.</span><span class="n">extracted_content</span><span class="p">)</span>
            <span class="k">print</span><span class="p">(</span><span class="s">"Extracted Data:"</span><span class="p">)</span>
            <span class="c1"># Print the extracted data nicely formatted
</span>            <span class="k">print</span><span class="p">(</span><span class="n">json</span><span class="p">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">extracted_data</span><span class="p">,</span> <span class="n">indent</span><span class="o">=</span><span class="mi">2</span><span class="p">))</span>
        <span class="k">elif</span> <span class="n">result</span><span class="p">.</span><span class="n">success</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="s">"</span><span class="se">\n</span><span class="s">Crawl successful, but no structured data extracted."</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"</span><span class="se">\n</span><span class="s">Crawl failed: </span><span class="si">{</span><span class="n">result</span><span class="p">.</span><span class="n">error_message</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>

<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="s">"__main__"</span><span class="p">:</span>
    <span class="n">asyncio</span><span class="p">.</span><span class="n">run</span><span class="p">(</span><span class="n">main</span><span class="p">())</span>
</code></pre></div></div> <p><strong>Explanation:</strong></p> <ol> <li><strong>Schema Definition:</strong> We create a Python dictionary <code class="language-plaintext highlighter-rouge">extraction_schema</code>. <ul> <li><code class="language-plaintext highlighter-rouge">baseSelector: "body"</code> tells the strategy to look for items within the <code class="language-plaintext highlighter-rouge">&lt;body&gt;</code> tag of the HTML.</li> <li><code class="language-plaintext highlighter-rouge">fields</code> is a list of dictionaries, each defining a field to extract: <ul> <li><code class="language-plaintext highlighter-rouge">name</code>: The key for this field in the output JSON (e.g., “page_title”).</li> <li><code class="language-plaintext highlighter-rouge">selector</code>: The CSS selector to find the element containing the data (e.g., “title” finds the <code class="language-plaintext highlighter-rouge">&lt;title&gt;</code> tag, “h1” finds the <code class="language-plaintext highlighter-rouge">&lt;h1&gt;</code> tag).</li> <li><code class="language-plaintext highlighter-rouge">type</code>: How to get the data from the selected element (<code class="language-plaintext highlighter-rouge">"text"</code> means get the text content).</li> </ul> </li> </ul> </li> <li><strong>Instantiate Strategy:</strong> We create an instance of <code class="language-plaintext highlighter-rouge">JsonCssExtractionStrategy</code>, passing our <code class="language-plaintext highlighter-rouge">extraction_schema</code>. This strategy knows its input format should be HTML.</li> <li><strong>Configure Run:</strong> We create a <code class="language-plaintext highlighter-rouge">CrawlerRunConfig</code> and assign our <code class="language-plaintext highlighter-rouge">css_extractor</code> instance to the <code class="language-plaintext highlighter-rouge">extraction_strategy</code> parameter.</li> <li><strong>Crawl:</strong> We run <code class="language-plaintext highlighter-rouge">crawler.arun</code>. After fetching and basic scraping, the <code class="language-plaintext highlighter-rouge">AsyncWebCrawler</code> will see the <code class="language-plaintext highlighter-rouge">extraction_strategy</code> in the config and call our <code class="language-plaintext highlighter-rouge">css_extractor</code>.</li> <li><strong>Result:</strong> The <code class="language-plaintext highlighter-rouge">CrawlResult</code> object now contains a field called <code class="language-plaintext highlighter-rouge">extracted_content</code>. This field holds the structured data found by the strategy, formatted as a <strong>JSON string</strong>. We use <code class="language-plaintext highlighter-rouge">json.loads()</code> to convert this string back into a Python list/dictionary.</li> </ol> <p><strong>Expected Output (Conceptual):</strong></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Extraction Schema defined using CSS selectors.
Using strategy: JsonCssExtractionStrategy

Crawling https://httpbin.org/html to extract structured data...

Extraction successful!
Extracted Data:
[
  {
    "page_title": "Herman Melville - Moby-Dick",
    "main_heading": "Moby Dick"
  }
]
</code></pre></div></div> <p><em>(Note: The actual output is a list containing one dictionary because <code class="language-plaintext highlighter-rouge">baseSelector: "body"</code> matches one element, and we extract fields relative to that.)</em></p> <h3 id="example-2-extracting-data-with-llmextractionstrategy-conceptual"> <a href="#example-2-extracting-data-with-llmextractionstrategy-conceptual" class="anchor-heading" aria-labelledby="example-2-extracting-data-with-llmextractionstrategy-conceptual"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Example 2: Extracting Data with <code class="language-plaintext highlighter-rouge">LLMExtractionStrategy</code> (Conceptual) </h3> <p>Now, let’s imagine we want the same information (title, heading) but using an AI. We’ll provide a schema describing what we want. (Note: This requires setting up LLM access separately, e.g., API keys).</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># chapter6_example_2.py
</span><span class="kn">import</span> <span class="nn">asyncio</span>
<span class="kn">import</span> <span class="nn">json</span>
<span class="kn">from</span> <span class="nn">crawl4ai</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">AsyncWebCrawler</span><span class="p">,</span>
    <span class="n">CrawlerRunConfig</span><span class="p">,</span>
    <span class="n">LLMExtractionStrategy</span><span class="p">,</span> <span class="c1"># Import the LLM strategy
</span>    <span class="n">LlmConfig</span>             <span class="c1"># Import LLM configuration helper
</span><span class="p">)</span>

<span class="c1"># Assume llm_config is properly configured with provider, API key, etc.
# This is just a placeholder - replace with your actual LLM setup
# E.g., llm_config = LlmConfig(provider="openai", api_token="env:OPENAI_API_KEY")
</span><span class="k">class</span> <span class="nc">MockLlmConfig</span><span class="p">:</span> <span class="n">provider</span><span class="o">=</span><span class="s">"mock"</span><span class="p">;</span> <span class="n">api_token</span><span class="o">=</span><span class="s">"mock"</span><span class="p">;</span> <span class="n">base_url</span><span class="o">=</span><span class="bp">None</span>
<span class="n">llm_config</span> <span class="o">=</span> <span class="n">MockLlmConfig</span><span class="p">()</span>


<span class="k">async</span> <span class="k">def</span> <span class="nf">main</span><span class="p">():</span>
    <span class="c1"># 1. Define the desired output schema (what fields we want)
</span>    <span class="c1">#    This helps guide the LLM.
</span>    <span class="n">output_schema</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s">"page_title"</span><span class="p">:</span> <span class="s">"string"</span><span class="p">,</span>
        <span class="s">"main_heading"</span><span class="p">:</span> <span class="s">"string"</span>
    <span class="p">}</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"Extraction Schema defined for LLM."</span><span class="p">)</span>

    <span class="c1"># 2. Create an instance of the LLM strategy
</span>    <span class="c1">#    We pass the schema and the LLM configuration.
</span>    <span class="c1">#    We also specify input_format='markdown' (common for LLMs).
</span>    <span class="n">llm_extractor</span> <span class="o">=</span> <span class="n">LLMExtractionStrategy</span><span class="p">(</span>
        <span class="n">schema</span><span class="o">=</span><span class="n">output_schema</span><span class="p">,</span>
        <span class="n">llmConfig</span><span class="o">=</span><span class="n">llm_config</span><span class="p">,</span> <span class="c1"># Pass the LLM provider details
</span>        <span class="n">input_format</span><span class="o">=</span><span class="s">"markdown"</span> <span class="c1"># Tell it to read the Markdown content
</span>    <span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Using strategy: </span><span class="si">{</span><span class="n">llm_extractor</span><span class="p">.</span><span class="n">__class__</span><span class="p">.</span><span class="n">__name__</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"LLM Provider (mocked): </span><span class="si">{</span><span class="n">llm_config</span><span class="p">.</span><span class="n">provider</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>

    <span class="c1"># 3. Create CrawlerRunConfig with the strategy
</span>    <span class="n">run_config</span> <span class="o">=</span> <span class="n">CrawlerRunConfig</span><span class="p">(</span>
        <span class="n">extraction_strategy</span><span class="o">=</span><span class="n">llm_extractor</span>
    <span class="p">)</span>

    <span class="c1"># 4. Run the crawl
</span>    <span class="k">async</span> <span class="k">with</span> <span class="n">AsyncWebCrawler</span><span class="p">()</span> <span class="k">as</span> <span class="n">crawler</span><span class="p">:</span>
        <span class="n">url_to_crawl</span> <span class="o">=</span> <span class="s">"https://httpbin.org/html"</span>
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"</span><span class="se">\n</span><span class="s">Crawling </span><span class="si">{</span><span class="n">url_to_crawl</span><span class="si">}</span><span class="s"> using LLM to extract..."</span><span class="p">)</span>

        <span class="c1"># This would make calls to the configured LLM API
</span>        <span class="n">result</span> <span class="o">=</span> <span class="k">await</span> <span class="n">crawler</span><span class="p">.</span><span class="n">arun</span><span class="p">(</span><span class="n">url</span><span class="o">=</span><span class="n">url_to_crawl</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">run_config</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">result</span><span class="p">.</span><span class="n">success</span> <span class="ow">and</span> <span class="n">result</span><span class="p">.</span><span class="n">extracted_content</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="s">"</span><span class="se">\n</span><span class="s">Extraction successful (using LLM)!"</span><span class="p">)</span>
            <span class="c1"># Extracted data is a JSON string
</span>            <span class="k">try</span><span class="p">:</span>
                <span class="n">extracted_data</span> <span class="o">=</span> <span class="n">json</span><span class="p">.</span><span class="n">loads</span><span class="p">(</span><span class="n">result</span><span class="p">.</span><span class="n">extracted_content</span><span class="p">)</span>
                <span class="k">print</span><span class="p">(</span><span class="s">"Extracted Data:"</span><span class="p">)</span>
                <span class="k">print</span><span class="p">(</span><span class="n">json</span><span class="p">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">extracted_data</span><span class="p">,</span> <span class="n">indent</span><span class="o">=</span><span class="mi">2</span><span class="p">))</span>
            <span class="k">except</span> <span class="n">json</span><span class="p">.</span><span class="n">JSONDecodeError</span><span class="p">:</span>
                <span class="k">print</span><span class="p">(</span><span class="s">"Could not parse LLM output as JSON:"</span><span class="p">)</span>
                <span class="k">print</span><span class="p">(</span><span class="n">result</span><span class="p">.</span><span class="n">extracted_content</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">result</span><span class="p">.</span><span class="n">success</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="s">"</span><span class="se">\n</span><span class="s">Crawl successful, but no structured data extracted by LLM."</span><span class="p">)</span>
            <span class="c1"># This might happen if the mock LLM doesn't return valid JSON
</span>            <span class="c1"># or if the content was too small/irrelevant for extraction.
</span>        <span class="k">else</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"</span><span class="se">\n</span><span class="s">Crawl failed: </span><span class="si">{</span><span class="n">result</span><span class="p">.</span><span class="n">error_message</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>

<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="s">"__main__"</span><span class="p">:</span>
    <span class="n">asyncio</span><span class="p">.</span><span class="n">run</span><span class="p">(</span><span class="n">main</span><span class="p">())</span>

</code></pre></div></div> <p><strong>Explanation:</strong></p> <ol> <li><strong>Schema Definition:</strong> We define a simple dictionary <code class="language-plaintext highlighter-rouge">output_schema</code> telling the LLM we want fields named “page_title” and “main_heading”, both expected to be strings.</li> <li><strong>Instantiate Strategy:</strong> We create <code class="language-plaintext highlighter-rouge">LLMExtractionStrategy</code>, passing: <ul> <li><code class="language-plaintext highlighter-rouge">schema=output_schema</code>: Our desired output structure.</li> <li><code class="language-plaintext highlighter-rouge">llmConfig=llm_config</code>: The configuration telling the strategy <em>which</em> LLM to use and how to authenticate (here, it’s mocked).</li> <li><code class="language-plaintext highlighter-rouge">input_format="markdown"</code>: Instructs the strategy to feed the generated Markdown content (from <code class="language-plaintext highlighter-rouge">result.markdown.raw_markdown</code>) to the LLM, which is often easier for LLMs to parse than raw HTML.</li> </ul> </li> <li><strong>Configure Run &amp; Crawl:</strong> Same as before, we set the <code class="language-plaintext highlighter-rouge">extraction_strategy</code> in <code class="language-plaintext highlighter-rouge">CrawlerRunConfig</code> and run the crawl.</li> <li><strong>Result:</strong> The <code class="language-plaintext highlighter-rouge">AsyncWebCrawler</code> calls the <code class="language-plaintext highlighter-rouge">llm_extractor</code>. The strategy sends the Markdown content and the schema instructions to the configured LLM. The LLM analyzes the text and (hopefully) returns a JSON object matching the schema. This JSON is stored as a string in <code class="language-plaintext highlighter-rouge">result.extracted_content</code>.</li> </ol> <p><strong>Expected Output (Conceptual, with a real LLM):</strong></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Extraction Schema defined for LLM.
Using strategy: LLMExtractionStrategy
LLM Provider (mocked): mock

Crawling https://httpbin.org/html using LLM to extract...

Extraction successful (using LLM)!
Extracted Data:
[
  {
    "page_title": "Herman Melville - Moby-Dick",
    "main_heading": "Moby Dick"
  }
]
</code></pre></div></div> <p><em>(Note: LLM output format might vary slightly, but it aims to match the requested schema based on the content it reads.)</em></p> <h2 id="how-it-works-inside-under-the-hood"> <a href="#how-it-works-inside-under-the-hood" class="anchor-heading" aria-labelledby="how-it-works-inside-under-the-hood"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> How It Works Inside (Under the Hood) </h2> <p>When you provide an <code class="language-plaintext highlighter-rouge">extraction_strategy</code> in the <code class="language-plaintext highlighter-rouge">CrawlerRunConfig</code>, how does <code class="language-plaintext highlighter-rouge">AsyncWebCrawler</code> use it?</p> <ol> <li><strong>Fetch &amp; Scrape:</strong> The crawler fetches the raw HTML (<a href="01_asynccrawlerstrategy.md">AsyncCrawlerStrategy</a>) and performs initial cleaning/scraping (<a href="04_contentscrapingstrategy.md">ContentScrapingStrategy</a>) to get <code class="language-plaintext highlighter-rouge">cleaned_html</code>, links, etc.</li> <li><strong>Markdown Generation:</strong> It usually generates Markdown representation (<a href="05_relevantcontentfilter.md#how-relevantcontentfilter-is-used-via-markdown-generation">DefaultMarkdownGenerator</a>).</li> <li><strong>Check for Strategy:</strong> The <code class="language-plaintext highlighter-rouge">AsyncWebCrawler</code> (specifically in its internal <code class="language-plaintext highlighter-rouge">aprocess_html</code> method) checks if <code class="language-plaintext highlighter-rouge">config.extraction_strategy</code> is set.</li> <li><strong>Execute Strategy:</strong> If a strategy exists: <ul> <li>It determines the required input format (e.g., “html” for <code class="language-plaintext highlighter-rouge">JsonCssExtractionStrategy</code>, “markdown” for <code class="language-plaintext highlighter-rouge">LLMExtractionStrategy</code> based on its <code class="language-plaintext highlighter-rouge">input_format</code> attribute).</li> <li>It retrieves the corresponding content (e.g., <code class="language-plaintext highlighter-rouge">result.cleaned_html</code> or <code class="language-plaintext highlighter-rouge">result.markdown.raw_markdown</code>).</li> <li>If the content is long and the strategy supports chunking (like <code class="language-plaintext highlighter-rouge">LLMExtractionStrategy</code>), it might first split the content into smaller chunks.</li> <li>It calls the strategy’s <code class="language-plaintext highlighter-rouge">run</code> method, passing the content chunk(s).</li> <li>The strategy performs its logic (applying selectors, calling LLM API).</li> <li>The strategy returns the extracted data (typically as a list of dictionaries).</li> </ul> </li> <li><strong>Store Result:</strong> The <code class="language-plaintext highlighter-rouge">AsyncWebCrawler</code> converts the returned structured data into a JSON string and stores it in <code class="language-plaintext highlighter-rouge">CrawlResult.extracted_content</code>.</li> </ol> <p>Here’s a simplified view:</p><pre><code class="language-mermaid">sequenceDiagram
    participant User
    participant AWC as AsyncWebCrawler
    participant Config as CrawlerRunConfig
    participant Processor as HTML Processing
    participant Extractor as ExtractionStrategy
    participant Result as CrawlResult

    User-&gt;&gt;AWC: arun(url, config=my_config)
    Note over AWC: Config includes an Extraction Strategy
    AWC-&gt;&gt;Processor: Process HTML (scrape, generate markdown)
    Processor--&gt;&gt;AWC: Processed Content (HTML, Markdown)
    AWC-&gt;&gt;Extractor: Run extraction on content (using Strategy's input format)
    Note over Extractor: Applying logic (CSS, XPath, LLM...)
    Extractor--&gt;&gt;AWC: Structured Data (List[Dict])
    AWC-&gt;&gt;AWC: Convert data to JSON String
    AWC-&gt;&gt;Result: Store JSON String in extracted_content
    AWC--&gt;&gt;User: Return CrawlResult
</code></pre><h3 id="code-glimpse-extraction_strategypy"> <a href="#code-glimpse-extraction_strategypy" class="anchor-heading" aria-labelledby="code-glimpse-extraction_strategypy"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Code Glimpse (<code class="language-plaintext highlighter-rouge">extraction_strategy.py</code>) </h3> <p>Inside the <code class="language-plaintext highlighter-rouge">crawl4ai</code> library, the file <code class="language-plaintext highlighter-rouge">extraction_strategy.py</code> defines the blueprint and the implementations.</p> <p><strong>The Blueprint (Abstract Base Class):</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Simplified from crawl4ai/extraction_strategy.py
</span><span class="kn">from</span> <span class="nn">abc</span> <span class="kn">import</span> <span class="n">ABC</span><span class="p">,</span> <span class="n">abstractmethod</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">List</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">Any</span>

<span class="k">class</span> <span class="nc">ExtractionStrategy</span><span class="p">(</span><span class="n">ABC</span><span class="p">):</span>
    <span class="s">"""Abstract base class for all extraction strategies."""</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_format</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s">"markdown"</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">input_format</span> <span class="o">=</span> <span class="n">input_format</span> <span class="c1"># e.g., 'html', 'markdown'
</span>        <span class="c1"># ... other common init ...
</span>
    <span class="o">@</span><span class="n">abstractmethod</span>
    <span class="k">def</span> <span class="nf">extract</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">url</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">content_chunk</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="o">*</span><span class="n">q</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]:</span>
        <span class="s">"""Extract structured data from a single chunk of content."""</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="nf">run</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">url</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">sections</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="o">*</span><span class="n">q</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]:</span>
        <span class="s">"""Process content sections (potentially chunked) and call extract."""</span>
        <span class="c1"># Default implementation might process sections in parallel or sequentially
</span>        <span class="n">all_extracted_data</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">section</span> <span class="ow">in</span> <span class="n">sections</span><span class="p">:</span>
             <span class="n">all_extracted_data</span><span class="p">.</span><span class="n">extend</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">extract</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">section</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">all_extracted_data</span>
</code></pre></div></div> <p><strong>Example Implementation (<code class="language-plaintext highlighter-rouge">JsonCssExtractionStrategy</code>):</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Simplified from crawl4ai/extraction_strategy.py
</span><span class="kn">from</span> <span class="nn">bs4</span> <span class="kn">import</span> <span class="n">BeautifulSoup</span> <span class="c1"># Uses BeautifulSoup for CSS selectors
</span>
<span class="k">class</span> <span class="nc">JsonCssExtractionStrategy</span><span class="p">(</span><span class="n">ExtractionStrategy</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">schema</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="c1"># Force input format to HTML for CSS selectors
</span>        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">(</span><span class="n">input_format</span><span class="o">=</span><span class="s">"html"</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">schema</span> <span class="o">=</span> <span class="n">schema</span> <span class="c1"># Store the user-defined schema
</span>
    <span class="k">def</span> <span class="nf">extract</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">url</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">html_content</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="o">*</span><span class="n">q</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]:</span>
        <span class="c1"># Parse the HTML content chunk
</span>        <span class="n">soup</span> <span class="o">=</span> <span class="n">BeautifulSoup</span><span class="p">(</span><span class="n">html_content</span><span class="p">,</span> <span class="s">"html.parser"</span><span class="p">)</span>
        <span class="n">extracted_items</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="c1"># Find base elements defined in the schema
</span>        <span class="n">base_elements</span> <span class="o">=</span> <span class="n">soup</span><span class="p">.</span><span class="n">select</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">schema</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">"baseSelector"</span><span class="p">,</span> <span class="s">"body"</span><span class="p">))</span>

        <span class="k">for</span> <span class="n">element</span> <span class="ow">in</span> <span class="n">base_elements</span><span class="p">:</span>
            <span class="n">item</span> <span class="o">=</span> <span class="p">{}</span>
            <span class="c1"># Extract fields based on schema selectors and types
</span>            <span class="n">fields_to_extract</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">schema</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">"fields"</span><span class="p">,</span> <span class="p">[])</span>
            <span class="k">for</span> <span class="n">field_def</span> <span class="ow">in</span> <span class="n">fields_to_extract</span><span class="p">:</span>
                <span class="k">try</span><span class="p">:</span>
                    <span class="c1"># Find the specific sub-element using CSS selector
</span>                    <span class="n">target_element</span> <span class="o">=</span> <span class="n">element</span><span class="p">.</span><span class="n">select_one</span><span class="p">(</span><span class="n">field_def</span><span class="p">[</span><span class="s">"selector"</span><span class="p">])</span>
                    <span class="k">if</span> <span class="n">target_element</span><span class="p">:</span>
                        <span class="k">if</span> <span class="n">field_def</span><span class="p">[</span><span class="s">"type"</span><span class="p">]</span> <span class="o">==</span> <span class="s">"text"</span><span class="p">:</span>
                            <span class="n">item</span><span class="p">[</span><span class="n">field_def</span><span class="p">[</span><span class="s">"name"</span><span class="p">]]</span> <span class="o">=</span> <span class="n">target_element</span><span class="p">.</span><span class="n">get_text</span><span class="p">(</span><span class="n">strip</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
                        <span class="k">elif</span> <span class="n">field_def</span><span class="p">[</span><span class="s">"type"</span><span class="p">]</span> <span class="o">==</span> <span class="s">"attribute"</span><span class="p">:</span>
                            <span class="n">item</span><span class="p">[</span><span class="n">field_def</span><span class="p">[</span><span class="s">"name"</span><span class="p">]]</span> <span class="o">=</span> <span class="n">target_element</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="n">field_def</span><span class="p">[</span><span class="s">"attribute"</span><span class="p">])</span>
                        <span class="c1"># ... other types like 'html', 'list', 'nested' ...
</span>                <span class="k">except</span> <span class="nb">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                    <span class="c1"># Handle errors, maybe log them if verbose
</span>                    <span class="k">pass</span>
            <span class="k">if</span> <span class="n">item</span><span class="p">:</span>
                <span class="n">extracted_items</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">item</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">extracted_items</span>

    <span class="c1"># run() method likely uses the default implementation from base class
</span></code></pre></div></div> <p><strong>Example Implementation (<code class="language-plaintext highlighter-rouge">LLMExtractionStrategy</code>):</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Simplified from crawl4ai/extraction_strategy.py
# Needs imports for LLM interaction (e.g., perform_completion_with_backoff)
</span><span class="kn">from</span> <span class="nn">.utils</span> <span class="kn">import</span> <span class="n">perform_completion_with_backoff</span><span class="p">,</span> <span class="n">chunk_documents</span><span class="p">,</span> <span class="n">escape_json_string</span>
<span class="kn">from</span> <span class="nn">.prompts</span> <span class="kn">import</span> <span class="n">PROMPT_EXTRACT_SCHEMA_WITH_INSTRUCTION</span> <span class="c1"># Example prompt
</span>
<span class="k">class</span> <span class="nc">LLMExtractionStrategy</span><span class="p">(</span><span class="n">ExtractionStrategy</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">schema</span><span class="p">:</span> <span class="n">Dict</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span> <span class="n">instruction</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span> <span class="n">llmConfig</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">input_format</span><span class="o">=</span><span class="s">"markdown"</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">(</span><span class="n">input_format</span><span class="o">=</span><span class="n">input_format</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">schema</span> <span class="o">=</span> <span class="n">schema</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">instruction</span> <span class="o">=</span> <span class="n">instruction</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">llmConfig</span> <span class="o">=</span> <span class="n">llmConfig</span> <span class="c1"># Contains provider, API key, etc.
</span>        <span class="c1"># ... other LLM specific setup ...
</span>
    <span class="k">def</span> <span class="nf">extract</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">url</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">content_chunk</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="o">*</span><span class="n">q</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]:</span>
        <span class="c1"># Prepare the prompt for the LLM
</span>        <span class="n">prompt</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">_build_llm_prompt</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">content_chunk</span><span class="p">)</span>

        <span class="c1"># Call the LLM API
</span>        <span class="n">response</span> <span class="o">=</span> <span class="n">perform_completion_with_backoff</span><span class="p">(</span>
            <span class="n">provider</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">llmConfig</span><span class="p">.</span><span class="n">provider</span><span class="p">,</span>
            <span class="n">prompt_with_variables</span><span class="o">=</span><span class="n">prompt</span><span class="p">,</span>
            <span class="n">api_token</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">llmConfig</span><span class="p">.</span><span class="n">api_token</span><span class="p">,</span>
            <span class="n">base_url</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">llmConfig</span><span class="p">.</span><span class="n">base_url</span><span class="p">,</span>
            <span class="n">json_response</span><span class="o">=</span><span class="bp">True</span> <span class="c1"># Often expect JSON from LLM for extraction
</span>            <span class="c1"># ... pass other necessary args ...
</span>        <span class="p">)</span>

        <span class="c1"># Parse the LLM's response (which should ideally be JSON)
</span>        <span class="k">try</span><span class="p">:</span>
            <span class="n">extracted_data</span> <span class="o">=</span> <span class="n">json</span><span class="p">.</span><span class="n">loads</span><span class="p">(</span><span class="n">response</span><span class="p">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">message</span><span class="p">.</span><span class="n">content</span><span class="p">)</span>
            <span class="c1"># Ensure it's a list
</span>            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">extracted_data</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
                <span class="n">extracted_data</span> <span class="o">=</span> <span class="p">[</span><span class="n">extracted_data</span><span class="p">]</span>
            <span class="k">return</span> <span class="n">extracted_data</span>
        <span class="k">except</span> <span class="nb">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="c1"># Handle LLM response parsing errors
</span>            <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Error parsing LLM response: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
            <span class="k">return</span> <span class="p">[{</span><span class="s">"error"</span><span class="p">:</span> <span class="s">"Failed to parse LLM output"</span><span class="p">,</span> <span class="s">"raw_output"</span><span class="p">:</span> <span class="n">response</span><span class="p">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">message</span><span class="p">.</span><span class="n">content</span><span class="p">}]</span>

    <span class="k">def</span> <span class="nf">_build_llm_prompt</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">url</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">content_chunk</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="c1"># Logic to construct the prompt using self.schema or self.instruction
</span>        <span class="c1"># and the content_chunk. Example:
</span>        <span class="n">prompt_template</span> <span class="o">=</span> <span class="n">PROMPT_EXTRACT_SCHEMA_WITH_INSTRUCTION</span> <span class="c1"># Choose appropriate prompt
</span>        <span class="n">variable_values</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s">"URL"</span><span class="p">:</span> <span class="n">url</span><span class="p">,</span>
            <span class="s">"CONTENT"</span><span class="p">:</span> <span class="n">escape_json_string</span><span class="p">(</span><span class="n">content_chunk</span><span class="p">),</span> <span class="c1"># Send Markdown or HTML chunk
</span>            <span class="s">"SCHEMA"</span><span class="p">:</span> <span class="n">json</span><span class="p">.</span><span class="n">dumps</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">schema</span><span class="p">)</span> <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">schema</span> <span class="k">else</span> <span class="s">"{}"</span><span class="p">,</span>
            <span class="s">"REQUEST"</span><span class="p">:</span> <span class="bp">self</span><span class="p">.</span><span class="n">instruction</span> <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">instruction</span> <span class="k">else</span> <span class="s">"Extract relevant data based on the schema."</span>
        <span class="p">}</span>
        <span class="n">prompt</span> <span class="o">=</span> <span class="n">prompt_template</span>
        <span class="k">for</span> <span class="n">var</span><span class="p">,</span> <span class="n">val</span> <span class="ow">in</span> <span class="n">variable_values</span><span class="p">.</span><span class="n">items</span><span class="p">():</span>
            <span class="n">prompt</span> <span class="o">=</span> <span class="n">prompt</span><span class="p">.</span><span class="n">replace</span><span class="p">(</span><span class="s">"{"</span> <span class="o">+</span> <span class="n">var</span> <span class="o">+</span> <span class="s">"}"</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">val</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">prompt</span>

    <span class="c1"># run() method might override the base to handle chunking specifically for LLMs
</span>    <span class="k">def</span> <span class="nf">run</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">url</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">sections</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="o">*</span><span class="n">q</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]:</span>
        <span class="c1"># Potentially chunk sections based on token limits before calling extract
</span>        <span class="c1"># chunked_content = chunk_documents(sections, ...)
</span>        <span class="c1"># extracted_data = []
</span>        <span class="c1"># for chunk in chunked_content:
</span>        <span class="c1">#    extracted_data.extend(self.extract(url, chunk, **kwargs))
</span>        <span class="c1"># return extracted_data
</span>        <span class="c1"># Simplified for now:
</span>        <span class="k">return</span> <span class="nb">super</span><span class="p">().</span><span class="n">run</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">sections</span><span class="p">,</span> <span class="o">*</span><span class="n">q</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

</code></pre></div></div> <h2 id="conclusion"> <a href="#conclusion" class="anchor-heading" aria-labelledby="conclusion"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Conclusion </h2> <p>You’ve learned about <code class="language-plaintext highlighter-rouge">ExtractionStrategy</code>, Crawl4AI’s way of giving instructions to an “Analyst” to pull out specific, structured data from web content.</p> <ul> <li>It solves the problem of needing precise data points (like product names, prices) in an organized format, not just blocks of text.</li> <li>You can choose your “Analyst”: <ul> <li><strong>Precise Locators (<code class="language-plaintext highlighter-rouge">JsonCssExtractionStrategy</code>, <code class="language-plaintext highlighter-rouge">JsonXPathExtractionStrategy</code>):</strong> Use exact CSS/XPath selectors defined in a schema. Fast but brittle.</li> <li><strong>Smart Interpreter (<code class="language-plaintext highlighter-rouge">LLMExtractionStrategy</code>):</strong> Uses an AI (LLM) guided by a schema or instructions. More flexible but slower and needs setup.</li> </ul> </li> <li>You configure the desired strategy within the <a href="03_crawlerrunconfig.md">CrawlerRunConfig</a>.</li> <li>The extracted structured data is returned as a JSON string in the <code class="language-plaintext highlighter-rouge">CrawlResult.extracted_content</code> field.</li> </ul> <p>Now that we understand how to fetch, clean, filter, and extract data, let’s put it all together and look at the final package that Crawl4AI delivers after a crawl.</p> <p><strong>Next:</strong> Let’s dive into the details of the output with <a href="07_crawlresult.md">Chapter 7: Understanding the Results - CrawlResult</a>.</p><hr /> <p>Generated by <a href="https://github.com/The-Pocket/Tutorial-Codebase-Knowledge">AI Codebase Knowledge Builder</a></p> </main> <hr> <footer> <p class="text-small text-grey-dk-100 mb-0">Copyright &copy; 2023 Sam Kirk</p> </footer> </div> </div> <div class="search-overlay"></div> </div> <script type="module"> import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.6.0/dist/mermaid.esm.min.mjs'; var config = {} ; mermaid.initialize(config); mermaid.run({ querySelector: '.language-mermaid', }); </script> </body> </html>
