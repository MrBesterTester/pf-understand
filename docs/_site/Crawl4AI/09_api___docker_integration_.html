<h1 id="chapter-9-api--docker-integration">Chapter 9: API &amp; Docker Integration</h1>

<p>In <a href="08_async_logging_infrastructure_.md">Chapter 8: Async Logging Infrastructure</a>, we learned how to track what our crawler is doing. Now, let’s explore how to share our crawler with others by turning it into a service that can be accessed over a network!</p>

<h2 id="what-is-api--docker-integration">What is API &amp; Docker Integration?</h2>

<p>Imagine you’ve built a helpful robot (your crawler) that can gather information from websites. Initially, this robot lives only in your house and only you can use it. But what if your friends and family also want to use your robot? You could:</p>

<ol>
  <li>Make copies of the robot for everyone (inefficient)</li>
  <li>Set up a “robot service center” where people can call in their requests (better!)</li>
</ol>

<p>The API &amp; Docker Integration in crawl4ai is like setting up that service center:</p>

<ul>
  <li>The <strong>API</strong> (Application Programming Interface) is like a receptionist who takes requests from callers and passes them to your robot</li>
  <li><strong>Docker</strong> is like a standardized shipping container that lets you easily move and set up your robot service center anywhere</li>
</ul>

<p>This means instead of everyone needing to install and run crawl4ai themselves, they can simply send requests to your service!</p>

<pre><code class="language-mermaid">flowchart LR
    User1[User 1] --&gt;|Request| API
    User2[User 2] --&gt;|Request| API
    User3[User 3] --&gt;|Request| API
    API --&gt;|Processes| Docker[Docker Container]
    Docker --&gt;|Contains| Crawler[Web Crawler]
    Crawler --&gt;|Returns| API
    API --&gt;|Response| User1
    API --&gt;|Response| User2
    API --&gt;|Response| User3
</code></pre>

<h2 id="getting-started-with-the-docker-client">Getting Started with the Docker Client</h2>

<p>Let’s start by seeing how to connect to a crawler service that’s already running. The <code class="language-plaintext highlighter-rouge">Crawl4aiDockerClient</code> makes this easy:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">crawl4ai</span> <span class="kn">import</span> <span class="n">Crawl4aiDockerClient</span>

<span class="k">async</span> <span class="k">def</span> <span class="nf">simple_crawl</span><span class="p">():</span>
    <span class="c1"># Connect to the crawler service
</span>    <span class="k">async</span> <span class="k">with</span> <span class="n">Crawl4aiDockerClient</span><span class="p">()</span> <span class="k">as</span> <span class="n">client</span><span class="p">:</span>
        <span class="c1"># Authenticate (required for security)
</span>        <span class="k">await</span> <span class="n">client</span><span class="p">.</span><span class="n">authenticate</span><span class="p">(</span><span class="s">"your@email.com"</span><span class="p">)</span>
        
        <span class="c1"># Crawl a website
</span>        <span class="n">result</span> <span class="o">=</span> <span class="k">await</span> <span class="n">client</span><span class="p">.</span><span class="n">crawl</span><span class="p">([</span><span class="s">"https://example.com"</span><span class="p">])</span>
        
        <span class="c1"># Use the result
</span>        <span class="k">print</span><span class="p">(</span><span class="n">result</span><span class="p">.</span><span class="n">markdown</span><span class="p">)</span>
</code></pre></div></div>

<p>This code:</p>
<ol>
  <li>Creates a client that connects to a crawl4ai service</li>
  <li>Authenticates with the service using your email</li>
  <li>Sends a request to crawl example.com</li>
  <li>Prints the markdown content from the result</li>
</ol>

<p>It’s like calling the robot service center, identifying yourself, asking the robot to visit a website, and getting back what it found!</p>

<h2 id="using-configuration-with-the-docker-client">Using Configuration with the Docker Client</h2>

<p>Just like the local crawler we used in earlier chapters, you can customize how the remote crawler works:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">crawl4ai</span> <span class="kn">import</span> <span class="n">Crawl4aiDockerClient</span><span class="p">,</span> <span class="n">BrowserConfig</span><span class="p">,</span> <span class="n">CrawlerRunConfig</span>

<span class="k">async</span> <span class="k">def</span> <span class="nf">configured_crawl</span><span class="p">():</span>
    <span class="c1"># Create configurations
</span>    <span class="n">browser_config</span> <span class="o">=</span> <span class="n">BrowserConfig</span><span class="p">(</span><span class="n">headless</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">crawler_config</span> <span class="o">=</span> <span class="n">CrawlerRunConfig</span><span class="p">(</span><span class="n">screenshot</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    
    <span class="k">async</span> <span class="k">with</span> <span class="n">Crawl4aiDockerClient</span><span class="p">()</span> <span class="k">as</span> <span class="n">client</span><span class="p">:</span>
        <span class="k">await</span> <span class="n">client</span><span class="p">.</span><span class="n">authenticate</span><span class="p">(</span><span class="s">"your@email.com"</span><span class="p">)</span>
        
        <span class="c1"># Use configurations with the crawl
</span>        <span class="n">result</span> <span class="o">=</span> <span class="k">await</span> <span class="n">client</span><span class="p">.</span><span class="n">crawl</span><span class="p">(</span>
            <span class="n">urls</span><span class="o">=</span><span class="p">[</span><span class="s">"https://example.com"</span><span class="p">],</span>
            <span class="n">browser_config</span><span class="o">=</span><span class="n">browser_config</span><span class="p">,</span>
            <span class="n">crawler_config</span><span class="o">=</span><span class="n">crawler_config</span>
        <span class="p">)</span>
</code></pre></div></div>

<p>This allows you to tell the remote crawler exactly how to behave, just like you would with a local crawler!</p>

<h2 id="streaming-results-for-faster-processing">Streaming Results for Faster Processing</h2>

<p>If you’re crawling multiple URLs, you can process the results as they come in, rather than waiting for all of them to finish:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">async</span> <span class="k">def</span> <span class="nf">stream_results</span><span class="p">():</span>
    <span class="k">async</span> <span class="k">with</span> <span class="n">Crawl4aiDockerClient</span><span class="p">()</span> <span class="k">as</span> <span class="n">client</span><span class="p">:</span>
        <span class="k">await</span> <span class="n">client</span><span class="p">.</span><span class="n">authenticate</span><span class="p">(</span><span class="s">"your@email.com"</span><span class="p">)</span>
        
        <span class="c1"># Enable streaming in the configuration
</span>        <span class="n">config</span> <span class="o">=</span> <span class="n">CrawlerRunConfig</span><span class="p">(</span><span class="n">stream</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        
        <span class="c1"># Get a streaming generator
</span>        <span class="n">results</span> <span class="o">=</span> <span class="k">await</span> <span class="n">client</span><span class="p">.</span><span class="n">crawl</span><span class="p">(</span>
            <span class="n">urls</span><span class="o">=</span><span class="p">[</span><span class="s">"https://example.com"</span><span class="p">,</span> <span class="s">"https://example.org"</span><span class="p">],</span>
            <span class="n">crawler_config</span><span class="o">=</span><span class="n">config</span>
        <span class="p">)</span>
        
        <span class="c1"># Process results as they arrive
</span>        <span class="k">async</span> <span class="k">for</span> <span class="n">result</span> <span class="ow">in</span> <span class="n">results</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Just received: </span><span class="si">{</span><span class="n">result</span><span class="p">.</span><span class="n">url</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
</code></pre></div></div>

<p>This is like getting updates from the robot as it explores each website, rather than waiting for it to explore all websites before reporting back.</p>

<h2 id="setting-up-your-own-crawler-service">Setting Up Your Own Crawler Service</h2>

<p>Now, let’s look at how to set up your own crawler service using Docker and FastAPI. This is a bit more advanced, but we’ll break it down:</p>

<ol>
  <li>First, you need to have Docker installed on your computer or server</li>
  <li>Then, you can create a Docker container with crawl4ai and its API server</li>
  <li>Finally, you start the container to make the service available</li>
</ol>

<p>Here’s a simple example using Docker Compose (a tool for defining and running Docker applications):</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># docker-compose.yml</span>
<span class="na">version</span><span class="pi">:</span> <span class="s1">'</span><span class="s">3'</span>
<span class="na">services</span><span class="pi">:</span>
  <span class="na">crawl4ai</span><span class="pi">:</span>
    <span class="na">image</span><span class="pi">:</span> <span class="s">crawl4ai/service:latest</span>
    <span class="na">ports</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="s2">"</span><span class="s">8000:8000"</span>
    <span class="na">environment</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="s">ALLOWED_EMAIL_DOMAINS=example.com,gmail.com</span>
</code></pre></div></div>

<p>This configuration:</p>
<ol>
  <li>Uses the pre-built <code class="language-plaintext highlighter-rouge">crawl4ai/service</code> Docker image</li>
  <li>Maps port 8000 on your computer to port 8000 in the container</li>
  <li>Sets an environment variable to control which email domains can authenticate</li>
</ol>

<p>To start the service, you would run:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker-compose up <span class="nt">-d</span>
</code></pre></div></div>

<p>Now your crawler service is running and can be accessed at <code class="language-plaintext highlighter-rouge">http://localhost:8000</code>!</p>

<h2 id="understanding-the-api-endpoints">Understanding the API Endpoints</h2>

<p>The FastAPI server provides several endpoints for different functions:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Available endpoints
</span><span class="o">/</span><span class="n">token</span>              <span class="c1"># Authenticate and get an access token
</span><span class="o">/</span><span class="n">crawl</span>              <span class="c1"># Crawl URLs and get results
</span><span class="o">/</span><span class="n">crawl</span><span class="o">/</span><span class="n">stream</span>       <span class="c1"># Stream crawl results as they complete
</span><span class="o">/</span><span class="n">md</span>                 <span class="c1"># Generate markdown from a URL
</span><span class="o">/</span><span class="n">llm</span>                <span class="c1"># Use LLM to process content from a URL
</span><span class="o">/</span><span class="n">schema</span>             <span class="c1"># Get configuration schemas
</span></code></pre></div></div>

<p>Each endpoint serves a specific purpose:</p>
<ul>
  <li><code class="language-plaintext highlighter-rouge">/token</code> is for authentication</li>
  <li><code class="language-plaintext highlighter-rouge">/crawl</code> is for basic crawling</li>
  <li><code class="language-plaintext highlighter-rouge">/crawl/stream</code> is for streaming crawl results</li>
  <li><code class="language-plaintext highlighter-rouge">/md</code> is for directly getting markdown from a URL</li>
  <li><code class="language-plaintext highlighter-rouge">/llm</code> is for using AI models to process web content</li>
  <li><code class="language-plaintext highlighter-rouge">/schema</code> returns information about configuration options</li>
</ul>

<h2 id="what-happens-under-the-hood">What Happens Under the Hood</h2>

<p>Let’s look at what happens when you send a request to crawl a website:</p>

<pre><code class="language-mermaid">sequenceDiagram
    participant Client as Docker Client
    participant API as FastAPI Server
    participant Pool as Crawler Pool
    participant Crawler as Web Crawler
    participant Website as Website

    Client-&gt;&gt;API: POST /crawl with URLs
    API-&gt;&gt;API: Authenticate request
    API-&gt;&gt;Pool: Get crawler from pool
    Pool-&gt;&gt;Pool: Check available crawlers
    Pool--&gt;&gt;API: Return crawler instance
    API-&gt;&gt;Crawler: Crawl URLs
    Crawler-&gt;&gt;Website: Request pages
    Website--&gt;&gt;Crawler: Return HTML
    Crawler--&gt;&gt;API: Return crawl results
    API--&gt;&gt;Client: Return JSON response
</code></pre>

<ol>
  <li>The client sends a request to the <code class="language-plaintext highlighter-rouge">/crawl</code> endpoint with URLs to crawl</li>
  <li>The server authenticates the request using the provided token</li>
  <li>The server gets a crawler from the crawler pool (which manages browser instances)</li>
  <li>The crawler fetches the requested URLs</li>
  <li>The server formats the results and returns them to the client</li>
</ol>

<h2 id="crawler-pool-efficient-resource-management">Crawler Pool: Efficient Resource Management</h2>

<p>One of the most important parts of the API server is the crawler pool, which efficiently manages browser instances:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Simplified crawler pool code
</span><span class="k">async</span> <span class="k">def</span> <span class="nf">get_crawler</span><span class="p">(</span><span class="n">cfg</span><span class="p">:</span> <span class="n">BrowserConfig</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">AsyncWebCrawler</span><span class="p">:</span>
    <span class="n">sig</span> <span class="o">=</span> <span class="n">_sig</span><span class="p">(</span><span class="n">cfg</span><span class="p">)</span>  <span class="c1"># Create a unique signature for this config
</span>    
    <span class="k">async</span> <span class="k">with</span> <span class="n">LOCK</span><span class="p">:</span>
        <span class="c1"># Return existing crawler if available
</span>        <span class="k">if</span> <span class="n">sig</span> <span class="ow">in</span> <span class="n">POOL</span><span class="p">:</span>
            <span class="n">LAST_USED</span><span class="p">[</span><span class="n">sig</span><span class="p">]</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="n">time</span><span class="p">()</span>
            <span class="k">return</span> <span class="n">POOL</span><span class="p">[</span><span class="n">sig</span><span class="p">]</span>
            
        <span class="c1"># Create new crawler if memory allows
</span>        <span class="k">if</span> <span class="n">psutil</span><span class="p">.</span><span class="n">virtual_memory</span><span class="p">().</span><span class="n">percent</span> <span class="o">&lt;</span> <span class="n">MEM_LIMIT</span><span class="p">:</span>
            <span class="n">crawler</span> <span class="o">=</span> <span class="n">AsyncWebCrawler</span><span class="p">(</span><span class="n">config</span><span class="o">=</span><span class="n">cfg</span><span class="p">)</span>
            <span class="k">await</span> <span class="n">crawler</span><span class="p">.</span><span class="n">start</span><span class="p">()</span>
            <span class="n">POOL</span><span class="p">[</span><span class="n">sig</span><span class="p">]</span> <span class="o">=</span> <span class="n">crawler</span>
            <span class="n">LAST_USED</span><span class="p">[</span><span class="n">sig</span><span class="p">]</span> <span class="o">=</span> <span class="n">time</span><span class="p">.</span><span class="n">time</span><span class="p">()</span>
            <span class="k">return</span> <span class="n">crawler</span>
</code></pre></div></div>

<p>The crawler pool:</p>
<ol>
  <li>Reuses existing crawler instances when possible</li>
  <li>Creates new instances when needed (if memory allows)</li>
  <li>Tracks when each crawler was last used</li>
  <li>Closes idle crawlers to free up resources</li>
</ol>

<p>This is like having a team of robots that are put to sleep when not in use, and woken up when needed!</p>

<h2 id="authentication-and-security">Authentication and Security</h2>

<p>Security is important for any web service. The API server includes authentication to ensure only authorized users can access it:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">@</span><span class="n">app</span><span class="p">.</span><span class="n">post</span><span class="p">(</span><span class="s">"/token"</span><span class="p">)</span>
<span class="k">async</span> <span class="k">def</span> <span class="nf">get_token</span><span class="p">(</span><span class="n">req</span><span class="p">:</span> <span class="n">TokenRequest</span><span class="p">):</span>
    <span class="c1"># Check if email domain is allowed
</span>    <span class="k">if</span> <span class="ow">not</span> <span class="n">verify_email_domain</span><span class="p">(</span><span class="n">req</span><span class="p">.</span><span class="n">email</span><span class="p">):</span>
        <span class="k">raise</span> <span class="n">HTTPException</span><span class="p">(</span><span class="mi">400</span><span class="p">,</span> <span class="s">"Invalid email domain"</span><span class="p">)</span>
        
    <span class="c1"># Create a JWT token for the user
</span>    <span class="n">token</span> <span class="o">=</span> <span class="n">create_access_token</span><span class="p">({</span><span class="s">"sub"</span><span class="p">:</span> <span class="n">req</span><span class="p">.</span><span class="n">email</span><span class="p">})</span>
    
    <span class="k">return</span> <span class="p">{</span>
        <span class="s">"email"</span><span class="p">:</span> <span class="n">req</span><span class="p">.</span><span class="n">email</span><span class="p">,</span> 
        <span class="s">"access_token"</span><span class="p">:</span> <span class="n">token</span><span class="p">,</span> 
        <span class="s">"token_type"</span><span class="p">:</span> <span class="s">"bearer"</span>
    <span class="p">}</span>
</code></pre></div></div>

<p>This endpoint:</p>
<ol>
  <li>Verifies that the user’s email domain is in the allowed list</li>
  <li>Creates a token (like a temporary ID card) for the user</li>
  <li>Returns the token, which must be included in subsequent requests</li>
</ol>

<h2 id="rate-limiting-preventing-overuse">Rate Limiting: Preventing Overuse</h2>

<p>To prevent any single user from overwhelming the service, the API includes rate limiting:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">@</span><span class="n">app</span><span class="p">.</span><span class="n">post</span><span class="p">(</span><span class="s">"/crawl"</span><span class="p">)</span>
<span class="o">@</span><span class="n">limiter</span><span class="p">.</span><span class="n">limit</span><span class="p">(</span><span class="n">config</span><span class="p">[</span><span class="s">"rate_limiting"</span><span class="p">][</span><span class="s">"default_limit"</span><span class="p">])</span>
<span class="k">async</span> <span class="k">def</span> <span class="nf">crawl</span><span class="p">(</span><span class="n">request</span><span class="p">:</span> <span class="n">Request</span><span class="p">,</span> <span class="n">crawl_request</span><span class="p">:</span> <span class="n">CrawlRequest</span><span class="p">):</span>
    <span class="c1"># Rate limiting is applied by the decorator above
</span>    <span class="c1"># ...process the crawl request...
</span></code></pre></div></div>

<p>This decorator ensures that each user can only make a certain number of requests per time period, like limiting how many calls a person can make to your robot service center in an hour.</p>

<h2 id="creating-a-complete-crawl-service">Creating a Complete Crawl Service</h2>

<p>Putting it all together, here’s how you would create a complete crawl service:</p>

<ol>
  <li>Set up the FastAPI server with all the endpoints</li>
  <li>Configure authentication, rate limiting, and security</li>
  <li>Create the crawler pool for efficient resource management</li>
  <li>Start the server in a Docker container</li>
</ol>

<p>The <code class="language-plaintext highlighter-rouge">server.py</code> file in the code snippets shows a complete implementation of this service.</p>

<h2 id="real-world-example-content-processing-service">Real-World Example: Content Processing Service</h2>

<p>Let’s see a real-world example of setting up a content processing service:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">crawl4ai</span> <span class="kn">import</span> <span class="n">Crawl4aiDockerClient</span><span class="p">,</span> <span class="n">CrawlerRunConfig</span>
<span class="kn">from</span> <span class="nn">fastapi</span> <span class="kn">import</span> <span class="n">FastAPI</span><span class="p">,</span> <span class="n">HTTPException</span>

<span class="n">app</span> <span class="o">=</span> <span class="n">FastAPI</span><span class="p">()</span>

<span class="o">@</span><span class="n">app</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">"/process/{url:path}"</span><span class="p">)</span>
<span class="k">async</span> <span class="k">def</span> <span class="nf">process_url</span><span class="p">(</span><span class="n">url</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">query</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="bp">None</span><span class="p">):</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="k">async</span> <span class="k">with</span> <span class="n">Crawl4aiDockerClient</span><span class="p">()</span> <span class="k">as</span> <span class="n">client</span><span class="p">:</span>
            <span class="k">await</span> <span class="n">client</span><span class="p">.</span><span class="n">authenticate</span><span class="p">(</span><span class="s">"service@example.com"</span><span class="p">)</span>
            
            <span class="n">config</span> <span class="o">=</span> <span class="n">CrawlerRunConfig</span><span class="p">()</span>
            <span class="k">if</span> <span class="n">query</span><span class="p">:</span>
                <span class="c1"># Use LLM processing if query is provided
</span>                <span class="n">config</span> <span class="o">=</span> <span class="n">CrawlerRunConfig</span><span class="p">(</span>
                    <span class="n">extraction_strategy</span><span class="o">=</span><span class="s">"llm"</span><span class="p">,</span>
                    <span class="n">instruction</span><span class="o">=</span><span class="n">query</span>
                <span class="p">)</span>
                
            <span class="n">result</span> <span class="o">=</span> <span class="k">await</span> <span class="n">client</span><span class="p">.</span><span class="n">crawl</span><span class="p">([</span><span class="n">url</span><span class="p">],</span> <span class="n">crawler_config</span><span class="o">=</span><span class="n">config</span><span class="p">)</span>
            <span class="k">return</span> <span class="p">{</span><span class="s">"content"</span><span class="p">:</span> <span class="n">result</span><span class="p">.</span><span class="n">markdown</span><span class="p">,</span> <span class="s">"url"</span><span class="p">:</span> <span class="n">url</span><span class="p">}</span>
            
    <span class="k">except</span> <span class="nb">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="k">raise</span> <span class="n">HTTPException</span><span class="p">(</span><span class="mi">500</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">e</span><span class="p">))</span>
</code></pre></div></div>

<p>This example creates a simple API that uses the crawl4ai service to process URLs and optionally extract specific information based on a query.</p>

<h2 id="conclusion">Conclusion</h2>

<p>The API &amp; Docker Integration in crawl4ai transforms your crawler from a local tool into a powerful service that can be accessed from anywhere. By wrapping the crawler in a Docker container and exposing it through a FastAPI server, you make it available to multiple users and applications.</p>

<p>In this chapter, we’ve learned:</p>
<ul>
  <li>How to connect to a crawler service using the Docker client</li>
  <li>How to configure and customize remote crawling operations</li>
  <li>How to stream results for faster processing</li>
  <li>How to set up your own crawler service using Docker</li>
  <li>How the server manages resources with the crawler pool</li>
  <li>How authentication and rate limiting protect the service</li>
</ul>

<p>This chapter completes our exploration of the crawl4ai library. You now have all the knowledge you need to build powerful web crawling applications, from simple scripts to full-featured services!</p>

<p>Remember that with great power comes great responsibility - always respect robots.txt rules, rate limits, and privacy considerations when crawling websites.</p>

<p>Thank you for joining this crawl4ai tutorial journey!</p>

<hr />

<p>Generated by <a href="https://github.com/The-Pocket/Tutorial-Codebase-Knowledge">AI Codebase Knowledge Builder</a></p>
