<!DOCTYPE html> <html lang="en-US"> <head> <meta charset="UTF-8"> <meta http-equiv="X-UA-Compatible" content="IE=Edge"> <link rel="stylesheet" href="/assets/css/just-the-docs-default.css"> <script src="/assets/js/vendor/lunr.min.js"></script> <script src="/assets/js/just-the-docs.js"></script> <meta name="viewport" content="width=device-width, initial-scale=1"> <!-- Begin Jekyll SEO tag v2.8.0 --> <title>Chapter 4: URL Filtering &amp; Scoring | Two Tutorials for Mojo using Pocket Flow</title> <meta name="generator" content="Jekyll v4.3.4" /> <meta property="og:title" content="Chapter 4: URL Filtering &amp; Scoring" /> <meta name="author" content="Sam Kirk" /> <meta property="og:locale" content="en_US" /> <meta name="description" content="Documentation generated using AI to explain codebases" /> <meta property="og:description" content="Documentation generated using AI to explain codebases" /> <link rel="canonical" href="http://localhost:4000/Crawl4AI/04_url_filtering___scoring_.html" /> <meta property="og:url" content="http://localhost:4000/Crawl4AI/04_url_filtering___scoring_.html" /> <meta property="og:site_name" content="Two Tutorials for Mojo using Pocket Flow" /> <meta property="og:type" content="website" /> <meta name="twitter:card" content="summary" /> <meta property="twitter:title" content="Chapter 4: URL Filtering &amp; Scoring" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"WebPage","author":{"@type":"Person","name":"Sam Kirk","url":"https://www.columbia.edu/~zh2408/"},"description":"Documentation generated using AI to explain codebases","headline":"Chapter 4: URL Filtering &amp; Scoring","url":"http://localhost:4000/Crawl4AI/04_url_filtering___scoring_.html"}</script> <!-- End Jekyll SEO tag --> <!-- Add Mermaid support --> <script src="https://cdn.jsdelivr.net/npm/mermaid@11.6.0/dist/mermaid.min.js"></script> <script> document.addEventListener("DOMContentLoaded", function() { mermaid.initialize({ startOnLoad: true, theme: "default" }); // Process code blocks document.querySelectorAll('pre code.language-mermaid').forEach(function(block) { // Create a div with class 'mermaid' var mermaidDiv = document.createElement('div'); mermaidDiv.className = 'mermaid'; mermaidDiv.innerHTML = block.textContent; // Replace the parent pre with the mermaid div block.parentNode.parentNode.replaceChild(mermaidDiv, block.parentNode); console.log("Processed Mermaid block:", mermaidDiv.innerHTML.substring(0, 50) + "..."); }); console.log("Mermaid initialization complete. Version:", mermaid.version()); }); </script> </head> <body> <a class="skip-to-main" href="#main-content">Skip to main content</a> <svg xmlns="http://www.w3.org/2000/svg" class="d-none"> <symbol id="svg-link" viewBox="0 0 24 24"> <title>Link</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-link"> <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path> </svg> </symbol> <symbol id="svg-menu" viewBox="0 0 24 24"> <title>Menu</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu"> <line x1="3" y1="12" x2="21" y2="12"></line><line x1="3" y1="6" x2="21" y2="6"></line><line x1="3" y1="18" x2="21" y2="18"></line> </svg> </symbol> <symbol id="svg-arrow-right" viewBox="0 0 24 24"> <title>Expand</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-chevron-right"> <polyline points="9 18 15 12 9 6"></polyline> </svg> </symbol> <!-- Feather. MIT License: https://github.com/feathericons/feather/blob/master/LICENSE --> <symbol id="svg-external-link" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-external-link"> <title id="svg-external-link-title">(external link)</title> <path d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"></path><polyline points="15 3 21 3 21 9"></polyline><line x1="10" y1="14" x2="21" y2="3"></line> </symbol> <symbol id="svg-doc" viewBox="0 0 24 24"> <title>Document</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file"> <path d="M13 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V9z"></path><polyline points="13 2 13 9 20 9"></polyline> </svg> </symbol> <symbol id="svg-search" viewBox="0 0 24 24"> <title>Search</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-search"> <circle cx="11" cy="11" r="8"></circle><line x1="21" y1="21" x2="16.65" y2="16.65"></line> </svg> </symbol> <!-- Bootstrap Icons. MIT License: https://github.com/twbs/icons/blob/main/LICENSE.md --> <symbol id="svg-copy" viewBox="0 0 16 16"> <title>Copy</title> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard" viewBox="0 0 16 16"> <path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1h1a1 1 0 0 1 1 1V14a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V3.5a1 1 0 0 1 1-1h1v-1z"/> <path d="M9.5 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3zm-3-1A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3z"/> </svg> </symbol> <symbol id="svg-copied" viewBox="0 0 16 16"> <title>Copied</title> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard-check-fill" viewBox="0 0 16 16"> <path d="M6.5 0A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3Zm3 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3Z"/> <path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1A2.5 2.5 0 0 1 9.5 5h-3A2.5 2.5 0 0 1 4 2.5v-1Zm6.854 7.354-3 3a.5.5 0 0 1-.708 0l-1.5-1.5a.5.5 0 0 1 .708-.708L7.5 10.793l2.646-2.647a.5.5 0 0 1 .708.708Z"/> </svg> </symbol> </svg> <div class="side-bar"> <div class="site-header" role="banner"> <a href="/" class="site-title lh-tight"> Two Tutorials for Mojo using Pocket Flow </a> <button id="menu-button" class="site-button btn-reset" aria-label="Toggle menu" aria-pressed="false"> <svg viewBox="0 0 24 24" class="icon" aria-hidden="true"><use xlink:href="#svg-menu"></use></svg> </a> </div> <nav aria-label="Main" id="site-nav" class="site-nav"> <ul class="nav-list"><li class="nav-list-item"><a href="/" class="nav-list-link">Home</a></li><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in My Tutorial for Crawl4ai category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button><a href="/my-crawl4AI/" class="nav-list-link">My Tutorial for Crawl4ai</a><ul class="nav-list"></ul></li><li class="nav-list-item"><a href="/design.html" class="nav-list-link">System Design</a></li><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in My Tutorial for Modular's Max category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button><a href="/modular_max/" class="nav-list-link">My Tutorial for Modular's Max</a><ul class="nav-list"><li class="nav-list-item "><a href="/modular_max/01_settings___settings__class__.html" class="nav-list-link">Chapter 1: Settings Class</a></li><li class="nav-list-item "><a href="/modular_max/02_serving_api_layer__fastapi_app___routers__.html" class="nav-list-link">Chapter 2: Serving API Layer</a></li><li class="nav-list-item "><a href="/modular_max/03_llm_pipeline_orchestrator___tokengeneratorpipeline___.html" class="nav-list-link">Chapter 3: LLM Pipeline Orchestrator</a></li><li class="nav-list-item "><a href="/modular_max/04_model_worker_.html" class="nav-list-link">Chapter 4: Model Worker</a></li><li class="nav-list-item "><a href="/modular_max/05_scheduler___tokengenerationscheduler____embeddingsscheduler___.html" class="nav-list-link">Chapter 5: Scheduler</a></li><li class="nav-list-item "><a href="/modular_max/06_kv_cache_management_.html" class="nav-list-link">Chapter 6: KV Cache Management</a></li><li class="nav-list-item "><a href="/modular_max/07_enginequeue_.html" class="nav-list-link">Chapter 7: EngineQueue</a></li><li class="nav-list-item "><a href="/modular_max/08_telemetry_and_metrics___metrics____metricclient___.html" class="nav-list-link">Chapter 8: Telemetry and Metrics</a></li></ul></li><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in My Tutorial for Mojo v1 category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button><a href="/mojo-v1/" class="nav-list-link">My Tutorial for Mojo v1</a><ul class="nav-list"><li class="nav-list-item "><a href="/mojo-v1/01_addressspace_.html" class="nav-list-link">Chapter 1: AddressSpace</a></li><li class="nav-list-item "><a href="/mojo-v1/02_unsafepointer_.html" class="nav-list-link">Chapter 2: UnsafePointer</a></li><li class="nav-list-item "><a href="/mojo-v1/03_indexlist_.html" class="nav-list-link">Chapter 3: IndexList</a></li><li class="nav-list-item "><a href="/mojo-v1/04_dimlist_.html" class="nav-list-link">Chapter 4: DimList</a></li><li class="nav-list-item "><a href="/mojo-v1/05_ndbuffer_.html" class="nav-list-link">Chapter 5: NDBuffer</a></li><li class="nav-list-item "><a href="/mojo-v1/06_n_d_to_1d_indexing_logic__strided_memory_access__.html" class="nav-list-link">Chapter 6: N-D to 1D Indexing Logic</a></li></ul></li><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in My Tutorial for Mojo v2 category" aria-pressed="false"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button><a href="/mojo-v2/" class="nav-list-link">My Tutorial for Mojo v2</a><ul class="nav-list"><li class="nav-list-item "><a href="/mojo-v2/01_unsafepointer__as_used_by_ndbuffer__.html" class="nav-list-link">Chapter 1: UnsafePointer</a></li><li class="nav-list-item "><a href="/mojo-v2/02_dimlist_and_dim_.html" class="nav-list-link">Chapter 2: DimList and Dim</a></li><li class="nav-list-item "><a href="/mojo-v2/03_ndbuffer_.html" class="nav-list-link">Chapter 3: NDBuffer</a></li><li class="nav-list-item "><a href="/mojo-v2/04_strides_and_offset_computation_.html" class="nav-list-link">Chapter 4: Strides and Offset Computation</a></li><li class="nav-list-item "><a href="/mojo-v2/05_simd_data_access_.html" class="nav-list-link">Chapter 5: SIMD Data Access</a></li></ul></li><li class="nav-list-item active"><button class="nav-list-expander btn-reset" aria-label="toggle items in Crawl4AI category" aria-pressed="true"> <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg> </button><a href="/Crawl4AI/" class="nav-list-link">Crawl4AI</a><ul class="nav-list"><li class="nav-list-item "><a href="/Crawl4AI/01_asynccrawlerstrategy.html" class="nav-list-link">AsyncCrawlerStrategy</a></li><li class="nav-list-item "><a href="/Crawl4AI/01_configuration_system_.html" class="nav-list-link">Chapter 1: Configuration System</a></li><li class="nav-list-item "><a href="/Crawl4AI/02_asyncwebcrawler.html" class="nav-list-link">AsyncWebCrawler</a></li><li class="nav-list-item "><a href="/Crawl4AI/02_asyncwebcrawler_.html" class="nav-list-link">Chapter 2: AsyncWebCrawler</a></li><li class="nav-list-item "><a href="/Crawl4AI/03_content_extraction_pipeline_.html" class="nav-list-link">Chapter 3: Content Extraction Pipeline</a></li><li class="nav-list-item "><a href="/Crawl4AI/03_crawlerrunconfig.html" class="nav-list-link">CrawlerRunConfig</a></li><li class="nav-list-item "><a href="/Crawl4AI/04_contentscrapingstrategy.html" class="nav-list-link">ContentScrapingStrategy</a></li><li class="nav-list-item active"><a href="/Crawl4AI/04_url_filtering___scoring_.html" class="nav-list-link active">Chapter 4: URL Filtering & Scoring</a></li><li class="nav-list-item "><a href="/Crawl4AI/05_deep_crawling_system_.html" class="nav-list-link">Chapter 5: Deep Crawling System</a></li><li class="nav-list-item "><a href="/Crawl4AI/05_relevantcontentfilter.html" class="nav-list-link">RelevantContentFilter</a></li><li class="nav-list-item "><a href="/Crawl4AI/06_caching_system_.html" class="nav-list-link">Chapter 6: Caching System</a></li><li class="nav-list-item "><a href="/Crawl4AI/06_extractionstrategy.html" class="nav-list-link">ExtractionStrategy</a></li><li class="nav-list-item "><a href="/Crawl4AI/07_dispatcher_framework_.html" class="nav-list-link">Chapter 7: Dispatcher Framework</a></li><li class="nav-list-item "><a href="/Crawl4AI/07_crawlresult.html" class="nav-list-link">CrawlResult</a></li><li class="nav-list-item "><a href="/Crawl4AI/08_async_logging_infrastructure_.html" class="nav-list-link">Chapter 8: Async Logging Infrastructure</a></li><li class="nav-list-item "><a href="/Crawl4AI/08_deepcrawlstrategy.html" class="nav-list-link">DeepCrawlStrategy</a></li><li class="nav-list-item "><a href="/Crawl4AI/09_api___docker_integration_.html" class="nav-list-link">Chapter 9: API & Docker Integration</a></li><li class="nav-list-item "><a href="/Crawl4AI/09_cachecontext___cachemode.html" class="nav-list-link">CacheContext & CacheMode</a></li><li class="nav-list-item "><a href="/Crawl4AI/10_basedispatcher.html" class="nav-list-link">BaseDispatcher</a></li></ul></li></ul> <div class="nav-category">Crawl4AI</div> <ul class="nav-list"></ul> <div class="nav-category">Modular Max</div> <ul class="nav-list"></ul> <div class="nav-category">Mojo (v1)</div> <ul class="nav-list"></ul> <div class="nav-category">Mojo (v2)</div> <ul class="nav-list"></ul> </nav> <footer class="site-footer"> This site uses <a href="https://github.com/just-the-docs/just-the-docs">Just the Docs</a>, a documentation theme for Jekyll. </footer> </div> <div class="main" id="top"> <div id="main-header" class="main-header"> <div class="search" role="search"> <div class="search-input-wrap"> <input type="text" id="search-input" class="search-input" tabindex="0" placeholder="Search Two Tutorials for Mojo using Pocket Flow" aria-label="Search Two Tutorials for Mojo using Pocket Flow" autocomplete="off"> <label for="search-input" class="search-label"><svg viewBox="0 0 24 24" class="search-icon"><use xlink:href="#svg-search"></use></svg></label> </div> <div id="search-results" class="search-results"></div> </div> <nav aria-label="Auxiliary" class="aux-nav"> <ul class="aux-nav-list"> <li class="aux-nav-list-item"> <a href="https://github.com/MrBesterTester/pf-understand" class="site-button" > View on GitHub </a> </li> </ul> </nav> </div> <div id="main-content-wrap" class="main-content-wrap"> <nav aria-label="Breadcrumb" class="breadcrumb-nav"> <ol class="breadcrumb-nav-list"> <li class="breadcrumb-nav-list-item"><a href="/Crawl4AI/">Crawl4AI</a></li> <li class="breadcrumb-nav-list-item"><span>Chapter 4: URL Filtering & Scoring</span></li> </ol> </nav> <div id="main-content" class="main-content"> <main> <h1 id="chapter-4-url-filtering--scoring"> <a href="#chapter-4-url-filtering--scoring" class="anchor-heading" aria-labelledby="chapter-4-url-filtering--scoring"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Chapter 4: URL Filtering &amp; Scoring </h1> <p>In <a href="03_content_extraction_pipeline_.md">Chapter 3: Content Extraction Pipeline</a>, we learned how to process and extract useful information from web pages. But how do we decide which URLs to crawl in the first place, and in what order? That’s where URL filtering and scoring come in!</p> <h2 id="understanding-url-filtering-and-scoring"> <a href="#understanding-url-filtering-and-scoring" class="anchor-heading" aria-labelledby="understanding-url-filtering-and-scoring"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Understanding URL Filtering and Scoring </h2> <p>Imagine you’re hosting a party with a VIP section. You need two people:</p> <ol> <li>A bouncer who decides who gets in based on specific rules (the URL filter)</li> <li>A host who decides the order people enter based on their importance (the URL scorer)</li> </ol> <p>When crawling the web, you don’t want to crawl everything - that would be inefficient and might get you blocked! Instead, you want to:</p> <ul> <li>Filter out URLs you don’t care about (like advertisements or irrelevant domains)</li> <li>Prioritize the most valuable URLs to crawl first</li> </ul> <p>Let’s see how <code class="language-plaintext highlighter-rouge">crawl4ai</code> helps us do this!</p> <h2 id="url-filters-the-bouncers"> <a href="#url-filters-the-bouncers" class="anchor-heading" aria-labelledby="url-filters-the-bouncers"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> URL Filters: The Bouncers </h2> <p>URL filters are simple yes/no decision makers. They look at a URL and decide: “Should we crawl this or not?”</p> <h3 id="creating-a-simple-domain-filter"> <a href="#creating-a-simple-domain-filter" class="anchor-heading" aria-labelledby="creating-a-simple-domain-filter"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Creating a Simple Domain Filter </h3> <p>Let’s say you only want to crawl pages from <code class="language-plaintext highlighter-rouge">example.com</code>:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">crawl4ai.deep_crawling.filters</span> <span class="kn">import</span> <span class="n">DomainFilter</span>

<span class="c1"># Create a filter that only allows example.com
</span><span class="n">domain_filter</span> <span class="o">=</span> <span class="n">DomainFilter</span><span class="p">(</span><span class="n">allowed_domains</span><span class="o">=</span><span class="p">[</span><span class="s">"example.com"</span><span class="p">])</span>

<span class="c1"># Test it
</span><span class="n">is_allowed</span> <span class="o">=</span> <span class="n">domain_filter</span><span class="p">.</span><span class="nb">apply</span><span class="p">(</span><span class="s">"https://example.com/page1"</span><span class="p">)</span>  <span class="c1"># True
</span><span class="n">is_blocked</span> <span class="o">=</span> <span class="n">domain_filter</span><span class="p">.</span><span class="nb">apply</span><span class="p">(</span><span class="s">"https://other-site.com/page1"</span><span class="p">)</span>  <span class="c1"># False
</span></code></pre></div></div> <p>This filter acts like a bouncer who only lets in people from a specific city!</p> <h3 id="filtering-by-content-type"> <a href="#filtering-by-content-type" class="anchor-heading" aria-labelledby="filtering-by-content-type"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Filtering by Content Type </h3> <p>Maybe you only want HTML pages and PDFs, not images or other files:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">crawl4ai.deep_crawling.filters</span> <span class="kn">import</span> <span class="n">ContentTypeFilter</span>

<span class="c1"># Create a filter for HTML and PDF files
</span><span class="n">content_filter</span> <span class="o">=</span> <span class="n">ContentTypeFilter</span><span class="p">(</span><span class="n">allowed_types</span><span class="o">=</span><span class="p">[</span><span class="s">"text/html"</span><span class="p">,</span> <span class="s">"application/pdf"</span><span class="p">])</span>

<span class="c1"># Now only HTML and PDF URLs will pass through
</span></code></pre></div></div> <p>This filter checks the content type (often inferred from the URL extension) and only allows specified types.</p> <h3 id="pattern-based-filtering"> <a href="#pattern-based-filtering" class="anchor-heading" aria-labelledby="pattern-based-filtering"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Pattern-Based Filtering </h3> <p>You can also filter URLs based on patterns:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">crawl4ai.deep_crawling.filters</span> <span class="kn">import</span> <span class="n">URLPatternFilter</span>

<span class="c1"># Exclude admin pages and pages with "login" in the URL
</span><span class="n">pattern_filter</span> <span class="o">=</span> <span class="n">URLPatternFilter</span><span class="p">(</span>
    <span class="n">patterns</span><span class="o">=</span><span class="p">[</span><span class="s">"/admin/*"</span><span class="p">,</span> <span class="s">"*/login*"</span><span class="p">],</span> 
    <span class="n">reverse</span><span class="o">=</span><span class="bp">True</span>  <span class="c1"># Exclude matching patterns instead of including them
</span><span class="p">)</span>
</code></pre></div></div> <p>This filter looks for specific patterns in URLs and rejects or accepts them accordingly.</p> <h2 id="url-scorers-the-judges"> <a href="#url-scorers-the-judges" class="anchor-heading" aria-labelledby="url-scorers-the-judges"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> URL Scorers: The Judges </h2> <p>While filters make yes/no decisions, scorers assign a numerical value to each URL. Higher scores mean higher priority!</p> <h3 id="scoring-by-keyword-relevance"> <a href="#scoring-by-keyword-relevance" class="anchor-heading" aria-labelledby="scoring-by-keyword-relevance"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Scoring by Keyword Relevance </h3> <p>Let’s score URLs higher if they contain keywords we’re interested in:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">crawl4ai.deep_crawling.scorers</span> <span class="kn">import</span> <span class="n">KeywordRelevanceScorer</span>

<span class="c1"># URLs with these keywords will score higher
</span><span class="n">keyword_scorer</span> <span class="o">=</span> <span class="n">KeywordRelevanceScorer</span><span class="p">(</span>
    <span class="n">keywords</span><span class="o">=</span><span class="p">[</span><span class="s">"python"</span><span class="p">,</span> <span class="s">"tutorial"</span><span class="p">,</span> <span class="s">"programming"</span><span class="p">],</span>
    <span class="n">weight</span><span class="o">=</span><span class="mf">1.0</span>
<span class="p">)</span>
</code></pre></div></div> <p>This scorer gives higher scores to URLs containing our keywords. The <code class="language-plaintext highlighter-rouge">weight</code> parameter lets us control how important this particular scoring factor is.</p> <h3 id="scoring-by-freshness"> <a href="#scoring-by-freshness" class="anchor-heading" aria-labelledby="scoring-by-freshness"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Scoring by Freshness </h3> <p>For news or time-sensitive information, you might want to prioritize recent content:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">crawl4ai.deep_crawling.scorers</span> <span class="kn">import</span> <span class="n">FreshnessScorer</span>

<span class="c1"># Prioritize URLs with recent dates in them
</span><span class="n">freshness_scorer</span> <span class="o">=</span> <span class="n">FreshnessScorer</span><span class="p">(</span><span class="n">weight</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">current_year</span><span class="o">=</span><span class="mi">2024</span><span class="p">)</span>
</code></pre></div></div> <p>This scorer looks for dates in URLs (like <code class="language-plaintext highlighter-rouge">/2024/04/</code> or <code class="language-plaintext highlighter-rouge">2023-news</code>) and scores newer content higher.</p> <h3 id="combining-multiple-scorers"> <a href="#combining-multiple-scorers" class="anchor-heading" aria-labelledby="combining-multiple-scorers"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Combining Multiple Scorers </h3> <p>You’ll often want to combine multiple scoring criteria:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">crawl4ai.deep_crawling.scorers</span> <span class="kn">import</span> <span class="n">CompositeScorer</span>

<span class="c1"># Combine our scorers
</span><span class="n">combined_scorer</span> <span class="o">=</span> <span class="n">CompositeScorer</span><span class="p">([</span>
    <span class="n">keyword_scorer</span><span class="p">,</span>  <span class="c1"># From previous example
</span>    <span class="n">freshness_scorer</span><span class="p">,</span>  <span class="c1"># From previous example
</span>    <span class="c1"># Add more scorers as needed
</span><span class="p">])</span>
</code></pre></div></div> <p>Each scorer contributes to the final score, letting you balance different factors when deciding crawling priority.</p> <h2 id="putting-it-all-together"> <a href="#putting-it-all-together" class="anchor-heading" aria-labelledby="putting-it-all-together"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Putting It All Together </h2> <p>Now let’s see how to use filters and scorers together in a complete crawling setup:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">crawl4ai</span> <span class="kn">import</span> <span class="n">AsyncWebCrawler</span><span class="p">,</span> <span class="n">CrawlerRunConfig</span>
<span class="kn">from</span> <span class="nn">crawl4ai.deep_crawling</span> <span class="kn">import</span> <span class="n">BestFirstCrawlingStrategy</span>
<span class="kn">from</span> <span class="nn">crawl4ai.deep_crawling.filters</span> <span class="kn">import</span> <span class="n">FilterChain</span>

<span class="c1"># Create a chain of filters
</span><span class="n">filter_chain</span> <span class="o">=</span> <span class="n">FilterChain</span><span class="p">([</span><span class="n">domain_filter</span><span class="p">,</span> <span class="n">content_filter</span><span class="p">,</span> <span class="n">pattern_filter</span><span class="p">])</span>

<span class="c1"># Create a crawler strategy with our filters and scorer
</span><span class="n">strategy</span> <span class="o">=</span> <span class="n">BestFirstCrawlingStrategy</span><span class="p">(</span>
    <span class="n">max_depth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>  <span class="c1"># How deep to crawl
</span>    <span class="n">filter_chain</span><span class="o">=</span><span class="n">filter_chain</span><span class="p">,</span>  <span class="c1"># Our filters
</span>    <span class="n">url_scorer</span><span class="o">=</span><span class="n">combined_scorer</span><span class="p">,</span>  <span class="c1"># Our scorer
</span>    <span class="n">max_pages</span><span class="o">=</span><span class="mi">100</span>  <span class="c1"># Limit total pages
</span><span class="p">)</span>

<span class="c1"># Set up the crawler with our strategy
</span><span class="n">config</span> <span class="o">=</span> <span class="n">CrawlerRunConfig</span><span class="p">(</span><span class="n">deep_crawl_strategy</span><span class="o">=</span><span class="n">strategy</span><span class="p">)</span>

<span class="c1"># Start crawling!
</span><span class="k">async</span> <span class="k">with</span> <span class="n">AsyncWebCrawler</span><span class="p">()</span> <span class="k">as</span> <span class="n">crawler</span><span class="p">:</span>
    <span class="n">results</span> <span class="o">=</span> <span class="k">await</span> <span class="n">crawler</span><span class="p">.</span><span class="n">arun</span><span class="p">(</span>
        <span class="n">url</span><span class="o">=</span><span class="s">"https://example.com"</span><span class="p">,</span> 
        <span class="n">config</span><span class="o">=</span><span class="n">config</span>
    <span class="p">)</span>
</code></pre></div></div> <p>This sets up a complete crawling system that:</p> <ol> <li>Starts at <code class="language-plaintext highlighter-rouge">example.com</code></li> <li>Only crawls URLs that pass all our filters</li> <li>Prioritizes URLs based on our scoring criteria</li> <li>Stops after 100 pages or when it reaches depth 3</li> </ol> <h2 id="what-happens-under-the-hood"> <a href="#what-happens-under-the-hood" class="anchor-heading" aria-labelledby="what-happens-under-the-hood"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> What Happens Under the Hood </h2> <p>Let’s look at what happens when you run a crawler with filters and scorers:</p><pre><code class="language-mermaid">sequenceDiagram
    participant WC as WebCrawler
    participant ST as BestFirstStrategy
    participant FC as FilterChain
    participant SC as URLScorer
    participant Q as PriorityQueue

    WC-&gt;&gt;ST: arun(start_url)
    ST-&gt;&gt;Q: add start_url (score=0)
    loop For each URL in queue
        ST-&gt;&gt;Q: get highest-scored URL
        ST-&gt;&gt;FC: can_process_url(url)?
        FC-&gt;&gt;ST: yes/no
        Note over ST: Skip if filter says no
        ST-&gt;&gt;WC: crawl(url)
        WC-&gt;&gt;ST: crawl_result
        ST-&gt;&gt;ST: extract links from result
        loop For each new link
            ST-&gt;&gt;FC: can_process_url(link)?
            FC-&gt;&gt;ST: yes/no
            Note over ST: Skip if filter says no
            ST-&gt;&gt;SC: score(link)
            SC-&gt;&gt;ST: priority score
            ST-&gt;&gt;Q: add link with score
        end
    end
</code></pre><p>When you run a crawler with URL filtering and scoring:</p> <ol> <li>The crawler adds the start URL to a priority queue</li> <li>It takes the highest-scored URL from the queue</li> <li>It checks if the URL passes all filters</li> <li>If it passes, the URL is crawled</li> <li>New URLs found during crawling go through the same process: <ul> <li>Check if they pass the filters</li> <li>Score them to determine priority</li> <li>Add them to the priority queue</li> </ul> </li> <li>The process repeats until the queue is empty or limits are reached</li> </ol> <h2 id="implementation-details"> <a href="#implementation-details" class="anchor-heading" aria-labelledby="implementation-details"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Implementation Details </h2> <p>Let’s look at how the <code class="language-plaintext highlighter-rouge">BestFirstCrawlingStrategy</code> implements this process:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># From crawl4ai/deep_crawling/bff_strategy.py
</span><span class="k">async</span> <span class="k">def</span> <span class="nf">_arun_best_first</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">start_url</span><span class="p">,</span> <span class="n">crawler</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
    <span class="n">queue</span> <span class="o">=</span> <span class="n">asyncio</span><span class="p">.</span><span class="n">PriorityQueue</span><span class="p">()</span>
    <span class="c1"># Start with the initial URL (score 0, depth 0)
</span>    <span class="k">await</span> <span class="n">queue</span><span class="p">.</span><span class="n">put</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">start_url</span><span class="p">,</span> <span class="bp">None</span><span class="p">))</span>
    <span class="n">visited</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
    
    <span class="k">while</span> <span class="ow">not</span> <span class="n">queue</span><span class="p">.</span><span class="n">empty</span><span class="p">():</span>
        <span class="c1"># Get highest priority URL
</span>        <span class="n">score</span><span class="p">,</span> <span class="n">depth</span><span class="p">,</span> <span class="n">url</span><span class="p">,</span> <span class="n">parent_url</span> <span class="o">=</span> <span class="k">await</span> <span class="n">queue</span><span class="p">.</span><span class="n">get</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">url</span> <span class="ow">in</span> <span class="n">visited</span><span class="p">:</span>
            <span class="k">continue</span>
        <span class="n">visited</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">url</span><span class="p">)</span>
        
        <span class="c1"># Crawl the URL
</span>        <span class="n">result</span> <span class="o">=</span> <span class="k">await</span> <span class="n">crawler</span><span class="p">.</span><span class="n">arun</span><span class="p">(</span><span class="n">url</span><span class="o">=</span><span class="n">url</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">)</span>
        <span class="k">yield</span> <span class="n">result</span>
        
        <span class="c1"># Only process successful crawls
</span>        <span class="k">if</span> <span class="n">result</span><span class="p">.</span><span class="n">success</span><span class="p">:</span>
            <span class="c1"># Discover new links
</span>            <span class="n">new_links</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">await</span> <span class="bp">self</span><span class="p">.</span><span class="n">link_discovery</span><span class="p">(</span><span class="n">result</span><span class="p">,</span> <span class="n">url</span><span class="p">,</span> <span class="n">depth</span><span class="p">,</span> <span class="n">visited</span><span class="p">,</span> <span class="n">new_links</span><span class="p">)</span>
            
            <span class="c1"># Score and add links to queue
</span>            <span class="k">for</span> <span class="n">new_url</span><span class="p">,</span> <span class="n">new_parent</span> <span class="ow">in</span> <span class="n">new_links</span><span class="p">:</span>
                <span class="n">new_score</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">url_scorer</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">new_url</span><span class="p">)</span>
                <span class="k">await</span> <span class="n">queue</span><span class="p">.</span><span class="n">put</span><span class="p">((</span><span class="n">new_score</span><span class="p">,</span> <span class="n">depth</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">new_url</span><span class="p">,</span> <span class="n">new_parent</span><span class="p">))</span>
</code></pre></div></div> <p>The key parts of this implementation are:</p> <ol> <li>Using a priority queue to always process the highest-scored URL next</li> <li>Checking if URLs have been visited to avoid duplicates</li> <li>Using filters during link discovery to filter out unwanted URLs</li> <li>Scoring new URLs to determine their priority in the queue</li> </ol> <p>Let’s also look at how filters are applied in the filter chain:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># From crawl4ai/deep_crawling/filters.py
</span><span class="k">async</span> <span class="k">def</span> <span class="nf">apply</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">url</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
    <span class="s">"""Apply all filters concurrently when possible"""</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">stats</span><span class="p">.</span><span class="n">_counters</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>  <span class="c1"># Count total URLs processed
</span>    
    <span class="c1"># Apply each filter
</span>    <span class="k">for</span> <span class="n">filter_</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">filters</span><span class="p">:</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">filter_</span><span class="p">.</span><span class="nb">apply</span><span class="p">(</span><span class="n">url</span><span class="p">)</span>
        
        <span class="c1"># If filter returns False, URL is rejected
</span>        <span class="k">if</span> <span class="ow">not</span> <span class="n">result</span><span class="p">:</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">stats</span><span class="p">.</span><span class="n">_counters</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>  <span class="c1"># Count rejections
</span>            <span class="k">return</span> <span class="bp">False</span>
    
    <span class="c1"># All filters passed
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">stats</span><span class="p">.</span><span class="n">_counters</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>  <span class="c1"># Count accepted URLs
</span>    <span class="k">return</span> <span class="bp">True</span>
</code></pre></div></div> <p>This shows how the filter chain works - it applies each filter in sequence, and if any filter says “no,” the URL is immediately rejected.</p> <h2 id="real-world-examples"> <a href="#real-world-examples" class="anchor-heading" aria-labelledby="real-world-examples"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Real-World Examples </h2> <h3 id="technical-documentation-crawler"> <a href="#technical-documentation-crawler" class="anchor-heading" aria-labelledby="technical-documentation-crawler"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Technical Documentation Crawler </h3> <p>Let’s say you want to crawl Python documentation but only care about tutorials and library references:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Filter for docs.python.org
</span><span class="n">domain_filter</span> <span class="o">=</span> <span class="n">DomainFilter</span><span class="p">(</span><span class="n">allowed_domains</span><span class="o">=</span><span class="p">[</span><span class="s">"docs.python.org"</span><span class="p">])</span>

<span class="c1"># Only interested in tutorial and library sections
</span><span class="n">pattern_filter</span> <span class="o">=</span> <span class="n">URLPatternFilter</span><span class="p">(</span><span class="n">patterns</span><span class="o">=</span><span class="p">[</span><span class="s">"/tutorial/*"</span><span class="p">,</span> <span class="s">"/library/*"</span><span class="p">])</span>

<span class="c1"># Prioritize based on keywords
</span><span class="n">keyword_scorer</span> <span class="o">=</span> <span class="n">KeywordRelevanceScorer</span><span class="p">([</span>
    <span class="s">"beginners"</span><span class="p">,</span> <span class="s">"tutorial"</span><span class="p">,</span> <span class="s">"examples"</span><span class="p">,</span> <span class="s">"howto"</span>
<span class="p">],</span> <span class="n">weight</span><span class="o">=</span><span class="mf">1.5</span><span class="p">)</span>  <span class="c1"># Give higher weight to beginner-friendly content
</span></code></pre></div></div> <h3 id="news-crawler"> <a href="#news-crawler" class="anchor-heading" aria-labelledby="news-crawler"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> News Crawler </h3> <p>For a news crawler that focuses on recent technology articles:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Only allow news sites
</span><span class="n">domain_filter</span> <span class="o">=</span> <span class="n">DomainFilter</span><span class="p">(</span><span class="n">allowed_domains</span><span class="o">=</span><span class="p">[</span>
    <span class="s">"techcrunch.com"</span><span class="p">,</span> <span class="s">"theverge.com"</span><span class="p">,</span> <span class="s">"wired.com"</span>
<span class="p">])</span>

<span class="c1"># Exclude category pages and author pages
</span><span class="n">pattern_filter</span> <span class="o">=</span> <span class="n">URLPatternFilter</span><span class="p">(</span>
    <span class="n">patterns</span><span class="o">=</span><span class="p">[</span><span class="s">"/author/*"</span><span class="p">,</span> <span class="s">"/category/*"</span><span class="p">,</span> <span class="s">"/tag/*"</span><span class="p">],</span> 
    <span class="n">reverse</span><span class="o">=</span><span class="bp">True</span>
<span class="p">)</span>

<span class="c1"># Prioritize recent articles and tech keywords
</span><span class="n">freshness</span> <span class="o">=</span> <span class="n">FreshnessScorer</span><span class="p">(</span><span class="n">weight</span><span class="o">=</span><span class="mf">1.2</span><span class="p">)</span>  <span class="c1"># High weight on recency
</span><span class="n">tech_terms</span> <span class="o">=</span> <span class="n">KeywordRelevanceScorer</span><span class="p">(</span>
    <span class="p">[</span><span class="s">"ai"</span><span class="p">,</span> <span class="s">"python"</span><span class="p">,</span> <span class="s">"machine learning"</span><span class="p">,</span> <span class="s">"data science"</span><span class="p">],</span> 
    <span class="n">weight</span><span class="o">=</span><span class="mf">1.0</span>
<span class="p">)</span>
</code></pre></div></div> <h2 id="conclusion"> <a href="#conclusion" class="anchor-heading" aria-labelledby="conclusion"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Conclusion </h2> <p>URL filtering and scoring are powerful tools that help you focus your web crawling on the most relevant and valuable content. Filters act as gatekeepers that eliminate unwanted URLs, while scorers help you prioritize the most important content first.</p> <p>By combining different filters and scorers, you can create sophisticated crawling strategies that efficiently collect exactly the data you need. This makes your crawlers more effective and less likely to waste resources on irrelevant content.</p> <p>In the next chapter, <a href="05_deep_crawling_system_.md">Deep Crawling System</a>, we’ll explore how to use these filtering and scoring techniques to build complete web crawling systems that can navigate complex website structures.</p><hr /> <p>Generated by <a href="https://github.com/The-Pocket/Tutorial-Codebase-Knowledge">AI Codebase Knowledge Builder</a></p> </main> <hr> <footer> <p class="text-small text-grey-dk-100 mb-0">Copyright &copy; 2023 Sam Kirk</p> </footer> </div> </div> <div class="search-overlay"></div> </div> <script type="module"> import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11.6.0/dist/mermaid.esm.min.mjs'; var config = {} ; mermaid.initialize(config); mermaid.run({ querySelector: '.language-mermaid', }); </script> </body> </html>
