<h1 id="chapter-5-deep-crawling-system">Chapter 5: Deep Crawling System</h1>

<p>In <a href="04_url_filtering___scoring_.md">Chapter 4: URL Filtering &amp; Scoring</a>, we learned how to filter and prioritize URLs. Now, let’s take the next step: exploring websites beyond a single page using the Deep Crawling System.</p>

<h2 id="what-is-a-deep-crawling-system">What is a Deep Crawling System?</h2>

<p>Imagine you’re exploring a vast library. You could just look at one book (a single webpage), or you could navigate through the entire library systematically (deep crawling).</p>

<p>The Deep Crawling System is like your exploration strategy for that library. It answers questions like:</p>

<ul>
  <li>Which doors should I go through first?</li>
  <li>How far should I venture from the entrance?</li>
  <li>Which rooms are most likely to contain what I’m looking for?</li>
</ul>

<p>When crawling websites, these become decisions about:</p>
<ul>
  <li>Which links to follow first</li>
  <li>How many pages deep to go</li>
  <li>When to stop exploring</li>
</ul>

<p>Let’s learn how to explore websites intelligently using <code class="language-plaintext highlighter-rouge">crawl4ai</code>!</p>

<h2 id="a-simple-example-exploring-a-documentation-site">A Simple Example: Exploring a Documentation Site</h2>

<p>Let’s say you want to crawl the Python documentation site to collect all tutorial pages:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">crawl4ai</span> <span class="kn">import</span> <span class="n">AsyncWebCrawler</span><span class="p">,</span> <span class="n">CrawlerRunConfig</span>
<span class="kn">from</span> <span class="nn">crawl4ai.deep_crawling</span> <span class="kn">import</span> <span class="n">BFSDeepCrawlStrategy</span>
<span class="kn">from</span> <span class="nn">crawl4ai.deep_crawling.filters</span> <span class="kn">import</span> <span class="n">DomainFilter</span>

<span class="k">async</span> <span class="k">def</span> <span class="nf">crawl_python_docs</span><span class="p">():</span>
    <span class="c1"># Create a strategy that explores up to 3 levels deep
</span>    <span class="n">strategy</span> <span class="o">=</span> <span class="n">BFSDeepCrawlStrategy</span><span class="p">(</span>
        <span class="n">max_depth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
        <span class="n">filter_chain</span><span class="o">=</span><span class="n">DomainFilter</span><span class="p">(</span><span class="n">allowed_domains</span><span class="o">=</span><span class="p">[</span><span class="s">"docs.python.org"</span><span class="p">])</span>
    <span class="p">)</span>
    
    <span class="c1"># Configure the crawler to use our strategy
</span>    <span class="n">config</span> <span class="o">=</span> <span class="n">CrawlerRunConfig</span><span class="p">(</span><span class="n">deep_crawl_strategy</span><span class="o">=</span><span class="n">strategy</span><span class="p">)</span>
    
    <span class="c1"># Start crawling from the tutorials page
</span>    <span class="k">async</span> <span class="k">with</span> <span class="n">AsyncWebCrawler</span><span class="p">()</span> <span class="k">as</span> <span class="n">crawler</span><span class="p">:</span>
        <span class="n">results</span> <span class="o">=</span> <span class="k">await</span> <span class="n">crawler</span><span class="p">.</span><span class="n">arun</span><span class="p">(</span>
            <span class="n">url</span><span class="o">=</span><span class="s">"https://docs.python.org/3/tutorial/"</span><span class="p">,</span>
            <span class="n">config</span><span class="o">=</span><span class="n">config</span>
        <span class="p">)</span>
        
    <span class="c1"># Process the results
</span>    <span class="k">for</span> <span class="n">result</span> <span class="ow">in</span> <span class="n">results</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Page: </span><span class="si">{</span><span class="n">result</span><span class="p">.</span><span class="n">url</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
</code></pre></div></div>

<p>This code sets up a crawler that:</p>
<ol>
  <li>Starts at the Python tutorial page</li>
  <li>Explores up to 3 links deep</li>
  <li>Only stays within the docs.python.org domain</li>
  <li>Returns all the pages it found</li>
</ol>

<h2 id="understanding-crawling-strategies">Understanding Crawling Strategies</h2>

<p>The Deep Crawling System offers different ways to explore a website, just like you’d have different strategies for exploring a city:</p>

<h3 id="1-breadth-first-search-bfs">1. Breadth-First Search (BFS)</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">crawl4ai.deep_crawling</span> <span class="kn">import</span> <span class="n">BFSDeepCrawlStrategy</span>

<span class="n">bfs_strategy</span> <span class="o">=</span> <span class="n">BFSDeepCrawlStrategy</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</code></pre></div></div>

<p>BFS is like exploring a city one block at a time, checking all streets on your current block before going to the next block. In web terms, it explores all links on the current page before going deeper.</p>

<h3 id="2-depth-first-search-dfs">2. Depth-First Search (DFS)</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">crawl4ai.deep_crawling</span> <span class="kn">import</span> <span class="n">DFSDeepCrawlStrategy</span>

<span class="n">dfs_strategy</span> <span class="o">=</span> <span class="n">DFSDeepCrawlStrategy</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</code></pre></div></div>

<p>DFS is like following a single street as far as it goes before coming back to try the next street. In web terms, it follows each path to its maximum depth before trying other paths.</p>

<h3 id="3-best-first-search">3. Best-First Search</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">crawl4ai.deep_crawling</span> <span class="kn">import</span> <span class="n">BestFirstCrawlingStrategy</span>
<span class="kn">from</span> <span class="nn">crawl4ai.deep_crawling.scorers</span> <span class="kn">import</span> <span class="n">KeywordRelevanceScorer</span>

<span class="n">scorer</span> <span class="o">=</span> <span class="n">KeywordRelevanceScorer</span><span class="p">(</span><span class="n">keywords</span><span class="o">=</span><span class="p">[</span><span class="s">"tutorial"</span><span class="p">,</span> <span class="s">"guide"</span><span class="p">])</span>
<span class="n">best_first</span> <span class="o">=</span> <span class="n">BestFirstCrawlingStrategy</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">url_scorer</span><span class="o">=</span><span class="n">scorer</span><span class="p">)</span>
</code></pre></div></div>

<p>Best-First is like having a treasure map that guides you to the most promising areas first. It uses scorers to decide which links are most likely to have what you’re looking for.</p>

<h2 id="setting-crawling-boundaries">Setting Crawling Boundaries</h2>

<p>Just as you’d set limits on a real-world exploration, you need to set boundaries for your web crawler:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Limit by depth (how many clicks from the start page)
</span><span class="n">strategy</span> <span class="o">=</span> <span class="n">BFSDeepCrawlStrategy</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>

<span class="c1"># Limit by total pages
</span><span class="n">strategy</span> <span class="o">=</span> <span class="n">BFSDeepCrawlStrategy</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">max_pages</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>

<span class="c1"># Limit by domain
</span><span class="kn">from</span> <span class="nn">crawl4ai.deep_crawling.filters</span> <span class="kn">import</span> <span class="n">DomainFilter</span>
<span class="n">filter_chain</span> <span class="o">=</span> <span class="n">DomainFilter</span><span class="p">(</span><span class="n">allowed_domains</span><span class="o">=</span><span class="p">[</span><span class="s">"example.com"</span><span class="p">])</span>
<span class="n">strategy</span> <span class="o">=</span> <span class="n">BFSDeepCrawlStrategy</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">filter_chain</span><span class="o">=</span><span class="n">filter_chain</span><span class="p">)</span>
</code></pre></div></div>

<p>These boundaries help keep your crawler focused and efficient.</p>

<h2 id="streaming-vs-batch-processing">Streaming vs. Batch Processing</h2>

<p>You can process crawled pages as they come in (streaming) or wait for all results (batch):</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Streaming: Process pages as they're found
</span><span class="n">config</span> <span class="o">=</span> <span class="n">CrawlerRunConfig</span><span class="p">(</span><span class="n">deep_crawl_strategy</span><span class="o">=</span><span class="n">strategy</span><span class="p">,</span> <span class="n">stream</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="k">async</span> <span class="k">with</span> <span class="n">AsyncWebCrawler</span><span class="p">()</span> <span class="k">as</span> <span class="n">crawler</span><span class="p">:</span>
    <span class="k">async</span> <span class="k">for</span> <span class="n">result</span> <span class="ow">in</span> <span class="k">await</span> <span class="n">crawler</span><span class="p">.</span><span class="n">arun</span><span class="p">(</span><span class="n">url</span><span class="o">=</span><span class="s">"https://example.com"</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">):</span>
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Just found: </span><span class="si">{</span><span class="n">result</span><span class="p">.</span><span class="n">url</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
</code></pre></div></div>

<p>Streaming is great when you want to start processing right away, without waiting for the entire crawl to finish.</p>

<h2 id="a-real-world-example-news-aggregator">A Real-World Example: News Aggregator</h2>

<p>Let’s build a simple news aggregator that collects recent articles from a news site:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">crawl4ai</span> <span class="kn">import</span> <span class="n">AsyncWebCrawler</span><span class="p">,</span> <span class="n">CrawlerRunConfig</span>
<span class="kn">from</span> <span class="nn">crawl4ai.deep_crawling</span> <span class="kn">import</span> <span class="n">BestFirstCrawlingStrategy</span>
<span class="kn">from</span> <span class="nn">crawl4ai.deep_crawling.scorers</span> <span class="kn">import</span> <span class="n">FreshnessScorer</span><span class="p">,</span> <span class="n">KeywordRelevanceScorer</span>
<span class="kn">from</span> <span class="nn">crawl4ai.deep_crawling.filters</span> <span class="kn">import</span> <span class="n">URLPatternFilter</span>

<span class="k">async</span> <span class="k">def</span> <span class="nf">crawl_news</span><span class="p">():</span>
    <span class="c1"># Create a scorer that prioritizes recent articles about technology
</span>    <span class="n">scorer</span> <span class="o">=</span> <span class="n">FreshnessScorer</span><span class="p">(</span><span class="n">weight</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
    
    <span class="c1"># Exclude category pages and author pages
</span>    <span class="nb">filter</span> <span class="o">=</span> <span class="n">URLPatternFilter</span><span class="p">(</span><span class="n">patterns</span><span class="o">=</span><span class="p">[</span><span class="s">"/author/*"</span><span class="p">,</span> <span class="s">"/category/*"</span><span class="p">],</span> <span class="n">reverse</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    
    <span class="c1"># Create a strategy that favors fresh content
</span>    <span class="n">strategy</span> <span class="o">=</span> <span class="n">BestFirstCrawlingStrategy</span><span class="p">(</span>
        <span class="n">max_depth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
        <span class="n">max_pages</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>  <span class="c1"># Limit to 10 articles
</span>        <span class="n">url_scorer</span><span class="o">=</span><span class="n">scorer</span><span class="p">,</span>
        <span class="n">filter_chain</span><span class="o">=</span><span class="nb">filter</span>
    <span class="p">)</span>
    
    <span class="c1"># Start crawling
</span>    <span class="n">config</span> <span class="o">=</span> <span class="n">CrawlerRunConfig</span><span class="p">(</span><span class="n">deep_crawl_strategy</span><span class="o">=</span><span class="n">strategy</span><span class="p">)</span>
    <span class="k">async</span> <span class="k">with</span> <span class="n">AsyncWebCrawler</span><span class="p">()</span> <span class="k">as</span> <span class="n">crawler</span><span class="p">:</span>
        <span class="n">results</span> <span class="o">=</span> <span class="k">await</span> <span class="n">crawler</span><span class="p">.</span><span class="n">arun</span><span class="p">(</span><span class="n">url</span><span class="o">=</span><span class="s">"https://news-site.com"</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">results</span>
</code></pre></div></div>

<p>This crawler will prioritize the most recent articles and ignore pages like author profiles and category listings.</p>

<h2 id="what-happens-under-the-hood">What Happens Under the Hood</h2>

<p>What actually happens when you start a deep crawl? Let’s visualize the process:</p>

<pre><code class="language-mermaid">sequenceDiagram
    participant User as Your Code
    participant Crawler as WebCrawler
    participant Strategy as DeepCrawlStrategy
    participant Queue as URL Queue
    participant FC as Filter Chain

    User-&gt;&gt;Crawler: arun(start_url, config)
    Crawler-&gt;&gt;Strategy: arun(start_url)
    Strategy-&gt;&gt;Queue: add start_url
    loop Until queue is empty
        Strategy-&gt;&gt;Queue: get next URL
        Strategy-&gt;&gt;FC: can_process_url(url)?
        FC-&gt;&gt;Strategy: yes/no
        Strategy-&gt;&gt;Crawler: crawler.arun(url)
        Crawler-&gt;&gt;Strategy: return result
        Strategy-&gt;&gt;Strategy: extract new URLs
        loop For each new URL
            Strategy-&gt;&gt;FC: can_process_url(new_url)?
            FC-&gt;&gt;Strategy: yes/no
            Strategy-&gt;&gt;Queue: add URL if approved
        end
        Strategy-&gt;&gt;User: yield result (if streaming)
    end
    Strategy-&gt;&gt;User: return all results (if batch mode)
</code></pre>

<p>This process repeats until either:</p>
<ul>
  <li>The queue is empty (no more URLs to crawl)</li>
  <li>We’ve reached the maximum depth</li>
  <li>We’ve crawled the maximum number of pages</li>
</ul>

<h2 id="implementation-details">Implementation Details</h2>

<p>Let’s look at how the BFS crawling strategy is implemented:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Simplified from crawl4ai/deep_crawling/bfs_strategy.py
</span><span class="k">async</span> <span class="k">def</span> <span class="nf">_arun_stream</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">start_url</span><span class="p">,</span> <span class="n">crawler</span><span class="p">,</span> <span class="n">config</span><span class="p">):</span>
    <span class="n">visited</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
    <span class="n">current_level</span> <span class="o">=</span> <span class="p">[(</span><span class="n">start_url</span><span class="p">,</span> <span class="bp">None</span><span class="p">)]</span>  <span class="c1"># (url, parent_url)
</span>    <span class="n">depths</span> <span class="o">=</span> <span class="p">{</span><span class="n">start_url</span><span class="p">:</span> <span class="mi">0</span><span class="p">}</span>

    <span class="k">while</span> <span class="n">current_level</span> <span class="ow">and</span> <span class="ow">not</span> <span class="bp">self</span><span class="p">.</span><span class="n">_cancel_event</span><span class="p">.</span><span class="n">is_set</span><span class="p">():</span>
        <span class="n">next_level</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">urls</span> <span class="o">=</span> <span class="p">[</span><span class="n">url</span> <span class="k">for</span> <span class="n">url</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">current_level</span><span class="p">]</span>
        
        <span class="c1"># Crawl all URLs at the current level
</span>        <span class="n">stream_gen</span> <span class="o">=</span> <span class="k">await</span> <span class="n">crawler</span><span class="p">.</span><span class="n">arun_many</span><span class="p">(</span><span class="n">urls</span><span class="o">=</span><span class="n">urls</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">)</span>
        
        <span class="k">async</span> <span class="k">for</span> <span class="n">result</span> <span class="ow">in</span> <span class="n">stream_gen</span><span class="p">:</span>
            <span class="c1"># Process each result
</span>            <span class="n">url</span> <span class="o">=</span> <span class="n">result</span><span class="p">.</span><span class="n">url</span>
            <span class="n">depth</span> <span class="o">=</span> <span class="n">depths</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
            <span class="n">result</span><span class="p">.</span><span class="n">metadata</span><span class="p">[</span><span class="s">"depth"</span><span class="p">]</span> <span class="o">=</span> <span class="n">depth</span>
            
            <span class="k">yield</span> <span class="n">result</span>
            
            <span class="c1"># Discover new links for the next level
</span>            <span class="k">if</span> <span class="n">result</span><span class="p">.</span><span class="n">success</span><span class="p">:</span>
                <span class="k">await</span> <span class="bp">self</span><span class="p">.</span><span class="n">link_discovery</span><span class="p">(</span>
                    <span class="n">result</span><span class="p">,</span> <span class="n">url</span><span class="p">,</span> <span class="n">depth</span><span class="p">,</span> <span class="n">visited</span><span class="p">,</span> <span class="n">next_level</span><span class="p">,</span> <span class="n">depths</span>
                <span class="p">)</span>
        
        <span class="c1"># Move to the next level
</span>        <span class="n">current_level</span> <span class="o">=</span> <span class="n">next_level</span>
</code></pre></div></div>

<p>This implementation shows how the BFS strategy works:</p>

<ol>
  <li>It starts with a single URL at depth 0</li>
  <li>It processes all URLs at the current depth</li>
  <li>It extracts new links and adds them to the next level</li>
  <li>It moves to the next level and repeats</li>
</ol>

<p>The <code class="language-plaintext highlighter-rouge">link_discovery</code> method is where URLs are filtered:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Simplified from link_discovery method
</span><span class="k">async</span> <span class="k">def</span> <span class="nf">link_discovery</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">result</span><span class="p">,</span> <span class="n">source_url</span><span class="p">,</span> <span class="n">current_depth</span><span class="p">,</span> <span class="n">visited</span><span class="p">,</span> <span class="n">next_level</span><span class="p">,</span> <span class="n">depths</span><span class="p">):</span>
    <span class="n">next_depth</span> <span class="o">=</span> <span class="n">current_depth</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="k">if</span> <span class="n">next_depth</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="p">.</span><span class="n">max_depth</span><span class="p">:</span>
        <span class="k">return</span>
        
    <span class="n">links</span> <span class="o">=</span> <span class="n">result</span><span class="p">.</span><span class="n">links</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">"internal"</span><span class="p">,</span> <span class="p">[])</span>
    
    <span class="k">for</span> <span class="n">link</span> <span class="ow">in</span> <span class="n">links</span><span class="p">:</span>
        <span class="n">url</span> <span class="o">=</span> <span class="n">link</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">"href"</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">url</span> <span class="ow">in</span> <span class="n">visited</span><span class="p">:</span>
            <span class="k">continue</span>
            
        <span class="k">if</span> <span class="ow">not</span> <span class="k">await</span> <span class="bp">self</span><span class="p">.</span><span class="n">can_process_url</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">next_depth</span><span class="p">):</span>
            <span class="k">continue</span>
            
        <span class="n">visited</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">url</span><span class="p">)</span>
        <span class="n">next_level</span><span class="p">.</span><span class="n">append</span><span class="p">((</span><span class="n">url</span><span class="p">,</span> <span class="n">source_url</span><span class="p">))</span>
        <span class="n">depths</span><span class="p">[</span><span class="n">url</span><span class="p">]</span> <span class="o">=</span> <span class="n">next_depth</span>
</code></pre></div></div>

<p>This method:</p>
<ol>
  <li>Checks if we’ve reached the maximum depth</li>
  <li>Gets all the links from the current page</li>
  <li>Filters out already visited URLs</li>
  <li>Applies URL filters to decide which URLs to follow</li>
  <li>Adds approved URLs to the next level</li>
</ol>

<h2 id="choosing-the-right-strategy">Choosing the Right Strategy</h2>

<p>Which strategy should you choose? Here’s a quick guide:</p>

<ul>
  <li>
    <p><strong>BFS</strong>: When you want a broad overview of a website, exploring many sections</p>
  </li>
  <li>
    <p><strong>DFS</strong>: When you’re looking for something specific and want to go deep into specific paths</p>
  </li>
  <li>
    <p><strong>Best-First</strong>: When you have a good idea of what you’re looking for and want to find the most relevant content first</p>
  </li>
</ul>

<h2 id="advanced-usage-combining-strategies">Advanced Usage: Combining Strategies</h2>

<p>You can combine different aspects of the Deep Crawling System for more sophisticated crawling:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">crawl4ai.deep_crawling</span> <span class="kn">import</span> <span class="n">BestFirstCrawlingStrategy</span>
<span class="kn">from</span> <span class="nn">crawl4ai.deep_crawling.filters</span> <span class="kn">import</span> <span class="n">FilterChain</span><span class="p">,</span> <span class="n">DomainFilter</span><span class="p">,</span> <span class="n">ContentTypeFilter</span>
<span class="kn">from</span> <span class="nn">crawl4ai.deep_crawling.scorers</span> <span class="kn">import</span> <span class="n">CompositeScorer</span><span class="p">,</span> <span class="n">KeywordScorer</span><span class="p">,</span> <span class="n">FreshnessScorer</span>

<span class="c1"># Create multiple filters
</span><span class="n">domain_filter</span> <span class="o">=</span> <span class="n">DomainFilter</span><span class="p">(</span><span class="n">allowed_domains</span><span class="o">=</span><span class="p">[</span><span class="s">"example.com"</span><span class="p">])</span>
<span class="n">content_filter</span> <span class="o">=</span> <span class="n">ContentTypeFilter</span><span class="p">(</span><span class="n">allowed_types</span><span class="o">=</span><span class="p">[</span><span class="s">"text/html"</span><span class="p">])</span>
<span class="n">filter_chain</span> <span class="o">=</span> <span class="n">FilterChain</span><span class="p">([</span><span class="n">domain_filter</span><span class="p">,</span> <span class="n">content_filter</span><span class="p">])</span>

<span class="c1"># Create multiple scorers
</span><span class="n">keyword_scorer</span> <span class="o">=</span> <span class="n">KeywordScorer</span><span class="p">(</span><span class="n">keywords</span><span class="o">=</span><span class="p">[</span><span class="s">"python"</span><span class="p">,</span> <span class="s">"tutorial"</span><span class="p">])</span>
<span class="n">freshness_scorer</span> <span class="o">=</span> <span class="n">FreshnessScorer</span><span class="p">()</span>
<span class="n">composite_scorer</span> <span class="o">=</span> <span class="n">CompositeScorer</span><span class="p">([</span><span class="n">keyword_scorer</span><span class="p">,</span> <span class="n">freshness_scorer</span><span class="p">])</span>

<span class="c1"># Combine them in a strategy
</span><span class="n">strategy</span> <span class="o">=</span> <span class="n">BestFirstCrawlingStrategy</span><span class="p">(</span>
    <span class="n">max_depth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
    <span class="n">filter_chain</span><span class="o">=</span><span class="n">filter_chain</span><span class="p">,</span>
    <span class="n">url_scorer</span><span class="o">=</span><span class="n">composite_scorer</span><span class="p">,</span>
    <span class="n">max_pages</span><span class="o">=</span><span class="mi">50</span>
<span class="p">)</span>
</code></pre></div></div>

<p>This creates a powerful crawler that stays within example.com, only crawls HTML pages, and prioritizes fresh content about Python tutorials.</p>

<h2 id="working-with-crawl-results">Working with Crawl Results</h2>

<p>Once you’ve collected pages, you can do many things with the results:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">async</span> <span class="k">def</span> <span class="nf">process_results</span><span class="p">(</span><span class="n">results</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">result</span> <span class="ow">in</span> <span class="n">results</span><span class="p">:</span>
        <span class="c1"># Access metadata about the crawl
</span>        <span class="n">depth</span> <span class="o">=</span> <span class="n">result</span><span class="p">.</span><span class="n">metadata</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">"depth"</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="n">parent</span> <span class="o">=</span> <span class="n">result</span><span class="p">.</span><span class="n">metadata</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">"parent_url"</span><span class="p">)</span>
        
        <span class="c1"># Process the content
</span>        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Page </span><span class="si">{</span><span class="n">result</span><span class="p">.</span><span class="n">url</span><span class="si">}</span><span class="s"> at depth </span><span class="si">{</span><span class="n">depth</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Title: </span><span class="si">{</span><span class="n">result</span><span class="p">.</span><span class="n">title</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Content: </span><span class="si">{</span><span class="n">result</span><span class="p">.</span><span class="n">markdown</span><span class="p">[</span><span class="si">:</span><span class="mi">100</span><span class="p">]</span><span class="si">}</span><span class="s">..."</span><span class="p">)</span>  <span class="c1"># First 100 chars
</span>        
        <span class="c1"># Check for specific patterns
</span>        <span class="k">if</span> <span class="s">"download"</span> <span class="ow">in</span> <span class="n">result</span><span class="p">.</span><span class="n">markdown</span><span class="p">.</span><span class="n">lower</span><span class="p">():</span>
            <span class="k">print</span><span class="p">(</span><span class="s">"Found a download page!"</span><span class="p">)</span>
</code></pre></div></div>

<p>Each result includes the page content, metadata about the crawl, and all the links that were found.</p>

<h2 id="conclusion">Conclusion</h2>

<p>The Deep Crawling System is a powerful tool that lets you explore websites systematically. By choosing the right strategy, setting appropriate boundaries, and configuring filters and scorers, you can efficiently find and extract exactly the content you need.</p>

<p>In this chapter, we’ve learned:</p>
<ul>
  <li>How to set up different crawling strategies (BFS, DFS, Best-First)</li>
  <li>How to set boundaries on your crawling</li>
  <li>How to process results in real-time or in batch</li>
  <li>How the Deep Crawling System works under the hood</li>
</ul>

<p>In the next chapter, <a href="06_caching_system_.md">Caching System</a>, we’ll learn how to store and reuse crawled content to make our crawlers even more efficient.</p>

<hr />

<p>Generated by <a href="https://github.com/The-Pocket/Tutorial-Codebase-Knowledge">AI Codebase Knowledge Builder</a></p>
