<h1 id="chapter-7-dispatcher-framework">Chapter 7: Dispatcher Framework</h1>

<p>In <a href="06_caching_system_.md">Chapter 6: Caching System</a>, we learned how to store and reuse web content to make our crawlers more efficient. Now, let’s explore how to manage multiple crawling operations happening at the same time with the Dispatcher Framework.</p>

<h2 id="what-is-the-dispatcher-framework">What is the Dispatcher Framework?</h2>

<p>Imagine you’re running a busy restaurant kitchen. You need a head chef (the dispatcher) who coordinates all the cooks (crawler instances), making sure they don’t overcrowd the kitchen or use too many ingredients at once.</p>

<p>The Dispatcher Framework in crawl4ai works the same way - it coordinates multiple crawler instances:</p>

<ul>
  <li>It controls how many web pages are crawled at the same time</li>
  <li>It makes sure your computer’s memory doesn’t get overwhelmed</li>
  <li>It ensures websites aren’t bombarded with too many requests at once</li>
</ul>

<p>Let’s see a simple example of how this works:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">crawl4ai</span> <span class="kn">import</span> <span class="n">AsyncWebCrawler</span><span class="p">,</span> <span class="n">SemaphoreDispatcher</span><span class="p">,</span> <span class="n">CrawlerRunConfig</span>

<span class="k">async</span> <span class="k">def</span> <span class="nf">crawl_multiple_sites</span><span class="p">():</span>
    <span class="c1"># Create a dispatcher that allows 3 crawls at a time
</span>    <span class="n">dispatcher</span> <span class="o">=</span> <span class="n">SemaphoreDispatcher</span><span class="p">(</span><span class="n">semaphore_count</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
    
    <span class="c1"># List of URLs to crawl
</span>    <span class="n">urls</span> <span class="o">=</span> <span class="p">[</span><span class="s">"https://example.com"</span><span class="p">,</span> <span class="s">"https://example.org"</span><span class="p">,</span> <span class="s">"https://example.net"</span><span class="p">]</span>
    
    <span class="c1"># Create a crawler and run multiple URLs using our dispatcher
</span>    <span class="k">async</span> <span class="k">with</span> <span class="n">AsyncWebCrawler</span><span class="p">()</span> <span class="k">as</span> <span class="n">crawler</span><span class="p">:</span>
        <span class="n">results</span> <span class="o">=</span> <span class="k">await</span> <span class="n">crawler</span><span class="p">.</span><span class="n">arun_many</span><span class="p">(</span>
            <span class="n">urls</span><span class="o">=</span><span class="n">urls</span><span class="p">,</span>
            <span class="n">dispatcher</span><span class="o">=</span><span class="n">dispatcher</span><span class="p">,</span>
            <span class="n">config</span><span class="o">=</span><span class="n">CrawlerRunConfig</span><span class="p">()</span>
        <span class="p">)</span>
    
    <span class="k">return</span> <span class="n">results</span>
</code></pre></div></div>

<p>In this example, we’re using a <code class="language-plaintext highlighter-rouge">SemaphoreDispatcher</code> to crawl three URLs at the same time, not one after another. This is much faster!</p>

<h2 id="two-types-of-dispatchers">Two Types of Dispatchers</h2>

<p>The Dispatcher Framework offers two main types of dispatchers, each with different strengths:</p>

<h3 id="1-semaphoredispatcher-simple-and-predictable">1. SemaphoreDispatcher: Simple and Predictable</h3>

<p>The <code class="language-plaintext highlighter-rouge">SemaphoreDispatcher</code> is like a restaurant with a fixed number of tables. It allows a specific number of crawls to happen at the same time - no more, no less.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">crawl4ai</span> <span class="kn">import</span> <span class="n">SemaphoreDispatcher</span>

<span class="c1"># Create a dispatcher that allows 5 simultaneous crawls
</span><span class="n">dispatcher</span> <span class="o">=</span> <span class="n">SemaphoreDispatcher</span><span class="p">(</span><span class="n">semaphore_count</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
</code></pre></div></div>

<p>This dispatcher is great when:</p>
<ul>
  <li>You want predictable behavior</li>
  <li>You know exactly how many concurrent crawls your system can handle</li>
  <li>You want to set a strict limit on resource usage</li>
</ul>

<h3 id="2-memoryadaptivedispatcher-smart-and-flexible">2. MemoryAdaptiveDispatcher: Smart and Flexible</h3>

<p>The <code class="language-plaintext highlighter-rouge">MemoryAdaptiveDispatcher</code> is like a restaurant that adjusts how many tables are available based on how busy the kitchen is. It monitors your computer’s memory and automatically adjusts how many crawls happen at once.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">crawl4ai</span> <span class="kn">import</span> <span class="n">MemoryAdaptiveDispatcher</span>

<span class="c1"># Create a dispatcher that adapts to memory usage
</span><span class="n">dispatcher</span> <span class="o">=</span> <span class="n">MemoryAdaptiveDispatcher</span><span class="p">(</span>
    <span class="n">memory_threshold_percent</span><span class="o">=</span><span class="mf">90.0</span><span class="p">,</span>  <span class="c1"># Slow down when memory reaches 90%
</span>    <span class="n">max_session_permit</span><span class="o">=</span><span class="mi">20</span>           <span class="c1"># Never exceed 20 simultaneous crawls
</span><span class="p">)</span>
</code></pre></div></div>

<p>This dispatcher is perfect when:</p>
<ul>
  <li>You want to maximize efficiency without crashing your program</li>
  <li>Your system’s available memory changes over time</li>
  <li>You’re crawling pages with unpredictable memory requirements</li>
</ul>

<h2 id="managing-rate-limits-with-ratelimiter">Managing Rate Limits with RateLimiter</h2>

<p>When crawling websites, it’s important to be polite and not send too many requests too quickly. The <code class="language-plaintext highlighter-rouge">RateLimiter</code> helps with this:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">crawl4ai</span> <span class="kn">import</span> <span class="n">RateLimiter</span><span class="p">,</span> <span class="n">MemoryAdaptiveDispatcher</span>

<span class="c1"># Create a rate limiter that waits 1-3 seconds between requests
</span><span class="n">rate_limiter</span> <span class="o">=</span> <span class="n">RateLimiter</span><span class="p">(</span>
    <span class="n">base_delay</span><span class="o">=</span><span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">),</span>  <span class="c1"># Wait 1-3 seconds between requests
</span>    <span class="n">max_delay</span><span class="o">=</span><span class="mf">60.0</span><span class="p">,</span>         <span class="c1"># Never wait more than 60 seconds
</span>    <span class="n">max_retries</span><span class="o">=</span><span class="mi">3</span>           <span class="c1"># Try a maximum of 3 times if rate limited
</span><span class="p">)</span>

<span class="c1"># Add the rate limiter to our dispatcher
</span><span class="n">dispatcher</span> <span class="o">=</span> <span class="n">MemoryAdaptiveDispatcher</span><span class="p">(</span><span class="n">rate_limiter</span><span class="o">=</span><span class="n">rate_limiter</span><span class="p">)</span>
</code></pre></div></div>

<p>The <code class="language-plaintext highlighter-rouge">RateLimiter</code> tracks each domain separately and adjusts wait times if it detects rate limiting (like HTTP 429 responses).</p>

<h2 id="processing-results-in-real-time-with-streaming">Processing Results in Real-Time with Streaming</h2>

<p>Sometimes you don’t want to wait for all URLs to finish crawling before processing results. Streaming allows you to process results as they come in:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">crawl4ai</span> <span class="kn">import</span> <span class="n">AsyncWebCrawler</span><span class="p">,</span> <span class="n">CrawlerRunConfig</span>

<span class="k">async</span> <span class="k">def</span> <span class="nf">stream_results</span><span class="p">():</span>
    <span class="n">urls</span> <span class="o">=</span> <span class="p">[</span><span class="s">"https://example.com"</span><span class="p">,</span> <span class="s">"https://example.org"</span><span class="p">,</span> <span class="s">"https://example.net"</span><span class="p">]</span>
    
    <span class="c1"># Enable streaming in the configuration
</span>    <span class="n">config</span> <span class="o">=</span> <span class="n">CrawlerRunConfig</span><span class="p">(</span><span class="n">stream</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    
    <span class="k">async</span> <span class="k">with</span> <span class="n">AsyncWebCrawler</span><span class="p">()</span> <span class="k">as</span> <span class="n">crawler</span><span class="p">:</span>
        <span class="c1"># This returns an async generator
</span>        <span class="n">results_generator</span> <span class="o">=</span> <span class="k">await</span> <span class="n">crawler</span><span class="p">.</span><span class="n">arun_many</span><span class="p">(</span><span class="n">urls</span><span class="o">=</span><span class="n">urls</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">)</span>
        
        <span class="c1"># Process results as they arrive
</span>        <span class="k">async</span> <span class="k">for</span> <span class="n">result</span> <span class="ow">in</span> <span class="n">results_generator</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Just finished: </span><span class="si">{</span><span class="n">result</span><span class="p">.</span><span class="n">url</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
            <span class="c1"># Process each result immediately
</span></code></pre></div></div>

<p>With streaming, you can start working with results from fast-loading pages while slower pages are still being crawled.</p>

<h2 id="a-complete-example-news-crawler-with-memory-adaptation">A Complete Example: News Crawler with Memory Adaptation</h2>

<p>Let’s put everything together in a more complete example. Imagine we’re building a news crawler that needs to handle many URLs efficiently:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">crawl4ai</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">AsyncWebCrawler</span><span class="p">,</span> <span class="n">MemoryAdaptiveDispatcher</span><span class="p">,</span> <span class="n">RateLimiter</span><span class="p">,</span> <span class="n">CrawlerRunConfig</span>
<span class="p">)</span>

<span class="k">async</span> <span class="k">def</span> <span class="nf">crawl_news_sites</span><span class="p">():</span>
    <span class="c1"># Create a memory-adaptive dispatcher with rate limiting
</span>    <span class="n">dispatcher</span> <span class="o">=</span> <span class="n">MemoryAdaptiveDispatcher</span><span class="p">(</span>
        <span class="n">memory_threshold_percent</span><span class="o">=</span><span class="mf">85.0</span><span class="p">,</span>  <span class="c1"># Be conservative with memory
</span>        <span class="n">rate_limiter</span><span class="o">=</span><span class="n">RateLimiter</span><span class="p">(</span><span class="n">base_delay</span><span class="o">=</span><span class="p">(</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">))</span>  <span class="c1"># Be extra polite
</span>    <span class="p">)</span>
    
    <span class="c1"># List of news sites to crawl
</span>    <span class="n">news_sites</span> <span class="o">=</span> <span class="p">[</span>
        <span class="s">"https://example-news.com/tech"</span><span class="p">,</span>
        <span class="s">"https://example-news.com/business"</span><span class="p">,</span>
        <span class="c1"># ... more URLs ...
</span>    <span class="p">]</span>
    
    <span class="c1"># Configure how we want to crawl
</span>    <span class="n">config</span> <span class="o">=</span> <span class="n">CrawlerRunConfig</span><span class="p">(</span>
        <span class="n">screenshot</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>  <span class="c1"># Take screenshots
</span>        <span class="n">stream</span><span class="o">=</span><span class="bp">True</span>       <span class="c1"># Process results as they arrive
</span>    <span class="p">)</span>
    
    <span class="c1"># Run the crawler with our dispatcher
</span>    <span class="k">async</span> <span class="k">with</span> <span class="n">AsyncWebCrawler</span><span class="p">()</span> <span class="k">as</span> <span class="n">crawler</span><span class="p">:</span>
        <span class="n">results_stream</span> <span class="o">=</span> <span class="k">await</span> <span class="n">crawler</span><span class="p">.</span><span class="n">arun_many</span><span class="p">(</span>
            <span class="n">urls</span><span class="o">=</span><span class="n">news_sites</span><span class="p">,</span>
            <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">,</span>
            <span class="n">dispatcher</span><span class="o">=</span><span class="n">dispatcher</span>
        <span class="p">)</span>
        
        <span class="c1"># Process each result as it completes
</span>        <span class="k">async</span> <span class="k">for</span> <span class="n">result</span> <span class="ow">in</span> <span class="n">results_stream</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">result</span><span class="p">.</span><span class="n">success</span><span class="p">:</span>
                <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Successfully crawled: </span><span class="si">{</span><span class="n">result</span><span class="p">.</span><span class="n">url</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
                <span class="c1"># Process the content...
</span>            <span class="k">else</span><span class="p">:</span>
                <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Failed to crawl: </span><span class="si">{</span><span class="n">result</span><span class="p">.</span><span class="n">url</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
</code></pre></div></div>

<p>This example shows how to:</p>
<ol>
  <li>Create a memory-adaptive dispatcher with rate limiting</li>
  <li>Configure it for cautious memory usage and polite crawling</li>
  <li>Stream and process results as they become available</li>
</ol>

<h2 id="what-happens-under-the-hood">What Happens Under the Hood</h2>

<p>When you use the Dispatcher Framework, a lot happens behind the scenes. Let’s look at a simplified view:</p>

<pre><code class="language-mermaid">sequenceDiagram
    participant App as Your Code
    participant WC as WebCrawler
    participant DP as Dispatcher
    participant RL as RateLimiter
    participant MM as Memory Monitor

    App-&gt;&gt;WC: arun_many(urls, dispatcher)
    WC-&gt;&gt;DP: run_urls(urls, crawler, config)
    
    activate DP
    DP-&gt;&gt;MM: Start monitoring memory
    
    loop For each URL (controlled by dispatcher)
        DP-&gt;&gt;RL: wait_if_needed(url)
        RL--&gt;&gt;DP: OK to proceed
        
        alt Memory usage is normal
            DP-&gt;&gt;WC: arun(url, config)
            WC--&gt;&gt;DP: Return result
        else Memory pressure is high
            DP-&gt;&gt;DP: Re-queue URL with higher wait time
        end
        
        DP-&gt;&gt;App: Yield result (if streaming)
    end
    
    DP-&gt;&gt;MM: Stop monitoring
    DP--&gt;&gt;WC: Return all results
    WC--&gt;&gt;App: Return results
    deactivate DP
</code></pre>

<p>Here’s what happens step by step:</p>

<ol>
  <li>Your code calls <code class="language-plaintext highlighter-rouge">crawler.arun_many()</code> with a list of URLs and a dispatcher</li>
  <li>The WebCrawler passes the URLs to the dispatcher’s <code class="language-plaintext highlighter-rouge">run_urls</code> method</li>
  <li>The dispatcher starts a memory monitor if it’s a <code class="language-plaintext highlighter-rouge">MemoryAdaptiveDispatcher</code></li>
  <li>For each URL:
    <ul>
      <li>The dispatcher checks with the rate limiter if it’s OK to crawl</li>
      <li>If memory usage is normal, it crawls the URL</li>
      <li>If memory is under pressure, it might delay some URLs</li>
      <li>It yields or collects the result</li>
    </ul>
  </li>
  <li>Finally, all results are returned to your code</li>
</ol>

<h2 id="implementation-details-memoryadaptivedispatcher">Implementation Details: MemoryAdaptiveDispatcher</h2>

<p>Let’s look at how the <code class="language-plaintext highlighter-rouge">MemoryAdaptiveDispatcher</code> is implemented:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># From crawl4ai/async_dispatcher.py (simplified)
</span><span class="k">class</span> <span class="nc">MemoryAdaptiveDispatcher</span><span class="p">(</span><span class="n">BaseDispatcher</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">memory_threshold_percent</span><span class="o">=</span><span class="mf">90.0</span><span class="p">,</span>
        <span class="n">critical_threshold_percent</span><span class="o">=</span><span class="mf">95.0</span><span class="p">,</span>
        <span class="n">recovery_threshold_percent</span><span class="o">=</span><span class="mf">85.0</span><span class="p">,</span>
        <span class="n">max_session_permit</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span>
        <span class="n">rate_limiter</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">memory_threshold_percent</span> <span class="o">=</span> <span class="n">memory_threshold_percent</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">critical_threshold_percent</span> <span class="o">=</span> <span class="n">critical_threshold_percent</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">max_session_permit</span> <span class="o">=</span> <span class="n">max_session_permit</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">rate_limiter</span> <span class="o">=</span> <span class="n">rate_limiter</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">memory_pressure_mode</span> <span class="o">=</span> <span class="bp">False</span>
</code></pre></div></div>

<p>The constructor sets up thresholds for memory management:</p>
<ul>
  <li><code class="language-plaintext highlighter-rouge">memory_threshold_percent</code>: When to start slowing down (90% by default)</li>
  <li><code class="language-plaintext highlighter-rouge">critical_threshold_percent</code>: When to pause crawling (95% by default)</li>
  <li><code class="language-plaintext highlighter-rouge">max_session_permit</code>: Maximum concurrent crawls (20 by default)</li>
</ul>

<p>The dispatcher monitors memory with a background task:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># From the _memory_monitor_task method (simplified)
</span><span class="k">async</span> <span class="k">def</span> <span class="nf">_memory_monitor_task</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
        <span class="c1"># Check current memory usage
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">current_memory_percent</span> <span class="o">=</span> <span class="n">psutil</span><span class="p">.</span><span class="n">virtual_memory</span><span class="p">().</span><span class="n">percent</span>
        
        <span class="c1"># Enter pressure mode if memory is high
</span>        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="p">.</span><span class="n">memory_pressure_mode</span> <span class="ow">and</span> <span class="bp">self</span><span class="p">.</span><span class="n">current_memory_percent</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="p">.</span><span class="n">memory_threshold_percent</span><span class="p">:</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">memory_pressure_mode</span> <span class="o">=</span> <span class="bp">True</span>
            
        <span class="c1"># Exit pressure mode if memory is lower
</span>        <span class="k">elif</span> <span class="bp">self</span><span class="p">.</span><span class="n">memory_pressure_mode</span> <span class="ow">and</span> <span class="bp">self</span><span class="p">.</span><span class="n">current_memory_percent</span> <span class="o">&lt;=</span> <span class="bp">self</span><span class="p">.</span><span class="n">recovery_threshold_percent</span><span class="p">:</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">memory_pressure_mode</span> <span class="o">=</span> <span class="bp">False</span>
            
        <span class="k">await</span> <span class="n">asyncio</span><span class="p">.</span><span class="n">sleep</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">check_interval</span><span class="p">)</span>
</code></pre></div></div>

<p>This continuously checks memory usage and adjusts the <code class="language-plaintext highlighter-rouge">memory_pressure_mode</code> flag, which affects how URLs are processed.</p>

<h2 id="implementation-details-semaphoredispatcher">Implementation Details: SemaphoreDispatcher</h2>

<p>The <code class="language-plaintext highlighter-rouge">SemaphoreDispatcher</code> is simpler but still powerful:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># From crawl4ai/async_dispatcher.py (simplified)
</span><span class="k">class</span> <span class="nc">SemaphoreDispatcher</span><span class="p">(</span><span class="n">BaseDispatcher</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">semaphore_count</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">rate_limiter</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">(</span><span class="n">rate_limiter</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">semaphore_count</span> <span class="o">=</span> <span class="n">semaphore_count</span>
        
    <span class="k">async</span> <span class="k">def</span> <span class="nf">crawl_url</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">url</span><span class="p">,</span> <span class="n">config</span><span class="p">,</span> <span class="n">task_id</span><span class="p">,</span> <span class="n">semaphore</span><span class="p">):</span>
        <span class="k">async</span> <span class="k">with</span> <span class="n">semaphore</span><span class="p">:</span>
            <span class="c1"># Crawl the URL with rate limiting
</span>            <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">rate_limiter</span><span class="p">:</span>
                <span class="k">await</span> <span class="bp">self</span><span class="p">.</span><span class="n">rate_limiter</span><span class="p">.</span><span class="n">wait_if_needed</span><span class="p">(</span><span class="n">url</span><span class="p">)</span>
                
            <span class="n">result</span> <span class="o">=</span> <span class="k">await</span> <span class="bp">self</span><span class="p">.</span><span class="n">crawler</span><span class="p">.</span><span class="n">arun</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">result</span>
</code></pre></div></div>

<p>The <code class="language-plaintext highlighter-rouge">SemaphoreDispatcher</code> uses an asyncio Semaphore to limit concurrent tasks. When a task is done, it releases its slot, allowing another task to start.</p>

<h2 id="advanced-usage-monitoring-crawl-progress">Advanced Usage: Monitoring Crawl Progress</h2>

<p>You can monitor the progress of your crawls using the <code class="language-plaintext highlighter-rouge">CrawlerMonitor</code>:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">crawl4ai</span> <span class="kn">import</span> <span class="n">AsyncWebCrawler</span><span class="p">,</span> <span class="n">MemoryAdaptiveDispatcher</span><span class="p">,</span> <span class="n">CrawlerMonitor</span>

<span class="k">async</span> <span class="k">def</span> <span class="nf">monitored_crawl</span><span class="p">():</span>
    <span class="c1"># Create a monitor
</span>    <span class="n">monitor</span> <span class="o">=</span> <span class="n">CrawlerMonitor</span><span class="p">()</span>
    
    <span class="c1"># Create a dispatcher with the monitor
</span>    <span class="n">dispatcher</span> <span class="o">=</span> <span class="n">MemoryAdaptiveDispatcher</span><span class="p">(</span><span class="n">monitor</span><span class="o">=</span><span class="n">monitor</span><span class="p">)</span>
    
    <span class="c1"># Run your crawl
</span>    <span class="k">async</span> <span class="k">with</span> <span class="n">AsyncWebCrawler</span><span class="p">()</span> <span class="k">as</span> <span class="n">crawler</span><span class="p">:</span>
        <span class="n">results</span> <span class="o">=</span> <span class="k">await</span> <span class="n">crawler</span><span class="p">.</span><span class="n">arun_many</span><span class="p">(</span>
            <span class="n">urls</span><span class="o">=</span><span class="p">[</span><span class="s">"https://example.com"</span><span class="p">,</span> <span class="s">"https://example.org"</span><span class="p">],</span>
            <span class="n">dispatcher</span><span class="o">=</span><span class="n">dispatcher</span>
        <span class="p">)</span>
        
    <span class="c1"># Get statistics about the crawl
</span>    <span class="n">stats</span> <span class="o">=</span> <span class="n">monitor</span><span class="p">.</span><span class="n">get_statistics</span><span class="p">()</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Completed: </span><span class="si">{</span><span class="n">stats</span><span class="p">[</span><span class="s">'completed'</span><span class="p">]</span><span class="si">}</span><span class="s"> URLs"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Failed: </span><span class="si">{</span><span class="n">stats</span><span class="p">[</span><span class="s">'failed'</span><span class="p">]</span><span class="si">}</span><span class="s"> URLs"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Average memory usage: </span><span class="si">{</span><span class="n">stats</span><span class="p">[</span><span class="s">'avg_memory_usage'</span><span class="p">]</span><span class="si">}</span><span class="s"> MB"</span><span class="p">)</span>
</code></pre></div></div>

<p>The monitor collects statistics about each URL, including success/failure, memory usage, and timing.</p>

<h2 id="conclusion">Conclusion</h2>

<p>The Dispatcher Framework is like having an expert traffic controller for your web crawling operations. It helps you crawl multiple websites simultaneously while being careful about system resources and polite to the websites you’re visiting.</p>

<p>In this chapter, we’ve learned:</p>
<ul>
  <li>How to use the <code class="language-plaintext highlighter-rouge">SemaphoreDispatcher</code> for simple concurrent crawling</li>
  <li>How to use the <code class="language-plaintext highlighter-rouge">MemoryAdaptiveDispatcher</code> for smart resource management</li>
  <li>How to enforce rate limits with the <code class="language-plaintext highlighter-rouge">RateLimiter</code></li>
  <li>How to process results in real-time with streaming</li>
  <li>How the dispatch process works under the hood</li>
</ul>

<p>With the Dispatcher Framework, you can build efficient, robust, and polite web crawlers that make the most of your system’s resources without overwhelming them.</p>

<p>In the next chapter, <a href="08_async_logging_infrastructure_.md">Async Logging Infrastructure</a>, we’ll learn how to track what’s happening in our crawlers with detailed logs, which is especially important when dealing with complex crawling operations.</p>

<hr />

<p>Generated by <a href="https://github.com/The-Pocket/Tutorial-Codebase-Knowledge">AI Codebase Knowledge Builder</a></p>
